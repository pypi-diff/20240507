# Comparing `tmp/openmetadata-ingestion-1.3.4.0.tar.gz` & `tmp/openmetadata-ingestion-1.4.0.0rc1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "openmetadata-ingestion-1.3.4.0.tar", last modified: Tue May  7 11:24:23 2024, max compression
+gzip compressed data, was "openmetadata-ingestion-1.4.0.0rc1.tar", last modified: Thu May  2 07:42:43 2024, max compression
```

## Comparing `openmetadata-ingestion-1.3.4.0.tar` & `openmetadata-ingestion-1.4.0.0rc1.tar`

### file list

```diff
@@ -1,1722 +1,1793 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.189407 openmetadata-ingestion-1.3.4.0/
--rw-r--r--   0 runner    (1001) docker     (127)    11356 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)    39020 2024-05-07 11:24:23.185407 openmetadata-ingestion-1.3.4.0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)      636 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     3531 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-07 11:24:23.189407 openmetadata-ingestion-1.3.4.0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)    11483 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.877407 openmetadata-ingestion-1.3.4.0/src/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.877407 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/
--rw-r--r--   0 runner    (1001) docker     (127)     1260 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.877407 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/hooks/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3561 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/hooks/openmetadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.877407 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2810 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     3652 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/callback.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.881407 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/config/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      659 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/config/commons.py
--rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/config/loader.py
--rw-r--r--   0 runner    (1001) docker     (127)     2958 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/operator.py
--rw-r--r--   0 runner    (1001) docker     (127)    15934 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/runner.py
--rw-r--r--   0 runner    (1001) docker     (127)     4200 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/status.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.881407 openmetadata-ingestion-1.3.4.0/src/metadata/
--rw-r--r--   0 runner    (1001) docker     (127)      700 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/__main__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2710 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/__version__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.881407 openmetadata-ingestion-1.3.4.0/src/metadata/antlr/
--rw-r--r--   0 runner    (1001) docker     (127)     1730 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/antlr/split_listener.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.881407 openmetadata-ingestion-1.3.4.0/src/metadata/applications/
--rw-r--r--   0 runner    (1001) docker     (127)     7136 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/applications/auto_tagger.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.881407 openmetadata-ingestion-1.3.4.0/src/metadata/automations/
--rw-r--r--   0 runner    (1001) docker     (127)     2349 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/automations/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.885407 openmetadata-ingestion-1.3.4.0/src/metadata/cli/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1570 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/app.py
--rw-r--r--   0 runner    (1001) docker     (127)     6733 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/backup.py
--rw-r--r--   0 runner    (1001) docker     (127)     1673 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/dataquality.py
--rw-r--r--   0 runner    (1001) docker     (127)     8076 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/db_dump.py
--rw-r--r--   0 runner    (1001) docker     (127)    13593 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/docker.py
--rw-r--r--   0 runner    (1001) docker     (127)     1611 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/ingest.py
--rw-r--r--   0 runner    (1001) docker     (127)     1628 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/insight.py
--rw-r--r--   0 runner    (1001) docker     (127)     2517 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     4455 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/openmetadata_dag_config_migration.py
--rw-r--r--   0 runner    (1001) docker     (127)     2806 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/openmetadata_imports_migration.py
--rw-r--r--   0 runner    (1001) docker     (127)     1652 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/profile.py
--rw-r--r--   0 runner    (1001) docker     (127)     2926 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/restore.py
--rw-r--r--   0 runner    (1001) docker     (127)     1594 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)     2095 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cli/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.885407 openmetadata-ingestion-1.3.4.0/src/metadata/clients/
--rw-r--r--   0 runner    (1001) docker     (127)     6689 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/clients/aws_client.py
--rw-r--r--   0 runner    (1001) docker     (127)     3244 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/clients/azure_client.py
--rw-r--r--   0 runner    (1001) docker     (127)     6112 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/clients/domo_client.py
--rw-r--r--   0 runner    (1001) docker     (127)    17142 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/cmd.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.885407 openmetadata-ingestion-1.3.4.0/src/metadata/config/
--rw-r--r--   0 runner    (1001) docker     (127)     3222 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/config/common.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.837407 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.885407 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.885407 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/kpi/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/kpi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5304 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/kpi/kpi_runner.py
--rw-r--r--   0 runner    (1001) docker     (127)     5130 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/kpi/run_result_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.885407 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9974 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/cost_analysis_report_data_processor.py
--rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/data_processor.py
--rw-r--r--   0 runner    (1001) docker     (127)     8703 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/entity_report_data_processor.py
--rw-r--r--   0 runner    (1001) docker     (127)    13601 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/web_analytic_report_data_processor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4996 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/cost_analysis_producer.py
--rw-r--r--   0 runner    (1001) docker     (127)     1968 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/entity_producer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2210 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/producer_factory.py
--rw-r--r--   0 runner    (1001) docker     (127)     1120 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/producer_interface.py
--rw-r--r--   0 runner    (1001) docker     (127)     2906 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/web_analytics_producer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/source/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/source/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6485 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/source/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.841407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/api/
--rw-r--r--   0 runner    (1001) docker     (127)     2456 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/api/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/pandas/
--rw-r--r--   0 runner    (1001) docker     (127)     3989 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/databricks/
--rw-r--r--   0 runner    (1001) docker     (127)     1029 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/databricks/test_suite_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/snowflake/
--rw-r--r--   0 runner    (1001) docker     (127)     1032 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/snowflake/test_suite_interface.py
--rw-r--r--   0 runner    (1001) docker     (127)     6166 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/unity_catalog/
--rw-r--r--   0 runner    (1001) docker     (127)     1323 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/unity_catalog/test_suite_interface.py
--rw-r--r--   0 runner    (1001) docker     (127)     3197 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/test_suite_interface.py
--rw-r--r--   0 runner    (1001) docker     (127)     4698 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/test_suite_interface_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/processor/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/processor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12471 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/processor/test_case_runner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/runner/
--rw-r--r--   0 runner    (1001) docker     (127)     4277 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/runner/base_test_suite_source.py
--rw-r--r--   0 runner    (1001) docker     (127)     1659 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/runner/core.py
--rw-r--r--   0 runner    (1001) docker     (127)     1477 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/runner/test_suite_source_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/source/
--rw-r--r--   0 runner    (1001) docker     (127)     6097 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/source/test_suite.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.889407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/
--rw-r--r--   0 runner    (1001) docker     (127)     5801 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/base_test_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.841407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.893407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/
--rw-r--r--   0 runner    (1001) docker     (127)     4308 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2682 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2693 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2715 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2682 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     3966 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py
--rw-r--r--   0 runner    (1001) docker     (127)     2680 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     5722 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     3534 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py
--rw-r--r--   0 runner    (1001) docker     (127)     3739 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py
--rw-r--r--   0 runner    (1001) docker     (127)     3260 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py
--rw-r--r--   0 runner    (1001) docker     (127)     3468 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py
--rw-r--r--   0 runner    (1001) docker     (127)     3641 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py
--rw-r--r--   0 runner    (1001) docker     (127)     3694 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.897407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/
--rw-r--r--   0 runner    (1001) docker     (127)     2617 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1772 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1783 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1783 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1811 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py
--rw-r--r--   0 runner    (1001) docker     (127)     1771 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2566 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2182 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py
--rw-r--r--   0 runner    (1001) docker     (127)     2120 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py
--rw-r--r--   0 runner    (1001) docker     (127)     2080 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py
--rw-r--r--   0 runner    (1001) docker     (127)     1940 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py
--rw-r--r--   0 runner    (1001) docker     (127)     2118 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py
--rw-r--r--   0 runner    (1001) docker     (127)     2138 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.901407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/
--rw-r--r--   0 runner    (1001) docker     (127)     2709 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1737 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1737 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1749 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1731 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1750 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1852 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py
--rw-r--r--   0 runner    (1001) docker     (127)     1737 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2526 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2045 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py
--rw-r--r--   0 runner    (1001) docker     (127)     2065 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py
--rw-r--r--   0 runner    (1001) docker     (127)     2121 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py
--rw-r--r--   0 runner    (1001) docker     (127)     2563 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py
--rw-r--r--   0 runner    (1001) docker     (127)     2520 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py
--rw-r--r--   0 runner    (1001) docker     (127)     2551 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.901407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/mixins/
--rw-r--r--   0 runner    (1001) docker     (127)     2558 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     4045 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.841407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.901407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/
--rw-r--r--   0 runner    (1001) docker     (127)     2313 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2302 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (127)     2437 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py
--rw-r--r--   0 runner    (1001) docker     (127)     3217 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py
--rw-r--r--   0 runner    (1001) docker     (127)     3296 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py
--rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     2419 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (127)     3505 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.901407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/
--rw-r--r--   0 runner    (1001) docker     (127)     1226 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1288 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (127)     1402 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py
--rw-r--r--   0 runner    (1001) docker     (127)     1439 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py
--rw-r--r--   0 runner    (1001) docker     (127)     1485 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py
--rw-r--r--   0 runner    (1001) docker     (127)     1300 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1280 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (127)     3095 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.905407 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/
--rw-r--r--   0 runner    (1001) docker     (127)     1426 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1406 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (127)     1427 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py
--rw-r--r--   0 runner    (1001) docker     (127)     1673 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py
--rw-r--r--   0 runner    (1001) docker     (127)     1760 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py
--rw-r--r--   0 runner    (1001) docker     (127)     1369 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)     1289 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (127)     2444 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (127)      954 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/validator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.841407 openmetadata-ingestion-1.3.4.0/src/metadata/examples/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.921407 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/
--rw-r--r--   0 runner    (1001) docker     (127)     1021 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/airbyte.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/airflow.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/airflow_backend.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1607 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/airflow_postgres.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1123 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/amundsen.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1199 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/athena.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      964 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/athena_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1086 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/athena_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1193 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/atlas.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1227 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/azuresql.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      941 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/azuresql_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1264 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/azuresql_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1540 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigquery.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      947 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigquery_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     2082 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigquery_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1664 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigquery_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1497 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigtable.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1179 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/clickhouse.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      951 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/clickhouse_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1240 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/clickhouse_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1059 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/couchbase.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1004 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/dagster.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1039 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/data_insight.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1157 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/databricks.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      951 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/databricks_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/databricks_pipeline.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1253 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/databricks_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1235 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_azure_client_secret.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1219 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_azure_default.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1646 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_gcs.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1233 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_s3.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1056 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/db2.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1649 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/db2_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     3796 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/dbt.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1105 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/deltalake.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1172 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/domodashboard.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1244 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/dynamodb.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1017 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/elasticsearch.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1016 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/fivetran.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1130 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/glue.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1147 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/gluepipeline.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1357 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/hive.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1049 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/impala.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1664 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/kafka.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1041 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/kinesis.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1128 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/lightdash.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1090 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/looker.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1081 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mariadb.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1109 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/metabase.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1083 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mlflow.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mode.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1086 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mongodb.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1043 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mssql.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      941 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mssql_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1220 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mssql_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1091 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mstr.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1151 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mysql.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1256 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mysql_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1772 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/openmetadata.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1068 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/oracle.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      939 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/oracle_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/oracle_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1117 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/pinotdb.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1087 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/postgres.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      969 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/postgres_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1265 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/postgres_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1566 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/powerbi.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1077 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/presto.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1829 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/qlik_sense.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1351 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/query_log_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1252 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/quicksight.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1083 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redash.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1116 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redpanda.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1186 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redshift.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      945 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redshift_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1663 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redshift_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1272 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redshift_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1107 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/sagemaker.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1131 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/salesforce.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1264 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/sas.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1092 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/singlestore.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1296 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/snowflake.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      937 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/snowflake_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1240 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/snowflake_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1200 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/spline.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1043 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/sqlite.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1487 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/superset.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1562 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/tableau.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1513 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/test_suite.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1351 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/trino.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      936 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/trino_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1280 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/trino_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1208 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/unity_catalog.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      976 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/unity_catalog_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1393 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/unity_catalog_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/vertica.yaml
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.841407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.925407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/
--rw-r--r--   0 runner    (1001) docker     (127)    34020 2024-05-07 11:23:55.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/EntityLinkLexer.py
--rw-r--r--   0 runner    (1001) docker     (127)     1976 2024-05-07 11:23:55.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/EntityLinkListener.py
--rw-r--r--   0 runner    (1001) docker     (127)    13358 2024-05-07 11:23:55.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/EntityLinkParser.py
--rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-07 11:23:55.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/FqnLexer.py
--rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-05-07 11:23:55.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/FqnListener.py
--rw-r--r--   0 runner    (1001) docker     (127)     6811 2024-05-07 11:23:55.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/FqnParser.py
--rw-r--r--   0 runner    (1001) docker     (127)     8675 2024-05-07 11:23:55.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/JdbcUriLexer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2181 2024-05-07 11:23:55.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/JdbcUriListener.py
--rw-r--r--   0 runner    (1001) docker     (127)    16824 2024-05-07 11:23:55.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/JdbcUriParser.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.853407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.925407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      948 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/basic.py
--rw-r--r--   0 runner    (1001) docker     (127)     1603 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportData.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.925407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2283 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/aggregatedCostAnalysisReportData.py
--rw-r--r--   0 runner    (1001) docker     (127)     1343 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py
--rw-r--r--   0 runner    (1001) docker     (127)      723 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/rawCostAnalysisReportData.py
--rw-r--r--   0 runner    (1001) docker     (127)     1094 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py
--rw-r--r--   0 runner    (1001) docker     (127)      958 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py
--rw-r--r--   0 runner    (1001) docker     (127)     2128 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEvent.py
--rw-r--r--   0 runner    (1001) docker     (127)      883 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEventData.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.925407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEventType/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEventType/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      978 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py
--rw-r--r--   0 runner    (1001) docker     (127)     1202 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.929407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1063 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/addGlossaryToAssetsRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.929407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/analytics/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/analytics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1021 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.929407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/automations/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/automations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1483 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/automations/createWorkflow.py
--rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/bulkAssets.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.929407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/classification/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1321 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/classification/createClassification.py
--rw-r--r--   0 runner    (1001) docker     (127)     1813 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/classification/createTag.py
--rw-r--r--   0 runner    (1001) docker     (127)      525 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/classification/loadTags.py
--rw-r--r--   0 runner    (1001) docker     (127)      928 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/createBot.py
--rw-r--r--   0 runner    (1001) docker     (127)     1403 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/createEventPublisherJob.py
--rw-r--r--   0 runner    (1001) docker     (127)     1017 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/createType.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.933407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1967 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createChart.py
--rw-r--r--   0 runner    (1001) docker     (127)     2984 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createContainer.py
--rw-r--r--   0 runner    (1001) docker     (127)      873 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createCustomProperty.py
--rw-r--r--   0 runner    (1001) docker     (127)     2789 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createDashboard.py
--rw-r--r--   0 runner    (1001) docker     (127)     2536 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createDashboardDataModel.py
--rw-r--r--   0 runner    (1001) docker     (127)     2559 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createDatabase.py
--rw-r--r--   0 runner    (1001) docker     (127)     2302 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createDatabaseSchema.py
--rw-r--r--   0 runner    (1001) docker     (127)     1803 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createGlossary.py
--rw-r--r--   0 runner    (1001) docker     (127)     2603 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createGlossaryTerm.py
--rw-r--r--   0 runner    (1001) docker     (127)     3132 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createMlModel.py
--rw-r--r--   0 runner    (1001) docker     (127)     2740 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     2516 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createQuery.py
--rw-r--r--   0 runner    (1001) docker     (127)     2307 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createSearchIndex.py
--rw-r--r--   0 runner    (1001) docker     (127)     2208 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createStoredProcedure.py
--rw-r--r--   0 runner    (1001) docker     (127)     2886 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createTable.py
--rw-r--r--   0 runner    (1001) docker     (127)      727 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createTableProfile.py
--rw-r--r--   0 runner    (1001) docker     (127)     3422 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createTopic.py
--rw-r--r--   0 runner    (1001) docker     (127)      541 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/loadGlossary.py
--rw-r--r--   0 runner    (1001) docker     (127)      424 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/restoreEntity.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.933407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/dataInsight/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/dataInsight/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1294 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.933407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/dataInsight/kpi/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/dataInsight/kpi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1322 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.933407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/docStore/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/docStore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      851 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/docStore/createDocument.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.937407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/domains/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/domains/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1554 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/domains/createDataProduct.py
--rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/domains/createDomain.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.937407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      597 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/closeTask.py
--rw-r--r--   0 runner    (1001) docker     (127)      526 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/createPost.py
--rw-r--r--   0 runner    (1001) docker     (127)      935 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/createSuggestion.py
--rw-r--r--   0 runner    (1001) docker     (127)     1898 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/createThread.py
--rw-r--r--   0 runner    (1001) docker     (127)      794 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/resolveTask.py
--rw-r--r--   0 runner    (1001) docker     (127)     1034 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/threadCount.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.937407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/lineage/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      408 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/lineage/addLineage.py
--rw-r--r--   0 runner    (1001) docker     (127)      672 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/openMetadataServerVersion.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.937407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/policies/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1082 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/policies/createPolicy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.937407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1534 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createDashboardService.py
--rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createDatabaseService.py
--rw-r--r--   0 runner    (1001) docker     (127)     1596 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createMessagingService.py
--rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createMetadataService.py
--rw-r--r--   0 runner    (1001) docker     (127)     1510 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createMlModelService.py
--rw-r--r--   0 runner    (1001) docker     (127)     1777 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createPipelineService.py
--rw-r--r--   0 runner    (1001) docker     (127)     1560 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createSearchService.py
--rw-r--r--   0 runner    (1001) docker     (127)     1582 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createStorageService.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.941407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/ingestionPipelines/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/ingestionPipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1463 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)      547 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/setOwner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.941407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      785 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/createPersona.py
--rw-r--r--   0 runner    (1001) docker     (127)      800 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/createRole.py
--rw-r--r--   0 runner    (1001) docker     (127)     2585 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/createTeam.py
--rw-r--r--   0 runner    (1001) docker     (127)     2405 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/createUser.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.941407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1252 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createCustomMetric.py
--rw-r--r--   0 runner    (1001) docker     (127)      585 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createLogicalTestCases.py
--rw-r--r--   0 runner    (1001) docker     (127)     1374 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createTestCase.py
--rw-r--r--   0 runner    (1001) docker     (127)     1067 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createTestCaseResolutionStatus.py
--rw-r--r--   0 runner    (1001) docker     (127)     1228 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createTestDefinition.py
--rw-r--r--   0 runner    (1001) docker     (127)     1311 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createTestSuite.py
--rw-r--r--   0 runner    (1001) docker     (127)      390 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/voteRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.945407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/basicAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)      462 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/basicLoginRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)      886 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/changePasswordRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)      437 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/createPersonalToken.py
--rw-r--r--   0 runner    (1001) docker     (127)      365 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/emailRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)     1120 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/emailVerificationToken.py
--rw-r--r--   0 runner    (1001) docker     (127)      347 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/generateToken.py
--rw-r--r--   0 runner    (1001) docker     (127)      751 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/jwtAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)      409 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/loginRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)      626 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/logoutRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)      683 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/passwordResetRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/passwordResetToken.py
--rw-r--r--   0 runner    (1001) docker     (127)      956 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/personalAccessToken.py
--rw-r--r--   0 runner    (1001) docker     (127)      975 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/refreshToken.py
--rw-r--r--   0 runner    (1001) docker     (127)      623 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/registrationRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)      476 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/revokePersonalToken.py
--rw-r--r--   0 runner    (1001) docker     (127)      324 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/revokeToken.py
--rw-r--r--   0 runner    (1001) docker     (127)      288 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/serviceTokenEnum.py
--rw-r--r--   0 runner    (1001) docker     (127)     1283 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/ssoAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)      378 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/tokenRefreshRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.945407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      928 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/appsPrivateConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)     1349 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/authConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     2098 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/authenticationConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/authorizerConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      363 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/changeEventConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      436 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/dataQualityConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)     1934 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      442 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/eventHandlerConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      357 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/fernetConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      658 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)     1930 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)     2857 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapConfiguration.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.945407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      894 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      544 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      451 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/jvmDefaultConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      462 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/trustAllConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      978 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      723 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/loginConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      655 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/logoConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)     3002 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      683 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/slackAppConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      436 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/taskNotificationConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      885 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.949407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3354 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/dataInsightChart.py
--rw-r--r--   0 runner    (1001) docker     (127)     3266 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.949407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/kpi/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/kpi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1517 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/kpi/basic.py
--rw-r--r--   0 runner    (1001) docker     (127)     2537 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/kpi/kpi.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.949407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      830 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/aggregatedUnusedAssetsCount.py
--rw-r--r--   0 runner    (1001) docker     (127)      828 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/aggregatedUnusedAssetsSize.py
--rw-r--r--   0 runner    (1001) docker     (127)     1002 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/aggregatedUsedVsUnusedAssetsCount.py
--rw-r--r--   0 runner    (1001) docker     (127)      987 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/aggregatedUsedVsUnusedAssetsSize.py
--rw-r--r--   0 runner    (1001) docker     (127)      561 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1077 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py
--rw-r--r--   0 runner    (1001) docker     (127)      791 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py
--rw-r--r--   0 runner    (1001) docker     (127)      635 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py
--rw-r--r--   0 runner    (1001) docker     (127)     1047 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py
--rw-r--r--   0 runner    (1001) docker     (127)      975 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py
--rw-r--r--   0 runner    (1001) docker     (127)     1024 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/percentageOfServicesWithDescription.py
--rw-r--r--   0 runner    (1001) docker     (127)      964 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/percentageOfServicesWithOwner.py
--rw-r--r--   0 runner    (1001) docker     (127)      831 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py
--rw-r--r--   0 runner    (1001) docker     (127)      843 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py
--rw-r--r--   0 runner    (1001) docker     (127)      662 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/unusedAssets.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.953407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/email/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/email/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1250 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/email/emailRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)     1312 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/email/smtpSettings.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.953407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.953407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5808 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/app.py
--rw-r--r--   0 runner    (1001) docker     (127)     1722 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/appRunRecord.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.953407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      856 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/applicationConfig.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.953407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/external/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/external/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      762 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/external/autoTaggerAppConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/external/metaPilotAppConfig.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.953407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/internal/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/internal/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      334 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/internal/dataInsightsAppConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      811 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/internal/dataInsightsReportAppConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1307 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/internal/searchIndexingAppConfig.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.845407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/private/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.953407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/private/external/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/private/external/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      855 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/private/external/metaPilotAppPrivateConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1146 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/createAppRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)      451 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/jobStatus.py
--rw-r--r--   0 runner    (1001) docker     (127)      499 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/liveExecutionContext.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.957407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/marketplace/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/marketplace/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4243 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/marketplace/appMarketPlaceDefinition.py
--rw-r--r--   0 runner    (1001) docker     (127)     3025 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/marketplace/createAppMarketPlaceDefinitionReq.py
--rw-r--r--   0 runner    (1001) docker     (127)      360 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/scheduledExecutionContext.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.957407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/automations/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/automations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1725 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/automations/testServiceConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2859 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/automations/workflow.py
--rw-r--r--   0 runner    (1001) docker     (127)     2032 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/bot.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.957407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/classification/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2864 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/classification/classification.py
--rw-r--r--   0 runner    (1001) docker     (127)     3407 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/classification/tag.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.961407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3897 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/chart.py
--rw-r--r--   0 runner    (1001) docker     (127)     5553 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/container.py
--rw-r--r--   0 runner    (1001) docker     (127)     4525 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/dashboard.py
--rw-r--r--   0 runner    (1001) docker     (127)     4204 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/dashboardDataModel.py
--rw-r--r--   0 runner    (1001) docker     (127)     5331 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/database.py
--rw-r--r--   0 runner    (1001) docker     (127)     5143 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/databaseSchema.py
--rw-r--r--   0 runner    (1001) docker     (127)     3317 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/glossary.py
--rw-r--r--   0 runner    (1001) docker     (127)     5556 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/glossaryTerm.py
--rw-r--r--   0 runner    (1001) docker     (127)     2589 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     7283 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/mlmodel.py
--rw-r--r--   0 runner    (1001) docker     (127)     7054 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3697 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/query.py
--rw-r--r--   0 runner    (1001) docker     (127)     2350 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/report.py
--rw-r--r--   0 runner    (1001) docker     (127)     6649 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/searchIndex.py
--rw-r--r--   0 runner    (1001) docker     (127)     4541 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/storedProcedure.py
--rw-r--r--   0 runner    (1001) docker     (127)    23832 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/table.py
--rw-r--r--   0 runner    (1001) docker     (127)     5421 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/topic.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.961407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/docStore/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/docStore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/docStore/document.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.961407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/domains/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/domains/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2340 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/domains/dataProduct.py
--rw-r--r--   0 runner    (1001) docker     (127)     2465 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/domains/domain.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.961407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/events/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/events/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1332 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/events/webhook.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.961407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/feed/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/feed/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2048 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/feed/suggestion.py
--rw-r--r--   0 runner    (1001) docker     (127)     5885 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/feed/thread.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.961407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.961407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/accessControl/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/accessControl/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1751 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py
--rw-r--r--   0 runner    (1001) docker     (127)     1821 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py
--rw-r--r--   0 runner    (1001) docker     (127)     1497 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/accessControl/rule.py
--rw-r--r--   0 runner    (1001) docker     (127)      681 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/filters.py
--rw-r--r--   0 runner    (1001) docker     (127)     3202 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/policy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.965407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.965407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.965407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/common/
--rw-r--r--   0 runner    (1001) docker     (127)      133 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      720 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/common/sslCertPaths.py
--rw-r--r--   0 runner    (1001) docker     (127)     1005 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/common/sslCertValues.py
--rw-r--r--   0 runner    (1001) docker     (127)      574 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/common/sslConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     3004 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.969407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      946 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1545 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1662 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/lightdashConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1803 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1300 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1349 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1370 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/mstrConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2025 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2418 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/qlikSenseConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1270 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1310 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1991 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.977407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2633 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     4634 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3071 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1311 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/bigTableConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3760 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.977407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      542 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/azureConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      541 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/basicAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)      536 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/iamAuthConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      522 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/jwtAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)     1649 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/couchbaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      938 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3562 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.977407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalake/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      531 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      521 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      518 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py
--rw-r--r--   0 runner    (1001) docker     (127)     2139 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2441 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3741 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1746 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3083 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/dorisConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2445 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1395 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1449 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3450 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/greenplumConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3846 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.977407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      530 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/dynamoDbCatalogConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      427 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/glueCatalogConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      623 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/hiveCatalogConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1346 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/icebergCatalog.py
--rw-r--r--   0 runner    (1001) docker     (127)      541 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/icebergFileSystem.py
--rw-r--r--   0 runner    (1001) docker     (127)     2044 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/restCatalogConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      981 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/icebergConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3535 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/impalaConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2816 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1881 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/mongoDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3354 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3414 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     4285 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2846 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3998 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2767 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3492 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2233 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.977407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sapHana/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sapHana/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sapHana/sapHanaHDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1301 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sapHana/sapHanaSQLConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2067 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sapHanaConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1684 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sasConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2887 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     4318 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2815 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3267 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3572 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/unityCatalogConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2936 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.981407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      946 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2908 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      901 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      729 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2721 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      389 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/saslMechanismType.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.981407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3633 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/alationConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1606 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1688 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1909 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     5062 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.981407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      931 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1087 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      915 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.985407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1154 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1536 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      545 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      938 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1199 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1412 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1371 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      916 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2241 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/sparkConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1018 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/splineConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.985407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      922 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/customSearchConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.985407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      741 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/apiAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)      682 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/basicAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)     1647 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/elasticSearchConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1911 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/openSearchConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1158 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/serviceConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.985407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/storage/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/storage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1167 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/storage/adlsConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)      930 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/storage/customStorageConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1146 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1119 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2930 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py
--rw-r--r--   0 runner    (1001) docker     (127)     1575 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py
--rw-r--r--   0 runner    (1001) docker     (127)     4568 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/dashboardService.py
--rw-r--r--   0 runner    (1001) docker     (127)     7076 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/databaseService.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.989407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/ingestionPipelines/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/ingestionPipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6278 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)      778 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/ingestionPipelines/pipelineServiceClientResponse.py
--rw-r--r--   0 runner    (1001) docker     (127)     1722 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/ingestionPipelines/status.py
--rw-r--r--   0 runner    (1001) docker     (127)     3943 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/messagingService.py
--rw-r--r--   0 runner    (1001) docker     (127)     3596 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/metadataService.py
--rw-r--r--   0 runner    (1001) docker     (127)     3708 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/mlmodelService.py
--rw-r--r--   0 runner    (1001) docker     (127)     4469 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/pipelineService.py
--rw-r--r--   0 runner    (1001) docker     (127)     3573 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/searchService.py
--rw-r--r--   0 runner    (1001) docker     (127)      413 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/serviceType.py
--rw-r--r--   0 runner    (1001) docker     (127)     3602 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/storageService.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.989407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1707 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/persona.py
--rw-r--r--   0 runner    (1001) docker     (127)     2472 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/role.py
--rw-r--r--   0 runner    (1001) docker     (127)     4585 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/team.py
--rw-r--r--   0 runner    (1001) docker     (127)     1856 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/teamHierarchy.py
--rw-r--r--   0 runner    (1001) docker     (127)     4338 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/user.py
--rw-r--r--   0 runner    (1001) docker     (127)     2761 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/type.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.989407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/utils/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1254 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/utils/entitiesCount.py
--rw-r--r--   0 runner    (1001) docker     (127)      951 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/utils/servicesCount.py
--rw-r--r--   0 runner    (1001) docker     (127)     1170 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/utils/supersetApiConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.989407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      780 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/alertMetrics.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.993407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/api/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1950 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/api/createEventSubscription.py
--rw-r--r--   0 runner    (1001) docker     (127)      736 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/emailAlertConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1362 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/eventFilterRule.py
--rw-r--r--   0 runner    (1001) docker     (127)     7594 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/eventSubscription.py
--rw-r--r--   0 runner    (1001) docker     (127)      541 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/eventSubscriptionOffset.py
--rw-r--r--   0 runner    (1001) docker     (127)      861 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/failedEvent.py
--rw-r--r--   0 runner    (1001) docker     (127)      936 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/filterResourceDescriptor.py
--rw-r--r--   0 runner    (1001) docker     (127)     1232 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.993407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1399 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/application.py
--rw-r--r--   0 runner    (1001) docker     (127)     1009 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/applicationPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3177 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)      469 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dataInsightPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     4030 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3262 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     2206 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     1542 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     2410 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.997407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtAzureConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      676 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py
--rw-r--r--   0 runner    (1001) docker     (127)     1446 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1255 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1082 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1250 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py
--rw-r--r--   0 runner    (1001) docker     (127)     1401 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     2281 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     1197 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     2103 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     1584 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/searchServiceMetadataPipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.997407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1536 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1726 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/manifestMetadataConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      792 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageBucketDetails.py
--rw-r--r--   0 runner    (1001) docker     (127)      698 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataADLSConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      689 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataGCSConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      531 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataHttpConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      528 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataLocalConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      686 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataS3Config.py
--rw-r--r--   0 runner    (1001) docker     (127)     1922 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     1037 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     5056 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.997407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/monitoring/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/monitoring/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      282 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/monitoring/eventMonitorProvider.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.997407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.001407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      579 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      700 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/azureSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      636 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      669 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/googleSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1879 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/oidcClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      757 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      494 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/openMetadataJWTClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     3242 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/samlSSOClientConfig.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.001407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      677 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/accessTokenAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)      497 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/apiAccessTokenAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)     2001 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/awsCredentials.py
--rw-r--r--   0 runner    (1001) docker     (127)     1294 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/azureCredentials.py
--rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/basicAuth.py
--rw-r--r--   0 runner    (1001) docker     (127)      904 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/bitbucketCredentials.py
--rw-r--r--   0 runner    (1001) docker     (127)     1601 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/gcpCredentials.py
--rw-r--r--   0 runner    (1001) docker     (127)     1390 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/gcpExternalAccount.py
--rw-r--r--   0 runner    (1001) docker     (127)     2167 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/gcpValues.py
--rw-r--r--   0 runner    (1001) docker     (127)     1102 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/gitCredentials.py
--rw-r--r--   0 runner    (1001) docker     (127)      712 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/githubCredentials.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.001407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/secrets/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/secrets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      298 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/secrets/secretsManagerClientLoader.py
--rw-r--r--   0 runner    (1001) docker     (127)     1299 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (127)      454 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/secrets/secretsManagerProvider.py
--rw-r--r--   0 runner    (1001) docker     (127)      560 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/securityConfiguration.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.005407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/ssl/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/ssl/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      554 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)      852 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/ssl/verifySSLConfig.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.005407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/settings/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/settings/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2280 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/settings/settings.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.005407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      370 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/entityError.py
--rw-r--r--   0 runner    (1001) docker     (127)     2420 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/eventPublisherJob.py
--rw-r--r--   0 runner    (1001) docker     (127)      849 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/indexingError.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.005407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/ui/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/ui/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      595 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/ui/knowledgePanel.py
--rw-r--r--   0 runner    (1001) docker     (127)     1757 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/ui/page.py
--rw-r--r--   0 runner    (1001) docker     (127)     1323 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/validationResponse.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.005407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      429 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/assigned.py
--rw-r--r--   0 runner    (1001) docker     (127)     2630 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/basic.py
--rw-r--r--   0 runner    (1001) docker     (127)     1348 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/customMetric.py
--rw-r--r--   0 runner    (1001) docker     (127)      928 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/resolved.py
--rw-r--r--   0 runner    (1001) docker     (127)     3092 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/testCase.py
--rw-r--r--   0 runner    (1001) docker     (127)     2093 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/testCaseResolutionStatus.py
--rw-r--r--   0 runner    (1001) docker     (127)     3686 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/testDefinition.py
--rw-r--r--   0 runner    (1001) docker     (127)     3901 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/testSuite.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.013407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1200 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/auditLog.py
--rw-r--r--   0 runner    (1001) docker     (127)     4823 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/basic.py
--rw-r--r--   0 runner    (1001) docker     (127)     1691 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/bulkOperationResult.py
--rw-r--r--   0 runner    (1001) docker     (127)     2307 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/changeEvent.py
--rw-r--r--   0 runner    (1001) docker     (127)      950 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/changeEventType.py
--rw-r--r--   0 runner    (1001) docker     (127)      870 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/collectionDescriptor.py
--rw-r--r--   0 runner    (1001) docker     (127)      526 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/csvDocumentation.py
--rw-r--r--   0 runner    (1001) docker     (127)      422 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/csvErrorType.py
--rw-r--r--   0 runner    (1001) docker     (127)      942 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/csvFile.py
--rw-r--r--   0 runner    (1001) docker     (127)     1264 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/csvImportResult.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.013407 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/customProperties/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/customProperties/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      396 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/customProperties/enumConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     1619 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/customProperty.py
--rw-r--r--   0 runner    (1001) docker     (127)      453 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/dailyCount.py
--rw-r--r--   0 runner    (1001) docker     (127)     1887 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/databaseConnectionConfig.py
--rw-r--r--   0 runner    (1001) docker     (127)     2457 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityHistory.py
--rw-r--r--   0 runner    (1001) docker     (127)     3430 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityLineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     1657 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityReference.py
--rw-r--r--   0 runner    (1001) docker     (127)      739 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityReferenceList.py
--rw-r--r--   0 runner    (1001) docker     (127)     2415 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityRelationship.py
--rw-r--r--   0 runner    (1001) docker     (127)      579 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityUsage.py
--rw-r--r--   0 runner    (1001) docker     (127)      703 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/filterPattern.py
--rw-r--r--   0 runner    (1001) docker     (127)     1398 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/function.py
--rw-r--r--   0 runner    (1001) docker     (127)      262 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/include.py
--rw-r--r--   0 runner    (1001) docker     (127)     1022 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/jdbcConnection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1306 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/lifeCycle.py
--rw-r--r--   0 runner    (1001) docker     (127)      834 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/paging.py
--rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/profile.py
--rw-r--r--   0 runner    (1001) docker     (127)     1591 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/queryParserData.py
--rw-r--r--   0 runner    (1001) docker     (127)      739 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/reaction.py
--rw-r--r--   0 runner    (1001) docker     (127)      604 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     2580 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/schema.py
--rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/tableQuery.py
--rw-r--r--   0 runner    (1001) docker     (127)     1724 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/tableUsageCount.py
--rw-r--r--   0 runner    (1001) docker     (127)     1944 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/tagLabel.py
--rw-r--r--   0 runner    (1001) docker     (127)     1216 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/usageDetails.py
--rw-r--r--   0 runner    (1001) docker     (127)      413 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/usageRequest.py
--rw-r--r--   0 runner    (1001) docker     (127)      904 2024-05-07 11:23:54.000000 openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/votes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.013407 openmetadata-ingestion-1.3.4.0/src/metadata/great_expectations/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/great_expectations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    16505 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/great_expectations/action.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.013407 openmetadata-ingestion-1.3.4.0/src/metadata/great_expectations/utils/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/great_expectations/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2795 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/great_expectations/utils/ometa_config_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.869407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.017407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/
--rw-r--r--   0 runner    (1001) docker     (127)      742 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/closeable.py
--rw-r--r--   0 runner    (1001) docker     (127)     1208 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/common.py
--rw-r--r--   0 runner    (1001) docker     (127)     2265 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/delete.py
--rw-r--r--   0 runner    (1001) docker     (127)     1132 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/models.py
--rw-r--r--   0 runner    (1001) docker     (127)    19815 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     3151 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/status.py
--rw-r--r--   0 runner    (1001) docker     (127)     8126 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/step.py
--rw-r--r--   0 runner    (1001) docker     (127)     2532 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/steps.py
--rw-r--r--   0 runner    (1001) docker     (127)    16660 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/topology_runner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.017407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/bulksink/
--rw-r--r--   0 runner    (1001) docker     (127)    16024 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/bulksink/metadata_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.017407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/
--rw-r--r--   0 runner    (1001) docker     (127)     6525 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/builders.py
--rw-r--r--   0 runner    (1001) docker     (127)     2440 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/headers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1920 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/secrets.py
--rw-r--r--   0 runner    (1001) docker     (127)     1229 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/session.py
--rw-r--r--   0 runner    (1001) docker     (127)    13519 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/test_connections.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.017407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/lineage/
--rw-r--r--   0 runner    (1001) docker     (127)     4429 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/lineage/models.py
--rw-r--r--   0 runner    (1001) docker     (127)    17904 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/lineage/parser.py
--rw-r--r--   0 runner    (1001) docker     (127)    17272 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/lineage/sql_lineage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.021407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/
--rw-r--r--   0 runner    (1001) docker     (127)     1864 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/custom_properties.py
--rw-r--r--   0 runner    (1001) docker     (127)     3843 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/custom_pydantic.py
--rw-r--r--   0 runner    (1001) docker     (127)     1460 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/custom_types.py
--rw-r--r--   0 runner    (1001) docker     (127)      846 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/data_insight.py
--rw-r--r--   0 runner    (1001) docker     (127)      898 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/delete_entity.py
--rw-r--r--   0 runner    (1001) docker     (127)     1147 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/encoders.py
--rw-r--r--   0 runner    (1001) docker     (127)     1011 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/lf_tags_model.py
--rw-r--r--   0 runner    (1001) docker     (127)      836 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/life_cycle.py
--rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/ometa_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)      825 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/ometa_topic_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     8880 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/patch_request.py
--rw-r--r--   0 runner    (1001) docker     (127)      850 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/pipeline_status.py
--rw-r--r--   0 runner    (1001) docker     (127)      911 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/profile_data.py
--rw-r--r--   0 runner    (1001) docker     (127)      869 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/search_index_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     1439 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/table_metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1626 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/tests_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     9740 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/topology.py
--rw-r--r--   0 runner    (1001) docker     (127)     1078 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/user.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.021407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/
--rw-r--r--   0 runner    (1001) docker     (127)     3192 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/auth_provider.py
--rw-r--r--   0 runner    (1001) docker     (127)    10418 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     2237 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/client_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3964 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/credentials.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.025407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/
--rw-r--r--   0 runner    (1001) docker     (127)     2883 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/custom_property_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     1588 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     6419 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     7098 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/es_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     4253 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     6755 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/lineage_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     5563 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)    18898 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/patch_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     4056 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4145 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     4376 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/query_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)    15790 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     2563 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/search_index_mixin.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2608 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/server_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     3163 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/service_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     1361 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/suggestions_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)    10292 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/table_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)    11215 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/tests_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     1472 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/topic_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     5510 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/user_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     3280 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/version_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     1048 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/models.py
--rw-r--r--   0 runner    (1001) docker     (127)    18202 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/ometa_api.py
--rw-r--r--   0 runner    (1001) docker     (127)    12690 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/routes.py
--rw-r--r--   0 runner    (1001) docker     (127)     2296 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.025407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/processor/
--rw-r--r--   0 runner    (1001) docker     (127)     4422 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/processor/query_parser.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.029407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/sink/
--rw-r--r--   0 runner    (1001) docker     (127)     2083 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/sink/file.py
--rw-r--r--   0 runner    (1001) docker     (127)    22209 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/sink/metadata_rest.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.029407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/
--rw-r--r--   0 runner    (1001) docker     (127)     1543 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/connections.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.029407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/
--rw-r--r--   0 runner    (1001) docker     (127)    21709 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/dashboard_service.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.029407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/domodashboard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/domodashboard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2455 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/domodashboard/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     9770 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.029407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4723 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     2236 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     6415 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)      954 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.033407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4314 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/bulk_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     4747 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/columns.py
--rw-r--r--   0 runner    (1001) docker     (127)     2725 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1106 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/links.py
--rw-r--r--   0 runner    (1001) docker     (127)    39518 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2646 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     7207 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     2315 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.033407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6119 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1871 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    13624 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2243 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.033407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mode/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mode/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5716 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mode/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1895 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mode/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     9281 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mode/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.033407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mstr/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mstr/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6518 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mstr/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1770 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mstr/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     7002 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mstr/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2681 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mstr/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.033407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11440 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1810 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    28035 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     5074 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.037407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/
--rw-r--r--   0 runner    (1001) docker     (127)     7710 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1833 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1870 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)    13763 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2905 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.037407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/quicksight/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/quicksight/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2021 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/quicksight/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    13079 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/quicksight/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)      761 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/quicksight/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.037407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/redash/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/redash/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2183 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/redash/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     2000 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/redash/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    12074 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/redash/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.037407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9787 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/api_source.py
--rw-r--r--   0 runner    (1001) docker     (127)     7649 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     3438 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    10187 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/db_source.py
--rw-r--r--   0 runner    (1001) docker     (127)     1865 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     9566 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     3944 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     1643 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.041407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/
--rw-r--r--   0 runner    (1001) docker     (127)      955 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10394 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     5346 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    19984 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     4147 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     1321 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.041407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.045407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2775 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     3427 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1655 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)    12183 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1376 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     5152 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     2416 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.045407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4032 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)      916 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     4468 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)      694 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     1672 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)      904 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.045407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6159 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     4621 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/helper.py
--rw-r--r--   0 runner    (1001) docker     (127)     1312 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)    26806 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     3675 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.049407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2548 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     4062 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     9145 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.049407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2932 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1336 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     5985 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2314 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     3847 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     1221 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)     6784 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      977 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/column_helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)    16314 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/column_type_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)    22636 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/common_db_source.py
--rw-r--r--   0 runner    (1001) docker     (127)    11143 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/common_nosql_source.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.049407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/couchbase/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/couchbase/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2948 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/couchbase/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     4987 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/couchbase/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)      918 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/couchbase/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)    20537 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/database_service.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.053407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7705 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     3903 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2001 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)    21969 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1557 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     2317 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     2415 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.053407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/datalake/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/datalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1084 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/datalake/columns.py
--rw-r--r--   0 runner    (1001) docker     (127)     5469 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/datalake/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    20974 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/datalake/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.053407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/db2/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/db2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1980 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/db2/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3115 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/db2/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.053407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2230 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)    14595 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/dbt_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     9463 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/dbt_service.py
--rw-r--r--   0 runner    (1001) docker     (127)     5826 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/dbt_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    38819 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1042 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.053407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/deltalake/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/deltalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5033 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/deltalake/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    16347 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/deltalake/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.053407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/domodatabase/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/domodatabase/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2177 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/domodatabase/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    10648 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/domodatabase/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1270 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/domodatabase/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.057407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2569 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    11625 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1120 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     1927 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.057407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/druid/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/druid/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2124 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/druid/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1547 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/druid/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.057407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dynamodb/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dynamodb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2117 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dynamodb/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     4712 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dynamodb/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)      825 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dynamodb/models.py
--rw-r--r--   0 runner    (1001) docker     (127)    21725 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/extended_sample_data.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.057407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/glue/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/glue/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2434 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/glue/connection.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    16890 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/glue/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1383 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/glue/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.057407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2697 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     7371 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     4647 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)    10493 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.061407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6522 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3589 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.061407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1777 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.061407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/
--rw-r--r--   0 runner    (1001) docker     (127)      912 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4235 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/dialect.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.061407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/
--rw-r--r--   0 runner    (1001) docker     (127)      930 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4522 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/dialect.py
--rw-r--r--   0 runner    (1001) docker     (127)      736 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     6164 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.061407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.061407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/
--rw-r--r--   0 runner    (1001) docker     (127)     2732 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1420 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     4005 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/dynamodb.py
--rw-r--r--   0 runner    (1001) docker     (127)     3481 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/glue.py
--rw-r--r--   0 runner    (1001) docker     (127)     1956 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/hive.py
--rw-r--r--   0 runner    (1001) docker     (127)     3182 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/rest.py
--rw-r--r--   0 runner    (1001) docker     (127)     2182 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/connection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.061407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/fs/
--rw-r--r--   0 runner    (1001) docker     (127)     1993 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/fs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1671 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/fs/azure.py
--rw-r--r--   0 runner    (1001) docker     (127)     1127 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/fs/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     2524 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/fs/s3.py
--rw-r--r--   0 runner    (1001) docker     (127)     4409 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/helper.py
--rw-r--r--   0 runner    (1001) docker     (127)    12180 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2155 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.065407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/impala/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/impala/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4095 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/impala/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     6788 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/impala/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)      742 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/impala/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     5772 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/life_cycle_query_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     6540 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/lineage_source.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.065407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mariadb/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mariadb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2021 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mariadb/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1984 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mariadb/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.065407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mongodb/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mongodb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2566 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mongodb/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3567 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mongodb/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.065407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2595 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1369 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     9042 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1059 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     8974 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     1647 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     1135 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)    15284 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1272 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/multi_db_source.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.065407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mysql/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mysql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3447 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mysql/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2000 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mysql/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     5080 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mysql/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.069407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4941 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1640 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     9095 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)      582 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     5786 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     1707 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     1364 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)     7759 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.069407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/pinotdb/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/pinotdb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2185 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/pinotdb/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1627 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/pinotdb/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.069407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3961 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3851 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     9091 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.073407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/pgspider/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/pgspider/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4784 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/pgspider/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     1733 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/pgspider/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     6391 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     4255 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     2754 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)    12624 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.073407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/presto/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/presto/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3403 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/presto/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     6453 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/presto/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)      722 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/presto/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.073407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/query/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/query/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1167 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/query/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     1380 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/query/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)     4056 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/query_parser_source.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.073407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2896 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1563 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)    11502 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)      814 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/models.py
--rw-r--r--   0 runner    (1001) docker     (127)    11654 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     2677 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     1181 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)    12164 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.073407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/salesforce/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/salesforce/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2029 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/salesforce/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    13248 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/salesforce/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)    65159 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sample_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     3736 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sample_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.077407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/saphana/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/saphana/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5726 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/saphana/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2851 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/saphana/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.077407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6813 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1665 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3568 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/extension_attr.py
--rw-r--r--   0 runner    (1001) docker     (127)    37674 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.077407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/singlestore/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/singlestore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2038 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/singlestore/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2022 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/singlestore/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.077407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7883 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1248 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)    23325 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2748 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     6325 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     3611 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)    11257 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    13449 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sql_column_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2970 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sqlalchemy_source.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.077407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sqlite/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sqlite/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2164 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sqlite/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1634 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sqlite/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     9479 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/stored_procedures_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.081407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4076 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1266 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)     9739 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     1647 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)      987 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.081407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2702 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     3846 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     6675 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)    20480 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1878 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     2343 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     1176 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)     6459 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/usage_source.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.081407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2264 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1380 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/lineage.py
--rw-r--r--   0 runner    (1001) docker     (127)    11070 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     4794 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/queries.py
--rw-r--r--   0 runner    (1001) docker     (127)     2764 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     1216 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.081407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/
--rw-r--r--   0 runner    (1001) docker     (127)    11836 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/common_broker_source.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.085407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kafka/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kafka/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5891 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kafka/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1511 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kafka/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.085407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kinesis/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kinesis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1839 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kinesis/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    10916 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kinesis/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2433 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kinesis/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     7269 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/messaging_service.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.085407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/redpanda/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/redpanda/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1966 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/redpanda/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1532 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/redpanda/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.865407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.085407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3513 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     2551 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    18823 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     6272 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.085407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/atlas/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/atlas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2451 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/atlas/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/atlas/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    20708 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/atlas/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.085407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.085407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/mlflow/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/mlflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1843 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/mlflow/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     8564 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     6752 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/mlmodel_service.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.089407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/sagemaker/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/sagemaker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     7797 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1124 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.089407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.089407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airbyte/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airbyte/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3668 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airbyte/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1794 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airbyte/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    10585 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airbyte/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.089407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3781 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    10770 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)    21104 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1973 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     2221 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.089407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4571 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     2010 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    11420 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2162 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     3549 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.089407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/databrickspipeline/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/databrickspipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1864 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     9774 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.093407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/domopipeline/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/domopipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2087 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/domopipeline/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     7492 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.093407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/fivetran/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/fivetran/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3046 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/fivetran/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/fivetran/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     8990 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/fivetran/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.093407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/gluepipeline/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/gluepipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1780 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     7856 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.093407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/nifi/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/nifi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5583 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/nifi/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     2479 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/nifi/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     8557 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/nifi/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     8491 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/pipeline_service.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.093407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4139 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/client.py
--rw-r--r--   0 runner    (1001) docker     (127)     1807 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    11111 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     1925 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     2896 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.093407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/elasticsearch/
--rw-r--r--   0 runner    (1001) docker     (127)     6727 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/elasticsearch/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)     5659 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/elasticsearch/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2136 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/elasticsearch/parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     7506 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/search_service.py
--rw-r--r--   0 runner    (1001) docker     (127)     1525 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/sqa_types.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/s3/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/s3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/s3/connection.py
--rw-r--r--   0 runner    (1001) docker     (127)    19632 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/s3/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     2481 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/s3/models.py
--rw-r--r--   0 runner    (1001) docker     (127)    10635 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/storage_service.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/stage/
--rw-r--r--   0 runner    (1001) docker     (127)     7277 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/stage/table_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.869407 openmetadata-ingestion-1.3.4.0/src/metadata/mixins/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/mixins/pandas/
--rw-r--r--   0 runner    (1001) docker     (127)     4758 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/mixins/pandas/pandas_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/mixins/sqalchemy/
--rw-r--r--   0 runner    (1001) docker     (127)     3934 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/mixins/sqalchemy/sqa_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/parsers/
--rw-r--r--   0 runner    (1001) docker     (127)     9226 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/parsers/avro_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     2956 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/parsers/json_schema_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     8204 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/parsers/protobuf_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)     2304 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/parsers/schema_parsers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/pii/
--rw-r--r--   0 runner    (1001) docker     (127)      640 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/pii/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)      816 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/pii/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     2183 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/pii/ner.py
--rw-r--r--   0 runner    (1001) docker     (127)     6138 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/pii/processor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/pii/scanners/
--rw-r--r--   0 runner    (1001) docker     (127)     3334 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/pii/scanners/column_name_scanner.py
--rw-r--r--   0 runner    (1001) docker     (127)     5286 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/pii/scanners/ner_scanner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.097407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/api/
--rw-r--r--   0 runner    (1001) docker     (127)     4222 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/api/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/pandas/
--rw-r--r--   0 runner    (1001) docker     (127)    14644 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/pandas/profiler_interface.py
--rw-r--r--   0 runner    (1001) docker     (127)    16752 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/profiler_interface.py
--rw-r--r--   0 runner    (1001) docker     (127)     4925 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/profiler_interface_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/bigquery/
--rw-r--r--   0 runner    (1001) docker     (127)     2947 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/bigquery/profiler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/databricks/
--rw-r--r--   0 runner    (1001) docker     (127)     1018 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/databricks/profiler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/db2/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/db2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1453 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/db2/profiler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/mariadb/
--rw-r--r--   0 runner    (1001) docker     (127)     2965 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/mariadb/profiler_interface.py
--rw-r--r--   0 runner    (1001) docker     (127)    20255 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/profiler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/single_store/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/single_store/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/single_store/profiler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/snowflake/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/snowflake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1693 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/snowflake/profiler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/trino/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/trino/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2715 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/trino/profiler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/unity_catalog/
--rw-r--r--   0 runner    (1001) docker     (127)     1290 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/unity_catalog/profiler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.101407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.105407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/
--rw-r--r--   0 runner    (1001) docker     (127)     1784 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/distinct_ratio.py
--rw-r--r--   0 runner    (1001) docker     (127)     1688 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/duplicate_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     1621 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/ilike_ratio.py
--rw-r--r--   0 runner    (1001) docker     (127)     1860 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/iqr.py
--rw-r--r--   0 runner    (1001) docker     (127)     1609 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/like_ratio.py
--rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/non_parametric_skew.py
--rw-r--r--   0 runner    (1001) docker     (127)     1757 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/null_ratio.py
--rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/unique_ratio.py
--rw-r--r--   0 runner    (1001) docker     (127)     6831 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/core.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.105407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/hybrid/
--rw-r--r--   0 runner    (1001) docker     (127)     8959 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/hybrid/histogram.py
--rw-r--r--   0 runner    (1001) docker     (127)     4291 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.109407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/
--rw-r--r--   0 runner    (1001) docker     (127)     2587 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/column_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     2501 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/column_names.py
--rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/count.py
--rw-r--r--   0 runner    (1001) docker     (127)     2654 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/count_in_set.py
--rw-r--r--   0 runner    (1001) docker     (127)     1994 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/distinct_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     1664 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/ilike_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     1633 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/like_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     3163 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/max.py
--rw-r--r--   0 runner    (1001) docker     (127)     2381 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/max_length.py
--rw-r--r--   0 runner    (1001) docker     (127)     4439 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/mean.py
--rw-r--r--   0 runner    (1001) docker     (127)     3163 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/min.py
--rw-r--r--   0 runner    (1001) docker     (127)     2382 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/min_length.py
--rw-r--r--   0 runner    (1001) docker     (127)     1684 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/not_like_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     2744 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/not_regexp_match_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     1468 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/null_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     2621 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/regexp_match_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     1306 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/row_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     4802 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/stddev.py
--rw-r--r--   0 runner    (1001) docker     (127)     1618 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/sum.py
--rw-r--r--   0 runner    (1001) docker     (127)     2724 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/unique_count.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.109407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/
--rw-r--r--   0 runner    (1001) docker     (127)     1207 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/dml_operation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.109407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/queries/
--rw-r--r--   0 runner    (1001) docker     (127)     1644 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/queries/bigquery.py
--rw-r--r--   0 runner    (1001) docker     (127)     2774 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/queries/redshift.py
--rw-r--r--   0 runner    (1001) docker     (127)     6743 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/queries/snowflake.py
--rw-r--r--   0 runner    (1001) docker     (127)    15386 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/system.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.109407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/window/
--rw-r--r--   0 runner    (1001) docker     (127)     3367 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/window/first_quartile.py
--rw-r--r--   0 runner    (1001) docker     (127)     3217 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/window/median.py
--rw-r--r--   0 runner    (1001) docker     (127)      323 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/window/percentille_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     3358 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/window/third_quartile.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.109407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.109407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/
--rw-r--r--   0 runner    (1001) docker     (127)     6147 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/base.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.109407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/bigquery/
--rw-r--r--   0 runner    (1001) docker     (127)     1362 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/bigquery/converter.py
--rw-r--r--   0 runner    (1001) docker     (127)     3222 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/common.py
--rw-r--r--   0 runner    (1001) docker     (127)     1197 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/converter_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.109407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/mssql/
--rw-r--r--   0 runner    (1001) docker     (127)      859 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/mssql/converter.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.109407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/snowflake/
--rw-r--r--   0 runner    (1001) docker     (127)     1520 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/snowflake/converter.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.113407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/
--rw-r--r--   0 runner    (1001) docker     (127)     1521 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/concat.py
--rw-r--r--   0 runner    (1001) docker     (127)     1637 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/conn_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1688 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/count.py
--rw-r--r--   0 runner    (1001) docker     (127)    10286 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/datetime.py
--rw-r--r--   0 runner    (1001) docker     (127)     2666 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/length.py
--rw-r--r--   0 runner    (1001) docker     (127)     6448 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/median.py
--rw-r--r--   0 runner    (1001) docker     (127)     2931 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/modulo.py
--rw-r--r--   0 runner    (1001) docker     (127)     3178 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/random_num.py
--rw-r--r--   0 runner    (1001) docker     (127)     3430 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/sum.py
--rw-r--r--   0 runner    (1001) docker     (127)    16271 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/table_metric_computer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2221 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/unique_count.py
--rw-r--r--   0 runner    (1001) docker     (127)     4934 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.113407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/
--rw-r--r--   0 runner    (1001) docker     (127)     2119 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/bytea_to_string.py
--rw-r--r--   0 runner    (1001) docker     (127)     1442 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_array.py
--rw-r--r--   0 runner    (1001) docker     (127)     1820 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_datetimerange.py
--rw-r--r--   0 runner    (1001) docker     (127)     2271 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_hex_byte_string.py
--rw-r--r--   0 runner    (1001) docker     (127)      933 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_image.py
--rw-r--r--   0 runner    (1001) docker     (127)      924 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_ip.py
--rw-r--r--   0 runner    (1001) docker     (127)     1515 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_timestamp.py
--rw-r--r--   0 runner    (1001) docker     (127)     1865 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/uuid.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/
--rw-r--r--   0 runner    (1001) docker     (127)    22853 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/core.py
--rw-r--r--   0 runner    (1001) docker     (127)     3050 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     4149 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/handle_partition.py
--rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     3598 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/processor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4724 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/runner.py
--rw-r--r--   0 runner    (1001) docker     (127)     4559 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sample_data_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/pandas/
--rw-r--r--   0 runner    (1001) docker     (127)     6088 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/pandas/sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2471 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sampler_factory.py
--rw-r--r--   0 runner    (1001) docker     (127)     2564 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sampler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sqlalchemy/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sqlalchemy/bigquery/
--rw-r--r--   0 runner    (1001) docker     (127)     3725 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sqlalchemy/bigquery/sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)    10456 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sqlalchemy/sampler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sqlalchemy/trino/
--rw-r--r--   0 runner    (1001) docker     (127)     1737 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sqlalchemy/trino/sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3007 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/base/
--rw-r--r--   0 runner    (1001) docker     (127)    10696 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/base/profiler_source.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/bigquery/
--rw-r--r--   0 runner    (1001) docker     (127)     2328 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/bigquery/profiler_source.py
--rw-r--r--   0 runner    (1001) docker     (127)     1748 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/bigquery/type_mapper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/databricks/
--rw-r--r--   0 runner    (1001) docker     (127)     1237 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/databricks/profiler_source.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.873407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/mariadb/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/mariadb/functions/
--rw-r--r--   0 runner    (1001) docker     (127)      768 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/mariadb/functions/median.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.873407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/mariadb/metrics/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/mariadb/metrics/window/
--rw-r--r--   0 runner    (1001) docker     (127)      444 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/mariadb/metrics/window/first_quartile.py
--rw-r--r--   0 runner    (1001) docker     (127)      415 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/mariadb/metrics/window/median.py
--rw-r--r--   0 runner    (1001) docker     (127)      444 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/mariadb/metrics/window/third_quartile.py
--rw-r--r--   0 runner    (1001) docker     (127)    11417 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)    10854 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/metadata_ext.py
--rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/profiler_source_factory.py
--rw-r--r--   0 runner    (1001) docker     (127)     1643 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/profiler_source_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.873407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/single_store/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.117407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/single_store/functions/
--rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/single_store/functions/median.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:22.873407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/single_store/metrics/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.121407 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/single_store/metrics/window/
--rw-r--r--   0 runner    (1001) docker     (127)      465 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/single_store/metrics/window/first_quartile.py
--rw-r--r--   0 runner    (1001) docker     (127)      436 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/single_store/metrics/window/median.py
--rw-r--r--   0 runner    (1001) docker     (127)      465 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/single_store/metrics/window/third_quartile.py
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/py.typed
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.121407 openmetadata-ingestion-1.3.4.0/src/metadata/readers/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.121407 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3172 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/avro.py
--rw-r--r--   0 runner    (1001) docker     (127)     2502 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/base.py
--rw-r--r--   0 runner    (1001) docker     (127)      911 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/common.py
--rw-r--r--   0 runner    (1001) docker     (127)     4454 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/dsv.py
--rw-r--r--   0 runner    (1001) docker     (127)     3265 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/json.py
--rw-r--r--   0 runner    (1001) docker     (127)     1859 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     5032 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/parquet.py
--rw-r--r--   0 runner    (1001) docker     (127)     2547 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/reader_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.125407 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2508 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/adls.py
--rw-r--r--   0 runner    (1001) docker     (127)     1810 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/api_reader.py
--rw-r--r--   0 runner    (1001) docker     (127)     1676 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     3782 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/bitbucket.py
--rw-r--r--   0 runner    (1001) docker     (127)     2608 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/config_source_factory.py
--rw-r--r--   0 runner    (1001) docker     (127)     2354 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/credentials.py
--rw-r--r--   0 runner    (1001) docker     (127)     1672 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/gcs.py
--rw-r--r--   0 runner    (1001) docker     (127)     4339 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/github.py
--rw-r--r--   0 runner    (1001) docker     (127)     2662 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/local.py
--rw-r--r--   0 runner    (1001) docker     (127)     1633 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/s3.py
--rw-r--r--   0 runner    (1001) docker     (127)     1164 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/readers/models.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.125407 openmetadata-ingestion-1.3.4.0/src/metadata/timer/
--rw-r--r--   0 runner    (1001) docker     (127)     1410 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/timer/repeated_timer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.129407 openmetadata-ingestion-1.3.4.0/src/metadata/utils/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1920 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/bigquery_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6284 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/class_helper.py
--rw-r--r--   0 runner    (1001) docker     (127)     1647 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/client_version.py
--rw-r--r--   0 runner    (1001) docker     (127)     4035 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     7316 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/credentials.py
--rw-r--r--   0 runner    (1001) docker     (127)     1847 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/custom_thread_pool.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.129407 openmetadata-ingestion-1.3.4.0/src/metadata/utils/datalake/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/datalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    19287 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/datalake/datalake_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3666 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/db_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1246 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/deprecation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2976 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/dispatch.py
--rw-r--r--   0 runner    (1001) docker     (127)     3280 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/elasticsearch.py
--rw-r--r--   0 runner    (1001) docker     (127)     3843 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/entity_link.py
--rw-r--r--   0 runner    (1001) docker     (127)     6499 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/execution_time_tracker.py
--rw-r--r--   0 runner    (1001) docker     (127)     8462 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/filters.py
--rw-r--r--   0 runner    (1001) docker     (127)    19254 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/fqn.py
--rw-r--r--   0 runner    (1001) docker     (127)    13071 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)     7150 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/importer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2774 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/life_cycle_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6976 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/logger.py
--rw-r--r--   0 runner    (1001) docker     (127)     1733 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/lru_cache.py
--rw-r--r--   0 runner    (1001) docker     (127)     2571 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/metadata_service_helper.py
--rw-r--r--   0 runner    (1001) docker     (127)     3816 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/partition.py
--rw-r--r--   0 runner    (1001) docker     (127)     3384 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/profiler_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1153 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/s3_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.133407 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3785 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/aws_based_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     2602 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/aws_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     2532 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/aws_ssm_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     4577 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/azure_kv_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     1115 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/db_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     1697 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/external_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     1226 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     3982 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/secrets_manager_factory.py
--rw-r--r--   0 runner    (1001) docker     (127)     1051 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/singleton.py
--rw-r--r--   0 runner    (1001) docker     (127)     1676 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/source_hash.py
--rw-r--r--   0 runner    (1001) docker     (127)      915 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/sqa_like_column.py
--rw-r--r--   0 runner    (1001) docker     (127)     8264 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/sqa_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3770 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/sqlalchemy_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1788 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/ssl_registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     7118 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/storage_metadata_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/stored_procedures.py
--rw-r--r--   0 runner    (1001) docker     (127)     4676 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/tag_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2595 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/test_suite.py
--rw-r--r--   0 runner    (1001) docker     (127)     3974 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/time_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2300 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/timeout.py
--rw-r--r--   0 runner    (1001) docker     (127)      916 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/utils/uuid_encoder.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.133407 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/
--rw-r--r--   0 runner    (1001) docker     (127)     5440 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/application.py
--rw-r--r--   0 runner    (1001) docker     (127)     1297 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/application_output_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    10149 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     3211 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/data_insight.py
--rw-r--r--   0 runner    (1001) docker     (127)     4830 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/data_quality.py
--rw-r--r--   0 runner    (1001) docker     (127)     7773 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/ingestion.py
--rw-r--r--   0 runner    (1001) docker     (127)     2817 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/metadata.py
--rw-r--r--   0 runner    (1001) docker     (127)     7803 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/output_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3381 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4153 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/usage.py
--rw-r--r--   0 runner    (1001) docker     (127)     4856 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/workflow_output_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     5020 2024-05-07 11:22:38.000000 openmetadata-ingestion-1.3.4.0/src/metadata/workflow/workflow_status_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-07 11:24:23.137407 openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    39020 2024-05-07 11:24:22.000000 openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    89592 2024-05-07 11:24:22.000000 openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-07 11:24:22.000000 openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)      144 2024-05-07 11:24:22.000000 openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (127)     9317 2024-05-07 11:24:22.000000 openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)       39 2024-05-07 11:24:22.000000 openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.109196 openmetadata-ingestion-1.4.0.0rc1/
+-rw-r--r--   0 runner    (1001) docker     (127)    11356 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)    38780 2024-05-02 07:42:43.109196 openmetadata-ingestion-1.4.0.0rc1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     1070 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-02 07:42:43.109196 openmetadata-ingestion-1.4.0.0rc1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)    11646 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.809197 openmetadata-ingestion-1.4.0.0rc1/src/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.809197 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/
+-rw-r--r--   0 runner    (1001) docker     (127)     1260 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.809197 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/hooks/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3561 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/hooks/openmetadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.809197 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2810 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3652 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/callback.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.809197 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/config/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      659 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/config/commons.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/config/loader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2958 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/operator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15934 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4200 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/status.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.813197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/
+-rw-r--r--   0 runner    (1001) docker     (127)      700 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2710 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/__version__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.813197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/antlr/
+-rw-r--r--   0 runner    (1001) docker     (127)     1730 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/antlr/split_listener.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.813197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/applications/
+-rw-r--r--   0 runner    (1001) docker     (127)     7136 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/applications/auto_tagger.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.813197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/automations/
+-rw-r--r--   0 runner    (1001) docker     (127)     2686 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/automations/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.813197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1570 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/app.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7053 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/backup.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1673 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/dataquality.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8076 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/db_dump.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1611 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/ingest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1628 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/insight.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2625 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1652 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/profile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3245 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/restore.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1594 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2095 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.813197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/clients/
+-rw-r--r--   0 runner    (1001) docker     (127)     6689 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/clients/aws_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3244 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/clients/azure_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6112 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/clients/domo_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11812 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/cmd.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.817197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/config/
+-rw-r--r--   0 runner    (1001) docker     (127)     3222 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/config/common.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.769198 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.817197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.817197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/kpi/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/kpi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5304 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/kpi/kpi_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5130 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/kpi/run_result_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.817197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9973 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/cost_analysis_report_data_processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/data_processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8703 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/entity_report_data_processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13601 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/web_analytic_report_data_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.817197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4996 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/cost_analysis_producer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1968 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/entity_producer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2210 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/producer_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1120 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/producer_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2906 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/web_analytics_producer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.817197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/source/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/source/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6485 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/source/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.773198 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.817197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/api/
+-rw-r--r--   0 runner    (1001) docker     (127)     2512 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/api/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.817197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.821197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/pandas/
+-rw-r--r--   0 runner    (1001) docker     (127)     3989 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.821197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.821197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/databricks/
+-rw-r--r--   0 runner    (1001) docker     (127)     1029 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/databricks/test_suite_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.821197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/snowflake/
+-rw-r--r--   0 runner    (1001) docker     (127)     1032 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/snowflake/test_suite_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6166 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.821197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/unity_catalog/
+-rw-r--r--   0 runner    (1001) docker     (127)     1323 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/unity_catalog/test_suite_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3197 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/test_suite_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4698 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/test_suite_interface_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.821197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/processor/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/processor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12753 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/processor/test_case_runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.821197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/runner/
+-rw-r--r--   0 runner    (1001) docker     (127)     4277 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/runner/base_test_suite_source.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1659 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/runner/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1477 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/runner/test_suite_source_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.821197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/source/
+-rw-r--r--   0 runner    (1001) docker     (127)     6173 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/source/test_suite.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.821197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/
+-rw-r--r--   0 runner    (1001) docker     (127)     5820 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/base_test_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.773198 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.825197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/
+-rw-r--r--   0 runner    (1001) docker     (127)     4308 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2682 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2693 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2715 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2682 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4021 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2680 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5722 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3534 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3739 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3260 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3468 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3641 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3694 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.825197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/
+-rw-r--r--   0 runner    (1001) docker     (127)     2617 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1772 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1783 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1783 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1811 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1771 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2566 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2182 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2120 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2080 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1940 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2118 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2138 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.829197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (127)     2709 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1737 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1737 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1749 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1731 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1750 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1852 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1737 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2526 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2045 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2065 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2121 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3186 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2520 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2551 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.829197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/mixins/
+-rw-r--r--   0 runner    (1001) docker     (127)     2558 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4383 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.773198 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.829197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/
+-rw-r--r--   0 runner    (1001) docker     (127)     2313 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2302 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2437 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3217 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3306 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2419 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3505 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.833197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/
+-rw-r--r--   0 runner    (1001) docker     (127)     1226 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1288 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1402 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1439 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1485 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1300 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1280 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3095 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.833197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (127)     1426 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1406 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1427 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1673 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1760 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1369 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1289 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2444 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (127)      954 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/validator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.773198 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.849197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/
+-rw-r--r--   0 runner    (1001) docker     (127)     1071 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/airbyte.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1235 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/airflow.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1087 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/airflow_backend.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1636 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/airflow_postgres.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1173 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/amundsen.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1249 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/athena.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      993 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/athena_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/athena_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1243 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/atlas.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1277 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/azuresql.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      991 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/azuresql_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1314 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/azuresql_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1590 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigquery.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      997 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigquery_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     2132 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigquery_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1714 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigquery_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1526 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigtable.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1229 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/clickhouse.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1001 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/clickhouse_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1290 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/clickhouse_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/couchbase.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/dagster.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1068 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/data_insight.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1207 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/databricks.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1001 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/databricks_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1158 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/databricks_pipeline.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1303 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/databricks_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1285 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_azure_client_secret.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1219 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_azure_default.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1696 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_gcs.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1256 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1283 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_s3.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1106 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/db2.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1699 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/db2_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     3884 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/dbt.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1155 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/deltalake.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1222 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/domodashboard.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1294 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/dynamodb.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1044 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/elasticsearch.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1066 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/fivetran.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/glue.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1197 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/gluepipeline.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1407 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/hive.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1099 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/impala.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1693 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/kafka.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1068 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/kinesis.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1178 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/lightdash.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1140 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/looker.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1131 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mariadb.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1159 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/metabase.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1133 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mlflow.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1441 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mode.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mongodb.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1093 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mssql.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      991 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mssql_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1270 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mssql_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1141 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mstr.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1201 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mysql.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1306 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mysql_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1822 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/openmetadata.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1118 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/oracle.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      969 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/oracle_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/oracle_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1167 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/pinotdb.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1137 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/postgres.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      999 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/postgres_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1294 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/postgres_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     3617 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/powerbi.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1127 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/presto.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1924 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/qlik_sense.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1260 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/qlikcloud.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1401 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/query_log_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1302 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/quicksight.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1133 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redash.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1166 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redpanda.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1236 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redshift.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      995 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redshift_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1713 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redshift_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1322 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redshift_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1134 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/sagemaker.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1181 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/salesforce.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1293 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/sas.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/singlestore.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1346 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/snowflake.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      987 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/snowflake_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1290 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/snowflake_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1261 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/spline.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1093 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/sqlite.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1537 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/superset.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/tableau.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1416 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/test_suite.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1401 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/trino.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      986 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/trino_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1330 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/trino_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1237 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/unity_catalog.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1005 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/unity_catalog_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1422 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/unity_catalog_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     1105 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/vertica.yaml
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.773198 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.849197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/
+-rw-r--r--   0 runner    (1001) docker     (127)    34377 2024-05-02 07:41:58.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/EntityLinkLexer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1976 2024-05-02 07:41:58.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/EntityLinkListener.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13358 2024-05-02 07:41:58.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/EntityLinkParser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-02 07:41:58.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/FqnLexer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-05-02 07:41:58.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/FqnListener.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6811 2024-05-02 07:41:58.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/FqnParser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8675 2024-05-02 07:41:58.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/JdbcUriLexer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2181 2024-05-02 07:41:58.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/JdbcUriListener.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16824 2024-05-02 07:41:58.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/JdbcUriParser.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.785197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.849197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      948 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/basic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1603 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportData.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.853197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2283 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/aggregatedCostAnalysisReportData.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1343 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py
+-rw-r--r--   0 runner    (1001) docker     (127)      723 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/rawCostAnalysisReportData.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1094 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py
+-rw-r--r--   0 runner    (1001) docker     (127)      958 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2128 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEvent.py
+-rw-r--r--   0 runner    (1001) docker     (127)      883 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEventData.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.853197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEventType/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEventType/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      978 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1202 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.853197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1063 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/addGlossaryToAssetsRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.853197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/analytics/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/analytics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1021 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.853197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/automations/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/automations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1483 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/automations/createWorkflow.py
+-rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/bulkAssets.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.853197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/classification/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1285 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/classification/createClassification.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1777 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/classification/createTag.py
+-rw-r--r--   0 runner    (1001) docker     (127)      525 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/classification/loadTags.py
+-rw-r--r--   0 runner    (1001) docker     (127)      928 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/createBot.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1403 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/createEventPublisherJob.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1017 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/createType.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.857197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1967 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createChart.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3088 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createContainer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      873 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createCustomProperty.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2789 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createDashboard.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2536 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createDashboardDataModel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2559 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createDatabase.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2302 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createDatabaseSchema.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1803 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createGlossary.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2603 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createGlossaryTerm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3132 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createMlModel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2740 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2516 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createQuery.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2307 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createSearchIndex.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2208 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createStoredProcedure.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2886 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createTable.py
+-rw-r--r--   0 runner    (1001) docker     (127)      727 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createTableProfile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3422 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createTopic.py
+-rw-r--r--   0 runner    (1001) docker     (127)      541 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/loadGlossary.py
+-rw-r--r--   0 runner    (1001) docker     (127)      424 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/restoreEntity.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.857197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/dataInsight/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/dataInsight/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1294 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.857197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/dataInsight/kpi/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/dataInsight/kpi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1322 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.857197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/docStore/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/docStore/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      851 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/docStore/createDocument.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.857197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/domains/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/domains/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1554 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/domains/createDataProduct.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/domains/createDomain.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.861197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      597 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/closeTask.py
+-rw-r--r--   0 runner    (1001) docker     (127)      526 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/createPost.py
+-rw-r--r--   0 runner    (1001) docker     (127)      935 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/createSuggestion.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1898 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/createThread.py
+-rw-r--r--   0 runner    (1001) docker     (127)      794 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/resolveTask.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1034 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/threadCount.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.861197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/lineage/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      408 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/lineage/addLineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)      672 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/openMetadataServerVersion.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.861197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1082 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/policies/createPolicy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.861197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1534 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createDashboardService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createDatabaseService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1596 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createMessagingService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createMetadataService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1510 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createMlModelService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1777 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createPipelineService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1560 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createSearchService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1582 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createStorageService.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.861197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/ingestionPipelines/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/ingestionPipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1463 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)      547 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/setOwner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.861197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      785 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/createPersona.py
+-rw-r--r--   0 runner    (1001) docker     (127)      800 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/createRole.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2585 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/createTeam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2405 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/createUser.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.861197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1252 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createCustomMetric.py
+-rw-r--r--   0 runner    (1001) docker     (127)      585 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createLogicalTestCases.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1374 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createTestCase.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1067 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createTestCaseResolutionStatus.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1228 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createTestDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1311 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createTestSuite.py
+-rw-r--r--   0 runner    (1001) docker     (127)      390 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/voteRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.865197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/basicAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)      462 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/basicLoginRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)      886 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/changePasswordRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)      437 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/createPersonalToken.py
+-rw-r--r--   0 runner    (1001) docker     (127)      365 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/emailRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1120 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/emailVerificationToken.py
+-rw-r--r--   0 runner    (1001) docker     (127)      347 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/generateToken.py
+-rw-r--r--   0 runner    (1001) docker     (127)      751 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/jwtAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)      409 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/loginRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)      626 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/logoutRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)      683 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/passwordResetRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/passwordResetToken.py
+-rw-r--r--   0 runner    (1001) docker     (127)      956 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/personalAccessToken.py
+-rw-r--r--   0 runner    (1001) docker     (127)      975 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/refreshToken.py
+-rw-r--r--   0 runner    (1001) docker     (127)      623 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/registrationRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)      476 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/revokePersonalToken.py
+-rw-r--r--   0 runner    (1001) docker     (127)      324 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/revokeToken.py
+-rw-r--r--   0 runner    (1001) docker     (127)      288 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/serviceTokenEnum.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1283 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/ssoAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)      378 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/tokenRefreshRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.869197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      928 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/appsPrivateConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1349 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/authConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2098 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/authenticationConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1667 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/authorizerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      363 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/changeEventConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      436 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/dataQualityConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2068 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      442 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/eventHandlerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      357 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/fernetConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      658 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1930 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2857 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapConfiguration.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.869197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      894 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      544 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      451 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/jvmDefaultConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      462 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/trustAllConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      978 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      723 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/loginConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      655 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/logoConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3002 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1827 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/profilerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      683 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/slackAppConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      436 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/taskNotificationConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      885 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1257 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/themeConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      816 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/uiThemePreference.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.869197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3354 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/dataInsightChart.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3266 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.869197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/kpi/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/kpi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1517 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/kpi/basic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2537 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/kpi/kpi.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.873197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      830 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/aggregatedUnusedAssetsCount.py
+-rw-r--r--   0 runner    (1001) docker     (127)      828 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/aggregatedUnusedAssetsSize.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1002 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/aggregatedUsedVsUnusedAssetsCount.py
+-rw-r--r--   0 runner    (1001) docker     (127)      987 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/aggregatedUsedVsUnusedAssetsSize.py
+-rw-r--r--   0 runner    (1001) docker     (127)      561 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1077 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py
+-rw-r--r--   0 runner    (1001) docker     (127)      791 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py
+-rw-r--r--   0 runner    (1001) docker     (127)      635 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1047 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py
+-rw-r--r--   0 runner    (1001) docker     (127)      975 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1024 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/percentageOfServicesWithDescription.py
+-rw-r--r--   0 runner    (1001) docker     (127)      964 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/percentageOfServicesWithOwner.py
+-rw-r--r--   0 runner    (1001) docker     (127)      831 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py
+-rw-r--r--   0 runner    (1001) docker     (127)      843 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py
+-rw-r--r--   0 runner    (1001) docker     (127)      662 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/unusedAssets.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.873197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/email/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/email/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1250 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/email/emailRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1312 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/email/smtpSettings.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.873197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.873197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5808 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/app.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1722 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/appRunRecord.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.873197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      853 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/applicationConfig.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.873197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.877197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/
+-rw-r--r--   0 runner    (1001) docker     (127)      133 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1253 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/addDescriptionAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      948 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/addDomainAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      937 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/addOwnerAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1183 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/addTagsAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      905 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/addTierAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      981 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/lineagePropagationAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      543 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/mlTaggingAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      897 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/removeDescriptionAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      561 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/removeDomainAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      555 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/removeOwnerAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      938 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/removeTagsAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      549 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/removeTierAction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2316 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automatorAppConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/metaPilotAppConfig.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.877197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/internal/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/internal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      334 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/internal/dataInsightsAppConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      811 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/internal/dataInsightsReportAppConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1307 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/internal/searchIndexingAppConfig.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.777197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/private/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.877197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/private/external/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/private/external/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      855 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/private/external/metaPilotAppPrivateConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1146 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/createAppRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)      451 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/jobStatus.py
+-rw-r--r--   0 runner    (1001) docker     (127)      499 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/liveExecutionContext.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.877197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/marketplace/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/marketplace/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4243 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/marketplace/appMarketPlaceDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3025 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/marketplace/createAppMarketPlaceDefinitionReq.py
+-rw-r--r--   0 runner    (1001) docker     (127)      360 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/scheduledExecutionContext.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.881197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/automations/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/automations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1725 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/automations/testServiceConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2859 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/automations/workflow.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2032 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/bot.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.881197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/classification/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2851 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/classification/classification.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3240 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/classification/tag.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.881197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3897 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/chart.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5657 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/container.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4525 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/dashboard.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4194 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/dashboardDataModel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5331 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/database.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5143 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/databaseSchema.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3317 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/glossary.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5676 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/glossaryTerm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2589 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7276 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/mlmodel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7054 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3697 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/query.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2350 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/report.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6649 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/searchIndex.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4541 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/storedProcedure.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23872 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/table.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5421 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/topic.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.881197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/docStore/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/docStore/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/docStore/document.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.885197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/domains/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/domains/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2340 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/domains/dataProduct.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2465 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/domains/domain.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.885197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/events/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/events/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1332 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/events/webhook.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.885197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      418 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/assets.py
+-rw-r--r--   0 runner    (1001) docker     (127)      503 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/customProperty.py
+-rw-r--r--   0 runner    (1001) docker     (127)      684 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/description.py
+-rw-r--r--   0 runner    (1001) docker     (127)      587 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/domain.py
+-rw-r--r--   0 runner    (1001) docker     (127)      456 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/entityInfo.py
+-rw-r--r--   0 runner    (1001) docker     (127)      583 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/owner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2048 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/suggestion.py
+-rw-r--r--   0 runner    (1001) docker     (127)      572 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/tag.py
+-rw-r--r--   0 runner    (1001) docker     (127)      646 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/testCaseResult.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7930 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/thread.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.885197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.885197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/accessControl/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/accessControl/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1887 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1821 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1497 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/accessControl/rule.py
+-rw-r--r--   0 runner    (1001) docker     (127)      679 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/filters.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3202 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/policy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.889197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.889197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.889197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      133 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      720 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/common/sslCertPaths.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1005 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/common/sslCertValues.py
+-rw-r--r--   0 runner    (1001) docker     (127)      574 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/common/sslConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3004 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.893197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      946 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1545 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1662 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/lightdashConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1803 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1300 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1349 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1370 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/mstrConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3037 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.893197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1116 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/azureConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      678 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/bucketDetails.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1100 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/gcsConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1094 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/s3Config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1093 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/qlikCloudConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2024 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/qlikSenseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1270 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1310 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1991 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.901197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2633 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4634 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3071 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1311 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/bigTableConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3760 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.901197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      542 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/azureConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      541 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/basicAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)      536 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/iamAuthConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      522 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/jwtAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1649 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/couchbaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      938 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3562 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.901197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/datalake/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/datalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      531 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      521 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      518 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2139 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2441 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3741 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1746 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2791 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/dorisConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2445 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1519 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1449 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3195 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/greenplumConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3846 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.901197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      530 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/dynamoDbCatalogConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      427 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/glueCatalogConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      623 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/hiveCatalogConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1346 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/icebergCatalog.py
+-rw-r--r--   0 runner    (1001) docker     (127)      541 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/icebergFileSystem.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2044 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/restCatalogConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      981 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/icebergConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3535 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/impalaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2816 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2005 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/mongoDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3354 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3122 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4285 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2846 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3744 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3091 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3238 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2233 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.905197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sapHana/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sapHana/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sapHana/sapHanaHDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1301 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sapHana/sapHanaSQLConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2067 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sapHanaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1684 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sasConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2887 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4318 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2815 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3429 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3572 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/unityCatalogConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2936 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.905197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      946 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3197 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      901 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      729 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2721 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      389 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/saslMechanismType.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.905197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3707 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/alationConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1606 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1688 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1909 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5062 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.905197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/mlmodel/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/mlmodel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      931 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1087 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      915 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.909197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1154 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1536 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      545 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      938 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1199 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1412 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1371 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      916 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1865 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/kafkaConnectConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2241 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2156 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/openLineageConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/sparkConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1018 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/splineConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.909197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      922 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/customSearchConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.909197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      741 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/apiAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)      682 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/basicAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1647 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/elasticSearchConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1911 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/openSearchConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1158 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/serviceConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.913197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1167 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/adlsConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      930 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/customStorageConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1146 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1119 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2930 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1575 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4674 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/dashboardService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7076 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/databaseService.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.913197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/ingestionPipelines/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/ingestionPipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6278 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)      778 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/ingestionPipelines/pipelineServiceClientResponse.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1722 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/ingestionPipelines/status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3943 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/messagingService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3596 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/metadataService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3708 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/mlmodelService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4706 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/pipelineService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3573 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/searchService.py
+-rw-r--r--   0 runner    (1001) docker     (127)      413 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/serviceType.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3602 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/storageService.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.913197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1707 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/persona.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2472 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/role.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4585 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/team.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1856 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/teamHierarchy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4338 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/user.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2761 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/type.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.913197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1254 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/utils/entitiesCount.py
+-rw-r--r--   0 runner    (1001) docker     (127)      951 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/utils/servicesCount.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1170 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/utils/supersetApiConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.917197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      780 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/alertMetrics.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.917197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/api/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1950 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/api/createEventSubscription.py
+-rw-r--r--   0 runner    (1001) docker     (127)      736 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/emailAlertConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1586 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/eventFilterRule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7773 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/eventSubscription.py
+-rw-r--r--   0 runner    (1001) docker     (127)      541 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/eventSubscriptionOffset.py
+-rw-r--r--   0 runner    (1001) docker     (127)      861 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/failedEvent.py
+-rw-r--r--   0 runner    (1001) docker     (127)      936 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/filterResourceDescriptor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1232 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.921197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1399 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/application.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1009 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/applicationPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3635 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)      469 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dataInsightPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5274 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3582 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2206 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1542 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2410 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.921197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtAzureConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      676 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1446 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1255 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1082 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1250 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1401 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2281 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1197 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2814 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1584 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/searchServiceMetadataPipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.921197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1536 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1726 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/manifestMetadataConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      792 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageBucketDetails.py
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataADLSConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      689 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataGCSConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      531 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataHttpConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      528 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataLocalConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      686 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataS3Config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1922 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1037 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5056 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.921197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/monitoring/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/monitoring/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      282 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/monitoring/eventMonitorProvider.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.921197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.925197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      579 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      700 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/azureSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      636 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      669 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/googleSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1879 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/oidcClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      757 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)      494 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/openMetadataJWTClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3242 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/samlSSOClientConfig.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.925197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      677 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/accessTokenAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)      497 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/apiAccessTokenAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2001 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/awsCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1294 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/azureCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/basicAuth.py
+-rw-r--r--   0 runner    (1001) docker     (127)      904 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/bitbucketCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1601 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/gcpCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1390 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/gcpExternalAccount.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2167 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/gcpValues.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1102 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/gitCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (127)      712 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/githubCredentials.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.929197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/secrets/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/secrets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      298 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/secrets/secretsManagerClientLoader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1299 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (127)      454 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/secrets/secretsManagerProvider.py
+-rw-r--r--   0 runner    (1001) docker     (127)      560 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/securityConfiguration.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.929197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/ssl/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/ssl/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      930 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1025 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/ssl/verifySSLConfig.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.929197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/settings/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/settings/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2416 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/settings/settings.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.929197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      370 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/entityError.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2420 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/eventPublisherJob.py
+-rw-r--r--   0 runner    (1001) docker     (127)      849 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/indexingError.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.929197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/ui/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/ui/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      595 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/ui/knowledgePanel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1757 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/ui/page.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1323 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/validationResponse.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.933197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      429 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/assigned.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3303 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/basic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1348 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/customMetric.py
+-rw-r--r--   0 runner    (1001) docker     (127)      928 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/resolved.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3621 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/testCase.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2093 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/testCaseResolutionStatus.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4288 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/testDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3901 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/testSuite.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.937197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1200 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/auditLog.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4823 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/basic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1691 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/bulkOperationResult.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2307 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/changeEvent.py
+-rw-r--r--   0 runner    (1001) docker     (127)      950 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/changeEventType.py
+-rw-r--r--   0 runner    (1001) docker     (127)      870 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/collectionDescriptor.py
+-rw-r--r--   0 runner    (1001) docker     (127)      526 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/csvDocumentation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      422 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/csvErrorType.py
+-rw-r--r--   0 runner    (1001) docker     (127)      942 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/csvFile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1264 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/csvImportResult.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.937197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/customProperties/
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/customProperties/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3448 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/customProperties/complexTypes.py
+-rw-r--r--   0 runner    (1001) docker     (127)      396 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/customProperties/enumConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1631 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/customProperty.py
+-rw-r--r--   0 runner    (1001) docker     (127)      453 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/dailyCount.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1887 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/databaseConnectionConfig.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1215 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityHierarchy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2457 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityHistory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3512 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityLineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1657 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityReference.py
+-rw-r--r--   0 runner    (1001) docker     (127)      739 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityReferenceList.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2415 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityRelationship.py
+-rw-r--r--   0 runner    (1001) docker     (127)      579 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityUsage.py
+-rw-r--r--   0 runner    (1001) docker     (127)      703 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/filterPattern.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1398 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/function.py
+-rw-r--r--   0 runner    (1001) docker     (127)      262 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/include.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1022 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/jdbcConnection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1306 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/lifeCycle.py
+-rw-r--r--   0 runner    (1001) docker     (127)      949 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/paging.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/profile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1591 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/queryParserData.py
+-rw-r--r--   0 runner    (1001) docker     (127)      739 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/reaction.py
+-rw-r--r--   0 runner    (1001) docker     (127)      604 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2580 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/schema.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/tableQuery.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1724 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/tableUsageCount.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1944 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/tagLabel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1216 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/usageDetails.py
+-rw-r--r--   0 runner    (1001) docker     (127)      413 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/usageRequest.py
+-rw-r--r--   0 runner    (1001) docker     (127)      904 2024-05-02 07:41:57.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/votes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.937197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/great_expectations/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/great_expectations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16484 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/great_expectations/action.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.937197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/great_expectations/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/great_expectations/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2795 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/great_expectations/utils/ometa_config_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.797197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.941197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/
+-rw-r--r--   0 runner    (1001) docker     (127)      742 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/closeable.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1208 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3385 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/delete.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1132 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19815 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3151 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8202 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/step.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2532 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/steps.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20088 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/topology_runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.941197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/bulksink/
+-rw-r--r--   0 runner    (1001) docker     (127)    16100 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/bulksink/metadata_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.941197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/
+-rw-r--r--   0 runner    (1001) docker     (127)     6525 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/builders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2440 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/headers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1920 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/secrets.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1229 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/session.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13519 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/test_connections.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.941197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/lineage/
+-rw-r--r--   0 runner    (1001) docker     (127)     5396 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/lineage/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18400 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/lineage/parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17948 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/lineage/sql_lineage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.945197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/
+-rw-r--r--   0 runner    (1001) docker     (127)     1882 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/custom_properties.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3843 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/custom_pydantic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1460 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/custom_types.py
+-rw-r--r--   0 runner    (1001) docker     (127)      846 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/data_insight.py
+-rw-r--r--   0 runner    (1001) docker     (127)      898 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/delete_entity.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1147 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/encoders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1011 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/lf_tags_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)      887 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/life_cycle.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/ometa_classification.py
+-rw-r--r--   0 runner    (1001) docker     (127)      825 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/ometa_topic_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14225 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/patch_request.py
+-rw-r--r--   0 runner    (1001) docker     (127)      850 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/pipeline_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)      911 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/profile_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)      869 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/search_index_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1439 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/table_metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1626 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/tests_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12399 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/topology.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1078 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/user.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.945197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/
+-rw-r--r--   0 runner    (1001) docker     (127)     3192 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/auth_provider.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10560 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2237 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/client_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3964 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/credentials.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.949197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/
+-rw-r--r--   0 runner    (1001) docker     (127)     2883 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/custom_property_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1588 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6419 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9214 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/es_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4253 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13257 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/lineage_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5563 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19153 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/patch_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4056 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4145 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4376 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/query_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15790 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2563 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/search_index_mixin.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4097 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/server_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3163 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/service_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1361 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/suggestions_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10292 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/table_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13140 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/tests_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1472 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/topic_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5508 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/user_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3280 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/version_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1048 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18409 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/ometa_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12813 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/routes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2417 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.949197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/processor/
+-rw-r--r--   0 runner    (1001) docker     (127)     4506 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/processor/query_parser.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.949197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/sink/
+-rw-r--r--   0 runner    (1001) docker     (127)     2162 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/sink/file.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22615 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/sink/metadata_rest.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.949197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/
+-rw-r--r--   0 runner    (1001) docker     (127)     1543 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/connections.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.949197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/
+-rw-r--r--   0 runner    (1001) docker     (127)    23747 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/dashboard_service.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.953197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/domodashboard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/domodashboard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2455 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/domodashboard/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9845 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.953197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4723 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2236 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6490 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)      954 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.953197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4314 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/bulk_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4747 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/columns.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2725 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1106 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/links.py
+-rw-r--r--   0 runner    (1001) docker     (127)    39710 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2646 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7207 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2315 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.953197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/metabase/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/metabase/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7255 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/metabase/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1954 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/metabase/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13721 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/metabase/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2237 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/metabase/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.957197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mode/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mode/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5716 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mode/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1895 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mode/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9356 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mode/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.957197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6518 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1770 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7102 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2681 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.957197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11764 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2106 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11415 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/file_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31295 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5512 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.957197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qlikcloud/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qlikcloud/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5873 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qlikcloud/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qlikcloud/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1744 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qlikcloud/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10336 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qlikcloud/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1003 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qlikcloud/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.961197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/
+-rw-r--r--   0 runner    (1001) docker     (127)     7191 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1833 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1870 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14527 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2935 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.961197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/quicksight/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/quicksight/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2021 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/quicksight/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13154 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/quicksight/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)      761 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/quicksight/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.961197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/redash/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/redash/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2183 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/redash/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2000 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/redash/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12174 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/redash/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.961197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9999 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/api_source.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7649 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3438 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10395 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/db_source.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1970 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10292 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4004 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1882 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.965197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/
+-rw-r--r--   0 runner    (1001) docker     (127)      955 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10394 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5346 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22371 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4147 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1321 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.965197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.969197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2775 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3427 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1655 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14599 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1376 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5231 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2416 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.969197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4032 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      916 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4525 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)      694 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1751 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)      904 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.973197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6159 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5182 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/helper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3018 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/incremental_table_processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1312 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33009 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2201 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5596 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3726 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.973197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2548 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4062 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9214 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.973197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2932 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1336 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6036 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2314 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3908 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1221 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6784 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      977 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/column_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16314 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/column_type_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24566 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/common_db_source.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11233 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/common_nosql_source.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.973197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/couchbase/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/couchbase/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2948 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/couchbase/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5072 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/couchbase/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)      918 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/couchbase/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21106 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/database_service.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.977197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7705 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3903 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2001 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23883 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1580 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2427 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2415 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.977197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/datalake/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/datalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1084 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/datalake/columns.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5461 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/datalake/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21137 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/datalake/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.977197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/db2/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/db2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1980 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/db2/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3194 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/db2/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.977197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2230 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14595 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/dbt_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9554 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/dbt_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5826 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/dbt_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41751 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1222 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.977197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/deltalake/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/deltalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5033 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/deltalake/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16488 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/deltalake/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.981197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/domodatabase/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/domodatabase/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2177 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/domodatabase/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10784 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/domodatabase/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1270 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/domodatabase/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.981197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2013 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12441 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1120 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1927 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.981197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/druid/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/druid/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2124 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/druid/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1627 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/druid/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.981197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dynamodb/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dynamodb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2117 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dynamodb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4763 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dynamodb/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)      825 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dynamodb/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21786 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/extended_sample_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3271 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/external_table_lineage_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.981197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/glue/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/glue/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2434 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/glue/connection.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    17061 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/glue/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1383 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/glue/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.985197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2188 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7769 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4647 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10493 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.985197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6522 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3650 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.985197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1777 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.985197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/
+-rw-r--r--   0 runner    (1001) docker     (127)      912 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4235 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/dialect.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.985197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/
+-rw-r--r--   0 runner    (1001) docker     (127)      930 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4522 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/dialect.py
+-rw-r--r--   0 runner    (1001) docker     (127)      736 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6164 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.985197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.989197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/
+-rw-r--r--   0 runner    (1001) docker     (127)     2732 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1420 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4005 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/dynamodb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3481 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/glue.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1956 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/hive.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3182 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/rest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2182 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/connection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.989197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/fs/
+-rw-r--r--   0 runner    (1001) docker     (127)     1993 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/fs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1671 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/fs/azure.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1127 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/fs/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2524 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/fs/s3.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5349 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/helper.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12321 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2468 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.989197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/impala/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/impala/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4095 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/impala/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6867 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/impala/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)      742 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/impala/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5297 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/incremental_metadata_extraction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5857 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/life_cycle_query_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6540 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/lineage_source.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.989197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mariadb/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mariadb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2021 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mariadb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2064 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mariadb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.989197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mongodb/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mongodb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2566 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mongodb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3628 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mongodb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.993197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2595 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1369 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9129 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1059 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9180 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1726 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1135 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15458 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1272 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/multi_db_source.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.993197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mysql/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mysql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2890 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mysql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2061 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mysql/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5080 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mysql/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.993197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4941 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1640 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9170 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)      582 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5800 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1786 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1364 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7759 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.993197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/pinotdb/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/pinotdb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2185 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/pinotdb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1706 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/pinotdb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.997197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3452 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3851 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9675 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.997197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/pgspider/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/pgspider/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4784 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/pgspider/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1733 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/pgspider/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6846 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4316 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2754 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16234 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.997197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/presto/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/presto/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3952 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/presto/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6553 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/presto/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)      722 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/presto/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.997197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/query/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/query/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1247 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/query/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1459 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/query/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4056 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/query_parser_source.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.001196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2387 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6859 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/incremental_table_processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1563 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16789 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3080 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12027 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2728 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1181 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12350 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.001196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/salesforce/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/salesforce/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2029 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/salesforce/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13359 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/salesforce/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)    67022 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sample_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3787 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sample_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.001196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/saphana/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/saphana/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5726 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/saphana/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2922 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/saphana/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.001196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6813 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1665 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3568 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/extension_attr.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37762 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.001196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/singlestore/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/singlestore/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2038 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/singlestore/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2102 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/singlestore/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.005197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7883 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1248 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28253 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3497 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9519 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3662 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16932 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13556 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sql_column_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2984 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sqlalchemy_source.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.005197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sqlite/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sqlite/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2164 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sqlite/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1714 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sqlite/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9573 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/stored_procedures_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.005197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4347 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1266 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9829 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1726 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)      987 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.005197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2702 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3846 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6726 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21027 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1878 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2422 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1176 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6459 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/usage_source.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.009197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2264 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1380 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11127 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4794 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/queries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2825 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1216 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.009197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/
+-rw-r--r--   0 runner    (1001) docker     (127)    11854 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/common_broker_source.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.009197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kafka/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kafka/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5891 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kafka/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2345 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kafka/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.009197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kinesis/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kinesis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1839 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kinesis/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11016 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kinesis/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2433 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kinesis/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7282 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/messaging_service.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.009197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/redpanda/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/redpanda/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1966 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/redpanda/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/redpanda/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.797197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.009197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3513 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2551 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18874 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6272 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.013197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/atlas/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/atlas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2451 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/atlas/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/atlas/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20759 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/atlas/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.013197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.013197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/mlflow/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/mlflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1843 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/mlflow/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8621 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6765 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/mlmodel_service.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.013197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/sagemaker/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/sagemaker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8060 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1124 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.013197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.013197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airbyte/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airbyte/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3668 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airbyte/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1794 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airbyte/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10676 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airbyte/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.013197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3781 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10770 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21741 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1973 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2221 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.017197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4571 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2010 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11555 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2162 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3549 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.017197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/databrickspipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/databrickspipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1864 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9871 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.017197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/domopipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/domopipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2087 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/domopipeline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7561 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.017197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/fivetran/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/fivetran/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3046 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/fivetran/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/fivetran/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9059 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/fivetran/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.017197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/gluepipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/gluepipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1780 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7935 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.017197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/nifi/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/nifi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5583 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/nifi/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2479 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/nifi/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8614 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/nifi/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.021196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/openlineage/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/openlineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3096 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/openlineage/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20228 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/openlineage/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1761 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/openlineage/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1635 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/openlineage/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8790 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/pipeline_service.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.021196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4139 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1807 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11168 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1925 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2896 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.021196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.021196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/elasticsearch/
+-rw-r--r--   0 runner    (1001) docker     (127)     6727 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/elasticsearch/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5740 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/elasticsearch/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2136 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/elasticsearch/parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7519 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/search_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2002 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/sqa_types.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.021196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.021196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/s3/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/s3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/s3/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20469 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/s3/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2586 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/s3/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10654 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/storage_service.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.021196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/stage/
+-rw-r--r--   0 runner    (1001) docker     (127)     7353 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/stage/table_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.801197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/mixins/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.021196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/mixins/pandas/
+-rw-r--r--   0 runner    (1001) docker     (127)     4761 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/mixins/pandas/pandas_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.021196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/mixins/sqalchemy/
+-rw-r--r--   0 runner    (1001) docker     (127)     3934 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/mixins/sqalchemy/sqa_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.025196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/parsers/
+-rw-r--r--   0 runner    (1001) docker     (127)     9226 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/parsers/avro_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2956 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/parsers/json_schema_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8204 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/parsers/protobuf_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2304 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/parsers/schema_parsers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.025196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/
+-rw-r--r--   0 runner    (1001) docker     (127)      640 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)      816 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2183 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/ner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6214 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/processor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.025196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/scanners/
+-rw-r--r--   0 runner    (1001) docker     (127)     3334 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/scanners/column_name_scanner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5286 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/scanners/ner_scanner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.025196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.025196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/adaptors/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/adaptors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1761 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/adaptors/adaptor_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1554 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/adaptors/dynamodb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2765 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/adaptors/factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5510 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/adaptors/mongodb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2076 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/adaptors/nosql_adaptor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.025196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/api/
+-rw-r--r--   0 runner    (1001) docker     (127)     4266 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/api/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1484 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.025196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.025196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/nosql/
+-rw-r--r--   0 runner    (1001) docker     (127)     8251 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/nosql/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/pandas/
+-rw-r--r--   0 runner    (1001) docker     (127)    15564 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/pandas/profiler_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17000 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/profiler_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4721 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/profiler_interface_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/bigquery/
+-rw-r--r--   0 runner    (1001) docker     (127)     2947 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/bigquery/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/databricks/
+-rw-r--r--   0 runner    (1001) docker     (127)     4043 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/databricks/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/db2/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/db2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1453 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/db2/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/mariadb/
+-rw-r--r--   0 runner    (1001) docker     (127)     2965 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/mariadb/profiler_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20357 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/single_store/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/single_store/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/single_store/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/snowflake/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/snowflake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1693 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/snowflake/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/trino/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/trino/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2715 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/trino/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/unity_catalog/
+-rw-r--r--   0 runner    (1001) docker     (127)     1165 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/unity_catalog/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.029196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.033197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/
+-rw-r--r--   0 runner    (1001) docker     (127)     1884 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/distinct_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1788 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/duplicate_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1721 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/ilike_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1960 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/iqr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1709 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/like_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2141 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/non_parametric_skew.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1950 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/null_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2752 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/unique_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7281 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/core.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.033197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/hybrid/
+-rw-r--r--   0 runner    (1001) docker     (127)     9059 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/hybrid/histogram.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4435 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.033197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/
+-rw-r--r--   0 runner    (1001) docker     (127)     2687 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/column_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2601 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/column_names.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1712 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2754 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/count_in_set.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2402 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/distinct_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1764 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/ilike_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1733 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/like_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4094 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/max.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2490 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/max_length.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4954 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/mean.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4094 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/min.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2491 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/min_length.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1784 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/not_like_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2844 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/not_regexp_match_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1568 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/null_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1987 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/null_missing_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2721 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/regexp_match_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1650 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/row_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4902 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/stddev.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2399 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/sum.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3137 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/unique_count.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.033197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/
+-rw-r--r--   0 runner    (1001) docker     (127)     1207 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/dml_operation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.037196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/queries/
+-rw-r--r--   0 runner    (1001) docker     (127)     1644 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/queries/bigquery.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2774 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/queries/redshift.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6743 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/queries/snowflake.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15486 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/system.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.037196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/window/
+-rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/window/first_quartile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3511 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/window/median.py
+-rw-r--r--   0 runner    (1001) docker     (127)      323 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/window/percentille_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3458 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/window/third_quartile.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.037196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.037196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/
+-rw-r--r--   0 runner    (1001) docker     (127)     6306 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/base.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.037196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/bigquery/
+-rw-r--r--   0 runner    (1001) docker     (127)     1668 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/bigquery/converter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5342 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1345 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/converter_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.037196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/mssql/
+-rw-r--r--   0 runner    (1001) docker     (127)      859 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/mssql/converter.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.037196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/redshift/
+-rw-r--r--   0 runner    (1001) docker     (127)     1848 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/redshift/converter.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.037196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/snowflake/
+-rw-r--r--   0 runner    (1001) docker     (127)     1885 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/snowflake/converter.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.041196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/
+-rw-r--r--   0 runner    (1001) docker     (127)     1521 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/concat.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1636 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/conn_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1688 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/count.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10286 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/datetime.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2666 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/length.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6448 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/median.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2931 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/modulo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3178 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/random_num.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3500 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/sum.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16271 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/table_metric_computer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2221 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/unique_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5023 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.041196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/
+-rw-r--r--   0 runner    (1001) docker     (127)     2119 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/bytea_to_string.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1442 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_array.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1820 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_datetimerange.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2271 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_hex_byte_string.py
+-rw-r--r--   0 runner    (1001) docker     (127)      933 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_image.py
+-rw-r--r--   0 runner    (1001) docker     (127)      924 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_ip.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1515 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_timestamp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1865 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/uuid.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.041196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/
+-rw-r--r--   0 runner    (1001) docker     (127)    21480 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3307 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4152 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/handle_partition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9562 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/metric_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3659 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/processor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4724 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4559 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sample_data_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.041196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.041196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/nosql/
+-rw-r--r--   0 runner    (1001) docker     (127)     2789 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/nosql/sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.041196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/pandas/
+-rw-r--r--   0 runner    (1001) docker     (127)     6130 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/pandas/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2925 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sampler_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2578 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sampler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.041196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sqlalchemy/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.041196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sqlalchemy/bigquery/
+-rw-r--r--   0 runner    (1001) docker     (127)     3725 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sqlalchemy/bigquery/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10459 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sqlalchemy/sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.041196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sqlalchemy/trino/
+-rw-r--r--   0 runner    (1001) docker     (127)     1737 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sqlalchemy/trino/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3007 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/base/
+-rw-r--r--   0 runner    (1001) docker     (127)    11166 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/base/profiler_source.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/bigquery/
+-rw-r--r--   0 runner    (1001) docker     (127)     2328 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/bigquery/profiler_source.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1748 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/bigquery/type_mapper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/databricks/
+-rw-r--r--   0 runner    (1001) docker     (127)     1421 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/databricks/profiler_source.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.805197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/mariadb/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/mariadb/functions/
+-rw-r--r--   0 runner    (1001) docker     (127)      768 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/mariadb/functions/median.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.805197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/mariadb/metrics/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/mariadb/metrics/window/
+-rw-r--r--   0 runner    (1001) docker     (127)      444 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/mariadb/metrics/window/first_quartile.py
+-rw-r--r--   0 runner    (1001) docker     (127)      415 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/mariadb/metrics/window/median.py
+-rw-r--r--   0 runner    (1001) docker     (127)      444 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/mariadb/metrics/window/third_quartile.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11615 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10984 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/metadata_ext.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/profiler_source_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1643 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/profiler_source_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.805197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/single_store/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/single_store/functions/
+-rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/single_store/functions/median.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:42.805197 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/single_store/metrics/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/single_store/metrics/window/
+-rw-r--r--   0 runner    (1001) docker     (127)      465 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/single_store/metrics/window/first_quartile.py
+-rw-r--r--   0 runner    (1001) docker     (127)      436 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/single_store/metrics/window/median.py
+-rw-r--r--   0 runner    (1001) docker     (127)      465 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/single_store/metrics/window/third_quartile.py
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/py.typed
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.045196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3204 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/avro.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2502 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)      911 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4454 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/dsv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3251 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/json.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1859 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5032 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/parquet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2547 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/reader_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.049196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3173 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/adls.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1810 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/api_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1856 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3782 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/bitbucket.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3137 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/config_source_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2354 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/credentials.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2196 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/gcs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4339 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/github.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2898 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/local.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2090 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/s3.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1164 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.049196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/timer/
+-rw-r--r--   0 runner    (1001) docker     (127)     1410 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/timer/repeated_timer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.053196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1920 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/bigquery_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6284 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/class_helper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1647 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/client_version.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4133 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7316 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/credentials.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1847 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/custom_thread_pool.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.053196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/datalake/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/datalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21221 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/datalake/datalake_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3666 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/db_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1246 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/deprecation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2976 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/dispatch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3280 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/elasticsearch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3843 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/entity_link.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7832 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/execution_time_tracker.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8462 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/filters.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21090 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/fqn.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13173 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7502 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/importer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2774 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/life_cycle_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6960 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/logger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1756 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/lru_cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2571 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/metadata_service_helper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6418 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/partition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3384 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/profiler_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1153 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/s3_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.057196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3785 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/aws_based_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2602 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/aws_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2532 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/aws_ssm_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4577 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/azure_kv_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1115 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/db_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1697 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/external_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1226 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3982 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/secrets_manager_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1051 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/singleton.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1676 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/source_hash.py
+-rw-r--r--   0 runner    (1001) docker     (127)      915 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/sqa_like_column.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8264 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/sqa_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4251 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/sqlalchemy_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6643 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/ssl_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1788 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/ssl_registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7118 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/storage_metadata_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/stored_procedures.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5218 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/tag_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2595 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/test_suite.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1995 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3974 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/time_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2300 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/timeout.py
+-rw-r--r--   0 runner    (1001) docker     (127)      916 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/uuid_encoder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.057196 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/
+-rw-r--r--   0 runner    (1001) docker     (127)     5516 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/application.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1297 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/application_output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10145 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3211 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/data_insight.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4830 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/data_quality.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8564 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/ingestion.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3016 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8515 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3381 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4153 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/usage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4921 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/workflow_output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5020 2024-05-02 07:40:33.000000 openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/workflow_status_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 07:42:43.057196 openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    38780 2024-05-02 07:42:42.000000 openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    94193 2024-05-02 07:42:42.000000 openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-02 07:42:42.000000 openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      144 2024-05-02 07:42:42.000000 openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     9175 2024-05-02 07:42:42.000000 openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       39 2024-05-02 07:42:42.000000 openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/top_level.txt
```

### Comparing `openmetadata-ingestion-1.3.4.0/LICENSE` & `openmetadata-ingestion-1.4.0.0rc1/LICENSE`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/PKG-INFO` & `openmetadata-ingestion-1.4.0.0rc1/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: openmetadata-ingestion
-Version: 1.3.4.0
+Version: 1.4.0.0rc1
 Summary: Ingestion Framework for OpenMetadata
 Author: OpenMetadata Committers
 License:                                  Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
            TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
@@ -206,246 +206,233 @@
            limitations under the License.
 Project-URL: Homepage, https://open-metadata.org/
 Project-URL: Documentation, https://docs.open-metadata.org/
 Project-URL: Source, https://github.com/open-metadata/OpenMetadata
 Requires-Python: >=3.8
 Description-Content-Type: text/x-rst
 License-File: LICENSE
-Requires-Dist: jsonschema
-Requires-Dist: pydantic~=1.10
-Requires-Dist: cached-property==1.5.2
-Requires-Dist: mypy_extensions>=0.4.3
-Requires-Dist: requests-aws4auth~=1.1
+Requires-Dist: boto3<2.0,>=1.20
 Requires-Dist: sqlalchemy<2,>=1.4.0
-Requires-Dist: collate-sqllineage~=1.3.0
-Requires-Dist: azure-keyvault-secrets
-Requires-Dist: avro~=1.11
+Requires-Dist: tabulate==0.9.0
+Requires-Dist: requests>=2.23
 Requires-Dist: setuptools~=66.0.0
-Requires-Dist: antlr4-python3-runtime==4.9.2
+Requires-Dist: typing-inspect
+Requires-Dist: requests-aws4auth~=1.1
+Requires-Dist: memory-profiler
+Requires-Dist: importlib-metadata>=4.13.0
+Requires-Dist: cached-property==1.5.2
 Requires-Dist: email-validator>=1.0.3
+Requires-Dist: mypy_extensions>=0.4.3
 Requires-Dist: azure-identity~=1.12
-Requires-Dist: croniter~=1.3.0
-Requires-Dist: requests>=2.23
-Requires-Dist: google-auth>=1.33.0
-Requires-Dist: Jinja2>=2.11.3
-Requires-Dist: jsonpatch<2.0,>=1.24
-Requires-Dist: cryptography
+Requires-Dist: antlr4-python3-runtime==4.9.2
+Requires-Dist: azure-keyvault-secrets
+Requires-Dist: cryptography>=42.0.0
+Requires-Dist: python-dateutil>=2.8.1
 Requires-Dist: chardet==4.0.0
-Requires-Dist: google>=3.0.0
-Requires-Dist: grpcio-tools>=1.47.2
 Requires-Dist: pymysql>=1.0.2
-Requires-Dist: python-jose~=3.3
-Requires-Dist: wheel~=0.38.4
-Requires-Dist: idna<3,>=2.5
-Requires-Dist: python-dateutil>=2.8.1
-Requires-Dist: importlib-metadata>=4.13.0
-Requires-Dist: boto3<2.0,>=1.20
-Requires-Dist: typing-inspect
-Requires-Dist: memory-profiler
+Requires-Dist: pydantic~=1.10
+Requires-Dist: collate-sqllineage~=1.3.0
+Requires-Dist: jsonpatch<2.0,>=1.24
+Requires-Dist: Jinja2>=2.11.3
 Requires-Dist: PyYAML~=6.0
-Requires-Dist: tabulate==0.9.0
 Provides-Extra: base
-Requires-Dist: jsonschema; extra == "base"
-Requires-Dist: pydantic~=1.10; extra == "base"
-Requires-Dist: cached-property==1.5.2; extra == "base"
-Requires-Dist: mypy_extensions>=0.4.3; extra == "base"
-Requires-Dist: requests-aws4auth~=1.1; extra == "base"
+Requires-Dist: boto3<2.0,>=1.20; extra == "base"
 Requires-Dist: sqlalchemy<2,>=1.4.0; extra == "base"
-Requires-Dist: collate-sqllineage~=1.3.0; extra == "base"
-Requires-Dist: azure-keyvault-secrets; extra == "base"
-Requires-Dist: avro~=1.11; extra == "base"
+Requires-Dist: tabulate==0.9.0; extra == "base"
+Requires-Dist: requests>=2.23; extra == "base"
 Requires-Dist: setuptools~=66.0.0; extra == "base"
-Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "base"
+Requires-Dist: typing-inspect; extra == "base"
+Requires-Dist: requests-aws4auth~=1.1; extra == "base"
+Requires-Dist: memory-profiler; extra == "base"
+Requires-Dist: importlib-metadata>=4.13.0; extra == "base"
+Requires-Dist: cached-property==1.5.2; extra == "base"
 Requires-Dist: email-validator>=1.0.3; extra == "base"
+Requires-Dist: mypy_extensions>=0.4.3; extra == "base"
 Requires-Dist: azure-identity~=1.12; extra == "base"
-Requires-Dist: croniter~=1.3.0; extra == "base"
-Requires-Dist: requests>=2.23; extra == "base"
-Requires-Dist: google-auth>=1.33.0; extra == "base"
-Requires-Dist: Jinja2>=2.11.3; extra == "base"
-Requires-Dist: jsonpatch<2.0,>=1.24; extra == "base"
-Requires-Dist: cryptography; extra == "base"
+Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "base"
+Requires-Dist: azure-keyvault-secrets; extra == "base"
+Requires-Dist: cryptography>=42.0.0; extra == "base"
+Requires-Dist: python-dateutil>=2.8.1; extra == "base"
 Requires-Dist: chardet==4.0.0; extra == "base"
-Requires-Dist: google>=3.0.0; extra == "base"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "base"
 Requires-Dist: pymysql>=1.0.2; extra == "base"
-Requires-Dist: python-jose~=3.3; extra == "base"
-Requires-Dist: wheel~=0.38.4; extra == "base"
-Requires-Dist: idna<3,>=2.5; extra == "base"
-Requires-Dist: python-dateutil>=2.8.1; extra == "base"
-Requires-Dist: importlib-metadata>=4.13.0; extra == "base"
-Requires-Dist: boto3<2.0,>=1.20; extra == "base"
-Requires-Dist: typing-inspect; extra == "base"
-Requires-Dist: memory-profiler; extra == "base"
+Requires-Dist: pydantic~=1.10; extra == "base"
+Requires-Dist: collate-sqllineage~=1.3.0; extra == "base"
+Requires-Dist: jsonpatch<2.0,>=1.24; extra == "base"
+Requires-Dist: Jinja2>=2.11.3; extra == "base"
 Requires-Dist: PyYAML~=6.0; extra == "base"
-Requires-Dist: tabulate==0.9.0; extra == "base"
 Provides-Extra: dev
+Requires-Dist: build; extra == "dev"
+Requires-Dist: boto3-stubs[essential]; extra == "dev"
+Requires-Dist: isort; extra == "dev"
 Requires-Dist: twine; extra == "dev"
-Requires-Dist: black==22.3.0; extra == "dev"
-Requires-Dist: pycln; extra == "dev"
 Requires-Dist: pylint~=3.0.0; extra == "dev"
-Requires-Dist: build; extra == "dev"
-Requires-Dist: pre-commit; extra == "dev"
 Requires-Dist: datamodel-code-generator==0.24.2; extra == "dev"
-Requires-Dist: isort; extra == "dev"
+Requires-Dist: pre-commit; extra == "dev"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "dev"
+Requires-Dist: pycln; extra == "dev"
+Requires-Dist: black==22.3.0; extra == "dev"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "dev"
 Provides-Extra: test
-Requires-Dist: lkml~=1.3; extra == "test"
-Requires-Dist: spacy==3.5.0; extra == "test"
-Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "test"
-Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "test"
-Requires-Dist: pyarrow~=14.0; extra == "test"
-Requires-Dist: scikit-learn~=1.0; extra == "test"
-Requires-Dist: pymongo~=4.3; extra == "test"
-Requires-Dist: elasticsearch8~=8.9.0; extra == "test"
-Requires-Dist: google>=3.0.0; extra == "test"
 Requires-Dist: pytest==7.0.0; extra == "test"
-Requires-Dist: pytest-cov; extra == "test"
-Requires-Dist: great-expectations~=0.18.0; extra == "test"
 Requires-Dist: trino[sqlalchemy]; extra == "test"
-Requires-Dist: moto==4.0.8; extra == "test"
 Requires-Dist: looker-sdk>=22.20.0; extra == "test"
-Requires-Dist: coverage; extra == "test"
-Requires-Dist: pytest-order; extra == "test"
-Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "test"
+Requires-Dist: lkml~=1.3; extra == "test"
 Requires-Dist: giturlparse; extra == "test"
+Requires-Dist: testcontainers==3.7.1; python_version < "3.9" and extra == "test"
 Requires-Dist: pydomo~=0.3; extra == "test"
-Requires-Dist: apache-airflow==2.7.3; extra == "test"
-Requires-Dist: dbt-artifacts-parser; extra == "test"
+Requires-Dist: minio==7.2.5; extra == "test"
+Requires-Dist: scikit-learn~=1.0; extra == "test"
+Requires-Dist: pytest-cov; extra == "test"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "test"
 Requires-Dist: sqlalchemy-databricks~=0.1; extra == "test"
+Requires-Dist: great-expectations~=0.18.0; extra == "test"
+Requires-Dist: coverage; extra == "test"
+Requires-Dist: pyarrow~=14.0; extra == "test"
+Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "test"
+Requires-Dist: pymongo~=4.3; extra == "test"
+Requires-Dist: elasticsearch8~=8.9.0; extra == "test"
 Requires-Dist: tableau-api-lib~=0.1; extra == "test"
+Requires-Dist: apache-airflow==2.7.3; extra == "test"
 Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "test"
+Requires-Dist: spacy==3.5.0; extra == "test"
+Requires-Dist: dbt-artifacts-parser; extra == "test"
+Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "test"
+Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "test"
+Requires-Dist: boto3-stubs[boto3]; extra == "test"
+Requires-Dist: pytest-order; extra == "test"
+Requires-Dist: testcontainers==4.4.0; python_version >= "3.9" and extra == "test"
+Requires-Dist: moto==4.0.8; extra == "test"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "test"
 Provides-Extra: e2e-test
 Requires-Dist: pytest-playwright; extra == "e2e-test"
 Requires-Dist: pytest-base-url; extra == "e2e-test"
 Provides-Extra: extended-testing
 Requires-Dist: Faker; extra == "extended-testing"
 Provides-Extra: data-insight
 Requires-Dist: elasticsearch8~=8.9.0; extra == "data-insight"
-Requires-Dist: elasticsearch==7.13.1; extra == "data-insight"
 Provides-Extra: airflow
-Requires-Dist: attrs; extra == "airflow"
 Requires-Dist: apache-airflow==2.7.3; extra == "airflow"
+Requires-Dist: attrs; extra == "airflow"
 Provides-Extra: amundsen
 Requires-Dist: neo4j~=5.3.0; extra == "amundsen"
 Provides-Extra: athena
 Requires-Dist: pyathena==3.0.8; extra == "athena"
 Provides-Extra: atlas
 Provides-Extra: azuresql
 Requires-Dist: pyodbc<5,>=4.0.35; extra == "azuresql"
 Provides-Extra: azure-sso
 Requires-Dist: msal~=1.2; extra == "azure-sso"
 Provides-Extra: backup
-Requires-Dist: azure-identity~=1.12; extra == "backup"
-Requires-Dist: azure-storage-blob; extra == "backup"
 Requires-Dist: boto3<2.0,>=1.20; extra == "backup"
+Requires-Dist: azure-storage-blob; extra == "backup"
+Requires-Dist: azure-identity~=1.12; extra == "backup"
 Provides-Extra: bigquery
-Requires-Dist: google-cloud-logging; extra == "bigquery"
-Requires-Dist: pyarrow~=14.0; extra == "bigquery"
 Requires-Dist: cachetools; extra == "bigquery"
+Requires-Dist: google-cloud-logging; extra == "bigquery"
 Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "bigquery"
+Requires-Dist: pyarrow~=14.0; extra == "bigquery"
 Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "bigquery"
 Provides-Extra: bigtable
 Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "bigtable"
 Requires-Dist: pandas~=2.0.0; extra == "bigtable"
 Provides-Extra: clickhouse
-Requires-Dist: clickhouse-driver~=0.2; extra == "clickhouse"
 Requires-Dist: clickhouse-sqlalchemy~=0.2; extra == "clickhouse"
+Requires-Dist: clickhouse-driver~=0.2; extra == "clickhouse"
 Provides-Extra: dagster
-Requires-Dist: GeoAlchemy2~=0.12; extra == "dagster"
 Requires-Dist: psycopg2-binary; extra == "dagster"
-Requires-Dist: dagster_graphql~=1.1; extra == "dagster"
 Requires-Dist: pymysql>=1.0.2; extra == "dagster"
+Requires-Dist: dagster_graphql~=1.1; extra == "dagster"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "dagster"
 Provides-Extra: dbt
+Requires-Dist: google-cloud; extra == "dbt"
 Requires-Dist: boto3<2.0,>=1.20; extra == "dbt"
 Requires-Dist: azure-storage-blob~=12.14; extra == "dbt"
-Requires-Dist: google-cloud; extra == "dbt"
-Requires-Dist: azure-identity~=1.12; extra == "dbt"
 Requires-Dist: google-cloud-storage==1.43.0; extra == "dbt"
 Requires-Dist: dbt-artifacts-parser; extra == "dbt"
+Requires-Dist: azure-identity~=1.12; extra == "dbt"
 Provides-Extra: db2
 Requires-Dist: ibm-db-sa~=0.3; extra == "db2"
 Provides-Extra: db2-ibmi
 Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "db2-ibmi"
 Provides-Extra: databricks
 Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "databricks"
+Requires-Dist: sqlalchemy-databricks~=0.1; extra == "databricks"
+Requires-Dist: ndg-httpsclient~=0.5.1; extra == "databricks"
 Requires-Dist: pyasn1~=0.6.0; extra == "databricks"
 Requires-Dist: pyOpenSSL~=24.1.0; extra == "databricks"
-Requires-Dist: ndg-httpsclient~=0.5.1; extra == "databricks"
-Requires-Dist: sqlalchemy-databricks~=0.1; extra == "databricks"
 Provides-Extra: datalake-azure
-Requires-Dist: adlfs~=2022.11; extra == "datalake-azure"
+Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-azure"
 Requires-Dist: azure-storage-blob~=12.14; extra == "datalake-azure"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "datalake-azure"
+Requires-Dist: adlfs~=2022.11; extra == "datalake-azure"
 Requires-Dist: cramjam~=2.7; extra == "datalake-azure"
+Requires-Dist: pyarrow~=14.0; extra == "datalake-azure"
 Requires-Dist: pandas~=2.0.0; extra == "datalake-azure"
-Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-azure"
 Requires-Dist: azure-identity~=1.12; extra == "datalake-azure"
-Requires-Dist: pyarrow~=14.0; extra == "datalake-azure"
 Provides-Extra: datalake-gcs
-Requires-Dist: google-cloud-storage==1.43.0; extra == "datalake-gcs"
+Requires-Dist: gcsfs~=2022.11; extra == "datalake-gcs"
 Requires-Dist: cramjam~=2.7; extra == "datalake-gcs"
+Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-gcs"
+Requires-Dist: google-cloud-storage==1.43.0; extra == "datalake-gcs"
 Requires-Dist: pandas~=2.0.0; extra == "datalake-gcs"
 Requires-Dist: pyarrow~=14.0; extra == "datalake-gcs"
-Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-gcs"
-Requires-Dist: gcsfs~=2022.11; extra == "datalake-gcs"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "datalake-gcs"
 Provides-Extra: datalake-s3
 Requires-Dist: s3fs==0.4.2; extra == "datalake-s3"
 Requires-Dist: cramjam~=2.7; extra == "datalake-s3"
-Requires-Dist: pandas~=2.0.0; extra == "datalake-s3"
 Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-s3"
 Requires-Dist: pyarrow~=14.0; extra == "datalake-s3"
+Requires-Dist: pandas~=2.0.0; extra == "datalake-s3"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "datalake-s3"
 Provides-Extra: deltalake
 Requires-Dist: delta-spark<=2.3.0; extra == "deltalake"
-Provides-Extra: docker
-Requires-Dist: python_on_whales==0.55.0; extra == "docker"
 Provides-Extra: domo
 Requires-Dist: pydomo~=0.3; extra == "domo"
 Provides-Extra: doris
 Requires-Dist: pydoris==1.0.2; extra == "doris"
 Provides-Extra: druid
 Requires-Dist: pydruid>=0.6.5; extra == "druid"
 Provides-Extra: dynamodb
 Requires-Dist: boto3<2.0,>=1.20; extra == "dynamodb"
 Provides-Extra: elasticsearch
 Requires-Dist: elasticsearch8~=8.9.0; extra == "elasticsearch"
-Requires-Dist: elasticsearch==7.13.1; extra == "elasticsearch"
 Provides-Extra: glue
 Requires-Dist: boto3<2.0,>=1.20; extra == "glue"
 Provides-Extra: great-expectations
 Requires-Dist: great-expectations~=0.18.0; extra == "great-expectations"
 Provides-Extra: hive
 Requires-Dist: impyla~=0.18.0; extra == "hive"
 Requires-Dist: thrift-sasl~=0.4; extra == "hive"
-Requires-Dist: thrift<1,>=0.13; extra == "hive"
+Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "hive"
 Requires-Dist: pure-sasl; extra == "hive"
+Requires-Dist: thrift<1,>=0.13; extra == "hive"
 Requires-Dist: presto-types-parser>=0.0.2; extra == "hive"
-Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "hive"
 Provides-Extra: iceberg
-Requires-Dist: pyiceberg; extra == "iceberg"
+Requires-Dist: gcsfs~=2022.11; extra == "iceberg"
+Requires-Dist: adlfs~=2022.11; extra == "iceberg"
 Requires-Dist: pydantic~=1.10; extra == "iceberg"
 Requires-Dist: pyarrow~=14.0; extra == "iceberg"
-Requires-Dist: adlfs~=2022.11; extra == "iceberg"
-Requires-Dist: gcsfs~=2022.11; extra == "iceberg"
+Requires-Dist: pyiceberg<1; extra == "iceberg"
 Provides-Extra: impala
-Requires-Dist: presto-types-parser>=0.0.2; extra == "impala"
+Requires-Dist: impyla[kerberos]~=0.18.0; extra == "impala"
 Requires-Dist: thrift-sasl~=0.4; extra == "impala"
-Requires-Dist: thrift<1,>=0.13; extra == "impala"
 Requires-Dist: pure-sasl; extra == "impala"
-Requires-Dist: impyla[kerberos]~=0.18.0; extra == "impala"
+Requires-Dist: thrift<1,>=0.13; extra == "impala"
+Requires-Dist: presto-types-parser>=0.0.2; extra == "impala"
 Provides-Extra: kafka
-Requires-Dist: avro~=1.11; extra == "kafka"
-Requires-Dist: protobuf; extra == "kafka"
 Requires-Dist: fastavro>=1.2.0; extra == "kafka"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "kafka"
 Requires-Dist: confluent_kafka==2.1.1; extra == "kafka"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "kafka"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "kafka"
+Requires-Dist: protobuf; extra == "kafka"
 Provides-Extra: kinesis
 Requires-Dist: boto3<2.0,>=1.20; extra == "kinesis"
-Provides-Extra: ldap-users
-Requires-Dist: ldap3==2.9.1; extra == "ldap-users"
 Provides-Extra: looker
-Requires-Dist: giturlparse; extra == "looker"
 Requires-Dist: gitpython~=3.1.34; extra == "looker"
+Requires-Dist: giturlparse; extra == "looker"
 Requires-Dist: looker-sdk>=22.20.0; extra == "looker"
 Requires-Dist: lkml~=1.3; extra == "looker"
 Provides-Extra: mlflow
 Requires-Dist: mlflow-skinny>=2.3.0; extra == "mlflow"
 Requires-Dist: alembic~=1.10.2; extra == "mlflow"
 Provides-Extra: mongo
 Requires-Dist: pymongo~=4.3; extra == "mongo"
@@ -455,313 +442,316 @@
 Provides-Extra: mssql
 Requires-Dist: sqlalchemy-pytds~=0.3; extra == "mssql"
 Provides-Extra: mssql-odbc
 Requires-Dist: pyodbc<5,>=4.0.35; extra == "mssql-odbc"
 Provides-Extra: mysql
 Requires-Dist: pymysql>=1.0.2; extra == "mysql"
 Provides-Extra: nifi
-Provides-Extra: okta
-Requires-Dist: okta~=2.3; extra == "okta"
+Provides-Extra: openlineage
+Requires-Dist: fastavro>=1.2.0; extra == "openlineage"
+Requires-Dist: confluent_kafka==2.1.1; extra == "openlineage"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "openlineage"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "openlineage"
+Requires-Dist: protobuf; extra == "openlineage"
 Provides-Extra: oracle
-Requires-Dist: oracledb~=1.2; extra == "oracle"
 Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "oracle"
+Requires-Dist: oracledb~=1.2; extra == "oracle"
 Provides-Extra: pgspider
 Requires-Dist: psycopg2-binary; extra == "pgspider"
 Requires-Dist: sqlalchemy-pgspider; extra == "pgspider"
 Provides-Extra: pinotdb
 Requires-Dist: pinotdb~=0.3; extra == "pinotdb"
 Provides-Extra: postgres
-Requires-Dist: GeoAlchemy2~=0.12; extra == "postgres"
 Requires-Dist: psycopg2-binary; extra == "postgres"
-Requires-Dist: packaging==21.3; extra == "postgres"
 Requires-Dist: pymysql>=1.0.2; extra == "postgres"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "postgres"
+Requires-Dist: packaging==21.3; extra == "postgres"
 Provides-Extra: powerbi
+Requires-Dist: boto3<2.0,>=1.20; extra == "powerbi"
+Requires-Dist: azure-storage-blob~=12.14; extra == "powerbi"
+Requires-Dist: google-cloud-storage==1.43.0; extra == "powerbi"
 Requires-Dist: msal~=1.2; extra == "powerbi"
+Requires-Dist: azure-identity~=1.12; extra == "powerbi"
 Provides-Extra: qliksense
 Requires-Dist: websocket-client~=1.6.1; extra == "qliksense"
 Provides-Extra: presto
-Requires-Dist: presto-types-parser>=0.0.2; extra == "presto"
 Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "presto"
+Requires-Dist: presto-types-parser>=0.0.2; extra == "presto"
 Provides-Extra: pymssql
 Requires-Dist: pymssql~=2.2.0; extra == "pymssql"
 Provides-Extra: quicksight
 Requires-Dist: boto3<2.0,>=1.20; extra == "quicksight"
 Provides-Extra: redash
 Requires-Dist: packaging==21.3; extra == "redash"
 Provides-Extra: redpanda
-Requires-Dist: avro~=1.11; extra == "redpanda"
-Requires-Dist: protobuf; extra == "redpanda"
 Requires-Dist: fastavro>=1.2.0; extra == "redpanda"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "redpanda"
 Requires-Dist: confluent_kafka==2.1.1; extra == "redpanda"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "redpanda"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "redpanda"
+Requires-Dist: protobuf; extra == "redpanda"
 Provides-Extra: redshift
-Requires-Dist: GeoAlchemy2~=0.12; extra == "redshift"
 Requires-Dist: psycopg2-binary; extra == "redshift"
 Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "redshift"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "redshift"
 Provides-Extra: sagemaker
 Requires-Dist: boto3<2.0,>=1.20; extra == "sagemaker"
 Provides-Extra: salesforce
 Requires-Dist: simple_salesforce==1.11.4; extra == "salesforce"
+Provides-Extra: sample-data
+Requires-Dist: grpcio-tools>=1.47.2; extra == "sample-data"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "sample-data"
 Provides-Extra: sap-hana
-Requires-Dist: sqlalchemy-hana; extra == "sap-hana"
 Requires-Dist: hdbcli; extra == "sap-hana"
+Requires-Dist: sqlalchemy-hana; extra == "sap-hana"
 Provides-Extra: sas
 Provides-Extra: singlestore
 Requires-Dist: pymysql>=1.0.2; extra == "singlestore"
 Provides-Extra: sklearn
 Requires-Dist: scikit-learn~=1.0; extra == "sklearn"
 Provides-Extra: snowflake
 Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "snowflake"
 Provides-Extra: superset
 Provides-Extra: tableau
+Requires-Dist: tableau-api-lib~=0.1; extra == "tableau"
 Requires-Dist: validators~=0.22.0; extra == "tableau"
 Requires-Dist: packaging==21.3; extra == "tableau"
-Requires-Dist: tableau-api-lib~=0.1; extra == "tableau"
 Provides-Extra: trino
 Requires-Dist: trino[sqlalchemy]; extra == "trino"
 Provides-Extra: vertica
 Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "vertica"
 Provides-Extra: pii-processor
 Requires-Dist: presidio-analyzer==2.2.32; extra == "pii-processor"
 Requires-Dist: spacy==3.5.0; extra == "pii-processor"
 Requires-Dist: pandas~=2.0.0; extra == "pii-processor"
 Provides-Extra: all
-Requires-Dist: jsonschema; extra == "all"
-Requires-Dist: presidio-analyzer==2.2.32; extra == "all"
-Requires-Dist: elasticsearch==7.13.1; extra == "all"
-Requires-Dist: cramjam~=2.7; extra == "all"
+Requires-Dist: trino[sqlalchemy]; extra == "all"
+Requires-Dist: looker-sdk>=22.20.0; extra == "all"
 Requires-Dist: lkml~=1.3; extra == "all"
-Requires-Dist: oracledb~=1.2; extra == "all"
-Requires-Dist: pymssql~=2.2.0; extra == "all"
-Requires-Dist: mypy_extensions>=0.4.3; extra == "all"
-Requires-Dist: google-cloud; extra == "all"
-Requires-Dist: ldap3==2.9.1; extra == "all"
-Requires-Dist: python_on_whales==0.55.0; extra == "all"
-Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "all"
-Requires-Dist: requests-aws4auth~=1.1; extra == "all"
-Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "all"
-Requires-Dist: simple_salesforce==1.11.4; extra == "all"
-Requires-Dist: spacy==3.5.0; extra == "all"
+Requires-Dist: boto3<2.0,>=1.20; extra == "all"
 Requires-Dist: sqlalchemy<2,>=1.4.0; extra == "all"
-Requires-Dist: collate-sqllineage~=1.3.0; extra == "all"
-Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "all"
-Requires-Dist: azure-keyvault-secrets; extra == "all"
-Requires-Dist: neo4j~=5.3.0; extra == "all"
-Requires-Dist: packaging==21.3; extra == "all"
-Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "all"
-Requires-Dist: azure-storage-blob; extra == "all"
-Requires-Dist: pyathena==3.0.8; extra == "all"
-Requires-Dist: pyarrow~=14.0; extra == "all"
-Requires-Dist: croniter~=1.3.0; extra == "all"
-Requires-Dist: ndg-httpsclient~=0.5.1; extra == "all"
-Requires-Dist: s3fs==0.4.2; extra == "all"
-Requires-Dist: Jinja2>=2.11.3; extra == "all"
-Requires-Dist: jsonpatch<2.0,>=1.24; extra == "all"
-Requires-Dist: pymongo~=4.3; extra == "all"
-Requires-Dist: scikit-learn~=1.0; extra == "all"
-Requires-Dist: elasticsearch8~=8.9.0; extra == "all"
-Requires-Dist: gitpython~=3.1.34; extra == "all"
 Requires-Dist: pyodbc<5,>=4.0.35; extra == "all"
-Requires-Dist: gcsfs~=2022.11; extra == "all"
-Requires-Dist: pyiceberg; extra == "all"
-Requires-Dist: pymysql>=1.0.2; extra == "all"
-Requires-Dist: impyla~=0.18.0; extra == "all"
 Requires-Dist: tabulate==0.9.0; extra == "all"
-Requires-Dist: pure-sasl; extra == "all"
-Requires-Dist: trino[sqlalchemy]; extra == "all"
-Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "all"
-Requires-Dist: msal~=1.2; extra == "all"
-Requires-Dist: looker-sdk>=22.20.0; extra == "all"
-Requires-Dist: wheel~=0.38.4; extra == "all"
-Requires-Dist: idna<3,>=2.5; extra == "all"
-Requires-Dist: python-dateutil>=2.8.1; extra == "all"
-Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "all"
-Requires-Dist: boto3<2.0,>=1.20; extra == "all"
-Requires-Dist: typing-inspect; extra == "all"
-Requires-Dist: giturlparse; extra == "all"
-Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "all"
 Requires-Dist: fastavro>=1.2.0; extra == "all"
-Requires-Dist: PyYAML~=6.0; extra == "all"
-Requires-Dist: google-cloud-storage==1.43.0; extra == "all"
-Requires-Dist: confluent_kafka==2.1.1; extra == "all"
-Requires-Dist: hdbcli; extra == "all"
-Requires-Dist: tableau-api-lib~=0.1; extra == "all"
-Requires-Dist: impyla[kerberos]~=0.18.0; extra == "all"
-Requires-Dist: pydantic~=1.10; extra == "all"
+Requires-Dist: scikit-learn~=1.0; extra == "all"
+Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "all"
+Requires-Dist: setuptools~=66.0.0; extra == "all"
 Requires-Dist: dagster_graphql~=1.1; extra == "all"
-Requires-Dist: cached-property==1.5.2; extra == "all"
+Requires-Dist: sqlalchemy-pytds~=0.3; extra == "all"
 Requires-Dist: thrift<1,>=0.13; extra == "all"
-Requires-Dist: protobuf; extra == "all"
-Requires-Dist: mlflow-skinny>=2.3.0; extra == "all"
-Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "all"
-Requires-Dist: sqlalchemy-hana; extra == "all"
-Requires-Dist: avro~=1.11; extra == "all"
-Requires-Dist: setuptools~=66.0.0; extra == "all"
-Requires-Dist: psycopg2-binary; extra == "all"
-Requires-Dist: couchbase~=4.1; extra == "all"
 Requires-Dist: pinotdb~=0.3; extra == "all"
+Requires-Dist: packaging==21.3; extra == "all"
+Requires-Dist: requests-aws4auth~=1.1; extra == "all"
+Requires-Dist: neo4j~=5.3.0; extra == "all"
+Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "all"
+Requires-Dist: memory-profiler; extra == "all"
+Requires-Dist: importlib-metadata>=4.13.0; extra == "all"
+Requires-Dist: impyla[kerberos]~=0.18.0; extra == "all"
+Requires-Dist: couchbase~=4.1; extra == "all"
 Requires-Dist: email-validator>=1.0.3; extra == "all"
-Requires-Dist: sqlalchemy-pgspider; extra == "all"
-Requires-Dist: pyOpenSSL~=24.1.0; extra == "all"
-Requires-Dist: thrift-sasl~=0.4; extra == "all"
-Requires-Dist: azure-identity~=1.12; extra == "all"
-Requires-Dist: google-cloud-logging; extra == "all"
-Requires-Dist: azure-storage-blob~=12.14; extra == "all"
-Requires-Dist: requests>=2.23; extra == "all"
-Requires-Dist: google-auth>=1.33.0; extra == "all"
-Requires-Dist: cryptography; extra == "all"
-Requires-Dist: validators~=0.22.0; extra == "all"
+Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "all"
+Requires-Dist: mypy_extensions>=0.4.3; extra == "all"
+Requires-Dist: google-cloud; extra == "all"
+Requires-Dist: pymssql~=2.2.0; extra == "all"
+Requires-Dist: presidio-analyzer==2.2.32; extra == "all"
+Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "all"
+Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "all"
+Requires-Dist: azure-keyvault-secrets; extra == "all"
+Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "all"
+Requires-Dist: python-dateutil>=2.8.1; extra == "all"
+Requires-Dist: pymongo~=4.3; extra == "all"
 Requires-Dist: chardet==4.0.0; extra == "all"
-Requires-Dist: google>=3.0.0; extra == "all"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "all"
+Requires-Dist: azure-storage-blob; extra == "all"
+Requires-Dist: pymysql>=1.0.2; extra == "all"
+Requires-Dist: pydantic~=1.10; extra == "all"
+Requires-Dist: sqlalchemy-pgspider; extra == "all"
+Requires-Dist: hdbcli; extra == "all"
+Requires-Dist: confluent_kafka==2.1.1; extra == "all"
+Requires-Dist: Jinja2>=2.11.3; extra == "all"
+Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "all"
 Requires-Dist: pandas~=2.0.0; extra == "all"
-Requires-Dist: pyasn1~=0.6.0; extra == "all"
-Requires-Dist: delta-spark<=2.3.0; extra == "all"
+Requires-Dist: websocket-client~=1.6.1; extra == "all"
+Requires-Dist: validators~=0.22.0; extra == "all"
+Requires-Dist: protobuf; extra == "all"
+Requires-Dist: gcsfs~=2022.11; extra == "all"
+Requires-Dist: PyYAML~=6.0; extra == "all"
+Requires-Dist: oracledb~=1.2; extra == "all"
+Requires-Dist: google-cloud-logging; extra == "all"
+Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "all"
+Requires-Dist: pyOpenSSL~=24.1.0; extra == "all"
 Requires-Dist: clickhouse-driver~=0.2; extra == "all"
+Requires-Dist: presto-types-parser>=0.0.2; extra == "all"
+Requires-Dist: s3fs==0.4.2; extra == "all"
+Requires-Dist: mlflow-skinny>=2.3.0; extra == "all"
+Requires-Dist: psycopg2-binary; extra == "all"
+Requires-Dist: giturlparse; extra == "all"
+Requires-Dist: pydomo~=0.3; extra == "all"
 Requires-Dist: alembic~=1.10.2; extra == "all"
-Requires-Dist: websocket-client~=1.6.1; extra == "all"
-Requires-Dist: python-jose~=3.3; extra == "all"
-Requires-Dist: okta~=2.3; extra == "all"
-Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "all"
-Requires-Dist: importlib-metadata>=4.13.0; extra == "all"
-Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "all"
+Requires-Dist: requests>=2.23; extra == "all"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "all"
+Requires-Dist: typing-inspect; extra == "all"
+Requires-Dist: simple_salesforce==1.11.4; extra == "all"
+Requires-Dist: sqlalchemy-databricks~=0.1; extra == "all"
+Requires-Dist: pyathena==3.0.8; extra == "all"
 Requires-Dist: clickhouse-sqlalchemy~=0.2; extra == "all"
+Requires-Dist: cached-property==1.5.2; extra == "all"
 Requires-Dist: pydoris==1.0.2; extra == "all"
-Requires-Dist: sqlalchemy-pytds~=0.3; extra == "all"
-Requires-Dist: GeoAlchemy2~=0.12; extra == "all"
+Requires-Dist: thrift-sasl~=0.4; extra == "all"
+Requires-Dist: cramjam~=2.7; extra == "all"
+Requires-Dist: pyarrow~=14.0; extra == "all"
+Requires-Dist: google-cloud-storage==1.43.0; extra == "all"
+Requires-Dist: impyla~=0.18.0; extra == "all"
+Requires-Dist: azure-identity~=1.12; extra == "all"
+Requires-Dist: gitpython~=3.1.34; extra == "all"
 Requires-Dist: cachetools; extra == "all"
-Requires-Dist: pydomo~=0.3; extra == "all"
+Requires-Dist: cryptography>=42.0.0; extra == "all"
+Requires-Dist: ndg-httpsclient~=0.5.1; extra == "all"
+Requires-Dist: tableau-api-lib~=0.1; extra == "all"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "all"
 Requires-Dist: adlfs~=2022.11; extra == "all"
-Requires-Dist: memory-profiler; extra == "all"
-Requires-Dist: presto-types-parser>=0.0.2; extra == "all"
-Requires-Dist: pydruid>=0.6.5; extra == "all"
+Requires-Dist: collate-sqllineage~=1.3.0; extra == "all"
+Requires-Dist: jsonpatch<2.0,>=1.24; extra == "all"
+Requires-Dist: pure-sasl; extra == "all"
+Requires-Dist: msal~=1.2; extra == "all"
+Requires-Dist: spacy==3.5.0; extra == "all"
 Requires-Dist: dbt-artifacts-parser; extra == "all"
-Requires-Dist: sqlalchemy-databricks~=0.1; extra == "all"
-Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "all"
+Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "all"
+Requires-Dist: delta-spark<=2.3.0; extra == "all"
+Requires-Dist: azure-storage-blob~=12.14; extra == "all"
+Requires-Dist: sqlalchemy-hana; extra == "all"
+Requires-Dist: pyasn1~=0.6.0; extra == "all"
+Requires-Dist: pydruid>=0.6.5; extra == "all"
+Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "all"
+Requires-Dist: elasticsearch8~=8.9.0; extra == "all"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "all"
+Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "all"
+Requires-Dist: pyiceberg<1; extra == "all"
 Provides-Extra: slim
-Requires-Dist: jsonschema; extra == "slim"
-Requires-Dist: presidio-analyzer==2.2.32; extra == "slim"
-Requires-Dist: elasticsearch==7.13.1; extra == "slim"
-Requires-Dist: cramjam~=2.7; extra == "slim"
+Requires-Dist: trino[sqlalchemy]; extra == "slim"
+Requires-Dist: looker-sdk>=22.20.0; extra == "slim"
 Requires-Dist: lkml~=1.3; extra == "slim"
-Requires-Dist: oracledb~=1.2; extra == "slim"
-Requires-Dist: pymssql~=2.2.0; extra == "slim"
-Requires-Dist: mypy_extensions>=0.4.3; extra == "slim"
-Requires-Dist: google-cloud; extra == "slim"
-Requires-Dist: ldap3==2.9.1; extra == "slim"
-Requires-Dist: python_on_whales==0.55.0; extra == "slim"
-Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "slim"
-Requires-Dist: requests-aws4auth~=1.1; extra == "slim"
-Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "slim"
-Requires-Dist: simple_salesforce==1.11.4; extra == "slim"
-Requires-Dist: spacy==3.5.0; extra == "slim"
+Requires-Dist: boto3<2.0,>=1.20; extra == "slim"
 Requires-Dist: sqlalchemy<2,>=1.4.0; extra == "slim"
-Requires-Dist: collate-sqllineage~=1.3.0; extra == "slim"
-Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "slim"
-Requires-Dist: azure-keyvault-secrets; extra == "slim"
-Requires-Dist: neo4j~=5.3.0; extra == "slim"
-Requires-Dist: packaging==21.3; extra == "slim"
-Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "slim"
-Requires-Dist: azure-storage-blob; extra == "slim"
-Requires-Dist: pyathena==3.0.8; extra == "slim"
-Requires-Dist: pyarrow~=14.0; extra == "slim"
-Requires-Dist: croniter~=1.3.0; extra == "slim"
-Requires-Dist: ndg-httpsclient~=0.5.1; extra == "slim"
-Requires-Dist: s3fs==0.4.2; extra == "slim"
-Requires-Dist: Jinja2>=2.11.3; extra == "slim"
-Requires-Dist: jsonpatch<2.0,>=1.24; extra == "slim"
-Requires-Dist: pymongo~=4.3; extra == "slim"
-Requires-Dist: elasticsearch8~=8.9.0; extra == "slim"
-Requires-Dist: gitpython~=3.1.34; extra == "slim"
 Requires-Dist: pyodbc<5,>=4.0.35; extra == "slim"
-Requires-Dist: gcsfs~=2022.11; extra == "slim"
-Requires-Dist: pyiceberg; extra == "slim"
-Requires-Dist: pymysql>=1.0.2; extra == "slim"
-Requires-Dist: impyla~=0.18.0; extra == "slim"
 Requires-Dist: tabulate==0.9.0; extra == "slim"
-Requires-Dist: pure-sasl; extra == "slim"
-Requires-Dist: trino[sqlalchemy]; extra == "slim"
-Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "slim"
-Requires-Dist: msal~=1.2; extra == "slim"
-Requires-Dist: looker-sdk>=22.20.0; extra == "slim"
-Requires-Dist: wheel~=0.38.4; extra == "slim"
-Requires-Dist: idna<3,>=2.5; extra == "slim"
-Requires-Dist: python-dateutil>=2.8.1; extra == "slim"
-Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "slim"
-Requires-Dist: boto3<2.0,>=1.20; extra == "slim"
-Requires-Dist: typing-inspect; extra == "slim"
-Requires-Dist: giturlparse; extra == "slim"
-Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "slim"
 Requires-Dist: fastavro>=1.2.0; extra == "slim"
-Requires-Dist: PyYAML~=6.0; extra == "slim"
-Requires-Dist: google-cloud-storage==1.43.0; extra == "slim"
-Requires-Dist: confluent_kafka==2.1.1; extra == "slim"
-Requires-Dist: hdbcli; extra == "slim"
-Requires-Dist: tableau-api-lib~=0.1; extra == "slim"
-Requires-Dist: impyla[kerberos]~=0.18.0; extra == "slim"
-Requires-Dist: pydantic~=1.10; extra == "slim"
+Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "slim"
+Requires-Dist: setuptools~=66.0.0; extra == "slim"
 Requires-Dist: dagster_graphql~=1.1; extra == "slim"
-Requires-Dist: cached-property==1.5.2; extra == "slim"
+Requires-Dist: sqlalchemy-pytds~=0.3; extra == "slim"
 Requires-Dist: thrift<1,>=0.13; extra == "slim"
-Requires-Dist: protobuf; extra == "slim"
-Requires-Dist: mlflow-skinny>=2.3.0; extra == "slim"
-Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "slim"
-Requires-Dist: sqlalchemy-hana; extra == "slim"
-Requires-Dist: avro~=1.11; extra == "slim"
-Requires-Dist: setuptools~=66.0.0; extra == "slim"
-Requires-Dist: psycopg2-binary; extra == "slim"
-Requires-Dist: couchbase~=4.1; extra == "slim"
 Requires-Dist: pinotdb~=0.3; extra == "slim"
+Requires-Dist: packaging==21.3; extra == "slim"
+Requires-Dist: requests-aws4auth~=1.1; extra == "slim"
+Requires-Dist: neo4j~=5.3.0; extra == "slim"
+Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "slim"
+Requires-Dist: memory-profiler; extra == "slim"
+Requires-Dist: importlib-metadata>=4.13.0; extra == "slim"
+Requires-Dist: impyla[kerberos]~=0.18.0; extra == "slim"
+Requires-Dist: couchbase~=4.1; extra == "slim"
 Requires-Dist: email-validator>=1.0.3; extra == "slim"
-Requires-Dist: sqlalchemy-pgspider; extra == "slim"
-Requires-Dist: pyOpenSSL~=24.1.0; extra == "slim"
-Requires-Dist: thrift-sasl~=0.4; extra == "slim"
-Requires-Dist: azure-identity~=1.12; extra == "slim"
-Requires-Dist: google-cloud-logging; extra == "slim"
-Requires-Dist: azure-storage-blob~=12.14; extra == "slim"
-Requires-Dist: requests>=2.23; extra == "slim"
-Requires-Dist: google-auth>=1.33.0; extra == "slim"
-Requires-Dist: cryptography; extra == "slim"
-Requires-Dist: validators~=0.22.0; extra == "slim"
+Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "slim"
+Requires-Dist: mypy_extensions>=0.4.3; extra == "slim"
+Requires-Dist: google-cloud; extra == "slim"
+Requires-Dist: pymssql~=2.2.0; extra == "slim"
+Requires-Dist: presidio-analyzer==2.2.32; extra == "slim"
+Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "slim"
+Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "slim"
+Requires-Dist: azure-keyvault-secrets; extra == "slim"
+Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "slim"
+Requires-Dist: python-dateutil>=2.8.1; extra == "slim"
+Requires-Dist: pymongo~=4.3; extra == "slim"
 Requires-Dist: chardet==4.0.0; extra == "slim"
-Requires-Dist: google>=3.0.0; extra == "slim"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "slim"
+Requires-Dist: azure-storage-blob; extra == "slim"
+Requires-Dist: pymysql>=1.0.2; extra == "slim"
+Requires-Dist: pydantic~=1.10; extra == "slim"
+Requires-Dist: sqlalchemy-pgspider; extra == "slim"
+Requires-Dist: hdbcli; extra == "slim"
+Requires-Dist: confluent_kafka==2.1.1; extra == "slim"
+Requires-Dist: Jinja2>=2.11.3; extra == "slim"
+Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "slim"
 Requires-Dist: pandas~=2.0.0; extra == "slim"
-Requires-Dist: pyasn1~=0.6.0; extra == "slim"
+Requires-Dist: websocket-client~=1.6.1; extra == "slim"
+Requires-Dist: validators~=0.22.0; extra == "slim"
+Requires-Dist: protobuf; extra == "slim"
+Requires-Dist: gcsfs~=2022.11; extra == "slim"
+Requires-Dist: PyYAML~=6.0; extra == "slim"
+Requires-Dist: oracledb~=1.2; extra == "slim"
+Requires-Dist: google-cloud-logging; extra == "slim"
+Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "slim"
+Requires-Dist: pyOpenSSL~=24.1.0; extra == "slim"
 Requires-Dist: clickhouse-driver~=0.2; extra == "slim"
+Requires-Dist: presto-types-parser>=0.0.2; extra == "slim"
+Requires-Dist: s3fs==0.4.2; extra == "slim"
+Requires-Dist: mlflow-skinny>=2.3.0; extra == "slim"
+Requires-Dist: psycopg2-binary; extra == "slim"
+Requires-Dist: giturlparse; extra == "slim"
+Requires-Dist: pydomo~=0.3; extra == "slim"
 Requires-Dist: alembic~=1.10.2; extra == "slim"
-Requires-Dist: websocket-client~=1.6.1; extra == "slim"
-Requires-Dist: python-jose~=3.3; extra == "slim"
-Requires-Dist: okta~=2.3; extra == "slim"
-Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "slim"
-Requires-Dist: importlib-metadata>=4.13.0; extra == "slim"
-Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "slim"
+Requires-Dist: requests>=2.23; extra == "slim"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "slim"
+Requires-Dist: typing-inspect; extra == "slim"
+Requires-Dist: simple_salesforce==1.11.4; extra == "slim"
+Requires-Dist: sqlalchemy-databricks~=0.1; extra == "slim"
+Requires-Dist: pyathena==3.0.8; extra == "slim"
 Requires-Dist: clickhouse-sqlalchemy~=0.2; extra == "slim"
+Requires-Dist: cached-property==1.5.2; extra == "slim"
 Requires-Dist: pydoris==1.0.2; extra == "slim"
-Requires-Dist: sqlalchemy-pytds~=0.3; extra == "slim"
-Requires-Dist: GeoAlchemy2~=0.12; extra == "slim"
+Requires-Dist: thrift-sasl~=0.4; extra == "slim"
+Requires-Dist: cramjam~=2.7; extra == "slim"
+Requires-Dist: pyarrow~=14.0; extra == "slim"
+Requires-Dist: google-cloud-storage==1.43.0; extra == "slim"
+Requires-Dist: impyla~=0.18.0; extra == "slim"
+Requires-Dist: azure-identity~=1.12; extra == "slim"
+Requires-Dist: gitpython~=3.1.34; extra == "slim"
 Requires-Dist: cachetools; extra == "slim"
-Requires-Dist: pydomo~=0.3; extra == "slim"
+Requires-Dist: cryptography>=42.0.0; extra == "slim"
+Requires-Dist: ndg-httpsclient~=0.5.1; extra == "slim"
+Requires-Dist: tableau-api-lib~=0.1; extra == "slim"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "slim"
 Requires-Dist: adlfs~=2022.11; extra == "slim"
-Requires-Dist: memory-profiler; extra == "slim"
-Requires-Dist: presto-types-parser>=0.0.2; extra == "slim"
-Requires-Dist: pydruid>=0.6.5; extra == "slim"
+Requires-Dist: collate-sqllineage~=1.3.0; extra == "slim"
+Requires-Dist: jsonpatch<2.0,>=1.24; extra == "slim"
+Requires-Dist: pure-sasl; extra == "slim"
+Requires-Dist: msal~=1.2; extra == "slim"
+Requires-Dist: spacy==3.5.0; extra == "slim"
 Requires-Dist: dbt-artifacts-parser; extra == "slim"
-Requires-Dist: sqlalchemy-databricks~=0.1; extra == "slim"
-Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "slim"
+Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "slim"
+Requires-Dist: azure-storage-blob~=12.14; extra == "slim"
+Requires-Dist: sqlalchemy-hana; extra == "slim"
+Requires-Dist: pyasn1~=0.6.0; extra == "slim"
+Requires-Dist: pydruid>=0.6.5; extra == "slim"
+Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "slim"
+Requires-Dist: elasticsearch8~=8.9.0; extra == "slim"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "slim"
+Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "slim"
+Requires-Dist: pyiceberg<1; extra == "slim"
 
 ---
 This guide will help you setup the Ingestion framework and connectors
 ---
 
-![Python version 3.7+](https://img.shields.io/badge/python-3.7%2B-blue)
+![Python version 3.8+](https://img.shields.io/badge/python-3.8%2B-blue)
 
 OpenMetadata Ingestion is a simple framework to build connectors and ingest metadata of various systems through OpenMetadata APIs. It could be used in an orchestration framework(e.g. Apache Airflow) to ingest metadata.
 **Prerequisites**
 
-- Python &gt;= 3.7.x
+- Python &gt;= 3.8.x
 
 ### Docs
 
 Please refer to the documentation here https://docs.open-metadata.org/connectors
 
 <img referrerpolicy="no-referrer-when-downgrade" src="https://static.scarf.sh/a.png?x-pxid=c1a30c7c-6dc7-4928-95bf-6ee08ca6aa6a" />
+
+### TopologyRunner
+
+All the Ingestion Workflows run through the TopologyRunner.
+
+The flow is depicted in the images below.
+
+**TopologyRunner Standard Flow**
+
+![image](../openmetadata-docs/images/v1.4/features/ingestion/workflows/metadata/multithreading/single-thread-flow.png)
+
+**TopologyRunner Multithread Flow**
+
+![image](../openmetadata-docs/images/v1.4/features/ingestion/workflows/metadata/multithreading/multi-thread-flow.png)
```

### Comparing `openmetadata-ingestion-1.3.4.0/setup.py` & `openmetadata-ingestion-1.4.0.0rc1/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 
 from setuptools import setup
 
 # Add here versions required for multiple plugins
 VERSIONS = {
     "airflow": "apache-airflow==2.7.3",
     "adlfs": "adlfs~=2022.11",
-    "avro": "avro~=1.11",
+    "avro": "avro>=1.11.3,<1.12",
     "boto3": "boto3>=1.20,<2.0",  # No need to add botocore separately. It's a dep from boto3
     "geoalchemy2": "GeoAlchemy2~=0.12",
     "google-cloud-storage": "google-cloud-storage==1.43.0",
     "gcsfs": "gcsfs~=2022.11",
     "great-expectations": "great-expectations~=0.18.0",
     "grpc-tools": "grpcio-tools>=1.47.2",
     "msal": "msal~=1.2",
@@ -38,15 +38,14 @@
     "pyodbc": "pyodbc>=4.0.35,<5",
     "scikit-learn": "scikit-learn~=1.0",  # Python 3.7 only goes up to 1.0.2
     "packaging": "packaging==21.3",
     "azure-storage-blob": "azure-storage-blob~=12.14",
     "azure-identity": "azure-identity~=1.12",
     "sqlalchemy-databricks": "sqlalchemy-databricks~=0.1",
     "databricks-sdk": "databricks-sdk>=0.18.0,<0.20.0",
-    "google": "google>=3.0.0",
     "trino": "trino[sqlalchemy]",
     "spacy": "spacy==3.5.0",
     "looker-sdk": "looker-sdk>=22.20.0",
     "lkml": "lkml~=1.3",
     "tableau": "tableau-api-lib~=0.1",
     "pyhive": "pyhive[hive_pure_sasl]~=0.7",
     "mongo": "pymongo~=4.3",
@@ -55,14 +54,15 @@
     "elasticsearch8": "elasticsearch8~=8.9.0",
     "giturlparse": "giturlparse",
     "validators": "validators~=0.22.0",
 }
 
 COMMONS = {
     "datalake": {
+        VERSIONS["avro"],
         VERSIONS["boto3"],
         VERSIONS["pandas"],
         VERSIONS["pyarrow"],
         # python-snappy does not work well on 3.11 https://github.com/aio-libs/aiokafka/discussions/931
         # Using this as an alternative
         "cramjam~=2.7",
     },
@@ -79,55 +79,40 @@
         VERSIONS[
             "grpc-tools"
         ],  # grpcio-tools already depends on grpcio. No need to add separately
         "protobuf",
     },
 }
 
-# required library for pii tagging
-pii_requirements = {
-    VERSIONS["spacy"],
-    VERSIONS["pandas"],
-    "presidio-analyzer==2.2.32",
-}
 
 base_requirements = {
     "antlr4-python3-runtime==4.9.2",
     VERSIONS["azure-identity"],
     "azure-keyvault-secrets",  # Azure Key Vault SM
-    VERSIONS["avro"],  # Used in sample data
     VERSIONS["boto3"],  # Required in base for the secrets manager
-    "cached-property==1.5.2",
-    "chardet==4.0.0",
-    "croniter~=1.3.0",
-    "cryptography",
-    "email-validator>=1.0.3",
-    VERSIONS["google"],
-    "google-auth>=1.33.0",
-    VERSIONS["grpc-tools"],  # Used in sample data
-    "idna<3,>=2.5",
+    "cached-property==1.5.2",  # LineageParser
+    "chardet==4.0.0",  # Used in the profiler
+    "cryptography>=42.0.0",
+    "email-validator>=1.0.3",  # For the pydantic generated models for Email
     "importlib-metadata>=4.13.0",  # From airflow constraints
     "Jinja2>=2.11.3",
     "jsonpatch<2.0, >=1.24",
-    "jsonschema",
     "memory-profiler",
     "mypy_extensions>=0.4.3",
     VERSIONS["pydantic"],
     VERSIONS["pymysql"],
     "python-dateutil>=2.8.1",
-    "python-jose~=3.3",
     "PyYAML~=6.0",
     "requests>=2.23",
     "requests-aws4auth~=1.1",  # Only depends on requests as external package. Leaving as base.
     "setuptools~=66.0.0",
     "sqlalchemy>=1.4.0,<2",
     "collate-sqllineage~=1.3.0",
     "tabulate==0.9.0",
     "typing-inspect",
-    "wheel~=0.38.4",
 }
 
 
 plugins: Dict[str, Set[str]] = {
     "airflow": {
         VERSIONS["airflow"],
         "attrs",
@@ -186,35 +171,33 @@
         # https://github.com/fsspec/s3fs/blob/9bf99f763edaf7026318e150c4bd3a8d18bb3a00/requirements.txt#L1
         # however, the latest version of `s3fs` conflicts its `aiobotocore` dep with `boto3`'s dep on `botocore`.
         # Leaving this marked to the automatic resolution to speed up installation.
         "s3fs==0.4.2",
         *COMMONS["datalake"],
     },
     "deltalake": {"delta-spark<=2.3.0"},
-    "docker": {"python_on_whales==0.55.0"},
     "domo": {VERSIONS["pydomo"]},
     "doris": {"pydoris==1.0.2"},
     "druid": {"pydruid>=0.6.5"},
     "dynamodb": {VERSIONS["boto3"]},
     "elasticsearch": {
-        "elasticsearch==7.13.1",
         VERSIONS["elasticsearch8"],
     },  # also requires requests-aws4auth which is in base
     "glue": {VERSIONS["boto3"]},
     "great-expectations": {VERSIONS["great-expectations"]},
     "hive": {
         *COMMONS["hive"],
         "thrift>=0.13,<1",
         # Replacing sasl with pure-sasl based on https://github.com/cloudera/python-sasl/issues/30 for py 3.11
         "pure-sasl",
         "thrift-sasl~=0.4",
         "impyla~=0.18.0",
     },
     "iceberg": {
-        "pyiceberg",
+        "pyiceberg<1",
         # Forcing the version of a few packages so it plays nicely with other requirements.
         VERSIONS["pydantic"],
         VERSIONS["adlfs"],
         VERSIONS["gcsfs"],
         VERSIONS["pyarrow"],
     },
     "impala": {
@@ -222,107 +205,124 @@
         "impyla[kerberos]~=0.18.0",
         "thrift>=0.13,<1",
         "pure-sasl",
         "thrift-sasl~=0.4",
     },
     "kafka": {*COMMONS["kafka"]},
     "kinesis": {VERSIONS["boto3"]},
-    "ldap-users": {"ldap3==2.9.1"},
     "looker": {
         VERSIONS["looker-sdk"],
         VERSIONS["lkml"],
         "gitpython~=3.1.34",
         VERSIONS["giturlparse"],
     },
     "mlflow": {"mlflow-skinny>=2.3.0", "alembic~=1.10.2"},
     "mongo": {VERSIONS["mongo"], VERSIONS["pandas"]},
     "couchbase": {"couchbase~=4.1"},
     "mssql": {"sqlalchemy-pytds~=0.3"},
     "mssql-odbc": {VERSIONS["pyodbc"]},
     "mysql": {VERSIONS["pymysql"]},
     "nifi": {},  # uses requests
-    "okta": {"okta~=2.3"},
+    "openlineage": {*COMMONS["kafka"]},
     "oracle": {"cx_Oracle>=8.3.0,<9", "oracledb~=1.2"},
     "pgspider": {"psycopg2-binary", "sqlalchemy-pgspider"},
     "pinotdb": {"pinotdb~=0.3"},
     "postgres": {
         VERSIONS["pymysql"],
         "psycopg2-binary",
         VERSIONS["geoalchemy2"],
         VERSIONS["packaging"],
     },
-    "powerbi": {VERSIONS["msal"]},
+    "powerbi": {
+        VERSIONS["msal"],
+        VERSIONS["boto3"],
+        VERSIONS["google-cloud-storage"],
+        VERSIONS["azure-storage-blob"],
+        VERSIONS["azure-identity"],
+    },
     "qliksense": {"websocket-client~=1.6.1"},
     "presto": {*COMMONS["hive"]},
     "pymssql": {"pymssql~=2.2.0"},
     "quicksight": {VERSIONS["boto3"]},
     "redash": {VERSIONS["packaging"]},
     "redpanda": {*COMMONS["kafka"]},
     "redshift": {
         # Going higher has memory and performance issues
         VERSIONS["redshift"],
         "psycopg2-binary",
         VERSIONS["geoalchemy2"],
     },
     "sagemaker": {VERSIONS["boto3"]},
     "salesforce": {"simple_salesforce==1.11.4"},
+    "sample-data": {VERSIONS["avro"], VERSIONS["grpc-tools"]},
     "sap-hana": {"hdbcli", "sqlalchemy-hana"},
     "sas": {},
     "singlestore": {VERSIONS["pymysql"]},
     "sklearn": {VERSIONS["scikit-learn"]},
     "snowflake": {VERSIONS["snowflake"]},
     "superset": {},  # uses requests
     "tableau": {VERSIONS["tableau"], VERSIONS["validators"], VERSIONS["packaging"]},
     "trino": {VERSIONS["trino"]},
     "vertica": {"sqlalchemy-vertica[vertica-python]>=0.0.5"},
-    "pii-processor": pii_requirements,
+    "pii-processor": {
+        VERSIONS["spacy"],
+        VERSIONS["pandas"],
+        "presidio-analyzer==2.2.32",
+    },
 }
 
 dev = {
     "black==22.3.0",
     "datamodel-code-generator==0.24.2",
+    "boto3-stubs[essential]",
     "isort",
     "pre-commit",
     "pycln",
     "pylint~=3.0.0",
     # For publishing
     "twine",
     "build",
+    *plugins["sample-data"],
 }
 
 
 test = {
     # Install Airflow as it's not part of `all` plugin
     VERSIONS["airflow"],
+    "boto3-stubs[boto3]",
     "coverage",
     # Install GE because it's not in the `all` plugin
     VERSIONS["great-expectations"],
     "moto==4.0.8",
     "pytest==7.0.0",
     "pytest-cov",
     "pytest-order",
     # install dbt dependency
     "dbt-artifacts-parser",
     VERSIONS["sqlalchemy-databricks"],
     VERSIONS["databricks-sdk"],
-    VERSIONS["google"],
     VERSIONS["scikit-learn"],
     VERSIONS["pyarrow"],
     VERSIONS["trino"],
     VERSIONS["spacy"],
     VERSIONS["pydomo"],
     VERSIONS["looker-sdk"],
     VERSIONS["lkml"],
     VERSIONS["tableau"],
     VERSIONS["pyhive"],
     VERSIONS["mongo"],
     VERSIONS["redshift"],
     VERSIONS["snowflake"],
     VERSIONS["elasticsearch8"],
     VERSIONS["giturlparse"],
+    VERSIONS["avro"],  # Sample Data
+    VERSIONS["grpc-tools"],
+    "testcontainers==3.7.1;python_version<'3.9'",
+    "testcontainers==4.4.0;python_version>='3.9'",
+    "minio==7.2.5",
 }
 
 e2e_test = {
     # playwright dependencies
     "pytest-playwright",
     "pytest-base-url",
 }
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/__init__.py` & `openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/hooks/openmetadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/hooks/openmetadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/backend.py` & `openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/backend.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/callback.py` & `openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/callback.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/config/commons.py` & `openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/config/commons.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/config/loader.py` & `openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/config/loader.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/operator.py` & `openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/operator.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/runner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/runner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/airflow_provider_openmetadata/lineage/status.py` & `openmetadata-ingestion-1.4.0.0rc1/src/airflow_provider_openmetadata/lineage/status.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/__main__.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/__main__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/__version__.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/__version__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/antlr/split_listener.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/antlr/split_listener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/applications/auto_tagger.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/applications/auto_tagger.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/automations/runner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kinesis/connection.py`

 * *Files 17% similar despite different names*

```diff
@@ -4,66 +4,54 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Run the Automation Workflow
+Source connection handler
 """
-from functools import singledispatch
-from typing import Any
 
-from metadata.generated.schema.entity.automations.testServiceConnection import (
-    TestServiceConnectionRequest,
-)
+from typing import Optional
+
+from metadata.clients.aws_client import AWSClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
+from metadata.generated.schema.entity.services.connections.messaging.kinesisConnection import (
+    KinesisConnection,
+)
+from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
-
-
-def execute(encrypted_automation_workflow: AutomationWorkflow) -> Any:
-    """
-    Execute the automation workflow.
-    The implementation depends on the request body type
-    """
+from metadata.utils.logger import ingestion_logger
 
-    # This will already instantiate the Secrets Manager
-    metadata = OpenMetadata(
-        config=encrypted_automation_workflow.openMetadataServerConnection
-    )
-
-    automation_workflow = metadata.get_by_name(
-        entity=AutomationWorkflow, fqn=encrypted_automation_workflow.name.__root__
-    )
-
-    return run_workflow(automation_workflow.request, automation_workflow, metadata)
+logger = ingestion_logger()
 
 
-@singledispatch
-def run_workflow(request: Any, *_, **__) -> Any:
+def get_connection(connection: KinesisConnection):
     """
-    Main entrypoint to execute the automation workflow
+    Create connection
     """
-    raise NotImplementedError(f"Workflow runner not implemented for {type(request)}")
+    return AWSClient(connection.awsConfig).get_kinesis_client()
 
 
-@run_workflow.register
-def _(
-    request: TestServiceConnectionRequest,
-    automation_workflow: AutomationWorkflow,
+def test_connection(
     metadata: OpenMetadata,
-):
-    """
-    Run the test connection
-    """
-
-    connection = get_connection(request.connection.config)
-
-    # Find the test_connection function in each <source>/connection.py file
-    test_connection_fn = get_test_connection_fn(request.connection.config)
-    test_connection_fn(
-        metadata, connection, request.connection.config, automation_workflow
+    client,
+    service_connection: KinesisConnection,
+    automation_workflow: Optional[AutomationWorkflow] = None,
+) -> None:
+    """
+    Test connection. This can be executed either as part
+    of a metadata workflow or during an Automation Workflow
+    """
+
+    test_fn = {"GetTopics": client.list_streams}
+
+    test_connection_steps(
+        metadata=metadata,
+        test_fn=test_fn,
+        service_type=service_connection.type.value,
+        automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/app.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/app.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/backup.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/backup.py`

 * *Files 4% similar despite different names*

```diff
@@ -173,14 +173,21 @@
     :param output: local path to store the backup
     :param filename: filename to store the backup
     :param upload_destination_type: Azure or AWS Destination Type
     :param upload: URI to upload result file
 
     """
     log_ansi_encoded_string(
+        color=ANSI.BRIGHT_RED,
+        bold=True,
+        message="WARNING: backup is deprecated starting 1.4.0. Use database native dump tools instead."
+        "For more information, please visit: "
+        "https://docs.open-metadata.org/v1.4.x/deployment/backup-restore-metadata",
+    )
+    log_ansi_encoded_string(
         color=ANSI.GREEN,
         bold=False,
         message="Creating OpenMetadata backup for "
         f"{common_backup_obj_instance.host}:{common_backup_obj_instance.port}/{common_backup_obj_instance.database}...",
     )
 
     out = get_output(output, filename)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/dataquality.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/dataquality.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/db_dump.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/db_dump.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/ingest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/ingest.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/insight.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/insight.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/lineage.py`

 * *Files 12% similar despite different names*

```diff
@@ -29,14 +29,15 @@
 
 logger = cli_logger()
 
 
 class LineageWorkflow(BaseModel):
     filePath: Optional[str]
     query: Optional[str]
+    checkPatch: Optional[bool] = True
     serviceName: str
     workflowConfig: WorkflowConfig
     parseTimeout: Optional[int] = 5 * 60  # default parsing timeout to be 5 mins
 
 
 def run_lineage(config_path: Path) -> None:
     """
@@ -63,11 +64,14 @@
 
     metadata = OpenMetadata(config=workflow.workflowConfig.openMetadataServerConfig)
     service: DatabaseService = metadata.get_by_name(
         entity=DatabaseService, fqn=workflow.serviceName
     )
     if service:
         metadata.add_lineage_by_query(
-            database_service=service, timeout=workflow.parseTimeout, sql=sql
+            database_service=service,
+            timeout=workflow.parseTimeout,
+            sql=sql,
+            check_patch=workflow.checkPatch,
         )
     else:
         logger.error(f"Service not found with name {workflow.serviceName}")
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/profile.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/profile.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/restore.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/restore.py`

 * *Files 11% similar despite different names*

```diff
@@ -68,14 +68,21 @@
     Run and restore the
     buckup. Optionally, download it from S3.
 
     :param common_restore_obj_instance: cls instance to fetch common args
     :param sql_file: local path of file to restore the backup
     """
     log_ansi_encoded_string(
+        color=ANSI.BRIGHT_RED,
+        bold=True,
+        message="WARNING: restore is deprecated starting 1.4.0. Use database native tools to restore."
+        "For more information, please visit: "
+        "https://docs.open-metadata.org/v1.4.x/deployment/backup-restore-metadata",
+    )
+    log_ansi_encoded_string(
         color=ANSI.GREEN,
         bold=False,
         message="Restoring OpenMetadata backup for "
         f"{common_restore_obj_instance.host}:{common_restore_obj_instance.port}/{common_restore_obj_instance.database}",
     )
 
     engine = get_engine(common_args=common_restore_obj_instance)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cli/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cli/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/clients/aws_client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/clients/aws_client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/clients/azure_client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/clients/azure_client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/clients/domo_client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/clients/domo_client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/cmd.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/cmd.py`

 * *Files 26% similar despite different names*

```diff
@@ -5,87 +5,62 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-This module defines the CLI commands for OpenMetada
+This module defines the CLI commands for OpenMetadata
 """
 import argparse
 import logging
 from enum import Enum
 from http.server import BaseHTTPRequestHandler, HTTPServer
 from pathlib import Path
 
 from metadata.__version__ import get_metadata_version
 from metadata.cli.app import run_app
 from metadata.cli.backup import UploadDestinationType, run_backup
 from metadata.cli.dataquality import run_test
-from metadata.cli.docker import BACKEND_DATABASES, DockerActions, run_docker
 from metadata.cli.ingest import run_ingest
 from metadata.cli.insight import run_insight
 from metadata.cli.lineage import run_lineage
-from metadata.cli.openmetadata_dag_config_migration import (
-    run_openmetadata_dag_config_migration,
-)
-from metadata.cli.openmetadata_imports_migration import (
-    run_openmetadata_imports_migration,
-)
 from metadata.cli.profile import run_profiler
 from metadata.cli.restore import run_restore
 from metadata.cli.usage import run_usage
 from metadata.utils.helpers import BackupRestoreArgs
 from metadata.utils.logger import cli_logger, set_loggers_level
 
 logger = cli_logger()
 
 
 class MetadataCommands(Enum):
     INGEST = "ingest"
     USAGE = "usage"
     PROFILE = "profile"
     TEST = "test"
-    DOCKER = "docker"
     BACKUP = "backup"
     RESTORE = "restore"
     WEBHOOK = "webhook"
     INSIGHT = "insight"
     LINEAGE = "lineage"
     APP = "app"
-    OPENMETADATA_IMPORTS_MIGRATION = "openmetadata_imports_migration"
-    OPENMETADATA_DAG_CONFIG_MIGRATION = "openmetadata_dag_config_migration"
 
 
 RUN_PATH_METHODS = {
     MetadataCommands.INGEST.value: run_ingest,
     MetadataCommands.USAGE.value: run_usage,
     MetadataCommands.LINEAGE.value: run_lineage,
     MetadataCommands.INSIGHT.value: run_insight,
     MetadataCommands.PROFILE.value: run_profiler,
     MetadataCommands.TEST.value: run_test,
     MetadataCommands.APP.value: run_app,
 }
 
 
-OM_IMPORTS_MIGRATION = """
-    Update DAG files generated after creating workflow in 0.11 and before.
-    In 0.12 the airflow managed API package name changed from `openmetadata` to 
-    `openmetadata_managed_apis` hence breaking existing DAGs. 
-    The `dag_generated_config` folder also changed location in Docker.
-    This small CLI utility allows you to update both elements.
-    """
-
-OM_DAG_CONFIG_MIGRATION = """
-    Update DAG Config files generated after creating workflow in 0.12 and before.
-    In 0.13 certains keys of the dag config. files have been removed. This small
-    utility command allows you to update legacy dag config files. Note this can
-    also be done manually through the UI by clicking on `redeploy`
-    """
-
 BACKUP_HELP = """
     Run a backup for the metadata DB. Uses a custom dump strategy for OpenMetadata tables.
 
     We can pass as many connection options as required with `-o <opt1>, -o <opt2> [...]`
     Same with connection arguments `-a <arg1>, -a <arg2> [...]`
 
     To run the upload, provide the information as
@@ -114,99 +89,14 @@
         "--config",
         help="path to the config file",
         type=Path,
         required=True,
     )
 
 
-def create_openmetadata_imports_migration_args(parser: argparse.ArgumentParser):
-    parser.add_argument(
-        "-d",
-        "--dir-path",
-        default="/opt/airflow/dags",
-        type=Path,
-        help="Path to the DAG folder. Default to `/opt/airflow/dags`",
-    )
-
-    parser.add_argument(
-        "--change-config-file-path",
-        help="Flag option. If pass this will try to change the path of the dag config files",
-        type=bool,
-    )
-
-
-def create_openmetadata_dag_config_migration_args(parser: argparse.ArgumentParser):
-    parser.add_argument(
-        "-d",
-        "--dir-path",
-        default="/opt/airflow/dag_generated_configs",
-        type=Path,
-        help="Path to the DAG folder. Default to `/opt/airflow/dag_generated_configs`",
-    )
-
-    parser.add_argument(
-        "--keep-backups",
-        help="Flag option. If passed, old files will be kept as backups <filename>.json.bak",
-        action="store_true",
-    )
-
-
-def docker_args(parser: argparse.ArgumentParser):
-    """
-    Additional Parser Arguments for Docker
-    """
-    parser.add_argument(
-        "--start", help="Start release docker containers", action="store_true"
-    )
-    parser.add_argument(
-        "--stop", help="Stops openmetadata docker containers", action="store_true"
-    )
-    parser.add_argument(
-        "--pause", help="Pause openmetadata docker containers", action="store_true"
-    )
-    parser.add_argument(
-        "--resume",
-        help="Resume/Unpause openmetadata docker containers",
-        action="store_true",
-    )
-    parser.add_argument(
-        "--clean",
-        help="Stops and remove openmetadata docker containers along with images, volumes, networks associated",
-        action="store_true",
-    )
-    parser.add_argument(
-        "-f",
-        "--file-path",
-        help="Path to Local docker-compose.yml",
-        type=Path,
-        required=False,
-    )
-    parser.add_argument(
-        "-env-file",
-        "--env-file-path",
-        help="Path to env file containing the environment variables",
-        type=Path,
-        required=False,
-    )
-    parser.add_argument(
-        "--reset-db", help="Reset OpenMetadata Data", action="store_true"
-    )
-    parser.add_argument(
-        "--ingest-sample-data",
-        help="Enable the sample metadata ingestion",
-        action="store_true",
-    )
-    parser.add_argument(
-        "-db",
-        "--database",
-        choices=list(BACKEND_DATABASES.keys()),
-        default="mysql",
-    )
-
-
 def webhook_args(parser: argparse.ArgumentParser):
     """
     Additional Parser Arguments for Webhook
     """
     parser.add_argument(
         "-H", "--host", help="Webserver Host", type=str, default="0.0.0.0"
     )
@@ -380,29 +270,14 @@
     )
     create_common_config_parser_args(
         sub_parser.add_parser(
             MetadataCommands.APP.value,
             help="Workflow for running external applications",
         )
     )
-    create_openmetadata_imports_migration_args(
-        sub_parser.add_parser(
-            MetadataCommands.OPENMETADATA_IMPORTS_MIGRATION.value,
-            help=OM_IMPORTS_MIGRATION,
-        )
-    )
-    create_openmetadata_dag_config_migration_args(
-        sub_parser.add_parser(
-            MetadataCommands.OPENMETADATA_DAG_CONFIG_MIGRATION.value,
-            help=OM_DAG_CONFIG_MIGRATION,
-        )
-    )
-    docker_args(
-        sub_parser.add_parser(MetadataCommands.DOCKER.value, help="Docker Quickstart")
-    )
     backup_args(
         sub_parser.add_parser(
             MetadataCommands.BACKUP.value,
             help=BACKUP_HELP,
         )
     )
     restore_args(
@@ -415,15 +290,15 @@
         sub_parser.add_parser(
             MetadataCommands.WEBHOOK.value,
             help="Simple Webserver to test webhook metadata events",
         )
     )
     create_common_config_parser_args(
         sub_parser.add_parser(
-            MetadataCommands.INSIGHT.value, help="Data Insigt Workflow"
+            MetadataCommands.INSIGHT.value, help="Data Insights Workflow"
         )
     )
 
     add_metadata_args(parser)
     parser.add_argument("--debug", help="Debug Mode", action="store_true")
     return parser.parse_args(args)
 
@@ -475,29 +350,14 @@
                 port=contains_args.get("port"),
                 options=contains_args.get("options"),
                 arguments=contains_args.get("arguments"),
                 schema=contains_args.get("schema"),
             ),
             sql_file=contains_args.get("input"),
         )
-    if metadata_workflow == MetadataCommands.DOCKER.value:
-        run_docker(
-            docker_obj_instance=DockerActions(
-                start=contains_args.get("start"),
-                stop=contains_args.get("stop"),
-                pause=contains_args.get("pause"),
-                resume=contains_args.get("resume"),
-                clean=contains_args.get("clean"),
-                reset_db=contains_args.get("reset_db"),
-            ),
-            file_path=contains_args.get("file_path"),
-            env_file_path=contains_args.get("env_file_path"),
-            ingest_sample_data=contains_args.get("ingest_sample_data"),
-            database=contains_args.get("database"),
-        )
     if metadata_workflow == MetadataCommands.WEBHOOK.value:
 
         class WebhookHandler(BaseHTTPRequestHandler):
             def do_GET(self):  # pylint: disable=invalid-name
                 self.send_response(200)
                 self.send_header("Content-type", "text/html")
                 self.end_headers()
@@ -514,17 +374,7 @@
         logger.info(
             f"Starting server at {contains_args.get('host')}:{contains_args.get('port')}"
         )
         with HTTPServer(
             (contains_args.get("host"), contains_args.get("port")), WebhookHandler
         ) as server:
             server.serve_forever()
-
-    if metadata_workflow == MetadataCommands.OPENMETADATA_IMPORTS_MIGRATION.value:
-        run_openmetadata_imports_migration(
-            contains_args.get("dir_path"), contains_args.get("change_config_file_path")
-        )
-
-    if metadata_workflow == MetadataCommands.OPENMETADATA_DAG_CONFIG_MIGRATION.value:
-        run_openmetadata_dag_config_migration(
-            contains_args.get("dir_path"), contains_args.get("keep_backups")
-        )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/config/common.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/config/common.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/kpi/kpi_runner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/kpi/kpi_runner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/kpi/run_result_registry.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/kpi/run_result_registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/cost_analysis_report_data_processor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/cost_analysis_report_data_processor.py`

 * *Files 0% similar despite different names*

```diff
@@ -153,15 +153,14 @@
 
     def refine(self, entity: Dict) -> None:
         """Aggregate data
         Returns:
             list:
         """
         try:
-
             for entity_fqn, cost_analysis_report_data in entity.items():
                 entity_type = str(cost_analysis_report_data.entity.__class__.__name__)
                 service_type = str(cost_analysis_report_data.entity.serviceType.name)
                 service_name = str(cost_analysis_report_data.entity.service.name)
                 if not self._refined_data[str(entity_type)][service_type].get(
                     service_name
                 ):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/data_processor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/data_processor.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/entity_report_data_processor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/entity_report_data_processor.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/processor/reports/web_analytic_report_data_processor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/processor/reports/web_analytic_report_data_processor.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/cost_analysis_producer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/cost_analysis_producer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/entity_producer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/entity_producer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/producer_factory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/producer_factory.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/producer_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/producer_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/producer/web_analytics_producer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/producer/web_analytics_producer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_insight/source/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_insight/source/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/api/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/api/models.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,14 +32,15 @@
 
     name: str
     displayName: Optional[str] = None
     description: Optional[str] = None
     testDefinitionName: str
     columnName: Optional[str] = None
     parameterValues: Optional[List[TestCaseParameterValue]]
+    computePassedFailedRowCount: Optional[bool] = False
 
 
 class TestSuiteProcessorConfig(ConfigModel):
     """class for the processor config"""
 
     testCases: Optional[List[TestCaseDefinition]] = None
     forceUpdate: Optional[bool] = False
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/databricks/test_suite_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/databricks/test_suite_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/snowflake/test_suite_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/snowflake/test_suite_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/sqlalchemy/unity_catalog/test_suite_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/sqlalchemy/unity_catalog/test_suite_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/test_suite_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/test_suite_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/interface/test_suite_interface_factory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/interface/test_suite_interface_factory.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/processor/test_case_runner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/processor/test_case_runner.py`

 * *Files 2% similar despite different names*

```diff
@@ -206,14 +206,15 @@
                             )
                         ),
                         testSuite=test_suite_fqn,
                         parameterValues=list(test_case_to_create.parameterValues)
                         if test_case_to_create.parameterValues
                         else None,
                         owner=None,
+                        computePassedFailedRowCount=test_case_to_create.computePassedFailedRowCount,
                     )
                 )
                 test_cases.append(test_case)
             except Exception as exc:
                 error = (
                     f"Couldn't create test case name {test_case_to_create.name}: {exc}"
                 )
@@ -247,21 +248,22 @@
             if test_case.name.__root__ in test_cases_to_update_names:
                 test_case_definition = next(
                     test_case_to_update
                     for test_case_to_update in test_cases_to_update
                     if test_case_to_update.name == test_case.name.__root__
                 )
                 updated_test_case = self.metadata.patch_test_case_definition(
-                    source=test_case,
+                    test_case=test_case,
                     entity_link=entity_link.get_entity_link(
                         Table,
                         fqn=table_fqn,
                         column_name=test_case_definition.columnName,
                     ),
                     test_case_parameter_values=test_case_definition.parameterValues,
+                    compute_passed_failed_row_count=test_case_definition.computePassedFailedRowCount,
                 )
                 if updated_test_case:
                     test_cases.pop(indx)
                     test_cases.append(updated_test_case)
 
         return test_cases
 
@@ -304,13 +306,18 @@
                     error=error,
                     stackTrace=traceback.format_exc(),
                 )
             )
         return None
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata) -> "Step":
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ) -> "Step":
         config = parse_workflow_config_gracefully(config_dict)
         return cls(config=config, metadata=metadata)
 
     def close(self) -> None:
         """Nothing to close"""
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/runner/base_test_suite_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/runner/base_test_suite_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/runner/core.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/runner/core.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/runner/test_suite_source_factory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/runner/test_suite_source_factory.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/source/test_suite.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/source/test_suite.py`

 * *Files 2% similar despite different names*

```diff
@@ -156,13 +156,18 @@
                     table=table,
                     test_cases=test_suite_cases,
                     service_type=self.config.source.serviceConnection.__root__.config.type.value,
                 )
             )
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata) -> "Step":
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ) -> "Step":
         config = parse_workflow_config_gracefully(config_dict)
         return cls(config=config, metadata=metadata)
 
     def close(self) -> None:
         """Nothing to close"""
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/base_test_handler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/base_test_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -109,15 +109,15 @@
             timestamp=execution_date,  # type: ignore
             testCaseStatus=status,
             result=result,
             testResultValue=test_result_value,
             sampleData=None,
         )
 
-        if (row_count is not None) and (
+        if (row_count is not None and row_count != 0) and (
             # we'll need at least one of these to be not None to compute the other
             (failed_rows is not None)
             or (passed_rows is not None)
         ):
             passed_rows = passed_rows if passed_rows is not None else (row_count - failed_rows)  # type: ignore
             failed_rows = (
                 failed_rows if failed_rows is not None else (row_count - passed_rows)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,15 +42,18 @@
         """Run validation for the given test case
 
         Returns:
             TestCaseResult:
         """
         try:
             column: Union[SQALikeColumn, Column] = self._get_column_name()
-            null_res = self._run_results(Metrics.NULL_COUNT, column)
+            null_res = self._run_results(
+                Metrics.NULL_MISSING_COUNT,
+                column,
+            )
         except (ValueError, RuntimeError) as exc:
             msg = f"Error computing {self.test_case.fullyQualifiedName}: {exc}"  # type: ignore
             logger.debug(traceback.format_exc())
             logger.warning(msg)
             return self.get_test_case_result_object(
                 self.execution_date,
                 TestCaseStatus.Aborted,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,68 +6,70 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to be unique test case
+Validator for column values to match regex test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
-from sqlalchemy.orm.util import AliasedClass
+from sqlalchemy.exc import CompileError, SQLAlchemyError
 
-from metadata.data_quality.validations.column.base.columnValuesToBeUnique import (
-    BaseColumnValuesToBeUniqueValidator,
+from metadata.data_quality.validations.column.base.columnValuesToMatchRegex import (
+    BaseColumnValuesToMatchRegexValidator,
 )
 from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
     SQAValidatorMixin,
 )
 from metadata.profiler.metrics.registry import Metrics
+from metadata.utils.logger import test_suite_logger
 
+logger = test_suite_logger()
 
-class ColumnValuesToBeUniqueValidator(
-    BaseColumnValuesToBeUniqueValidator, SQAValidatorMixin
+
+class ColumnValuesToMatchRegexValidator(
+    BaseColumnValuesToMatchRegexValidator, SQAValidatorMixin
 ):
-    """Validator for column values to be unique test case"""
+    """Validator for column values to match regex test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
             Column: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             inspect(self.runner.table).c,
         )
 
-    def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
+    def _run_results(self, metric: Metrics, column: Column, **kwargs) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        return self.run_query_results(self.runner, metric, column)
+        try:
+            return self.run_query_results(self.runner, metric, column, **kwargs)
+        except (CompileError, SQLAlchemyError) as err:
+            logger.warning(
+                f"Could not use `REGEXP` due to - {err}. Falling back to `LIKE`"
+            )
+            return self.run_query_results(
+                self.runner, Metrics.LIKE_COUNT, column, **kwargs
+            )
 
-    def _get_unique_count(self, metric: Metrics, column: Column) -> Optional[int]:
-        """Get unique count of values"""
-        unique_count = dict(
-            self.runner.select_all_from_query(
-                metric.value(column).query(
-                    sample=self.runner._sample  # pylint: disable=protected-access
-                    if isinstance(
-                        self.runner._sample,  # pylint: disable=protected-access
-                        AliasedClass,
-                    )
-                    else self.runner.table,
-                    session=self.runner._session,  # pylint: disable=protected-access
-                )  # type: ignore
-            )[
-                0
-            ]  # query result is a list of tuples
-        )
+    def compute_row_count(self, column: Column):
+        """Compute row count for the given column
 
-        return unique_count.get(metric.name)
+        Args:
+            column (Union[SQALikeColumn, Column]): column to compute row count for
+
+        Raises:
+            NotImplementedError:
+        """
+        return self._compute_row_count(self.runner, column)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py`

 * *Files 10% similar despite different names*

```diff
@@ -6,44 +6,44 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to match regex test case
+Validator for column values to not match regex test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 from sqlalchemy.exc import CompileError, SQLAlchemyError
 
-from metadata.data_quality.validations.column.base.columnValuesToMatchRegex import (
-    BaseColumnValuesToMatchRegexValidator,
+from metadata.data_quality.validations.column.base.columnValuesToNotMatchRegex import (
+    BaseColumnValuesToNotMatchRegexValidator,
 )
 from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
     SQAValidatorMixin,
 )
 from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
-class ColumnValuesToMatchRegexValidator(
-    BaseColumnValuesToMatchRegexValidator, SQAValidatorMixin
+class ColumnValuesToNotMatchRegexValidator(
+    BaseColumnValuesToNotMatchRegexValidator, SQAValidatorMixin
 ):
-    """Validator for column values to match regex test case"""
+    """Validator for column values to not match regex test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
-            Column: column
+            SQALikeColumn: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             inspect(self.runner.table).c,
         )
 
     def _run_results(self, metric: Metrics, column: Column, **kwargs) -> Optional[int]:
@@ -56,15 +56,15 @@
         try:
             return self.run_query_results(self.runner, metric, column, **kwargs)
         except (CompileError, SQLAlchemyError) as err:
             logger.warning(
                 f"Could not use `REGEXP` due to - {err}. Falling back to `LIKE`"
             )
             return self.run_query_results(
-                self.runner, Metrics.LIKE_COUNT, column, **kwargs
+                self.runner, Metrics.NOT_LIKE_COUNT, column, **kwargs
             )
 
     def compute_row_count(self, column: Column):
         """Compute row count for the given column
 
         Args:
             column (Union[SQALikeColumn, Column]): column to compute row count for
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,70 +6,79 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to not match regex test case
+Validator for column values to be unique test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
-from sqlalchemy.exc import CompileError, SQLAlchemyError
+from sqlalchemy.exc import SQLAlchemyError
+from sqlalchemy.orm.util import AliasedClass
 
-from metadata.data_quality.validations.column.base.columnValuesToNotMatchRegex import (
-    BaseColumnValuesToNotMatchRegexValidator,
+from metadata.data_quality.validations.column.base.columnValuesToBeUnique import (
+    BaseColumnValuesToBeUniqueValidator,
 )
 from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
     SQAValidatorMixin,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.utils.logger import test_suite_logger
 
-logger = test_suite_logger()
 
-
-class ColumnValuesToNotMatchRegexValidator(
-    BaseColumnValuesToNotMatchRegexValidator, SQAValidatorMixin
+class ColumnValuesToBeUniqueValidator(
+    BaseColumnValuesToBeUniqueValidator, SQAValidatorMixin
 ):
-    """Validator for column values to not match regex test case"""
+    """Validator for column values to be unique test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
-            SQALikeColumn: column
+            Column: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             inspect(self.runner.table).c,
         )
 
-    def _run_results(self, metric: Metrics, column: Column, **kwargs) -> Optional[int]:
+    def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        try:
-            return self.run_query_results(self.runner, metric, column, **kwargs)
-        except (CompileError, SQLAlchemyError) as err:
-            logger.warning(
-                f"Could not use `REGEXP` due to - {err}. Falling back to `LIKE`"
+        count = Metrics.COUNT.value(column).fn()
+        unique_count = Metrics.UNIQUE_COUNT.value(column).query(
+            sample=self.runner._sample  # pylint: disable=protected-access
+            if isinstance(
+                self.runner._sample,  # pylint: disable=protected-access
+                AliasedClass,
             )
-            return self.run_query_results(
-                self.runner, Metrics.NOT_LIKE_COUNT, column, **kwargs
+            else self.runner.table,
+            session=self.runner._session,  # pylint: disable=protected-access
+        )  # type: ignore
+
+        try:
+            self.value = dict(self.runner.dispatch_query_select_first(count, unique_count.scalar_subquery().label("uniqueCount")))  # type: ignore
+            res = self.value.get(Metrics.COUNT.name)
+        except Exception as exc:
+            raise SQLAlchemyError(exc)
+
+        if res is None:
+            raise ValueError(
+                f"\nQuery on table/column {column.name if column is not None else ''} returned None. Your table might be empty. "
+                "If you confirmed your table is not empty and are still seeing this message you can:\n"
+                "\t1. check the documentation: https://docs.open-metadata.org/v1.3.x/connectors/ingestion/workflows/data-quality/tests\n"
+                "\t2. reach out to the Collate team for support"
             )
 
-    def compute_row_count(self, column: Column):
-        """Compute row count for the given column
+        return res
 
-        Args:
-            column (Union[SQALikeColumn, Column]): column to compute row count for
+    def _get_unique_count(self, metric: Metrics, column: Column) -> Optional[int]:
+        """Get unique count of values"""
 
-        Raises:
-            NotImplementedError:
-        """
-        return self._compute_row_count(self.runner, column)
+        return self.value.get(metric.name)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py`

 * *Files 10% similar despite different names*

```diff
@@ -75,15 +75,18 @@
             value = dict(runner.dispatch_query_select_first(metric_fn))  # type: ignore
             res = value.get(metric.name)
         except Exception as exc:
             raise SQLAlchemyError(exc)
 
         if res is None:
             raise ValueError(
-                f"Query on table/column {column.name if column is not None else ''} returned None"
+                f"\nQuery on table/column {column.name if column is not None else ''} returned None. Your table might be empty. "
+                "If you confirmed your table is not empty and are still seeing this message you can:\n"
+                "\t1. check the documentation: https://docs.open-metadata.org/v1.3.x/connectors/ingestion/workflows/data-quality/tests\n"
+                "\t2. reach out to the Collate team for support"
             )
 
         return res
 
     def _compute_row_count_between(
         self,
         runner: QueryRunner,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py`

 * *Files 2% similar despite different names*

```diff
@@ -87,14 +87,14 @@
         else:
             status = TestCaseStatus.Failed
             result_value = len_rows
 
         return self.get_test_case_result_object(
             self.execution_date,
             status,
-            f"Found {result_value} row(s). Test query is expected to return 0 row.",
+            f"Found {result_value} row(s). Test query is expected to return {threshold} row.",
             [TestResultValue(name=RESULT_ROW_COUNT, value=str(result_value))],
         )
 
     @abstractmethod
     def _run_results(self, sql_expression: str, strategy: Strategy = Strategy.ROWS):
         raise NotImplementedError
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/data_quality/validations/validator.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/data_quality/validations/validator.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/airbyte.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/airbyte.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -10,12 +10,13 @@
   sourceConfig:
     config:
       type: PipelineMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/airflow.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/airflow.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -16,13 +16,13 @@
   sourceConfig:
     config:
       type: PipelineMetadata
 sink:
   type: metadata-rest
   config: { }
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/airflow_backend.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/postgres_lineage.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -1,23 +1,18 @@
 source:
-  type: airflow
-  serviceName: airflow_source
-  serviceConnection:
-    config:
-      type: Airflow
-      hostPort: http://localhost:8080
-      numberOfStatus: 10
-      connection:
-        type: Backend
+  type: postgres-lineage
+  serviceName: local_postgres11
   sourceConfig:
     config:
-      type: PipelineMetadata
+      type: DatabaseLineage
+      queryLogDuration: 1
+      resultLimit: 10000
 sink:
   type: metadata-rest
-  config: { }
+  config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/airflow_postgres.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/airflow_postgres.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -25,13 +25,13 @@
   sourceConfig:
     config:
       type: PipelineMetadata
 sink:
   type: metadata-rest
   config: { }
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/amundsen.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/amundsen.yaml`

 * *Files 5% similar despite different names*

```diff
@@ -13,12 +13,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/athena.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/athena.yaml`

 * *Files 13% similar despite different names*

```diff
@@ -14,12 +14,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/athena_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/azuresql_lineage.yaml`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 source:
-  type: athena-lineage
-  serviceName: local_athena
+  type: azuresql-lineage
+  serviceName: azuresql
   sourceConfig:
     config:
       type: DatabaseLineage
       queryLogDuration: 1
       resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/athena_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/oracle_usage.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 source:
-  type: athena-usage
-  serviceName: local_athena
+  type: oracle-usage
+  serviceName: local_oracle
   sourceConfig:
     config:
-      type: DatabaseUsage
-      queryLogDuration: 1
+      queryLogDuration: 2
       resultLimit: 1000
 processor:
   type: query-parser
   config: {}
 stage:
   type: table-usage
   config:
     filename: /tmp/athena_usage
 bulkSink:
   type: metadata-usage
   config:
     filename: /tmp/athena_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/atlas.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/pinotdb.yaml`

 * *Files 13% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 source:
-  type: Atlas
-  serviceName: local_atlas
+  type: pinotdb
+  serviceName: local_pinotdb
   serviceConnection:
     config:
-      type: Atlas
-      username: username
-      password: password
-      databaseServiceName: [name,of,database,service]
-      messagingServiceName: [name,of,messaging,service]
-      hostPort: http://192.168.1.8:21000
-      entity_type: examples/workflows/atlas_mapping.yaml
+      type: PinotDB
+      username: ''
+      password: ''
+      hostPort: localhost:8000
+      pinotControllerHost: http://localhost:9000/
+      connectionOptions: {}
+      connectionArguments: {}
   sourceConfig:
     config:
       type: DatabaseMetadata
-sink: 
+sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/azuresql.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/azuresql.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -18,12 +18,13 @@
         - information_schema.*
         - performance_schema.*
         - sys.*
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/azuresql_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/trino_lineage.yaml`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 source:
-  type: azuresql-lineage
-  serviceName: azuresql
+  type: trino-lineage
+  serviceName: local_trino
   sourceConfig:
     config:
       type: DatabaseLineage
       queryLogDuration: 1
       resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/azuresql_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/azuresql_usage.yaml`

 * *Files 5% similar despite different names*

```diff
@@ -21,12 +21,13 @@
   config:
     filename: /tmp/azuresql_usage
 bulkSink:
   type: metadata-usage
   config:
     filename: /tmp/azuresql_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigquery.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigquery.yaml`

 * *Files 9% similar despite different names*

```diff
@@ -21,12 +21,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigquery_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mssql_lineage.yaml`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 source:
-  type: bigquery-lineage
-  serviceName: local_bigquery
+  type: mssql-lineage
+  serviceName: local_mssql
   sourceConfig:
     config:
       type: DatabaseLineage
       queryLogDuration: 1
       resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigquery_profiler.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigquery_profiler.yaml`

 * *Files 9% similar despite different names*

```diff
@@ -43,12 +43,13 @@
             - a
             - b
 
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigquery_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigquery_usage.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -28,12 +28,13 @@
   config:
     filename: /tmp/bigquery_usage
 bulkSink:
   type: metadata-usage
   config:
     filename: /tmp/bigquery_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/bigtable.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigtable.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -20,13 +20,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/clickhouse.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/clickhouse.yaml`

 * *Files 4% similar despite different names*

```diff
@@ -16,12 +16,13 @@
         - system.*
         - information_schema.*
         - INFORMATION_SCHEMA.*
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/clickhouse_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redshift_lineage.yaml`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 source:
-  type: clickhouse-lineage
-  serviceName: local_clickhouse
+  type: redshift-lineage
+  serviceName: aws_redshift
   sourceConfig:
     config:
       type: DatabaseLineage
       queryLogDuration: 1
       resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/clickhouse_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/clickhouse_usage.yaml`

 * *Files 9% similar despite different names*

```diff
@@ -20,12 +20,13 @@
   config:
     filename: /tmp/clickhouse_usage
 bulkSink:
   type: metadata-usage
   config:
     filename: /tmp/clickhouse_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/couchbase.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/couchbase.yaml`

 * *Files 5% similar despite different names*

```diff
@@ -11,13 +11,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       "jwtToken": "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/dagster.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/dagster.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -9,13 +9,14 @@
   sourceConfig:
     config:
       type: PipelineMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/data_insight.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/data_insight.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -10,13 +10,13 @@
 sink:
   type: elasticsearch
   config:
     es_host: localhost
     es_port: 9200
     recreate_indexes: false
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/databricks.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/databricks.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -13,12 +13,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/databricks_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/databricks_lineage.yaml`

 * *Files 7% similar despite different names*

```diff
@@ -6,12 +6,13 @@
       type: DatabaseLineage
       queryLogDuration: 1
       resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/databricks_pipeline.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/databricks_pipeline.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -11,12 +11,13 @@
   sourceConfig:
     config:
       type: PipelineMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/databricks_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/databricks_usage.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -19,12 +19,13 @@
   config:
     filename: /tmp/databricks_usage
 bulkSink:
   type: metadata-usage
   config:
     filename: /tmp/databricks_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_azure_client_secret.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_azure_client_secret.yaml`

 * *Files 2% similar despite different names*

```diff
@@ -18,13 +18,14 @@
       tableFilterPattern:
         includes:
         - ''
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_azure_default.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_azure_default.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_gcs.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_gcs.yaml`

 * *Files 7% similar despite different names*

```diff
@@ -25,13 +25,14 @@
       tableFilterPattern:
         includes:
         - ''
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_profiler.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/unity_catalog_lineage.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,29 +1,18 @@
 source:
-  type: datalake
-  serviceName: local_datalake
-  serviceConnection:
-    config:
-      type: Datalake
-      configSource:      
-        securityConfig: 
-          awsAccessKeyId: aws access key id
-          awsSecretAccessKey: aws secret access key
-          awsRegion: aws region
-      bucketName: bucket name
-      prefix: prefix
+  type: unitycatalog-lineage
+  serviceName: local_unitycatalog
   sourceConfig:
     config:
-      type: Profiler
-processor:
-  type: "orm-profiler"
-  config: {}
+      type: DatabaseLineage
+      queryLogDuration: 1
+      resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
-
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/datalake_s3.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_s3.yaml`

 * *Files 4% similar despite different names*

```diff
@@ -17,13 +17,14 @@
       tableFilterPattern:
         includes:
         - ''
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/db2.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/db2.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -11,12 +11,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/db2_profiler.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/db2_profiler.yaml`

 * *Files 11% similar despite different names*

```diff
@@ -35,12 +35,13 @@
             - columnName: order_id
             - columnName: order_date
             - columnName: status
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/dbt.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/dbt.yaml`

 * *Files 2% similar despite different names*

```diff
@@ -7,14 +7,15 @@
       # For DBT, choose one of Cloud, Local, HTTP, S3 or GCS configurations
       # For cloud
       dbtConfigSource:
         dbtConfigType: cloud
         dbtCloudAuthToken: token
         dbtCloudAccountId: ID
         dbtCloudJobId: JOB ID
+        dbtCloudProjectId: PROJECT ID
         dbtCloudUrl: https://cloud.getdbt.com
       # # For Local
       # dbtConfigSource:
       #   dbtConfigType: local
       #   dbtCatalogFilePath: path-to-catalog.json
       #   dbtManifestFilePath: path-to-manifest.json
       #   dbtRunResultsFilePath: path-to-run_results.json
@@ -85,12 +86,13 @@
       #   excludes:
       #     - .*demo.*
       
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/deltalake.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/impala.yaml`

 * *Files 13% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 source:
-  type: deltalake
-  serviceName: local_deltalake
+  type: impala
+  serviceName: local_impala
   serviceConnection:
     config:
-      type: DeltaLake
-      metastoreConnection:
-        metastoreHostPort: localhost:9083
-        # metastoreFilePath: <path_to_metastore>/metastore_db
-      appName: OpenMetadata
+      type: Impala      
+      hostPort: localhost:21050
+      databaseSchema: default     
+      authMechanism: NOSASL      
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/domodashboard.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/domodashboard.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -14,12 +14,13 @@
       type: DashboardMetadata
       dashboardFilterPattern: {}
       chartFilterPattern: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/dynamodb.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/dynamodb.yaml`

 * *Files 11% similar despite different names*

```diff
@@ -16,12 +16,13 @@
       tableFilterPattern:
         includes:
         - ''
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/elasticsearch.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/elasticsearch.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -8,13 +8,13 @@
   sourceConfig:
     config:
       type: SearchMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  # loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/fivetran.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/fivetran.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -9,12 +9,13 @@
   sourceConfig:
     config:
       type: PipelineMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/glue.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/glue.yaml`

 * *Files 5% similar despite different names*

```diff
@@ -12,12 +12,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/gluepipeline.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/gluepipeline.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -12,12 +12,13 @@
   sourceConfig:
     config:
       type: PipelineMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/hive.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/trino.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,35 +1,34 @@
 source:
-  type: hive
-  serviceName: local_hive
+  type: trino
+  serviceName: local_trino
   serviceConnection:
     config:
-      type: Hive
-      databaseSchema: default
-      hostPort: localhost:10000
-
-      # metastoreConnection:
-      #   type: Mysql
-      #   username: APP
-      #   authType:
-      #     password: password
-      #   hostPort: localhost:3306
-      #   databaseSchema: demo_hive
-
-        # type: Postgres
-        # username: APP
-        # authType:
-        #   password: password
-        # hostPort: localhost:5432
-        # database: demo_hive
+      type: Trino
+      hostPort: localhost:8080
+      username: user
+      # For trino, choose one of basic or jwt auth configurations
+      authType:
+      # # For basic auth
+      #   password: password
+      # # For JWT auth
+      #   jwt: jwt_token
+      catalog: catalog_name
+      databaseSchema: schema_name
+      connectionOptions: {}
+      connectionArguments: {}
   sourceConfig:
     config:
       type: DatabaseMetadata
+      tableFilterPattern:
+        includes:
+          - customer.*
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/impala.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mysql.yaml`

 * *Files 13% similar despite different names*

```diff
@@ -1,21 +1,26 @@
 source:
-  type: impala
-  serviceName: local_impala
+  type: mysql
+  serviceName: local_mysql
   serviceConnection:
     config:
-      type: Impala      
-      hostPort: localhost:21050
-      databaseSchema: default     
-      authMechanism: NOSASL      
+      type: Mysql
+      username: openmetadata_user
+      authType:
+        password: openmetadata_password
+      hostPort: localhost:3306
+      databaseSchema: openmetadata_db
+      connectionOptions: {}
+      connectionArguments: {}
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"            
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/kafka.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/kafka.yaml`

 * *Files 7% similar despite different names*

```diff
@@ -21,13 +21,13 @@
         excludes:
         - _confluent.*
       generateSampleData: true
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/kinesis.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/kinesis.yaml`

 * *Files 5% similar despite different names*

```diff
@@ -10,13 +10,13 @@
     config:
       type: MessagingMetadata
       generateSampleData: false
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: "DEBUG"
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/lightdash.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/lightdash.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -13,12 +13,13 @@
       type: DashboardMetadata
       dashboardFilterPattern: {}
       chartFilterPattern: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/looker.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/looker.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -12,12 +12,13 @@
       type: DashboardMetadata
       dashboardFilterPattern: {}
       chartFilterPattern: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mariadb.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mariadb.yaml`

 * *Files 5% similar despite different names*

```diff
@@ -11,12 +11,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/metabase.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/metabase.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -13,12 +13,13 @@
       dashboardFilterPattern: {}
       chartFilterPattern: {}
       projectFilterPattern: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mlflow.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mlflow.yaml`

 * *Files 7% similar despite different names*

```diff
@@ -9,13 +9,14 @@
   sourceConfig:
     config:
       type: MlModelMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     enableVersionValidation: false
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mode.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mode.yaml`

 * *Files 9% similar despite different names*

```diff
@@ -17,17 +17,19 @@
           - "Number"
         excludes:
           - Total Revenue
       dashboardFilterPattern:
         includes:
           - Supplier Quality Analysis Sample
           - "Customer"
-      dbServiceNames: [local_redshift]
+      lineageInformation:
+        dbServiceNames: [local_redshift]
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mongodb.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mongodb.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -12,12 +12,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       "jwtToken": "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mssql.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mssql.yaml`

 * *Files 8% similar despite different names*

```diff
@@ -11,12 +11,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mssql_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/clickhouse_lineage.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 source:
-  type: mssql-lineage
-  serviceName: local_mssql
+  type: clickhouse-lineage
+  serviceName: local_clickhouse
   sourceConfig:
     config:
       type: DatabaseLineage
       queryLogDuration: 1
       resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mssql_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/postgres_usage.yaml`

 * *Files 13% similar despite different names*

```diff
@@ -1,31 +1,33 @@
 source:
-  type: mssql-usage
-  serviceName: local_mssql
+  type: postgres-usage
+  serviceName: local_postgres
   serviceConnection:
     config:
-      type: Mssql
-      database: catalog_test
-      username: sa
-      password: test!Password
-      hostPort: localhost:1433
+      type: Postgres
+      username: username
+      authType:
+        password: password
+      hostPort: localhost:5432
+      database: database_name
   sourceConfig:
     config:
       type: DatabaseUsage
-      queryLogDuration: '1'
+      queryLogDuration: 1
 processor:
   type: query-parser
   config: {}
 stage:
   type: table-usage
   config:
-    filename: /tmp/mssql_usage
+    filename: /tmp/postgres_usage
 bulkSink:
   type: metadata-usage
   config:
-    filename: /tmp/mssql_usage
+    filename: /tmp/postgres_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mstr.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mstr.yaml`

 * *Files 11% similar despite different names*

```diff
@@ -13,12 +13,13 @@
       type: DashboardMetadata
       dashboardFilterPattern: {}
       chartFilterPattern: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mysql.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mysql_profiler.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -4,22 +4,30 @@
   serviceConnection:
     config:
       type: Mysql
       username: openmetadata_user
       authType:
         password: openmetadata_password
       hostPort: localhost:3306
-      databaseSchema: openmetadata_db
       connectionOptions: {}
       connectionArguments: {}
   sourceConfig:
     config:
-      type: DatabaseMetadata
+      type: Profiler
+      generateSampleData: true
+      schemaFilterPattern:
+        includes:
+        - openmetadata_db*
+
+processor:
+  type: "orm-profiler"
+  config: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/mysql_profiler.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redshift_profiler.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -1,32 +1,49 @@
 source:
-  type: mysql
-  serviceName: local_mysql
+  type: redshift
+  serviceName: local_redshift
   serviceConnection:
     config:
-      type: Mysql
-      username: openmetadata_user
-      authType:
-        password: openmetadata_password
-      hostPort: localhost:3306
-      connectionOptions: {}
-      connectionArguments: {}
+      hostPort: my-host:5439
+      username: username
+      password: strongPassword
+      database: databseToConnect
+      type: Redshift
   sourceConfig:
     config:
       type: Profiler
       generateSampleData: true
+      databaseFilterPattern: 
+        includes: 
+          - database
       schemaFilterPattern:
-        includes:
-        - openmetadata_db*
+        includes: 
+          - schema_one
+        excludes:
+          - schema_two
+      tableFilterPattern:
+        includes: 
+          - orders
+          - customers
 
 processor:
-  type: "orm-profiler"
-  config: {}
+   type: "orm-profiler"
+   config:
+    tableConfig:
+      - fullyQualifiedName: local_redshift.database.schema_one.orders
+        profileSample: 85
+        columnConfig:
+          includeColumns:
+            - columnName: order_id
+            - columnName: order_date
+            - columnName: status
+
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/openmetadata.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/openmetadata.yaml`

 * *Files 8% similar despite different names*

```diff
@@ -16,12 +16,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/oracle.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/athena_lineage.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -1,23 +1,18 @@
 source:
-  type: oracle
-  serviceName: local_oracle
-  serviceConnection:
-    config:
-      hostPort: hostPort
-      username: username
-      password: password
-      type: Oracle
-      oracleConnectionType:
-        oracleServiceName: TESTDB
+  type: athena-lineage
+  serviceName: local_athena
   sourceConfig:
     config:
-      type: DatabaseMetadata
+      type: DatabaseLineage
+      queryLogDuration: 1
+      resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/oracle_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/oracle_lineage.yaml`

 * *Files 11% similar despite different names*

```diff
@@ -5,14 +5,14 @@
     config:
       queryLogDuration: 2
       resultLimit: 1000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: INFO
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: 'http://localhost:8585/api'
     authProvider: openmetadata
     securityConfig:
       jwtToken: >-
         eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/oracle_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redshift.yaml`

 * *Files 22% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 source:
-  type: oracle-usage
-  serviceName: local_oracle
+  type: redshift
+  serviceName: aws_redshift
+  serviceConnection:
+    config:
+      hostPort: cluster.name.region.redshift.amazonaws.com:5439
+      username: username
+      password: strong_password
+      database: dev
+      type: Redshift
   sourceConfig:
     config:
-      queryLogDuration: 2
-      resultLimit: 1000
-processor:
-  type: query-parser
+      type: DatabaseMetadata
+      schemaFilterPattern:
+        excludes:
+        - information_schema.*
+        - '[\w]*event_vw.*'
+sink:
+  type: metadata-rest
   config: {}
-stage:
-  type: table-usage
-  config:
-    filename: /tmp/athena_usage
-bulkSink:
-  type: metadata-usage
-  config:
-    filename: /tmp/athena_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/pinotdb.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/vertica.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -1,24 +1,23 @@
 source:
-  type: pinotdb
-  serviceName: local_pinotdb
+  type: vertica
+  serviceName: local_vertica
   serviceConnection:
     config:
-      type: PinotDB
-      username: ''
+      type: Vertica
+      username: openmetadata_user
       password: ''
-      hostPort: localhost:8000
-      pinotControllerHost: http://localhost:9000/
-      connectionOptions: {}
-      connectionArguments: {}
+      hostPort: localhost:5433
+      database: custom_database_name
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/postgres.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/postgres.yaml`

 * *Files 8% similar despite different names*

```diff
@@ -12,12 +12,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/postgres_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redpanda.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,18 +1,24 @@
 source:
-  type: postgres-lineage
-  serviceName: local_postgres11
+  type: redpanda
+  serviceName: local_redpanda
+  serviceConnection:
+    config:
+      type: Redpanda
+      bootstrapServers: localhost:9092
+      schemaRegistryURL: http://localhost:8081
+      consumerConfig: {}
+      schemaRegistryConfig: {}
   sourceConfig:
     config:
-      type: DatabaseLineage
-      queryLogDuration: 1
-      resultLimit: 10000
+      type: MessagingMetadata
+      generateSampleData: true
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: INFO
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/postgres_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/quicksight.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,33 +1,27 @@
 source:
-  type: postgres-usage
-  serviceName: local_postgres
+  type: quicksight
+  serviceName: local_quicksight
   serviceConnection:
     config:
-      type: Postgres
-      username: username
-      authType:
-        password: password
-      hostPort: localhost:5432
-      database: database_name
+      type: QuickSight
+      awsConfig:
+        awsAccessKeyId: aws_access_key_id
+        awsSecretAccessKey: aws_secret_access_key
+        awsRegion: aws region
+        endPointURL: https://quicksight.region_name.amazonaws.com
+      awsAccountId: aws_account_id
   sourceConfig:
     config:
-      type: DatabaseUsage
-      queryLogDuration: 1
-processor:
-  type: query-parser
+      type: DashboardMetadata
+      dashboardFilterPattern: {}
+      chartFilterPattern: {}
+sink:
+  type: metadata-rest
   config: {}
-stage:
-  type: table-usage
-  config:
-    filename: /tmp/postgres_usage
-bulkSink:
-  type: metadata-usage
-  config:
-    filename: /tmp/postgres_usage
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/powerbi.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/datalake_profiler.yaml`

 * *Files 22% similar despite different names*

```diff
@@ -1,40 +1,30 @@
 source:
-  type: powerbi
-  serviceName: local_powerbi
+  type: datalake
+  serviceName: local_datalake
   serviceConnection:
     config:
-      clientId: client_id
-      clientSecret: client_secret
-      tenantId: tenant_id
-      scope:
-        - https://analysis.windows.net/powerbi/api/.default
-      pagination_entity_per_page: 100
-      # useAdminApis: true or false
-      type: PowerBI
+      type: Datalake
+      configSource:      
+        securityConfig: 
+          awsAccessKeyId: aws access key id
+          awsSecretAccessKey: aws secret access key
+          awsRegion: aws region
+      bucketName: bucket name
+      prefix: prefix
   sourceConfig:
     config:
-      type: DashboardMetadata
-      chartFilterPattern:
-        includes:
-          - Gross Margin %
-          - Total Defect*
-          - "Number"
-        excludes:
-          - Total Revenue
-      dashboardFilterPattern:
-        includes:
-          - Supplier Quality Analysis Sample
-          - "Customer"
-      projectFilterPattern:
-        includes:
-          - Supplier Quality Analysis Sample
-          - "Customer"
+      type: Profiler
+processor:
+  type: "orm-profiler"
+  config: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/presto.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/presto.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -12,12 +12,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/qlik_sense.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/qlik_sense.yaml`

 * *Files 7% similar despite different names*

```diff
@@ -19,25 +19,27 @@
         # stagingDir: /tmp/stage
         
       userId: user_id
       userDirectory: user_dir
   sourceConfig:
     config:
       type: DashboardMetadata
-      dbServiceNames:
-      - mysql
-      - postgres
+      lineageInformation:
+        dbServiceNames:
+        - mysql
+        - postgres
       chartFilterPattern: {}
       # dashboardFilterPattern:
       #   includes:
       #   - .*mysql.*
+      includeDraftDashboard: True
 
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/query_log_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/query_log_usage.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -23,12 +23,13 @@
   config:
     filename: /tmp/query_log_usage
 bulkSink:
   type: metadata-usage
   config:
     filename: /tmp/query_log_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/quicksight.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redash.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,26 +1,24 @@
 source:
-  type: quicksight
-  serviceName: local_quicksight
+  type: redash
+  serviceName: local_redash
   serviceConnection:
     config:
-      type: QuickSight
-      awsConfig:
-        awsAccessKeyId: aws_access_key_id
-        awsSecretAccessKey: aws_secret_access_key
-        awsRegion: aws region
-        endPointURL: https://quicksight.region_name.amazonaws.com
-      awsAccountId: aws_account_id
+      type: Redash
+      hostPort: http://localhost:5000
+      apiKey: api_key
+      username: random
   sourceConfig:
     config:
       type: DashboardMetadata
       dashboardFilterPattern: {}
       chartFilterPattern: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redash.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/mssql_usage.yaml`

 * *Files 17% similar despite different names*

```diff
@@ -1,23 +1,32 @@
 source:
-  type: redash
-  serviceName: local_redash
+  type: mssql-usage
+  serviceName: local_mssql
   serviceConnection:
     config:
-      type: Redash
-      hostPort: http://localhost:5000
-      apiKey: api_key
-      username: random
+      type: Mssql
+      database: catalog_test
+      username: sa
+      password: test!Password
+      hostPort: localhost:1433
   sourceConfig:
     config:
-      type: DashboardMetadata
-      dashboardFilterPattern: {}
-      chartFilterPattern: {}
-sink:
-  type: metadata-rest
+      type: DatabaseUsage
+      queryLogDuration: '1'
+processor:
+  type: query-parser
   config: {}
+stage:
+  type: table-usage
+  config:
+    filename: /tmp/mssql_usage
+bulkSink:
+  type: metadata-usage
+  config:
+    filename: /tmp/mssql_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redpanda.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/airflow_backend.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 source:
-  type: redpanda
-  serviceName: local_redpanda
+  type: airflow
+  serviceName: airflow_source
   serviceConnection:
     config:
-      type: Redpanda
-      bootstrapServers: localhost:9092
-      schemaRegistryURL: http://localhost:8081
-      consumerConfig: {}
-      schemaRegistryConfig: {}
+      type: Airflow
+      hostPort: http://localhost:8080
+      numberOfStatus: 10
+      connection:
+        type: Backend
   sourceConfig:
     config:
-      type: MessagingMetadata
-      generateSampleData: true
+      type: PipelineMetadata
 sink:
   type: metadata-rest
-  config: {}
+  config: { }
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redshift.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/superset.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -1,26 +1,35 @@
 source:
-  type: redshift
-  serviceName: aws_redshift
+  type: superset
+  serviceName: local_superset_12
   serviceConnection:
     config:
-      hostPort: cluster.name.region.redshift.amazonaws.com:5439
-      username: username
-      password: strong_password
-      database: dev
-      type: Redshift
+      hostPort: http://localhost:8088
+      connection:
+        type: Postgres
+        username: superset
+        authType:
+          password: superset
+        hostPort: localhost:5432
+        database: superset
+        # username: admin
+        # password: admin
+        # provider: db
+        # verifySSL: no-ssl / ignore / validate
+        # sslConfig:
+        #   certificatePath: CA certificate path. E.g., /path/to/public.cert. Will be used if Verify SSL is set to `validate`.
+      type: Superset
   sourceConfig:
     config:
-      type: DatabaseMetadata
-      schemaFilterPattern:
-        excludes:
-        - information_schema.*
-        - '[\w]*event_vw.*'
+      type: DashboardMetadata
+      chartFilterPattern: {}
+      dashboardFilterPattern: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redshift_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/snowflake_lineage.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 source:
-  type: redshift-lineage
-  serviceName: aws_redshift
+  type: snowflake-lineage
+  serviceName: snowflake
   sourceConfig:
     config:
       type: DatabaseLineage
       queryLogDuration: 1
       resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redshift_profiler.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/tableau.yaml`

 * *Files 27% similar despite different names*

```diff
@@ -1,48 +1,37 @@
 source:
-  type: redshift
-  serviceName: local_redshift
+  type: tableau
+  serviceName: local_tableau
   serviceConnection:
     config:
-      hostPort: my-host:5439
-      username: username
-      password: strongPassword
-      database: databseToConnect
-      type: Redshift
+      type: Tableau
+      # For Tableau, choose one of basic or access token authentication
+      # # For basic authentication
+      # authType:
+      #   username: username
+      #   password: password
+      # # For access token authentication
+      # authType:
+      #   personalAccessTokenName: personal_access_token_name
+      #   personalAccessTokenSecret: personal_access_token_secret
+      env: tableau_prod
+      hostPort: http://localhost
+      siteName: site_name
+      siteUrl: site_url
+      apiVersion: api_version
+      paginationLimit: 10
   sourceConfig:
     config:
-      type: Profiler
-      generateSampleData: true
-      databaseFilterPattern: 
-        includes: 
-          - database
-      schemaFilterPattern:
-        includes: 
-          - schema_one
-        excludes:
-          - schema_two
-      tableFilterPattern:
-        includes: 
-          - orders
-          - customers
-
-processor:
-   type: "orm-profiler"
-   config:
-    tableConfig:
-      - fullyQualifiedName: local_redshift.database.schema_one.orders
-        profileSample: 85
-        columnConfig:
-          includeColumns:
-            - columnName: order_id
-            - columnName: order_date
-            - columnName: status
-
+      type: DashboardMetadata
+      dashboardFilterPattern: {}
+      chartFilterPattern: {}
+      projectFilterPattern: {}
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/redshift_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/redshift_usage.yaml`

 * *Files 7% similar despite different names*

```diff
@@ -20,12 +20,13 @@
   config:
     filename: /tmp/redshift_usage
 bulkSink:
   type: metadata-usage
   config:
     filename: /tmp/redshift_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/sagemaker.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/sagemaker.yaml`

 * *Files 19% similar despite different names*

```diff
@@ -11,13 +11,13 @@
   sourceConfig:
     config:
       type: MlModelMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: "DEBUG"
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/salesforce.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/salesforce.yaml`

 * *Files 5% similar despite different names*

```diff
@@ -13,12 +13,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/sas.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/sas.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -16,13 +16,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort:  http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJvcGVuLW1ldGFkYXRhLm9yZyIsInN1YiI6ImluZ2VzdGlvbi1ib3QiLCJlbWFpbCI6ImluZ2VzdGlvbi1ib3RAb3Blbm1ldGFkYXRhLm9yZyIsImlzQm90Ijp0cnVlLCJ0b2tlblR5cGUiOiJCT1QiLCJpYXQiOjE3MDQ3NDY0MzYsImV4cCI6bnVsbH0.qRWByaRw3F1B0bDqdpmxDsHEUx9Npk5VNelpabuvmVERjIG8AY88p1dv5gBME6Y1-kfTtCAMtSxsll0_gTR35D1foVdXgGMRAPyNXH0JHRpENBnT1V3OVO0yRWmeqsp5K7yqiaVa-CeqxitSgYrns58BFRD_vFX5vxMHirFBrFddH7b8af8823a8Oh-wMDCuJJd7Ya61Kv6gUpssE7Y403PwomLK6pE7gsAgZ5YOyjHlQ889C9z8oLQ268BX3ndmTp6t1J7MYgOEqeIzeCoBQZf6aBdyCqYajB3TTwb3SHFz2TEYF5xeLvYlZ77ek3l22m6Ehh5d4t2jB-ZOFDUkqA
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/singlestore.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/singlestore.yaml`

 * *Files 11% similar despite different names*

```diff
@@ -11,12 +11,13 @@
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/snowflake.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/deltalake.yaml`

 * *Files 22% similar despite different names*

```diff
@@ -1,31 +1,23 @@
 source:
-  type: snowflake
-  serviceName: snowflake
+  type: deltalake
+  serviceName: local_deltalake
   serviceConnection:
     config:
-      type: Snowflake
-      username: username
-      password: password
-      database: database_name
-      warehouse: warehouse_name
-      account: account.region_name.cloud_service
-      connectionArguments:
-        private_key: private_key
+      type: DeltaLake
+      metastoreConnection:
+        metastoreHostPort: localhost:9083
+        # metastoreFilePath: <path_to_metastore>/metastore_db
+      appName: OpenMetadata
   sourceConfig:
     config:
       type: DatabaseMetadata
-      schemaFilterPattern:
-        excludes:
-        - mysql.*
-        - information_schema.*
-        - performance_schema.*
-        - sys.*
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/snowflake_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/qlikcloud.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -1,17 +1,31 @@
 source:
-  type: snowflake-lineage
-  serviceName: snowflake
+  type: qlikcloud
+  serviceName: qlikcloud_connector
+  serviceConnection:
+    config:
+      type: QlikCloud
+      token: <jwt_token>
+      hostPort: http://localhost:2000
+
   sourceConfig:
     config:
-      type: DatabaseLineage
-      queryLogDuration: 1
-      resultLimit: 10000
+      type: DashboardMetadata
+      # dashboardFilterPattern: {}
+      # chartFilterPattern: {}
+      # projectFilterPattern: {}
+      lineageInformation:
+        dbServiceNames:
+        - postgres
+      includeDraftDashboard: True
+
+
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"      
+ingestionPipelineFQN: qlikcloud_pipeline
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/snowflake_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/snowflake_usage.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -20,12 +20,13 @@
   config:
     filename: /tmp/snowflake_usage
 bulkSink:
   type: metadata-usage
   config:
     filename: /tmp/snowflake_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/spline.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/spline.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -5,25 +5,26 @@
     config:
       type: Spline
       hostPort: http://localhost:8081/
       uiHostPort: http://localhost:9090
   sourceConfig:
     config:
       type: PipelineMetadata
-      dbServiceNames:
-      - local_databricks
-      - local_postgres_empty1
+      lineageInformation:
+        dbServiceNames:
+        - local_databricks
+        - local_postgres_empty1
 
       pipelineFilterPattern:
         includes:
         - .*jdbc.*
         - .*databricks.*
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/sqlite.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/snowflake.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,22 +1,32 @@
 source:
-  type: sqlite
-  serviceName: local_sqlite
+  type: snowflake
+  serviceName: snowflake
   serviceConnection:
     config:
-      hostPort: localhost:1433
-      username: <username>
-      password: <password>
-      database: <database>
-      type: SQLite
+      type: Snowflake
+      username: username
+      password: password
+      database: database_name
+      warehouse: warehouse_name
+      account: account.region_name.cloud_service
+      connectionArguments:
+        private_key: private_key
   sourceConfig:
     config:
       type: DatabaseMetadata
+      schemaFilterPattern:
+        excludes:
+        - mysql.*
+        - information_schema.*
+        - performance_schema.*
+        - sys.*
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/superset.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/athena_usage.yaml`

 * *Files 25% similar despite different names*

```diff
@@ -1,34 +1,26 @@
 source:
-  type: superset
-  serviceName: local_superset_12
-  serviceConnection:
-    config:
-      hostPort: http://localhost:8088
-      connection:
-        type: Postgres
-        username: superset
-        authType:
-          password: superset
-        hostPort: localhost:5432
-        database: superset
-        # username: admin
-        # password: admin
-        # provider: db
-        # verifySSL: no-ssl / ignore / validate
-        # sslConfig:
-        #   certificatePath: CA certificate path. E.g., /path/to/public.cert. Will be used if Verify SSL is set to `validate`.
-      type: Superset
+  type: athena-usage
+  serviceName: local_athena
   sourceConfig:
     config:
-      type: DashboardMetadata
-      chartFilterPattern: {}
-      dashboardFilterPattern: {}
-sink:
-  type: metadata-rest
+      type: DatabaseUsage
+      queryLogDuration: 1
+      resultLimit: 1000
+processor:
+  type: query-parser
   config: {}
+stage:
+  type: table-usage
+  config:
+    filename: /tmp/athena_usage
+bulkSink:
+  type: metadata-usage
+  config:
+    filename: /tmp/athena_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/test_suite.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/test_suite.yaml`

 * *Files 24% similar despite different names*

```diff
@@ -1,34 +1,31 @@
 source:
   type: TestSuite
   serviceName: service_name
-  serviceConnection: {}
   sourceConfig:
     config:
       type: TestSuite
-      entityFullyQualifiedName: db.schema.columns
+      entityFullyQualifiedName: my.service.db.schema.columns
 processor:
   type: orm-test-runner
   config:
-    testSuites:
-      - name: test suite name
-        description: this is a description
-        testCases:
-          - name: test case name
-            description: test case description
-            testDefinitionName: name of the test definition for this test case
-            entityLink: "<#E::table::fqn> or <#E::table::fqn::columns::column_name>"     
-            parameterValues:
-              - name: parameter name
-                value: value
-              - name: parameter name
-                value: value
+    testCases:
+      - name: test case name
+        description: test case description
+        testDefinitionName: name of the test definition for this test case
+        entityLink: "<#E::table::fqn> or <#E::table::fqn::columns::column_name>"     
+        parameterValues:
+          - name: parameter name
+            value: value
+          - name: parameter name
+            value: value
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
-    hostPort: http://localhost:8585/api
+    hostPort: http://host:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/trino.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/trino_usage.yaml`

 * *Files 17% similar despite different names*

```diff
@@ -1,33 +1,35 @@
 source:
-  type: trino
+  type: trino-usage
   serviceName: local_trino
   serviceConnection:
     config:
       type: Trino
       hostPort: localhost:8080
       username: user
-      # For trino, choose one of basic or jwt auth configurations
-      authType:
-      # # For basic auth
-      #   password: password
-      # # For JWT auth
-      #   jwt: jwt_token
-      catalog: catalog_name
-      databaseSchema: schema_name
+      catalog: system
       connectionOptions: {}
       connectionArguments: {}
   sourceConfig:
     config:
-      type: DatabaseMetadata
-      tableFilterPattern:
-        includes:
-          - customer.*
-sink:
-  type: metadata-rest
+      resultLimit: 1000
+      # tableFilterPattern:
+      #   includes:
+      #     - customer.*
+processor:
+  type: query-parser
   config: {}
+stage:
+  type: table-usage
+  config:
+    filename: /tmp/trino_usage
+bulkSink:
+  type: metadata-usage
+  config:
+    filename: /tmp/trino_usage
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/trino_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/sqlite.yaml`

 * *Files 17% similar despite different names*

```diff
@@ -1,18 +1,23 @@
 source:
-  type: trino-lineage
-  serviceName: local_trino
+  type: sqlite
+  serviceName: local_sqlite
+  serviceConnection:
+    config:
+      hostPort: localhost:1433
+      username: <username>
+      password: <password>
+      database: <database>
+      type: SQLite
   sourceConfig:
     config:
-      type: DatabaseLineage
-      queryLogDuration: 1
-      resultLimit: 10000
+      type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
-
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/unity_catalog.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/atlas.yaml`

 * *Files 20% similar despite different names*

```diff
@@ -1,27 +1,25 @@
 source:
-  type: unitycatalog
-  serviceName: local_unitycatalog
+  type: Atlas
+  serviceName: local_atlas
   serviceConnection:
     config:
-      type: UnityCatalog
-      catalog: hive_metastore
-      databaseSchema: default
-      token: <databricks token>
-      hostPort: localhost:443
-      connectionTimeout: 120
-      connectionArguments:
-        http_path: <http path of databricks cluster>
-
+      type: Atlas
+      username: username
+      password: password
+      databaseServiceName: [name,of,database,service]
+      messagingServiceName: [name,of,messaging,service]
+      hostPort: http://192.168.1.8:21000
+      entity_type: examples/workflows/atlas_mapping.yaml
   sourceConfig:
     config:
       type: DatabaseMetadata
-sink:
+sink: 
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/unity_catalog_lineage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/bigquery_lineage.yaml`

 * *Files 17% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 source:
-  type: unitycatalog-lineage
-  serviceName: local_unitycatalog
+  type: bigquery-lineage
+  serviceName: local_bigquery
   sourceConfig:
     config:
       type: DatabaseLineage
       queryLogDuration: 1
       resultLimit: 10000
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/unity_catalog_usage.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/unity_catalog_usage.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -23,13 +23,13 @@
   config:
     filename: /tmp/databricks_usage
 bulkSink:
   type: metadata-usage
   config:
     filename: /tmp/databricks_usage
 workflowConfig:
-  loggerLevel: DEBUG
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
       jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/examples/workflows/vertica.yaml` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/examples/workflows/hive.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,22 +1,36 @@
 source:
-  type: vertica
-  serviceName: local_vertica
+  type: hive
+  serviceName: local_hive
   serviceConnection:
     config:
-      type: Vertica
-      username: openmetadata_user
-      password: ''
-      hostPort: localhost:5433
-      database: custom_database_name
+      type: Hive
+      databaseSchema: default
+      hostPort: localhost:10000
+
+      # metastoreConnection:
+      #   type: Mysql
+      #   username: APP
+      #   authType:
+      #     password: password
+      #   hostPort: localhost:3306
+      #   databaseSchema: demo_hive
+
+        # type: Postgres
+        # username: APP
+        # authType:
+        #   password: password
+        # hostPort: localhost:5432
+        # database: demo_hive
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
+#  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/EntityLinkLexer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/EntityLinkLexer.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,15 +8,15 @@
     from typing.io import TextIO
 
 
 
 def serializedATN():
     with StringIO() as buf:
         buf.write("\3\u608b\ua72a\u8133\ub9ed\u417c\u3be7\u7786\u5964\2\b")
-        buf.write("\u03ce\b\1\4\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7")
+        buf.write("\u03da\b\1\4\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7")
         buf.write("\t\7\3\2\3\2\3\3\3\3\3\3\3\4\3\4\3\4\3\4\3\5\3\5\3\5\3")
         buf.write("\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5")
         buf.write("\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3")
         buf.write("\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5")
         buf.write("\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3")
         buf.write("\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5")
         buf.write("\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3")
@@ -45,15 +45,17 @@
         buf.write("\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5")
         buf.write("\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3")
         buf.write("\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5")
         buf.write("\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3")
         buf.write("\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5")
         buf.write("\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3")
         buf.write("\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5")
-        buf.write("\3\5\3\5\3\5\3\5\5\5\u01f9\n\5\3\6\3\6\3\6\3\6\3\6\3\6")
+        buf.write("\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3")
+        buf.write("\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\5\5\u020a\n\5\3\6\3\6\3")
+        buf.write("\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6")
         buf.write("\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3")
         buf.write("\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6")
         buf.write("\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3")
         buf.write("\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6")
         buf.write("\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3")
         buf.write("\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6")
         buf.write("\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3")
@@ -78,361 +80,364 @@
         buf.write("\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6")
         buf.write("\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3")
         buf.write("\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6")
         buf.write("\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3")
         buf.write("\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6")
         buf.write("\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3")
         buf.write("\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6")
-        buf.write("\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\3\6\5\6\u03bd")
-        buf.write("\n\6\3\7\6\7\u03c0\n\7\r\7\16\7\u03c1\3\7\7\7\u03c5\n")
-        buf.write("\7\f\7\16\7\u03c8\13\7\3\7\6\7\u03cb\n\7\r\7\16\7\u03cc")
-        buf.write("\3\u03c6\2\b\3\3\5\4\7\5\t\6\13\7\r\b\3\2\4\3\2<<\4\2")
-        buf.write("<<@@\2\u042e\2\3\3\2\2\2\2\5\3\2\2\2\2\7\3\2\2\2\2\t\3")
-        buf.write("\2\2\2\2\13\3\2\2\2\2\r\3\2\2\2\3\17\3\2\2\2\5\21\3\2")
-        buf.write("\2\2\7\24\3\2\2\2\t\u01f8\3\2\2\2\13\u03bc\3\2\2\2\r\u03bf")
-        buf.write("\3\2\2\2\17\20\7@\2\2\20\4\3\2\2\2\21\22\7<\2\2\22\23")
-        buf.write("\7<\2\2\23\6\3\2\2\2\24\25\7>\2\2\25\26\7%\2\2\26\27\7")
-        buf.write("G\2\2\27\b\3\2\2\2\30\31\7v\2\2\31\32\7c\2\2\32\33\7d")
-        buf.write("\2\2\33\34\7n\2\2\34\u01f9\7g\2\2\35\36\7v\2\2\36\37\7")
-        buf.write("q\2\2\37 \7r\2\2 !\7k\2\2!\u01f9\7e\2\2\"#\7e\2\2#$\7")
-        buf.write("n\2\2$%\7c\2\2%&\7u\2\2&\'\7u\2\2\'(\7k\2\2()\7h\2\2)")
-        buf.write("*\7k\2\2*+\7e\2\2+,\7c\2\2,-\7v\2\2-.\7k\2\2./\7q\2\2")
-        buf.write("/\u01f9\7p\2\2\60\61\7f\2\2\61\62\7c\2\2\62\63\7u\2\2")
-        buf.write("\63\64\7j\2\2\64\65\7d\2\2\65\66\7q\2\2\66\67\7c\2\2\67")
-        buf.write("8\7t\2\28\u01f9\7f\2\29:\7r\2\2:;\7k\2\2;<\7r\2\2<=\7")
-        buf.write("g\2\2=>\7n\2\2>?\7k\2\2?@\7p\2\2@\u01f9\7g\2\2AB\7f\2")
-        buf.write("\2BC\7c\2\2CD\7v\2\2DE\7c\2\2EF\7d\2\2FG\7c\2\2GH\7u\2")
-        buf.write("\2H\u01f9\7g\2\2IJ\7f\2\2JK\7c\2\2KL\7v\2\2LM\7c\2\2M")
-        buf.write("N\7d\2\2NO\7c\2\2OP\7u\2\2PQ\7g\2\2QR\7U\2\2RS\7e\2\2")
-        buf.write("ST\7j\2\2TU\7g\2\2UV\7o\2\2V\u01f9\7c\2\2WX\7i\2\2XY\7")
-        buf.write("n\2\2YZ\7q\2\2Z[\7u\2\2[\\\7u\2\2\\]\7c\2\2]^\7t\2\2^")
-        buf.write("\u01f9\7{\2\2_`\7i\2\2`a\7n\2\2ab\7q\2\2bc\7u\2\2cd\7")
-        buf.write("u\2\2de\7c\2\2ef\7t\2\2fg\7{\2\2gh\7V\2\2hi\7g\2\2ij\7")
-        buf.write("t\2\2j\u01f9\7o\2\2kl\7f\2\2lm\7c\2\2mn\7v\2\2no\7c\2")
-        buf.write("\2op\7d\2\2pq\7c\2\2qr\7u\2\2rs\7g\2\2st\7U\2\2tu\7g\2")
-        buf.write("\2uv\7t\2\2vw\7x\2\2wx\7k\2\2xy\7e\2\2y\u01f9\7g\2\2z")
-        buf.write("{\7o\2\2{|\7g\2\2|}\7u\2\2}~\7u\2\2~\177\7c\2\2\177\u0080")
-        buf.write("\7i\2\2\u0080\u0081\7k\2\2\u0081\u0082\7p\2\2\u0082\u0083")
-        buf.write("\7i\2\2\u0083\u0084\7U\2\2\u0084\u0085\7g\2\2\u0085\u0086")
-        buf.write("\7t\2\2\u0086\u0087\7x\2\2\u0087\u0088\7k\2\2\u0088\u0089")
-        buf.write("\7e\2\2\u0089\u01f9\7g\2\2\u008a\u008b\7o\2\2\u008b\u008c")
-        buf.write("\7g\2\2\u008c\u008d\7v\2\2\u008d\u008e\7c\2\2\u008e\u008f")
-        buf.write("\7f\2\2\u008f\u0090\7c\2\2\u0090\u0091\7v\2\2\u0091\u0092")
-        buf.write("\7c\2\2\u0092\u0093\7U\2\2\u0093\u0094\7g\2\2\u0094\u0095")
-        buf.write("\7t\2\2\u0095\u0096\7x\2\2\u0096\u0097\7k\2\2\u0097\u0098")
-        buf.write("\7e\2\2\u0098\u01f9\7g\2\2\u0099\u009a\7f\2\2\u009a\u009b")
-        buf.write("\7c\2\2\u009b\u009c\7u\2\2\u009c\u009d\7j\2\2\u009d\u009e")
-        buf.write("\7d\2\2\u009e\u009f\7q\2\2\u009f\u00a0\7c\2\2\u00a0\u00a1")
-        buf.write("\7t\2\2\u00a1\u00a2\7f\2\2\u00a2\u00a3\7U\2\2\u00a3\u00a4")
-        buf.write("\7g\2\2\u00a4\u00a5\7t\2\2\u00a5\u00a6\7x\2\2\u00a6\u00a7")
-        buf.write("\7k\2\2\u00a7\u00a8\7e\2\2\u00a8\u01f9\7g\2\2\u00a9\u00aa")
-        buf.write("\7r\2\2\u00aa\u00ab\7k\2\2\u00ab\u00ac\7r\2\2\u00ac\u00ad")
-        buf.write("\7g\2\2\u00ad\u00ae\7n\2\2\u00ae\u00af\7k\2\2\u00af\u00b0")
-        buf.write("\7p\2\2\u00b0\u00b1\7g\2\2\u00b1\u00b2\7U\2\2\u00b2\u00b3")
-        buf.write("\7g\2\2\u00b3\u00b4\7t\2\2\u00b4\u00b5\7x\2\2\u00b5\u00b6")
-        buf.write("\7k\2\2\u00b6\u00b7\7e\2\2\u00b7\u01f9\7g\2\2\u00b8\u00b9")
-        buf.write("\7o\2\2\u00b9\u00ba\7n\2\2\u00ba\u00bb\7o\2\2\u00bb\u00bc")
-        buf.write("\7q\2\2\u00bc\u00bd\7f\2\2\u00bd\u00be\7g\2\2\u00be\u00bf")
-        buf.write("\7n\2\2\u00bf\u00c0\7U\2\2\u00c0\u00c1\7g\2\2\u00c1\u00c2")
-        buf.write("\7t\2\2\u00c2\u00c3\7x\2\2\u00c3\u00c4\7k\2\2\u00c4\u00c5")
-        buf.write("\7e\2\2\u00c5\u01f9\7g\2\2\u00c6\u00c7\7u\2\2\u00c7\u00c8")
-        buf.write("\7v\2\2\u00c8\u00c9\7q\2\2\u00c9\u00ca\7t\2\2\u00ca\u00cb")
-        buf.write("\7c\2\2\u00cb\u00cc\7i\2\2\u00cc\u00cd\7g\2\2\u00cd\u00ce")
-        buf.write("\7U\2\2\u00ce\u00cf\7g\2\2\u00cf\u00d0\7t\2\2\u00d0\u00d1")
-        buf.write("\7x\2\2\u00d1\u00d2\7k\2\2\u00d2\u00d3\7e\2\2\u00d3\u01f9")
-        buf.write("\7g\2\2\u00d4\u00d5\7u\2\2\u00d5\u00d6\7g\2\2\u00d6\u00d7")
-        buf.write("\7c\2\2\u00d7\u00d8\7t\2\2\u00d8\u00d9\7e\2\2\u00d9\u00da")
-        buf.write("\7j\2\2\u00da\u00db\7U\2\2\u00db\u00dc\7g\2\2\u00dc\u00dd")
-        buf.write("\7t\2\2\u00dd\u00de\7x\2\2\u00de\u00df\7k\2\2\u00df\u00e0")
-        buf.write("\7e\2\2\u00e0\u01f9\7g\2\2\u00e1\u00e2\7y\2\2\u00e2\u00e3")
-        buf.write("\7g\2\2\u00e3\u00e4\7d\2\2\u00e4\u00e5\7j\2\2\u00e5\u00e6")
-        buf.write("\7q\2\2\u00e6\u00e7\7q\2\2\u00e7\u01f9\7m\2\2\u00e8\u00e9")
-        buf.write("\7o\2\2\u00e9\u00ea\7n\2\2\u00ea\u00eb\7o\2\2\u00eb\u00ec")
-        buf.write("\7q\2\2\u00ec\u00ed\7f\2\2\u00ed\u00ee\7g\2\2\u00ee\u01f9")
-        buf.write("\7n\2\2\u00ef\u00f0\7v\2\2\u00f0\u00f1\7{\2\2\u00f1\u00f2")
-        buf.write("\7r\2\2\u00f2\u01f9\7g\2\2\u00f3\u00f4\7v\2\2\u00f4\u00f5")
-        buf.write("\7g\2\2\u00f5\u00f6\7c\2\2\u00f6\u01f9\7o\2\2\u00f7\u00f8")
-        buf.write("\7w\2\2\u00f8\u00f9\7u\2\2\u00f9\u00fa\7g\2\2\u00fa\u01f9")
-        buf.write("\7t\2\2\u00fb\u00fc\7d\2\2\u00fc\u00fd\7q\2\2\u00fd\u01f9")
-        buf.write("\7v\2\2\u00fe\u00ff\7t\2\2\u00ff\u0100\7q\2\2\u0100\u0101")
-        buf.write("\7n\2\2\u0101\u01f9\7g\2\2\u0102\u0103\7r\2\2\u0103\u0104")
-        buf.write("\7q\2\2\u0104\u0105\7n\2\2\u0105\u0106\7k\2\2\u0106\u0107")
-        buf.write("\7e\2\2\u0107\u01f9\7{\2\2\u0108\u0109\7v\2\2\u0109\u010a")
-        buf.write("\7g\2\2\u010a\u010b\7u\2\2\u010b\u010c\7v\2\2\u010c\u010d")
-        buf.write("\7U\2\2\u010d\u010e\7w\2\2\u010e\u010f\7k\2\2\u010f\u0110")
-        buf.write("\7v\2\2\u0110\u01f9\7g\2\2\u0111\u0112\7v\2\2\u0112\u0113")
-        buf.write("\7g\2\2\u0113\u0114\7u\2\2\u0114\u0115\7v\2\2\u0115\u0116")
-        buf.write("\7E\2\2\u0116\u0117\7c\2\2\u0117\u0118\7u\2\2\u0118\u01f9")
-        buf.write("\7g\2\2\u0119\u011a\7f\2\2\u011a\u011b\7c\2\2\u011b\u011c")
-        buf.write("\7v\2\2\u011c\u011d\7c\2\2\u011d\u011e\7K\2\2\u011e\u011f")
-        buf.write("\7p\2\2\u011f\u0120\7u\2\2\u0120\u0121\7k\2\2\u0121\u0122")
-        buf.write("\7i\2\2\u0122\u0123\7j\2\2\u0123\u0124\7v\2\2\u0124\u0125")
-        buf.write("\7E\2\2\u0125\u0126\7j\2\2\u0126\u0127\7c\2\2\u0127\u0128")
-        buf.write("\7t\2\2\u0128\u01f9\7v\2\2\u0129\u012a\7m\2\2\u012a\u012b")
-        buf.write("\7r\2\2\u012b\u01f9\7k\2\2\u012c\u012d\7c\2\2\u012d\u012e")
-        buf.write("\7n\2\2\u012e\u012f\7g\2\2\u012f\u0130\7t\2\2\u0130\u01f9")
-        buf.write("\7v\2\2\u0131\u0132\7e\2\2\u0132\u0133\7q\2\2\u0133\u0134")
-        buf.write("\7p\2\2\u0134\u0135\7v\2\2\u0135\u0136\7c\2\2\u0136\u0137")
-        buf.write("\7k\2\2\u0137\u0138\7p\2\2\u0138\u0139\7g\2\2\u0139\u01f9")
-        buf.write("\7t\2\2\u013a\u013b\7v\2\2\u013b\u013c\7c\2\2\u013c\u01f9")
-        buf.write("\7i\2\2\u013d\u013e\7f\2\2\u013e\u013f\7c\2\2\u013f\u0140")
-        buf.write("\7u\2\2\u0140\u0141\7j\2\2\u0141\u0142\7d\2\2\u0142\u0143")
-        buf.write("\7q\2\2\u0143\u0144\7c\2\2\u0144\u0145\7t\2\2\u0145\u0146")
-        buf.write("\7f\2\2\u0146\u0147\7F\2\2\u0147\u0148\7c\2\2\u0148\u0149")
-        buf.write("\7v\2\2\u0149\u014a\7c\2\2\u014a\u014b\7O\2\2\u014b\u014c")
-        buf.write("\7q\2\2\u014c\u014d\7f\2\2\u014d\u014e\7g\2\2\u014e\u01f9")
-        buf.write("\7n\2\2\u014f\u0150\7u\2\2\u0150\u0151\7w\2\2\u0151\u0152")
-        buf.write("\7d\2\2\u0152\u0153\7u\2\2\u0153\u0154\7e\2\2\u0154\u0155")
-        buf.write("\7t\2\2\u0155\u0156\7k\2\2\u0156\u0157\7r\2\2\u0157\u0158")
-        buf.write("\7v\2\2\u0158\u0159\7k\2\2\u0159\u015a\7q\2\2\u015a\u01f9")
-        buf.write("\7p\2\2\u015b\u015c\7e\2\2\u015c\u015d\7j\2\2\u015d\u015e")
-        buf.write("\7c\2\2\u015e\u015f\7t\2\2\u015f\u01f9\7v\2\2\u0160\u0161")
-        buf.write("\7f\2\2\u0161\u0162\7q\2\2\u0162\u0163\7o\2\2\u0163\u0164")
-        buf.write("\7c\2\2\u0164\u0165\7k\2\2\u0165\u01f9\7p\2\2\u0166\u0167")
-        buf.write("\7f\2\2\u0167\u0168\7c\2\2\u0168\u0169\7v\2\2\u0169\u016a")
-        buf.write("\7c\2\2\u016a\u016b\7R\2\2\u016b\u016c\7t\2\2\u016c\u016d")
-        buf.write("\7q\2\2\u016d\u016e\7f\2\2\u016e\u016f\7w\2\2\u016f\u0170")
-        buf.write("\7e\2\2\u0170\u01f9\7v\2\2\u0171\u0172\7u\2\2\u0172\u0173")
-        buf.write("\7c\2\2\u0173\u0174\7o\2\2\u0174\u0175\7r\2\2\u0175\u0176")
-        buf.write("\7n\2\2\u0176\u0177\7g\2\2\u0177\u0178\7F\2\2\u0178\u0179")
-        buf.write("\7c\2\2\u0179\u017a\7v\2\2\u017a\u01f9\7c\2\2\u017b\u017c")
-        buf.write("\7u\2\2\u017c\u017d\7v\2\2\u017d\u017e\7q\2\2\u017e\u017f")
-        buf.write("\7t\2\2\u017f\u0180\7g\2\2\u0180\u0181\7f\2\2\u0181\u0182")
-        buf.write("\7R\2\2\u0182\u0183\7t\2\2\u0183\u0184\7q\2\2\u0184\u0185")
-        buf.write("\7e\2\2\u0185\u0186\7g\2\2\u0186\u0187\7f\2\2\u0187\u0188")
-        buf.write("\7w\2\2\u0188\u0189\7t\2\2\u0189\u01f9\7g\2\2\u018a\u018b")
-        buf.write("\7u\2\2\u018b\u018c\7g\2\2\u018c\u018d\7c\2\2\u018d\u018e")
-        buf.write("\7t\2\2\u018e\u018f\7e\2\2\u018f\u0190\7j\2\2\u0190\u0191")
-        buf.write("\7K\2\2\u0191\u0192\7p\2\2\u0192\u0193\7f\2\2\u0193\u0194")
-        buf.write("\7g\2\2\u0194\u01f9\7z\2\2\u0195\u0196\7c\2\2\u0196\u0197")
-        buf.write("\7r\2\2\u0197\u0198\7r\2\2\u0198\u0199\7O\2\2\u0199\u019a")
-        buf.write("\7c\2\2\u019a\u019b\7t\2\2\u019b\u019c\7m\2\2\u019c\u019d")
-        buf.write("\7g\2\2\u019d\u019e\7v\2\2\u019e\u019f\7R\2\2\u019f\u01a0")
-        buf.write("\7n\2\2\u01a0\u01a1\7c\2\2\u01a1\u01a2\7e\2\2\u01a2\u01a3")
-        buf.write("\7g\2\2\u01a3\u01a4\7F\2\2\u01a4\u01a5\7g\2\2\u01a5\u01a6")
-        buf.write("\7h\2\2\u01a6\u01a7\7k\2\2\u01a7\u01a8\7p\2\2\u01a8\u01a9")
-        buf.write("\7k\2\2\u01a9\u01aa\7v\2\2\u01aa\u01ab\7k\2\2\u01ab\u01ac")
-        buf.write("\7q\2\2\u01ac\u01f9\7p\2\2\u01ad\u01ae\7c\2\2\u01ae\u01af")
-        buf.write("\7r\2\2\u01af\u01f9\7r\2\2\u01b0\u01b1\7r\2\2\u01b1\u01b2")
-        buf.write("\7g\2\2\u01b2\u01b3\7t\2\2\u01b3\u01b4\7u\2\2\u01b4\u01b5")
-        buf.write("\7q\2\2\u01b5\u01b6\7p\2\2\u01b6\u01f9\7c\2\2\u01b7\u01b8")
-        buf.write("\7f\2\2\u01b8\u01b9\7q\2\2\u01b9\u01ba\7e\2\2\u01ba\u01bb")
-        buf.write("\7U\2\2\u01bb\u01bc\7v\2\2\u01bc\u01bd\7q\2\2\u01bd\u01be")
-        buf.write("\7t\2\2\u01be\u01f9\7g\2\2\u01bf\u01c0\7r\2\2\u01c0\u01c1")
-        buf.write("\7c\2\2\u01c1\u01c2\7i\2\2\u01c2\u01f9\7g\2\2\u01c3\u01c4")
-        buf.write("\7M\2\2\u01c4\u01c5\7p\2\2\u01c5\u01c6\7q\2\2\u01c6\u01c7")
-        buf.write("\7y\2\2\u01c7\u01c8\7N\2\2\u01c8\u01c9\7g\2\2\u01c9\u01ca")
-        buf.write("\7f\2\2\u01ca\u01cb\7i\2\2\u01cb\u01cc\7g\2\2\u01cc\u01cd")
-        buf.write("\7R\2\2\u01cd\u01ce\7c\2\2\u01ce\u01cf\7p\2\2\u01cf\u01d0")
-        buf.write("\7g\2\2\u01d0\u01d1\7n\2\2\u01d1\u01f9\7u\2\2\u01d2\u01d3")
-        buf.write("\7i\2\2\u01d3\u01d4\7q\2\2\u01d4\u01d5\7x\2\2\u01d5\u01d6")
-        buf.write("\7g\2\2\u01d6\u01d7\7t\2\2\u01d7\u01f9\7p\2\2\u01d8\u01d9")
-        buf.write("\7c\2\2\u01d9\u01da\7n\2\2\u01da\u01f9\7n\2\2\u01db\u01dc")
-        buf.write("\7e\2\2\u01dc\u01dd\7w\2\2\u01dd\u01de\7u\2\2\u01de\u01df")
-        buf.write("\7v\2\2\u01df\u01e0\7q\2\2\u01e0\u01e1\7o\2\2\u01e1\u01e2")
-        buf.write("\7O\2\2\u01e2\u01e3\7g\2\2\u01e3\u01e4\7v\2\2\u01e4\u01e5")
-        buf.write("\7t\2\2\u01e5\u01e6\7k\2\2\u01e6\u01f9\7e\2\2\u01e7\u01e8")
-        buf.write("\7g\2\2\u01e8\u01e9\7x\2\2\u01e9\u01ea\7g\2\2\u01ea\u01eb")
-        buf.write("\7p\2\2\u01eb\u01ec\7v\2\2\u01ec\u01ed\7u\2\2\u01ed\u01ee")
-        buf.write("\7w\2\2\u01ee\u01ef\7d\2\2\u01ef\u01f0\7u\2\2\u01f0\u01f1")
-        buf.write("\7e\2\2\u01f1\u01f2\7t\2\2\u01f2\u01f3\7k\2\2\u01f3\u01f4")
-        buf.write("\7r\2\2\u01f4\u01f5\7v\2\2\u01f5\u01f6\7k\2\2\u01f6\u01f7")
-        buf.write("\7q\2\2\u01f7\u01f9\7p\2\2\u01f8\30\3\2\2\2\u01f8\35\3")
-        buf.write("\2\2\2\u01f8\"\3\2\2\2\u01f8\60\3\2\2\2\u01f89\3\2\2\2")
-        buf.write("\u01f8A\3\2\2\2\u01f8I\3\2\2\2\u01f8W\3\2\2\2\u01f8_\3")
-        buf.write("\2\2\2\u01f8k\3\2\2\2\u01f8z\3\2\2\2\u01f8\u008a\3\2\2")
-        buf.write("\2\u01f8\u0099\3\2\2\2\u01f8\u00a9\3\2\2\2\u01f8\u00b8")
-        buf.write("\3\2\2\2\u01f8\u00c6\3\2\2\2\u01f8\u00d4\3\2\2\2\u01f8")
-        buf.write("\u00e1\3\2\2\2\u01f8\u00e8\3\2\2\2\u01f8\u00ef\3\2\2\2")
-        buf.write("\u01f8\u00f3\3\2\2\2\u01f8\u00f7\3\2\2\2\u01f8\u00fb\3")
-        buf.write("\2\2\2\u01f8\u00fe\3\2\2\2\u01f8\u0102\3\2\2\2\u01f8\u0108")
-        buf.write("\3\2\2\2\u01f8\u0111\3\2\2\2\u01f8\u0119\3\2\2\2\u01f8")
-        buf.write("\u0129\3\2\2\2\u01f8\u012c\3\2\2\2\u01f8\u0131\3\2\2\2")
-        buf.write("\u01f8\u013a\3\2\2\2\u01f8\u013d\3\2\2\2\u01f8\u014f\3")
-        buf.write("\2\2\2\u01f8\u015b\3\2\2\2\u01f8\u0160\3\2\2\2\u01f8\u0166")
-        buf.write("\3\2\2\2\u01f8\u0171\3\2\2\2\u01f8\u017b\3\2\2\2\u01f8")
-        buf.write("\u018a\3\2\2\2\u01f8\u0195\3\2\2\2\u01f8\u01ad\3\2\2\2")
-        buf.write("\u01f8\u01b0\3\2\2\2\u01f8\u01b7\3\2\2\2\u01f8\u01bf\3")
-        buf.write("\2\2\2\u01f8\u01c3\3\2\2\2\u01f8\u01d2\3\2\2\2\u01f8\u01d8")
-        buf.write("\3\2\2\2\u01f8\u01db\3\2\2\2\u01f8\u01e7\3\2\2\2\u01f9")
-        buf.write("\n\3\2\2\2\u01fa\u01fb\7f\2\2\u01fb\u01fc\7g\2\2\u01fc")
-        buf.write("\u01fd\7u\2\2\u01fd\u01fe\7e\2\2\u01fe\u01ff\7t\2\2\u01ff")
-        buf.write("\u0200\7k\2\2\u0200\u0201\7r\2\2\u0201\u0202\7v\2\2\u0202")
-        buf.write("\u0203\7k\2\2\u0203\u0204\7q\2\2\u0204\u03bd\7p\2\2\u0205")
-        buf.write("\u0206\7e\2\2\u0206\u0207\7q\2\2\u0207\u0208\7n\2\2\u0208")
-        buf.write("\u0209\7w\2\2\u0209\u020a\7o\2\2\u020a\u020b\7p\2\2\u020b")
-        buf.write("\u03bd\7u\2\2\u020c\u020d\7u\2\2\u020d\u020e\7e\2\2\u020e")
-        buf.write("\u020f\7j\2\2\u020f\u0210\7g\2\2\u0210\u0211\7o\2\2\u0211")
-        buf.write("\u0212\7c\2\2\u0212\u0213\7H\2\2\u0213\u0214\7k\2\2\u0214")
-        buf.write("\u0215\7g\2\2\u0215\u0216\7n\2\2\u0216\u0217\7f\2\2\u0217")
-        buf.write("\u03bd\7u\2\2\u0218\u0219\7v\2\2\u0219\u021a\7c\2\2\u021a")
-        buf.write("\u021b\7i\2\2\u021b\u03bd\7u\2\2\u021c\u021d\7v\2\2\u021d")
-        buf.write("\u021e\7c\2\2\u021e\u021f\7u\2\2\u021f\u0220\7m\2\2\u0220")
-        buf.write("\u03bd\7u\2\2\u0221\u0222\7o\2\2\u0222\u0223\7n\2\2\u0223")
-        buf.write("\u0224\7H\2\2\u0224\u0225\7g\2\2\u0225\u0226\7c\2\2\u0226")
-        buf.write("\u0227\7v\2\2\u0227\u0228\7w\2\2\u0228\u0229\7t\2\2\u0229")
-        buf.write("\u022a\7g\2\2\u022a\u03bd\7u\2\2\u022b\u022c\7u\2\2\u022c")
-        buf.write("\u022d\7e\2\2\u022d\u022e\7j\2\2\u022e\u022f\7g\2\2\u022f")
-        buf.write("\u0230\7o\2\2\u0230\u0231\7c\2\2\u0231\u0232\7V\2\2\u0232")
-        buf.write("\u0233\7g\2\2\u0233\u0234\7z\2\2\u0234\u03bd\7v\2\2\u0235")
-        buf.write("\u0236\7q\2\2\u0236\u0237\7y\2\2\u0237\u0238\7p\2\2\u0238")
-        buf.write("\u0239\7g\2\2\u0239\u03bd\7t\2\2\u023a\u023b\7t\2\2\u023b")
-        buf.write("\u023c\7g\2\2\u023c\u023d\7x\2\2\u023d\u023e\7k\2\2\u023e")
-        buf.write("\u023f\7g\2\2\u023f\u0240\7y\2\2\u0240\u0241\7g\2\2\u0241")
-        buf.write("\u0242\7t\2\2\u0242\u03bd\7u\2\2\u0243\u0244\7u\2\2\u0244")
-        buf.write("\u0245\7{\2\2\u0245\u0246\7p\2\2\u0246\u0247\7q\2\2\u0247")
-        buf.write("\u0248\7p\2\2\u0248\u0249\7{\2\2\u0249\u024a\7o\2\2\u024a")
-        buf.write("\u03bd\7u\2\2\u024b\u024c\7t\2\2\u024c\u024d\7g\2\2\u024d")
-        buf.write("\u024e\7n\2\2\u024e\u024f\7c\2\2\u024f\u0250\7v\2\2\u0250")
-        buf.write("\u0251\7g\2\2\u0251\u0252\7f\2\2\u0252\u0253\7V\2\2\u0253")
-        buf.write("\u0254\7g\2\2\u0254\u0255\7t\2\2\u0255\u0256\7o\2\2\u0256")
-        buf.write("\u03bd\7u\2\2\u0257\u0258\7t\2\2\u0258\u0259\7g\2\2\u0259")
-        buf.write("\u025a\7h\2\2\u025a\u025b\7g\2\2\u025b\u025c\7t\2\2\u025c")
-        buf.write("\u025d\7g\2\2\u025d\u025e\7p\2\2\u025e\u025f\7e\2\2\u025f")
-        buf.write("\u0260\7g\2\2\u0260\u03bd\7u\2\2\u0261\u0262\7g\2\2\u0262")
-        buf.write("\u0263\7z\2\2\u0263\u0264\7v\2\2\u0264\u0265\7g\2\2\u0265")
-        buf.write("\u0266\7p\2\2\u0266\u0267\7u\2\2\u0267\u0268\7k\2\2\u0268")
-        buf.write("\u0269\7q\2\2\u0269\u03bd\7p\2\2\u026a\u026b\7f\2\2\u026b")
-        buf.write("\u026c\7k\2\2\u026c\u026d\7u\2\2\u026d\u026e\7r\2\2\u026e")
-        buf.write("\u026f\7n\2\2\u026f\u0270\7c\2\2\u0270\u0271\7{\2\2\u0271")
-        buf.write("\u0272\7P\2\2\u0272\u0273\7c\2\2\u0273\u0274\7o\2\2\u0274")
-        buf.write("\u03bd\7g\2\2\u0275\u0276\7p\2\2\u0276\u0277\7c\2\2\u0277")
-        buf.write("\u0278\7o\2\2\u0278\u03bd\7g\2\2\u0279\u027a\7o\2\2\u027a")
-        buf.write("\u027b\7g\2\2\u027b\u027c\7u\2\2\u027c\u027d\7u\2\2\u027d")
-        buf.write("\u027e\7c\2\2\u027e\u027f\7i\2\2\u027f\u0280\7g\2\2\u0280")
-        buf.write("\u0281\7U\2\2\u0281\u0282\7e\2\2\u0282\u0283\7j\2\2\u0283")
-        buf.write("\u0284\7g\2\2\u0284\u0285\7o\2\2\u0285\u03bd\7c\2\2\u0286")
-        buf.write("\u0287\7e\2\2\u0287\u0288\7j\2\2\u0288\u0289\7c\2\2\u0289")
-        buf.write("\u028a\7t\2\2\u028a\u028b\7v\2\2\u028b\u03bd\7u\2\2\u028c")
-        buf.write("\u028d\7f\2\2\u028d\u028e\7c\2\2\u028e\u028f\7v\2\2\u028f")
-        buf.write("\u0290\7c\2\2\u0290\u0291\7O\2\2\u0291\u0292\7q\2\2\u0292")
-        buf.write("\u0293\7f\2\2\u0293\u0294\7g\2\2\u0294\u03bd\7n\2\2\u0295")
-        buf.write("\u0296\7e\2\2\u0296\u0297\7q\2\2\u0297\u0298\7p\2\2\u0298")
-        buf.write("\u0299\7u\2\2\u0299\u029a\7v\2\2\u029a\u029b\7t\2\2\u029b")
-        buf.write("\u029c\7c\2\2\u029c\u029d\7k\2\2\u029d\u029e\7p\2\2\u029e")
-        buf.write("\u03bd\7v\2\2\u029f\u02a0\7v\2\2\u02a0\u02a1\7c\2\2\u02a1")
-        buf.write("\u02a2\7d\2\2\u02a2\u02a3\7n\2\2\u02a3\u02a4\7g\2\2\u02a4")
-        buf.write("\u02a5\7E\2\2\u02a5\u02a6\7q\2\2\u02a6\u02a7\7p\2\2\u02a7")
-        buf.write("\u02a8\7u\2\2\u02a8\u02a9\7v\2\2\u02a9\u02aa\7t\2\2\u02aa")
-        buf.write("\u02ab\7c\2\2\u02ab\u02ac\7k\2\2\u02ac\u02ad\7p\2\2\u02ad")
-        buf.write("\u02ae\7v\2\2\u02ae\u03bd\7u\2\2\u02af\u02b0\7r\2\2\u02b0")
-        buf.write("\u02b1\7c\2\2\u02b1\u02b2\7t\2\2\u02b2\u02b3\7v\2\2\u02b3")
-        buf.write("\u02b4\7k\2\2\u02b4\u02b5\7v\2\2\u02b5\u02b6\7k\2\2\u02b6")
-        buf.write("\u02b7\7q\2\2\u02b7\u02b8\7p\2\2\u02b8\u03bd\7u\2\2\u02b9")
-        buf.write("\u02ba\7t\2\2\u02ba\u02bb\7g\2\2\u02bb\u02bc\7r\2\2\u02bc")
-        buf.write("\u02bd\7n\2\2\u02bd\u02be\7k\2\2\u02be\u02bf\7e\2\2\u02bf")
-        buf.write("\u02c0\7c\2\2\u02c0\u02c1\7v\2\2\u02c1\u02c2\7k\2\2\u02c2")
-        buf.write("\u02c3\7q\2\2\u02c3\u02c4\7p\2\2\u02c4\u02c5\7H\2\2\u02c5")
-        buf.write("\u02c6\7c\2\2\u02c6\u02c7\7e\2\2\u02c7\u02c8\7v\2\2\u02c8")
-        buf.write("\u02c9\7q\2\2\u02c9\u03bd\7t\2\2\u02ca\u02cb\7u\2\2\u02cb")
-        buf.write("\u02cc\7q\2\2\u02cc\u02cd\7w\2\2\u02cd\u02ce\7t\2\2\u02ce")
-        buf.write("\u02cf\7e\2\2\u02cf\u02d0\7g\2\2\u02d0\u02d1\7W\2\2\u02d1")
-        buf.write("\u02d2\7t\2\2\u02d2\u03bd\7n\2\2\u02d3\u02d4\7o\2\2\u02d4")
-        buf.write("\u02d5\7w\2\2\u02d5\u02d6\7v\2\2\u02d6\u02d7\7w\2\2\u02d7")
-        buf.write("\u02d8\7c\2\2\u02d8\u02d9\7n\2\2\u02d9\u02da\7n\2\2\u02da")
-        buf.write("\u02db\7{\2\2\u02db\u02dc\7G\2\2\u02dc\u02dd\7z\2\2\u02dd")
-        buf.write("\u02de\7e\2\2\u02de\u02df\7n\2\2\u02df\u02e0\7w\2\2\u02e0")
-        buf.write("\u02e1\7u\2\2\u02e1\u02e2\7k\2\2\u02e2\u02e3\7x\2\2\u02e3")
-        buf.write("\u03bd\7g\2\2\u02e4\u02e5\7g\2\2\u02e5\u02e6\7z\2\2\u02e6")
-        buf.write("\u02e7\7r\2\2\u02e7\u02e8\7g\2\2\u02e8\u02e9\7t\2\2\u02e9")
-        buf.write("\u02ea\7v\2\2\u02ea\u03bd\7u\2\2\u02eb\u02ec\7h\2\2\u02ec")
-        buf.write("\u02ed\7k\2\2\u02ed\u02ee\7g\2\2\u02ee\u02ef\7n\2\2\u02ef")
-        buf.write("\u02f0\7f\2\2\u02f0\u03bd\7u\2\2\u02f1\u02f2\7h\2\2\u02f2")
-        buf.write("\u02f3\7q\2\2\u02f3\u02f4\7n\2\2\u02f4\u02f5\7n\2\2\u02f5")
-        buf.write("\u02f6\7q\2\2\u02f6\u02f7\7y\2\2\u02f7\u02f8\7g\2\2\u02f8")
-        buf.write("\u02f9\7t\2\2\u02f9\u03bd\7u\2\2\u02fa\u02fb\7c\2\2\u02fb")
-        buf.write("\u02fc\7r\2\2\u02fc\u02fd\7r\2\2\u02fd\u02fe\7E\2\2\u02fe")
-        buf.write("\u02ff\7q\2\2\u02ff\u0300\7p\2\2\u0300\u0301\7h\2\2\u0301")
-        buf.write("\u0302\7k\2\2\u0302\u0303\7i\2\2\u0303\u0304\7w\2\2\u0304")
-        buf.write("\u0305\7t\2\2\u0305\u0306\7c\2\2\u0306\u0307\7v\2\2\u0307")
-        buf.write("\u0308\7k\2\2\u0308\u0309\7q\2\2\u0309\u03bd\7p\2\2\u030a")
-        buf.write("\u030b\7c\2\2\u030b\u030c\7r\2\2\u030c\u030d\7r\2\2\u030d")
-        buf.write("\u030e\7U\2\2\u030e\u030f\7e\2\2\u030f\u0310\7j\2\2\u0310")
-        buf.write("\u0311\7g\2\2\u0311\u0312\7f\2\2\u0312\u0313\7w\2\2\u0313")
-        buf.write("\u0314\7n\2\2\u0314\u03bd\7g\2\2\u0315\u0316\7x\2\2\u0316")
-        buf.write("\u0317\7q\2\2\u0317\u0318\7v\2\2\u0318\u0319\7g\2\2\u0319")
-        buf.write("\u03bd\7u\2\2\u031a\u031b\7r\2\2\u031b\u031c\7t\2\2\u031c")
-        buf.write("\u031d\7q\2\2\u031d\u031e\7h\2\2\u031e\u031f\7k\2\2\u031f")
-        buf.write("\u0320\7n\2\2\u0320\u03bd\7g\2\2\u0321\u0322\7t\2\2\u0322")
-        buf.write("\u0323\7q\2\2\u0323\u0324\7n\2\2\u0324\u0325\7g\2\2\u0325")
-        buf.write("\u03bd\7u\2\2\u0326\u0327\7f\2\2\u0327\u0328\7g\2\2\u0328")
-        buf.write("\u0329\7n\2\2\u0329\u032a\7g\2\2\u032a\u032b\7v\2\2\u032b")
-        buf.write("\u032c\7g\2\2\u032c\u03bd\7f\2\2\u032d\u032e\7n\2\2\u032e")
-        buf.write("\u032f\7k\2\2\u032f\u0330\7h\2\2\u0330\u0331\7g\2\2\u0331")
-        buf.write("\u0332\7E\2\2\u0332\u0333\7{\2\2\u0333\u0334\7e\2\2\u0334")
-        buf.write("\u0335\7n\2\2\u0335\u03bd\7g\2\2\u0336\u0337\7c\2\2\u0337")
-        buf.write("\u0338\7r\2\2\u0338\u0339\7k\2\2\u0339\u033a\7a\2\2\u033a")
-        buf.write("\u033b\7e\2\2\u033b\u033c\7n\2\2\u033c\u033d\7k\2\2\u033d")
-        buf.write("\u033e\7g\2\2\u033e\u033f\7p\2\2\u033f\u0340\7v\2\2\u0340")
-        buf.write("\u0341\7a\2\2\u0341\u0342\7k\2\2\u0342\u03bd\7f\2\2\u0343")
-        buf.write("\u0344\7u\2\2\u0344\u0345\7q\2\2\u0345\u0346\7w\2\2\u0346")
-        buf.write("\u0347\7t\2\2\u0347\u0348\7e\2\2\u0348\u0349\7g\2\2\u0349")
-        buf.write("\u034a\7J\2\2\u034a\u034b\7c\2\2\u034b\u034c\7u\2\2\u034c")
-        buf.write("\u03bd\7j\2\2\u034d\u034e\7v\2\2\u034e\u034f\7g\2\2\u034f")
-        buf.write("\u0350\7u\2\2\u0350\u0351\7v\2\2\u0351\u0352\7E\2\2\u0352")
-        buf.write("\u0353\7c\2\2\u0353\u0354\7u\2\2\u0354\u0355\7g\2\2\u0355")
-        buf.write("\u0356\7T\2\2\u0356\u0357\7g\2\2\u0357\u0358\7u\2\2\u0358")
-        buf.write("\u0359\7w\2\2\u0359\u035a\7n\2\2\u035a\u03bd\7v\2\2\u035b")
-        buf.write("\u035c\7v\2\2\u035c\u035d\7g\2\2\u035d\u035e\7u\2\2\u035e")
-        buf.write("\u035f\7v\2\2\u035f\u03bd\7u\2\2\u0360\u0361\7r\2\2\u0361")
-        buf.write("\u0362\7k\2\2\u0362\u0363\7r\2\2\u0363\u0364\7g\2\2\u0364")
-        buf.write("\u0365\7n\2\2\u0365\u0366\7k\2\2\u0366\u0367\7p\2\2\u0367")
-        buf.write("\u0368\7g\2\2\u0368\u0369\7U\2\2\u0369\u036a\7v\2\2\u036a")
-        buf.write("\u036b\7c\2\2\u036b\u036c\7v\2\2\u036c\u036d\7w\2\2\u036d")
-        buf.write("\u03bd\7u\2\2\u036e\u036f\7f\2\2\u036f\u0370\7c\2\2\u0370")
-        buf.write("\u0371\7v\2\2\u0371\u0372\7c\2\2\u0372\u0373\7R\2\2\u0373")
-        buf.write("\u0374\7t\2\2\u0374\u0375\7q\2\2\u0375\u0376\7f\2\2\u0376")
-        buf.write("\u0377\7w\2\2\u0377\u0378\7e\2\2\u0378\u0379\7v\2\2\u0379")
-        buf.write("\u03bd\7u\2\2\u037a\u037b\7r\2\2\u037b\u037c\7c\2\2\u037c")
-        buf.write("\u037d\7t\2\2\u037d\u037e\7c\2\2\u037e\u037f\7o\2\2\u037f")
-        buf.write("\u0380\7g\2\2\u0380\u0381\7v\2\2\u0381\u0382\7g\2\2\u0382")
-        buf.write("\u0383\7t\2\2\u0383\u0384\7X\2\2\u0384\u0385\7c\2\2\u0385")
-        buf.write("\u0386\7n\2\2\u0386\u0387\7w\2\2\u0387\u0388\7g\2\2\u0388")
-        buf.write("\u03bd\7u\2\2\u0389\u038a\7t\2\2\u038a\u038b\7g\2\2\u038b")
-        buf.write("\u038c\7v\2\2\u038c\u038d\7g\2\2\u038d\u038e\7p\2\2\u038e")
-        buf.write("\u038f\7v\2\2\u038f\u0390\7k\2\2\u0390\u0391\7q\2\2\u0391")
-        buf.write("\u0392\7p\2\2\u0392\u0393\7R\2\2\u0393\u0394\7g\2\2\u0394")
-        buf.write("\u0395\7t\2\2\u0395\u0396\7k\2\2\u0396\u0397\7q\2\2\u0397")
-        buf.write("\u03bd\7f\2\2\u0398\u0399\7r\2\2\u0399\u039a\7c\2\2\u039a")
-        buf.write("\u039b\7t\2\2\u039b\u039c\7g\2\2\u039c\u039d\7p\2\2\u039d")
-        buf.write("\u03bd\7v\2\2\u039e\u039f\7v\2\2\u039f\u03a0\7g\2\2\u03a0")
-        buf.write("\u03a1\7c\2\2\u03a1\u03a2\7o\2\2\u03a2\u03bd\7u\2\2\u03a3")
-        buf.write("\u03a4\7r\2\2\u03a4\u03a5\7g\2\2\u03a5\u03a6\7t\2\2\u03a6")
-        buf.write("\u03a7\7u\2\2\u03a7\u03a8\7q\2\2\u03a8\u03a9\7p\2\2\u03a9")
-        buf.write("\u03aa\7c\2\2\u03aa\u03bd\7u\2\2\u03ab\u03ac\7k\2\2\u03ac")
-        buf.write("\u03ad\7p\2\2\u03ad\u03ae\7i\2\2\u03ae\u03af\7g\2\2\u03af")
-        buf.write("\u03b0\7u\2\2\u03b0\u03b1\7v\2\2\u03b1\u03b2\7k\2\2\u03b2")
-        buf.write("\u03b3\7q\2\2\u03b3\u03b4\7p\2\2\u03b4\u03b5\7R\2\2\u03b5")
-        buf.write("\u03b6\7k\2\2\u03b6\u03b7\7r\2\2\u03b7\u03b8\7g\2\2\u03b8")
-        buf.write("\u03b9\7n\2\2\u03b9\u03ba\7k\2\2\u03ba\u03bb\7p\2\2\u03bb")
-        buf.write("\u03bd\7g\2\2\u03bc\u01fa\3\2\2\2\u03bc\u0205\3\2\2\2")
-        buf.write("\u03bc\u020c\3\2\2\2\u03bc\u0218\3\2\2\2\u03bc\u021c\3")
-        buf.write("\2\2\2\u03bc\u0221\3\2\2\2\u03bc\u022b\3\2\2\2\u03bc\u0235")
-        buf.write("\3\2\2\2\u03bc\u023a\3\2\2\2\u03bc\u0243\3\2\2\2\u03bc")
-        buf.write("\u024b\3\2\2\2\u03bc\u0257\3\2\2\2\u03bc\u0261\3\2\2\2")
-        buf.write("\u03bc\u026a\3\2\2\2\u03bc\u0275\3\2\2\2\u03bc\u0279\3")
-        buf.write("\2\2\2\u03bc\u0286\3\2\2\2\u03bc\u028c\3\2\2\2\u03bc\u0295")
-        buf.write("\3\2\2\2\u03bc\u029f\3\2\2\2\u03bc\u02af\3\2\2\2\u03bc")
-        buf.write("\u02b9\3\2\2\2\u03bc\u02ca\3\2\2\2\u03bc\u02d3\3\2\2\2")
-        buf.write("\u03bc\u02e4\3\2\2\2\u03bc\u02eb\3\2\2\2\u03bc\u02f1\3")
-        buf.write("\2\2\2\u03bc\u02fa\3\2\2\2\u03bc\u030a\3\2\2\2\u03bc\u0315")
-        buf.write("\3\2\2\2\u03bc\u031a\3\2\2\2\u03bc\u0321\3\2\2\2\u03bc")
-        buf.write("\u0326\3\2\2\2\u03bc\u032d\3\2\2\2\u03bc\u0336\3\2\2\2")
-        buf.write("\u03bc\u0343\3\2\2\2\u03bc\u034d\3\2\2\2\u03bc\u035b\3")
-        buf.write("\2\2\2\u03bc\u0360\3\2\2\2\u03bc\u036e\3\2\2\2\u03bc\u037a")
-        buf.write("\3\2\2\2\u03bc\u0389\3\2\2\2\u03bc\u0398\3\2\2\2\u03bc")
-        buf.write("\u039e\3\2\2\2\u03bc\u03a3\3\2\2\2\u03bc\u03ab\3\2\2\2")
-        buf.write("\u03bd\f\3\2\2\2\u03be\u03c0\n\2\2\2\u03bf\u03be\3\2\2")
-        buf.write("\2\u03c0\u03c1\3\2\2\2\u03c1\u03bf\3\2\2\2\u03c1\u03c2")
-        buf.write("\3\2\2\2\u03c2\u03c6\3\2\2\2\u03c3\u03c5\7@\2\2\u03c4")
-        buf.write("\u03c3\3\2\2\2\u03c5\u03c8\3\2\2\2\u03c6\u03c7\3\2\2\2")
-        buf.write("\u03c6\u03c4\3\2\2\2\u03c7\u03ca\3\2\2\2\u03c8\u03c6\3")
-        buf.write("\2\2\2\u03c9\u03cb\n\3\2\2\u03ca\u03c9\3\2\2\2\u03cb\u03cc")
-        buf.write("\3\2\2\2\u03cc\u03ca\3\2\2\2\u03cc\u03cd\3\2\2\2\u03cd")
-        buf.write("\16\3\2\2\2\b\2\u01f8\u03bc\u03c1\u03c6\u03cc\2")
+        buf.write("\3\6\3\6\5\6\u03ce\n\6\3\7\7\7\u03d1\n\7\f\7\16\7\u03d4")
+        buf.write("\13\7\3\7\6\7\u03d7\n\7\r\7\16\7\u03d8\2\2\b\3\3\5\4\7")
+        buf.write("\5\t\6\13\7\r\b\3\2\4\3\2<<\4\2<<@@\2\u043a\2\3\3\2\2")
+        buf.write("\2\2\5\3\2\2\2\2\7\3\2\2\2\2\t\3\2\2\2\2\13\3\2\2\2\2")
+        buf.write("\r\3\2\2\2\3\17\3\2\2\2\5\21\3\2\2\2\7\24\3\2\2\2\t\u0209")
+        buf.write("\3\2\2\2\13\u03cd\3\2\2\2\r\u03d2\3\2\2\2\17\20\7@\2\2")
+        buf.write("\20\4\3\2\2\2\21\22\7<\2\2\22\23\7<\2\2\23\6\3\2\2\2\24")
+        buf.write("\25\7>\2\2\25\26\7%\2\2\26\27\7G\2\2\27\b\3\2\2\2\30\31")
+        buf.write("\7v\2\2\31\32\7c\2\2\32\33\7d\2\2\33\34\7n\2\2\34\u020a")
+        buf.write("\7g\2\2\35\36\7v\2\2\36\37\7q\2\2\37 \7r\2\2 !\7k\2\2")
+        buf.write("!\u020a\7e\2\2\"#\7e\2\2#$\7n\2\2$%\7c\2\2%&\7u\2\2&\'")
+        buf.write("\7u\2\2\'(\7k\2\2()\7h\2\2)*\7k\2\2*+\7e\2\2+,\7c\2\2")
+        buf.write(",-\7v\2\2-.\7k\2\2./\7q\2\2/\u020a\7p\2\2\60\61\7f\2\2")
+        buf.write("\61\62\7c\2\2\62\63\7u\2\2\63\64\7j\2\2\64\65\7d\2\2\65")
+        buf.write("\66\7q\2\2\66\67\7c\2\2\678\7t\2\28\u020a\7f\2\29:\7r")
+        buf.write("\2\2:;\7k\2\2;<\7r\2\2<=\7g\2\2=>\7n\2\2>?\7k\2\2?@\7")
+        buf.write("p\2\2@\u020a\7g\2\2AB\7f\2\2BC\7c\2\2CD\7v\2\2DE\7c\2")
+        buf.write("\2EF\7d\2\2FG\7c\2\2GH\7u\2\2H\u020a\7g\2\2IJ\7f\2\2J")
+        buf.write("K\7c\2\2KL\7v\2\2LM\7c\2\2MN\7d\2\2NO\7c\2\2OP\7u\2\2")
+        buf.write("PQ\7g\2\2QR\7U\2\2RS\7e\2\2ST\7j\2\2TU\7g\2\2UV\7o\2\2")
+        buf.write("V\u020a\7c\2\2WX\7i\2\2XY\7n\2\2YZ\7q\2\2Z[\7u\2\2[\\")
+        buf.write("\7u\2\2\\]\7c\2\2]^\7t\2\2^\u020a\7{\2\2_`\7i\2\2`a\7")
+        buf.write("n\2\2ab\7q\2\2bc\7u\2\2cd\7u\2\2de\7c\2\2ef\7t\2\2fg\7")
+        buf.write("{\2\2gh\7V\2\2hi\7g\2\2ij\7t\2\2j\u020a\7o\2\2kl\7f\2")
+        buf.write("\2lm\7c\2\2mn\7v\2\2no\7c\2\2op\7d\2\2pq\7c\2\2qr\7u\2")
+        buf.write("\2rs\7g\2\2st\7U\2\2tu\7g\2\2uv\7t\2\2vw\7x\2\2wx\7k\2")
+        buf.write("\2xy\7e\2\2y\u020a\7g\2\2z{\7o\2\2{|\7g\2\2|}\7u\2\2}")
+        buf.write("~\7u\2\2~\177\7c\2\2\177\u0080\7i\2\2\u0080\u0081\7k\2")
+        buf.write("\2\u0081\u0082\7p\2\2\u0082\u0083\7i\2\2\u0083\u0084\7")
+        buf.write("U\2\2\u0084\u0085\7g\2\2\u0085\u0086\7t\2\2\u0086\u0087")
+        buf.write("\7x\2\2\u0087\u0088\7k\2\2\u0088\u0089\7e\2\2\u0089\u020a")
+        buf.write("\7g\2\2\u008a\u008b\7o\2\2\u008b\u008c\7g\2\2\u008c\u008d")
+        buf.write("\7v\2\2\u008d\u008e\7c\2\2\u008e\u008f\7f\2\2\u008f\u0090")
+        buf.write("\7c\2\2\u0090\u0091\7v\2\2\u0091\u0092\7c\2\2\u0092\u0093")
+        buf.write("\7U\2\2\u0093\u0094\7g\2\2\u0094\u0095\7t\2\2\u0095\u0096")
+        buf.write("\7x\2\2\u0096\u0097\7k\2\2\u0097\u0098\7e\2\2\u0098\u020a")
+        buf.write("\7g\2\2\u0099\u009a\7f\2\2\u009a\u009b\7c\2\2\u009b\u009c")
+        buf.write("\7u\2\2\u009c\u009d\7j\2\2\u009d\u009e\7d\2\2\u009e\u009f")
+        buf.write("\7q\2\2\u009f\u00a0\7c\2\2\u00a0\u00a1\7t\2\2\u00a1\u00a2")
+        buf.write("\7f\2\2\u00a2\u00a3\7U\2\2\u00a3\u00a4\7g\2\2\u00a4\u00a5")
+        buf.write("\7t\2\2\u00a5\u00a6\7x\2\2\u00a6\u00a7\7k\2\2\u00a7\u00a8")
+        buf.write("\7e\2\2\u00a8\u020a\7g\2\2\u00a9\u00aa\7r\2\2\u00aa\u00ab")
+        buf.write("\7k\2\2\u00ab\u00ac\7r\2\2\u00ac\u00ad\7g\2\2\u00ad\u00ae")
+        buf.write("\7n\2\2\u00ae\u00af\7k\2\2\u00af\u00b0\7p\2\2\u00b0\u00b1")
+        buf.write("\7g\2\2\u00b1\u00b2\7U\2\2\u00b2\u00b3\7g\2\2\u00b3\u00b4")
+        buf.write("\7t\2\2\u00b4\u00b5\7x\2\2\u00b5\u00b6\7k\2\2\u00b6\u00b7")
+        buf.write("\7e\2\2\u00b7\u020a\7g\2\2\u00b8\u00b9\7o\2\2\u00b9\u00ba")
+        buf.write("\7n\2\2\u00ba\u00bb\7o\2\2\u00bb\u00bc\7q\2\2\u00bc\u00bd")
+        buf.write("\7f\2\2\u00bd\u00be\7g\2\2\u00be\u00bf\7n\2\2\u00bf\u00c0")
+        buf.write("\7U\2\2\u00c0\u00c1\7g\2\2\u00c1\u00c2\7t\2\2\u00c2\u00c3")
+        buf.write("\7x\2\2\u00c3\u00c4\7k\2\2\u00c4\u00c5\7e\2\2\u00c5\u020a")
+        buf.write("\7g\2\2\u00c6\u00c7\7u\2\2\u00c7\u00c8\7v\2\2\u00c8\u00c9")
+        buf.write("\7q\2\2\u00c9\u00ca\7t\2\2\u00ca\u00cb\7c\2\2\u00cb\u00cc")
+        buf.write("\7i\2\2\u00cc\u00cd\7g\2\2\u00cd\u00ce\7U\2\2\u00ce\u00cf")
+        buf.write("\7g\2\2\u00cf\u00d0\7t\2\2\u00d0\u00d1\7x\2\2\u00d1\u00d2")
+        buf.write("\7k\2\2\u00d2\u00d3\7e\2\2\u00d3\u020a\7g\2\2\u00d4\u00d5")
+        buf.write("\7u\2\2\u00d5\u00d6\7g\2\2\u00d6\u00d7\7c\2\2\u00d7\u00d8")
+        buf.write("\7t\2\2\u00d8\u00d9\7e\2\2\u00d9\u00da\7j\2\2\u00da\u00db")
+        buf.write("\7U\2\2\u00db\u00dc\7g\2\2\u00dc\u00dd\7t\2\2\u00dd\u00de")
+        buf.write("\7x\2\2\u00de\u00df\7k\2\2\u00df\u00e0\7e\2\2\u00e0\u020a")
+        buf.write("\7g\2\2\u00e1\u00e2\7y\2\2\u00e2\u00e3\7g\2\2\u00e3\u00e4")
+        buf.write("\7d\2\2\u00e4\u00e5\7j\2\2\u00e5\u00e6\7q\2\2\u00e6\u00e7")
+        buf.write("\7q\2\2\u00e7\u020a\7m\2\2\u00e8\u00e9\7o\2\2\u00e9\u00ea")
+        buf.write("\7n\2\2\u00ea\u00eb\7o\2\2\u00eb\u00ec\7q\2\2\u00ec\u00ed")
+        buf.write("\7f\2\2\u00ed\u00ee\7g\2\2\u00ee\u020a\7n\2\2\u00ef\u00f0")
+        buf.write("\7v\2\2\u00f0\u00f1\7{\2\2\u00f1\u00f2\7r\2\2\u00f2\u020a")
+        buf.write("\7g\2\2\u00f3\u00f4\7v\2\2\u00f4\u00f5\7g\2\2\u00f5\u00f6")
+        buf.write("\7c\2\2\u00f6\u020a\7o\2\2\u00f7\u00f8\7w\2\2\u00f8\u00f9")
+        buf.write("\7u\2\2\u00f9\u00fa\7g\2\2\u00fa\u020a\7t\2\2\u00fb\u00fc")
+        buf.write("\7d\2\2\u00fc\u00fd\7q\2\2\u00fd\u020a\7v\2\2\u00fe\u00ff")
+        buf.write("\7t\2\2\u00ff\u0100\7q\2\2\u0100\u0101\7n\2\2\u0101\u020a")
+        buf.write("\7g\2\2\u0102\u0103\7r\2\2\u0103\u0104\7q\2\2\u0104\u0105")
+        buf.write("\7n\2\2\u0105\u0106\7k\2\2\u0106\u0107\7e\2\2\u0107\u020a")
+        buf.write("\7{\2\2\u0108\u0109\7v\2\2\u0109\u010a\7g\2\2\u010a\u010b")
+        buf.write("\7u\2\2\u010b\u010c\7v\2\2\u010c\u010d\7U\2\2\u010d\u010e")
+        buf.write("\7w\2\2\u010e\u010f\7k\2\2\u010f\u0110\7v\2\2\u0110\u020a")
+        buf.write("\7g\2\2\u0111\u0112\7v\2\2\u0112\u0113\7g\2\2\u0113\u0114")
+        buf.write("\7u\2\2\u0114\u0115\7v\2\2\u0115\u0116\7E\2\2\u0116\u0117")
+        buf.write("\7c\2\2\u0117\u0118\7u\2\2\u0118\u020a\7g\2\2\u0119\u011a")
+        buf.write("\7f\2\2\u011a\u011b\7c\2\2\u011b\u011c\7v\2\2\u011c\u011d")
+        buf.write("\7c\2\2\u011d\u011e\7K\2\2\u011e\u011f\7p\2\2\u011f\u0120")
+        buf.write("\7u\2\2\u0120\u0121\7k\2\2\u0121\u0122\7i\2\2\u0122\u0123")
+        buf.write("\7j\2\2\u0123\u0124\7v\2\2\u0124\u0125\7E\2\2\u0125\u0126")
+        buf.write("\7j\2\2\u0126\u0127\7c\2\2\u0127\u0128\7t\2\2\u0128\u020a")
+        buf.write("\7v\2\2\u0129\u012a\7m\2\2\u012a\u012b\7r\2\2\u012b\u020a")
+        buf.write("\7k\2\2\u012c\u012d\7c\2\2\u012d\u012e\7n\2\2\u012e\u012f")
+        buf.write("\7g\2\2\u012f\u0130\7t\2\2\u0130\u020a\7v\2\2\u0131\u0132")
+        buf.write("\7e\2\2\u0132\u0133\7q\2\2\u0133\u0134\7p\2\2\u0134\u0135")
+        buf.write("\7v\2\2\u0135\u0136\7c\2\2\u0136\u0137\7k\2\2\u0137\u0138")
+        buf.write("\7p\2\2\u0138\u0139\7g\2\2\u0139\u020a\7t\2\2\u013a\u013b")
+        buf.write("\7v\2\2\u013b\u013c\7c\2\2\u013c\u020a\7i\2\2\u013d\u013e")
+        buf.write("\7f\2\2\u013e\u013f\7c\2\2\u013f\u0140\7u\2\2\u0140\u0141")
+        buf.write("\7j\2\2\u0141\u0142\7d\2\2\u0142\u0143\7q\2\2\u0143\u0144")
+        buf.write("\7c\2\2\u0144\u0145\7t\2\2\u0145\u0146\7f\2\2\u0146\u0147")
+        buf.write("\7F\2\2\u0147\u0148\7c\2\2\u0148\u0149\7v\2\2\u0149\u014a")
+        buf.write("\7c\2\2\u014a\u014b\7O\2\2\u014b\u014c\7q\2\2\u014c\u014d")
+        buf.write("\7f\2\2\u014d\u014e\7g\2\2\u014e\u020a\7n\2\2\u014f\u0150")
+        buf.write("\7u\2\2\u0150\u0151\7w\2\2\u0151\u0152\7d\2\2\u0152\u0153")
+        buf.write("\7u\2\2\u0153\u0154\7e\2\2\u0154\u0155\7t\2\2\u0155\u0156")
+        buf.write("\7k\2\2\u0156\u0157\7r\2\2\u0157\u0158\7v\2\2\u0158\u0159")
+        buf.write("\7k\2\2\u0159\u015a\7q\2\2\u015a\u020a\7p\2\2\u015b\u015c")
+        buf.write("\7e\2\2\u015c\u015d\7j\2\2\u015d\u015e\7c\2\2\u015e\u015f")
+        buf.write("\7t\2\2\u015f\u020a\7v\2\2\u0160\u0161\7f\2\2\u0161\u0162")
+        buf.write("\7q\2\2\u0162\u0163\7o\2\2\u0163\u0164\7c\2\2\u0164\u0165")
+        buf.write("\7k\2\2\u0165\u020a\7p\2\2\u0166\u0167\7f\2\2\u0167\u0168")
+        buf.write("\7c\2\2\u0168\u0169\7v\2\2\u0169\u016a\7c\2\2\u016a\u016b")
+        buf.write("\7R\2\2\u016b\u016c\7t\2\2\u016c\u016d\7q\2\2\u016d\u016e")
+        buf.write("\7f\2\2\u016e\u016f\7w\2\2\u016f\u0170\7e\2\2\u0170\u020a")
+        buf.write("\7v\2\2\u0171\u0172\7u\2\2\u0172\u0173\7c\2\2\u0173\u0174")
+        buf.write("\7o\2\2\u0174\u0175\7r\2\2\u0175\u0176\7n\2\2\u0176\u0177")
+        buf.write("\7g\2\2\u0177\u0178\7F\2\2\u0178\u0179\7c\2\2\u0179\u017a")
+        buf.write("\7v\2\2\u017a\u020a\7c\2\2\u017b\u017c\7u\2\2\u017c\u017d")
+        buf.write("\7v\2\2\u017d\u017e\7q\2\2\u017e\u017f\7t\2\2\u017f\u0180")
+        buf.write("\7g\2\2\u0180\u0181\7f\2\2\u0181\u0182\7R\2\2\u0182\u0183")
+        buf.write("\7t\2\2\u0183\u0184\7q\2\2\u0184\u0185\7e\2\2\u0185\u0186")
+        buf.write("\7g\2\2\u0186\u0187\7f\2\2\u0187\u0188\7w\2\2\u0188\u0189")
+        buf.write("\7t\2\2\u0189\u020a\7g\2\2\u018a\u018b\7u\2\2\u018b\u018c")
+        buf.write("\7g\2\2\u018c\u018d\7c\2\2\u018d\u018e\7t\2\2\u018e\u018f")
+        buf.write("\7e\2\2\u018f\u0190\7j\2\2\u0190\u0191\7K\2\2\u0191\u0192")
+        buf.write("\7p\2\2\u0192\u0193\7f\2\2\u0193\u0194\7g\2\2\u0194\u020a")
+        buf.write("\7z\2\2\u0195\u0196\7c\2\2\u0196\u0197\7r\2\2\u0197\u0198")
+        buf.write("\7r\2\2\u0198\u0199\7O\2\2\u0199\u019a\7c\2\2\u019a\u019b")
+        buf.write("\7t\2\2\u019b\u019c\7m\2\2\u019c\u019d\7g\2\2\u019d\u019e")
+        buf.write("\7v\2\2\u019e\u019f\7R\2\2\u019f\u01a0\7n\2\2\u01a0\u01a1")
+        buf.write("\7c\2\2\u01a1\u01a2\7e\2\2\u01a2\u01a3\7g\2\2\u01a3\u01a4")
+        buf.write("\7F\2\2\u01a4\u01a5\7g\2\2\u01a5\u01a6\7h\2\2\u01a6\u01a7")
+        buf.write("\7k\2\2\u01a7\u01a8\7p\2\2\u01a8\u01a9\7k\2\2\u01a9\u01aa")
+        buf.write("\7v\2\2\u01aa\u01ab\7k\2\2\u01ab\u01ac\7q\2\2\u01ac\u020a")
+        buf.write("\7p\2\2\u01ad\u01ae\7c\2\2\u01ae\u01af\7r\2\2\u01af\u020a")
+        buf.write("\7r\2\2\u01b0\u01b1\7r\2\2\u01b1\u01b2\7g\2\2\u01b2\u01b3")
+        buf.write("\7t\2\2\u01b3\u01b4\7u\2\2\u01b4\u01b5\7q\2\2\u01b5\u01b6")
+        buf.write("\7p\2\2\u01b6\u020a\7c\2\2\u01b7\u01b8\7f\2\2\u01b8\u01b9")
+        buf.write("\7q\2\2\u01b9\u01ba\7e\2\2\u01ba\u01bb\7U\2\2\u01bb\u01bc")
+        buf.write("\7v\2\2\u01bc\u01bd\7q\2\2\u01bd\u01be\7t\2\2\u01be\u020a")
+        buf.write("\7g\2\2\u01bf\u01c0\7r\2\2\u01c0\u01c1\7c\2\2\u01c1\u01c2")
+        buf.write("\7i\2\2\u01c2\u020a\7g\2\2\u01c3\u01c4\7M\2\2\u01c4\u01c5")
+        buf.write("\7p\2\2\u01c5\u01c6\7q\2\2\u01c6\u01c7\7y\2\2\u01c7\u01c8")
+        buf.write("\7N\2\2\u01c8\u01c9\7g\2\2\u01c9\u01ca\7f\2\2\u01ca\u01cb")
+        buf.write("\7i\2\2\u01cb\u01cc\7g\2\2\u01cc\u01cd\7R\2\2\u01cd\u01ce")
+        buf.write("\7c\2\2\u01ce\u01cf\7p\2\2\u01cf\u01d0\7g\2\2\u01d0\u01d1")
+        buf.write("\7n\2\2\u01d1\u020a\7u\2\2\u01d2\u01d3\7i\2\2\u01d3\u01d4")
+        buf.write("\7q\2\2\u01d4\u01d5\7x\2\2\u01d5\u01d6\7g\2\2\u01d6\u01d7")
+        buf.write("\7t\2\2\u01d7\u020a\7p\2\2\u01d8\u01d9\7c\2\2\u01d9\u01da")
+        buf.write("\7n\2\2\u01da\u020a\7n\2\2\u01db\u01dc\7e\2\2\u01dc\u01dd")
+        buf.write("\7w\2\2\u01dd\u01de\7u\2\2\u01de\u01df\7v\2\2\u01df\u01e0")
+        buf.write("\7q\2\2\u01e0\u01e1\7o\2\2\u01e1\u01e2\7O\2\2\u01e2\u01e3")
+        buf.write("\7g\2\2\u01e3\u01e4\7v\2\2\u01e4\u01e5\7t\2\2\u01e5\u01e6")
+        buf.write("\7k\2\2\u01e6\u020a\7e\2\2\u01e7\u01e8\7g\2\2\u01e8\u01e9")
+        buf.write("\7x\2\2\u01e9\u01ea\7g\2\2\u01ea\u01eb\7p\2\2\u01eb\u01ec")
+        buf.write("\7v\2\2\u01ec\u01ed\7u\2\2\u01ed\u01ee\7w\2\2\u01ee\u01ef")
+        buf.write("\7d\2\2\u01ef\u01f0\7u\2\2\u01f0\u01f1\7e\2\2\u01f1\u01f2")
+        buf.write("\7t\2\2\u01f2\u01f3\7k\2\2\u01f3\u01f4\7r\2\2\u01f4\u01f5")
+        buf.write("\7v\2\2\u01f5\u01f6\7k\2\2\u01f6\u01f7\7q\2\2\u01f7\u020a")
+        buf.write("\7p\2\2\u01f8\u01f9\7k\2\2\u01f9\u01fa\7p\2\2\u01fa\u01fb")
+        buf.write("\7i\2\2\u01fb\u01fc\7g\2\2\u01fc\u01fd\7u\2\2\u01fd\u01fe")
+        buf.write("\7v\2\2\u01fe\u01ff\7k\2\2\u01ff\u0200\7q\2\2\u0200\u0201")
+        buf.write("\7p\2\2\u0201\u0202\7R\2\2\u0202\u0203\7k\2\2\u0203\u0204")
+        buf.write("\7r\2\2\u0204\u0205\7g\2\2\u0205\u0206\7n\2\2\u0206\u0207")
+        buf.write("\7k\2\2\u0207\u0208\7p\2\2\u0208\u020a\7g\2\2\u0209\30")
+        buf.write("\3\2\2\2\u0209\35\3\2\2\2\u0209\"\3\2\2\2\u0209\60\3\2")
+        buf.write("\2\2\u02099\3\2\2\2\u0209A\3\2\2\2\u0209I\3\2\2\2\u0209")
+        buf.write("W\3\2\2\2\u0209_\3\2\2\2\u0209k\3\2\2\2\u0209z\3\2\2\2")
+        buf.write("\u0209\u008a\3\2\2\2\u0209\u0099\3\2\2\2\u0209\u00a9\3")
+        buf.write("\2\2\2\u0209\u00b8\3\2\2\2\u0209\u00c6\3\2\2\2\u0209\u00d4")
+        buf.write("\3\2\2\2\u0209\u00e1\3\2\2\2\u0209\u00e8\3\2\2\2\u0209")
+        buf.write("\u00ef\3\2\2\2\u0209\u00f3\3\2\2\2\u0209\u00f7\3\2\2\2")
+        buf.write("\u0209\u00fb\3\2\2\2\u0209\u00fe\3\2\2\2\u0209\u0102\3")
+        buf.write("\2\2\2\u0209\u0108\3\2\2\2\u0209\u0111\3\2\2\2\u0209\u0119")
+        buf.write("\3\2\2\2\u0209\u0129\3\2\2\2\u0209\u012c\3\2\2\2\u0209")
+        buf.write("\u0131\3\2\2\2\u0209\u013a\3\2\2\2\u0209\u013d\3\2\2\2")
+        buf.write("\u0209\u014f\3\2\2\2\u0209\u015b\3\2\2\2\u0209\u0160\3")
+        buf.write("\2\2\2\u0209\u0166\3\2\2\2\u0209\u0171\3\2\2\2\u0209\u017b")
+        buf.write("\3\2\2\2\u0209\u018a\3\2\2\2\u0209\u0195\3\2\2\2\u0209")
+        buf.write("\u01ad\3\2\2\2\u0209\u01b0\3\2\2\2\u0209\u01b7\3\2\2\2")
+        buf.write("\u0209\u01bf\3\2\2\2\u0209\u01c3\3\2\2\2\u0209\u01d2\3")
+        buf.write("\2\2\2\u0209\u01d8\3\2\2\2\u0209\u01db\3\2\2\2\u0209\u01e7")
+        buf.write("\3\2\2\2\u0209\u01f8\3\2\2\2\u020a\n\3\2\2\2\u020b\u020c")
+        buf.write("\7f\2\2\u020c\u020d\7g\2\2\u020d\u020e\7u\2\2\u020e\u020f")
+        buf.write("\7e\2\2\u020f\u0210\7t\2\2\u0210\u0211\7k\2\2\u0211\u0212")
+        buf.write("\7r\2\2\u0212\u0213\7v\2\2\u0213\u0214\7k\2\2\u0214\u0215")
+        buf.write("\7q\2\2\u0215\u03ce\7p\2\2\u0216\u0217\7e\2\2\u0217\u0218")
+        buf.write("\7q\2\2\u0218\u0219\7n\2\2\u0219\u021a\7w\2\2\u021a\u021b")
+        buf.write("\7o\2\2\u021b\u021c\7p\2\2\u021c\u03ce\7u\2\2\u021d\u021e")
+        buf.write("\7u\2\2\u021e\u021f\7e\2\2\u021f\u0220\7j\2\2\u0220\u0221")
+        buf.write("\7g\2\2\u0221\u0222\7o\2\2\u0222\u0223\7c\2\2\u0223\u0224")
+        buf.write("\7H\2\2\u0224\u0225\7k\2\2\u0225\u0226\7g\2\2\u0226\u0227")
+        buf.write("\7n\2\2\u0227\u0228\7f\2\2\u0228\u03ce\7u\2\2\u0229\u022a")
+        buf.write("\7v\2\2\u022a\u022b\7c\2\2\u022b\u022c\7i\2\2\u022c\u03ce")
+        buf.write("\7u\2\2\u022d\u022e\7v\2\2\u022e\u022f\7c\2\2\u022f\u0230")
+        buf.write("\7u\2\2\u0230\u0231\7m\2\2\u0231\u03ce\7u\2\2\u0232\u0233")
+        buf.write("\7o\2\2\u0233\u0234\7n\2\2\u0234\u0235\7H\2\2\u0235\u0236")
+        buf.write("\7g\2\2\u0236\u0237\7c\2\2\u0237\u0238\7v\2\2\u0238\u0239")
+        buf.write("\7w\2\2\u0239\u023a\7t\2\2\u023a\u023b\7g\2\2\u023b\u03ce")
+        buf.write("\7u\2\2\u023c\u023d\7u\2\2\u023d\u023e\7e\2\2\u023e\u023f")
+        buf.write("\7j\2\2\u023f\u0240\7g\2\2\u0240\u0241\7o\2\2\u0241\u0242")
+        buf.write("\7c\2\2\u0242\u0243\7V\2\2\u0243\u0244\7g\2\2\u0244\u0245")
+        buf.write("\7z\2\2\u0245\u03ce\7v\2\2\u0246\u0247\7q\2\2\u0247\u0248")
+        buf.write("\7y\2\2\u0248\u0249\7p\2\2\u0249\u024a\7g\2\2\u024a\u03ce")
+        buf.write("\7t\2\2\u024b\u024c\7t\2\2\u024c\u024d\7g\2\2\u024d\u024e")
+        buf.write("\7x\2\2\u024e\u024f\7k\2\2\u024f\u0250\7g\2\2\u0250\u0251")
+        buf.write("\7y\2\2\u0251\u0252\7g\2\2\u0252\u0253\7t\2\2\u0253\u03ce")
+        buf.write("\7u\2\2\u0254\u0255\7u\2\2\u0255\u0256\7{\2\2\u0256\u0257")
+        buf.write("\7p\2\2\u0257\u0258\7q\2\2\u0258\u0259\7p\2\2\u0259\u025a")
+        buf.write("\7{\2\2\u025a\u025b\7o\2\2\u025b\u03ce\7u\2\2\u025c\u025d")
+        buf.write("\7t\2\2\u025d\u025e\7g\2\2\u025e\u025f\7n\2\2\u025f\u0260")
+        buf.write("\7c\2\2\u0260\u0261\7v\2\2\u0261\u0262\7g\2\2\u0262\u0263")
+        buf.write("\7f\2\2\u0263\u0264\7V\2\2\u0264\u0265\7g\2\2\u0265\u0266")
+        buf.write("\7t\2\2\u0266\u0267\7o\2\2\u0267\u03ce\7u\2\2\u0268\u0269")
+        buf.write("\7t\2\2\u0269\u026a\7g\2\2\u026a\u026b\7h\2\2\u026b\u026c")
+        buf.write("\7g\2\2\u026c\u026d\7t\2\2\u026d\u026e\7g\2\2\u026e\u026f")
+        buf.write("\7p\2\2\u026f\u0270\7e\2\2\u0270\u0271\7g\2\2\u0271\u03ce")
+        buf.write("\7u\2\2\u0272\u0273\7g\2\2\u0273\u0274\7z\2\2\u0274\u0275")
+        buf.write("\7v\2\2\u0275\u0276\7g\2\2\u0276\u0277\7p\2\2\u0277\u0278")
+        buf.write("\7u\2\2\u0278\u0279\7k\2\2\u0279\u027a\7q\2\2\u027a\u03ce")
+        buf.write("\7p\2\2\u027b\u027c\7f\2\2\u027c\u027d\7k\2\2\u027d\u027e")
+        buf.write("\7u\2\2\u027e\u027f\7r\2\2\u027f\u0280\7n\2\2\u0280\u0281")
+        buf.write("\7c\2\2\u0281\u0282\7{\2\2\u0282\u0283\7P\2\2\u0283\u0284")
+        buf.write("\7c\2\2\u0284\u0285\7o\2\2\u0285\u03ce\7g\2\2\u0286\u0287")
+        buf.write("\7p\2\2\u0287\u0288\7c\2\2\u0288\u0289\7o\2\2\u0289\u03ce")
+        buf.write("\7g\2\2\u028a\u028b\7o\2\2\u028b\u028c\7g\2\2\u028c\u028d")
+        buf.write("\7u\2\2\u028d\u028e\7u\2\2\u028e\u028f\7c\2\2\u028f\u0290")
+        buf.write("\7i\2\2\u0290\u0291\7g\2\2\u0291\u0292\7U\2\2\u0292\u0293")
+        buf.write("\7e\2\2\u0293\u0294\7j\2\2\u0294\u0295\7g\2\2\u0295\u0296")
+        buf.write("\7o\2\2\u0296\u03ce\7c\2\2\u0297\u0298\7e\2\2\u0298\u0299")
+        buf.write("\7j\2\2\u0299\u029a\7c\2\2\u029a\u029b\7t\2\2\u029b\u029c")
+        buf.write("\7v\2\2\u029c\u03ce\7u\2\2\u029d\u029e\7f\2\2\u029e\u029f")
+        buf.write("\7c\2\2\u029f\u02a0\7v\2\2\u02a0\u02a1\7c\2\2\u02a1\u02a2")
+        buf.write("\7O\2\2\u02a2\u02a3\7q\2\2\u02a3\u02a4\7f\2\2\u02a4\u02a5")
+        buf.write("\7g\2\2\u02a5\u03ce\7n\2\2\u02a6\u02a7\7e\2\2\u02a7\u02a8")
+        buf.write("\7q\2\2\u02a8\u02a9\7p\2\2\u02a9\u02aa\7u\2\2\u02aa\u02ab")
+        buf.write("\7v\2\2\u02ab\u02ac\7t\2\2\u02ac\u02ad\7c\2\2\u02ad\u02ae")
+        buf.write("\7k\2\2\u02ae\u02af\7p\2\2\u02af\u03ce\7v\2\2\u02b0\u02b1")
+        buf.write("\7v\2\2\u02b1\u02b2\7c\2\2\u02b2\u02b3\7d\2\2\u02b3\u02b4")
+        buf.write("\7n\2\2\u02b4\u02b5\7g\2\2\u02b5\u02b6\7E\2\2\u02b6\u02b7")
+        buf.write("\7q\2\2\u02b7\u02b8\7p\2\2\u02b8\u02b9\7u\2\2\u02b9\u02ba")
+        buf.write("\7v\2\2\u02ba\u02bb\7t\2\2\u02bb\u02bc\7c\2\2\u02bc\u02bd")
+        buf.write("\7k\2\2\u02bd\u02be\7p\2\2\u02be\u02bf\7v\2\2\u02bf\u03ce")
+        buf.write("\7u\2\2\u02c0\u02c1\7r\2\2\u02c1\u02c2\7c\2\2\u02c2\u02c3")
+        buf.write("\7t\2\2\u02c3\u02c4\7v\2\2\u02c4\u02c5\7k\2\2\u02c5\u02c6")
+        buf.write("\7v\2\2\u02c6\u02c7\7k\2\2\u02c7\u02c8\7q\2\2\u02c8\u02c9")
+        buf.write("\7p\2\2\u02c9\u03ce\7u\2\2\u02ca\u02cb\7t\2\2\u02cb\u02cc")
+        buf.write("\7g\2\2\u02cc\u02cd\7r\2\2\u02cd\u02ce\7n\2\2\u02ce\u02cf")
+        buf.write("\7k\2\2\u02cf\u02d0\7e\2\2\u02d0\u02d1\7c\2\2\u02d1\u02d2")
+        buf.write("\7v\2\2\u02d2\u02d3\7k\2\2\u02d3\u02d4\7q\2\2\u02d4\u02d5")
+        buf.write("\7p\2\2\u02d5\u02d6\7H\2\2\u02d6\u02d7\7c\2\2\u02d7\u02d8")
+        buf.write("\7e\2\2\u02d8\u02d9\7v\2\2\u02d9\u02da\7q\2\2\u02da\u03ce")
+        buf.write("\7t\2\2\u02db\u02dc\7u\2\2\u02dc\u02dd\7q\2\2\u02dd\u02de")
+        buf.write("\7w\2\2\u02de\u02df\7t\2\2\u02df\u02e0\7e\2\2\u02e0\u02e1")
+        buf.write("\7g\2\2\u02e1\u02e2\7W\2\2\u02e2\u02e3\7t\2\2\u02e3\u03ce")
+        buf.write("\7n\2\2\u02e4\u02e5\7o\2\2\u02e5\u02e6\7w\2\2\u02e6\u02e7")
+        buf.write("\7v\2\2\u02e7\u02e8\7w\2\2\u02e8\u02e9\7c\2\2\u02e9\u02ea")
+        buf.write("\7n\2\2\u02ea\u02eb\7n\2\2\u02eb\u02ec\7{\2\2\u02ec\u02ed")
+        buf.write("\7G\2\2\u02ed\u02ee\7z\2\2\u02ee\u02ef\7e\2\2\u02ef\u02f0")
+        buf.write("\7n\2\2\u02f0\u02f1\7w\2\2\u02f1\u02f2\7u\2\2\u02f2\u02f3")
+        buf.write("\7k\2\2\u02f3\u02f4\7x\2\2\u02f4\u03ce\7g\2\2\u02f5\u02f6")
+        buf.write("\7g\2\2\u02f6\u02f7\7z\2\2\u02f7\u02f8\7r\2\2\u02f8\u02f9")
+        buf.write("\7g\2\2\u02f9\u02fa\7t\2\2\u02fa\u02fb\7v\2\2\u02fb\u03ce")
+        buf.write("\7u\2\2\u02fc\u02fd\7h\2\2\u02fd\u02fe\7k\2\2\u02fe\u02ff")
+        buf.write("\7g\2\2\u02ff\u0300\7n\2\2\u0300\u0301\7f\2\2\u0301\u03ce")
+        buf.write("\7u\2\2\u0302\u0303\7h\2\2\u0303\u0304\7q\2\2\u0304\u0305")
+        buf.write("\7n\2\2\u0305\u0306\7n\2\2\u0306\u0307\7q\2\2\u0307\u0308")
+        buf.write("\7y\2\2\u0308\u0309\7g\2\2\u0309\u030a\7t\2\2\u030a\u03ce")
+        buf.write("\7u\2\2\u030b\u030c\7c\2\2\u030c\u030d\7r\2\2\u030d\u030e")
+        buf.write("\7r\2\2\u030e\u030f\7E\2\2\u030f\u0310\7q\2\2\u0310\u0311")
+        buf.write("\7p\2\2\u0311\u0312\7h\2\2\u0312\u0313\7k\2\2\u0313\u0314")
+        buf.write("\7i\2\2\u0314\u0315\7w\2\2\u0315\u0316\7t\2\2\u0316\u0317")
+        buf.write("\7c\2\2\u0317\u0318\7v\2\2\u0318\u0319\7k\2\2\u0319\u031a")
+        buf.write("\7q\2\2\u031a\u03ce\7p\2\2\u031b\u031c\7c\2\2\u031c\u031d")
+        buf.write("\7r\2\2\u031d\u031e\7r\2\2\u031e\u031f\7U\2\2\u031f\u0320")
+        buf.write("\7e\2\2\u0320\u0321\7j\2\2\u0321\u0322\7g\2\2\u0322\u0323")
+        buf.write("\7f\2\2\u0323\u0324\7w\2\2\u0324\u0325\7n\2\2\u0325\u03ce")
+        buf.write("\7g\2\2\u0326\u0327\7x\2\2\u0327\u0328\7q\2\2\u0328\u0329")
+        buf.write("\7v\2\2\u0329\u032a\7g\2\2\u032a\u03ce\7u\2\2\u032b\u032c")
+        buf.write("\7r\2\2\u032c\u032d\7t\2\2\u032d\u032e\7q\2\2\u032e\u032f")
+        buf.write("\7h\2\2\u032f\u0330\7k\2\2\u0330\u0331\7n\2\2\u0331\u03ce")
+        buf.write("\7g\2\2\u0332\u0333\7t\2\2\u0333\u0334\7q\2\2\u0334\u0335")
+        buf.write("\7n\2\2\u0335\u0336\7g\2\2\u0336\u03ce\7u\2\2\u0337\u0338")
+        buf.write("\7f\2\2\u0338\u0339\7g\2\2\u0339\u033a\7n\2\2\u033a\u033b")
+        buf.write("\7g\2\2\u033b\u033c\7v\2\2\u033c\u033d\7g\2\2\u033d\u03ce")
+        buf.write("\7f\2\2\u033e\u033f\7n\2\2\u033f\u0340\7k\2\2\u0340\u0341")
+        buf.write("\7h\2\2\u0341\u0342\7g\2\2\u0342\u0343\7E\2\2\u0343\u0344")
+        buf.write("\7{\2\2\u0344\u0345\7e\2\2\u0345\u0346\7n\2\2\u0346\u03ce")
+        buf.write("\7g\2\2\u0347\u0348\7c\2\2\u0348\u0349\7r\2\2\u0349\u034a")
+        buf.write("\7k\2\2\u034a\u034b\7a\2\2\u034b\u034c\7e\2\2\u034c\u034d")
+        buf.write("\7n\2\2\u034d\u034e\7k\2\2\u034e\u034f\7g\2\2\u034f\u0350")
+        buf.write("\7p\2\2\u0350\u0351\7v\2\2\u0351\u0352\7a\2\2\u0352\u0353")
+        buf.write("\7k\2\2\u0353\u03ce\7f\2\2\u0354\u0355\7u\2\2\u0355\u0356")
+        buf.write("\7q\2\2\u0356\u0357\7w\2\2\u0357\u0358\7t\2\2\u0358\u0359")
+        buf.write("\7e\2\2\u0359\u035a\7g\2\2\u035a\u035b\7J\2\2\u035b\u035c")
+        buf.write("\7c\2\2\u035c\u035d\7u\2\2\u035d\u03ce\7j\2\2\u035e\u035f")
+        buf.write("\7v\2\2\u035f\u0360\7g\2\2\u0360\u0361\7u\2\2\u0361\u0362")
+        buf.write("\7v\2\2\u0362\u0363\7E\2\2\u0363\u0364\7c\2\2\u0364\u0365")
+        buf.write("\7u\2\2\u0365\u0366\7g\2\2\u0366\u0367\7T\2\2\u0367\u0368")
+        buf.write("\7g\2\2\u0368\u0369\7u\2\2\u0369\u036a\7w\2\2\u036a\u036b")
+        buf.write("\7n\2\2\u036b\u03ce\7v\2\2\u036c\u036d\7v\2\2\u036d\u036e")
+        buf.write("\7g\2\2\u036e\u036f\7u\2\2\u036f\u0370\7v\2\2\u0370\u03ce")
+        buf.write("\7u\2\2\u0371\u0372\7r\2\2\u0372\u0373\7k\2\2\u0373\u0374")
+        buf.write("\7r\2\2\u0374\u0375\7g\2\2\u0375\u0376\7n\2\2\u0376\u0377")
+        buf.write("\7k\2\2\u0377\u0378\7p\2\2\u0378\u0379\7g\2\2\u0379\u037a")
+        buf.write("\7U\2\2\u037a\u037b\7v\2\2\u037b\u037c\7c\2\2\u037c\u037d")
+        buf.write("\7v\2\2\u037d\u037e\7w\2\2\u037e\u03ce\7u\2\2\u037f\u0380")
+        buf.write("\7f\2\2\u0380\u0381\7c\2\2\u0381\u0382\7v\2\2\u0382\u0383")
+        buf.write("\7c\2\2\u0383\u0384\7R\2\2\u0384\u0385\7t\2\2\u0385\u0386")
+        buf.write("\7q\2\2\u0386\u0387\7f\2\2\u0387\u0388\7w\2\2\u0388\u0389")
+        buf.write("\7e\2\2\u0389\u038a\7v\2\2\u038a\u03ce\7u\2\2\u038b\u038c")
+        buf.write("\7r\2\2\u038c\u038d\7c\2\2\u038d\u038e\7t\2\2\u038e\u038f")
+        buf.write("\7c\2\2\u038f\u0390\7o\2\2\u0390\u0391\7g\2\2\u0391\u0392")
+        buf.write("\7v\2\2\u0392\u0393\7g\2\2\u0393\u0394\7t\2\2\u0394\u0395")
+        buf.write("\7X\2\2\u0395\u0396\7c\2\2\u0396\u0397\7n\2\2\u0397\u0398")
+        buf.write("\7w\2\2\u0398\u0399\7g\2\2\u0399\u03ce\7u\2\2\u039a\u039b")
+        buf.write("\7t\2\2\u039b\u039c\7g\2\2\u039c\u039d\7v\2\2\u039d\u039e")
+        buf.write("\7g\2\2\u039e\u039f\7p\2\2\u039f\u03a0\7v\2\2\u03a0\u03a1")
+        buf.write("\7k\2\2\u03a1\u03a2\7q\2\2\u03a2\u03a3\7p\2\2\u03a3\u03a4")
+        buf.write("\7R\2\2\u03a4\u03a5\7g\2\2\u03a5\u03a6\7t\2\2\u03a6\u03a7")
+        buf.write("\7k\2\2\u03a7\u03a8\7q\2\2\u03a8\u03ce\7f\2\2\u03a9\u03aa")
+        buf.write("\7r\2\2\u03aa\u03ab\7c\2\2\u03ab\u03ac\7t\2\2\u03ac\u03ad")
+        buf.write("\7g\2\2\u03ad\u03ae\7p\2\2\u03ae\u03ce\7v\2\2\u03af\u03b0")
+        buf.write("\7v\2\2\u03b0\u03b1\7g\2\2\u03b1\u03b2\7c\2\2\u03b2\u03b3")
+        buf.write("\7o\2\2\u03b3\u03ce\7u\2\2\u03b4\u03b5\7r\2\2\u03b5\u03b6")
+        buf.write("\7g\2\2\u03b6\u03b7\7t\2\2\u03b7\u03b8\7u\2\2\u03b8\u03b9")
+        buf.write("\7q\2\2\u03b9\u03ba\7p\2\2\u03ba\u03bb\7c\2\2\u03bb\u03ce")
+        buf.write("\7u\2\2\u03bc\u03bd\7k\2\2\u03bd\u03be\7p\2\2\u03be\u03bf")
+        buf.write("\7i\2\2\u03bf\u03c0\7g\2\2\u03c0\u03c1\7u\2\2\u03c1\u03c2")
+        buf.write("\7v\2\2\u03c2\u03c3\7k\2\2\u03c3\u03c4\7q\2\2\u03c4\u03c5")
+        buf.write("\7p\2\2\u03c5\u03c6\7R\2\2\u03c6\u03c7\7k\2\2\u03c7\u03c8")
+        buf.write("\7r\2\2\u03c8\u03c9\7g\2\2\u03c9\u03ca\7n\2\2\u03ca\u03cb")
+        buf.write("\7k\2\2\u03cb\u03cc\7p\2\2\u03cc\u03ce\7g\2\2\u03cd\u020b")
+        buf.write("\3\2\2\2\u03cd\u0216\3\2\2\2\u03cd\u021d\3\2\2\2\u03cd")
+        buf.write("\u0229\3\2\2\2\u03cd\u022d\3\2\2\2\u03cd\u0232\3\2\2\2")
+        buf.write("\u03cd\u023c\3\2\2\2\u03cd\u0246\3\2\2\2\u03cd\u024b\3")
+        buf.write("\2\2\2\u03cd\u0254\3\2\2\2\u03cd\u025c\3\2\2\2\u03cd\u0268")
+        buf.write("\3\2\2\2\u03cd\u0272\3\2\2\2\u03cd\u027b\3\2\2\2\u03cd")
+        buf.write("\u0286\3\2\2\2\u03cd\u028a\3\2\2\2\u03cd\u0297\3\2\2\2")
+        buf.write("\u03cd\u029d\3\2\2\2\u03cd\u02a6\3\2\2\2\u03cd\u02b0\3")
+        buf.write("\2\2\2\u03cd\u02c0\3\2\2\2\u03cd\u02ca\3\2\2\2\u03cd\u02db")
+        buf.write("\3\2\2\2\u03cd\u02e4\3\2\2\2\u03cd\u02f5\3\2\2\2\u03cd")
+        buf.write("\u02fc\3\2\2\2\u03cd\u0302\3\2\2\2\u03cd\u030b\3\2\2\2")
+        buf.write("\u03cd\u031b\3\2\2\2\u03cd\u0326\3\2\2\2\u03cd\u032b\3")
+        buf.write("\2\2\2\u03cd\u0332\3\2\2\2\u03cd\u0337\3\2\2\2\u03cd\u033e")
+        buf.write("\3\2\2\2\u03cd\u0347\3\2\2\2\u03cd\u0354\3\2\2\2\u03cd")
+        buf.write("\u035e\3\2\2\2\u03cd\u036c\3\2\2\2\u03cd\u0371\3\2\2\2")
+        buf.write("\u03cd\u037f\3\2\2\2\u03cd\u038b\3\2\2\2\u03cd\u039a\3")
+        buf.write("\2\2\2\u03cd\u03a9\3\2\2\2\u03cd\u03af\3\2\2\2\u03cd\u03b4")
+        buf.write("\3\2\2\2\u03cd\u03bc\3\2\2\2\u03ce\f\3\2\2\2\u03cf\u03d1")
+        buf.write("\n\2\2\2\u03d0\u03cf\3\2\2\2\u03d1\u03d4\3\2\2\2\u03d2")
+        buf.write("\u03d0\3\2\2\2\u03d2\u03d3\3\2\2\2\u03d3\u03d6\3\2\2\2")
+        buf.write("\u03d4\u03d2\3\2\2\2\u03d5\u03d7\n\3\2\2\u03d6\u03d5\3")
+        buf.write("\2\2\2\u03d7\u03d8\3\2\2\2\u03d8\u03d6\3\2\2\2\u03d8\u03d9")
+        buf.write("\3\2\2\2\u03d9\16\3\2\2\2\7\2\u0209\u03cd\u03d2\u03d8")
+        buf.write("\2")
         return buf.getvalue()
 
 
 class EntityLinkLexer(Lexer):
 
     atn = ATNDeserializer().deserialize(serializedATN())
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/EntityLinkListener.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/EntityLinkListener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/EntityLinkParser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/EntityLinkParser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/FqnLexer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/FqnLexer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/FqnListener.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/FqnListener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/FqnParser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/FqnParser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/JdbcUriLexer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/JdbcUriLexer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/JdbcUriListener.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/JdbcUriListener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/antlr/JdbcUriParser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/antlr/JdbcUriParser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/basic.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/basic.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/basic.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportData.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportData.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportData.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/aggregatedCostAnalysisReportData.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/aggregatedCostAnalysisReportData.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/aggregatedCostAnalysisReportData.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/entityReportData.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/rawCostAnalysisReportData.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/rawCostAnalysisReportData.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/rawCostAnalysisReportData.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/webAnalyticEntityViewReportData.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/webAnalyticUserActivityReportData.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEvent.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEvent.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEvent.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEventData.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEventData.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEventData.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEventType/customEvent.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEventType/pageViewEvent.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/addGlossaryToAssetsRequest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/addGlossaryToAssetsRequest.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/addGlossaryToAssetsRequest.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/analytics/createWebAnalyticEvent.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/automations/createWorkflow.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/automations/createWorkflow.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/automations/createWorkflow.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/bulkAssets.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/bulkAssets.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/bulkAssets.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/classification/createClassification.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/classification/createClassification.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 # generated by datamodel-codegen:
 #   filename:  api/classification/createClassification.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...entity.classification import tag
 from ...type import basic
 
 
 class CreateClassificationRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: tag.TagName
+    name: basic.EntityName
     displayName: Optional[str] = Field(
         None, description='Display Name that identifies this classification.'
     )
     description: basic.Markdown = Field(
         ..., description='Description of the classification.'
     )
     provider: Optional[basic.ProviderType] = basic.ProviderType.user
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/classification/createTag.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/classification/createTag.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,33 +1,32 @@
 # generated by datamodel-codegen:
 #   filename:  api/classification/createTag.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...entity.classification import tag
 from ...type import basic
 
 
 class CreateTagRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
     classification: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='Name of the classification that this tag is part of.'
     )
     parent: Optional[basic.FullyQualifiedEntityName] = Field(
         None,
         description='Fully qualified name of the parent tag. When null, the term is at the root of the classification.',
     )
-    name: tag.TagName
+    name: basic.EntityName
     displayName: Optional[str] = Field(
         None, description='Display Name that identifies this tag.'
     )
     description: basic.Markdown = Field(
         ..., description='Unique name of the classification'
     )
     style: Optional[basic.Style] = None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/classification/loadTags.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/eventSubscriptionOffset.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 # generated by datamodel-codegen:
-#   filename:  api/classification/loadTags.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  events/eventSubscriptionOffset.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
-from typing import List, Optional
+from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from . import createClassification, createTag
+from ..type import basic
 
 
-class LoadTags(BaseModel):
+class EventSubscriptionOffset(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    createClassification: createClassification.CreateClassificationRequest
-    createTags: Optional[List[createTag.CreateTagRequest]] = Field(None, min_items=1)
+    offset: int = Field(..., description='Name of this Event Filter.')
+    timestamp: Optional[basic.Timestamp] = Field(
+        None, description='Update time of the job status.'
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/createBot.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/createBot.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/createBot.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/createEventPublisherJob.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/createEventPublisherJob.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/createEventPublisherJob.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/createType.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/createType.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/createType.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createChart.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createChart.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createChart.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createContainer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createContainer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createContainer.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
 
@@ -58,14 +58,17 @@
     extension: Optional[basic.EntityExtension] = Field(
         None,
         description='Entity extension data with custom attributes added to the entity.',
     )
     sourceUrl: Optional[basic.SourceUrl] = Field(
         None, description='Source URL of container.'
     )
+    fullPath: Optional[str] = Field(
+        None, description='Full path of the container/file.'
+    )
     domain: Optional[str] = Field(
         None, description='Fully qualified name of the domain the Container belongs to.'
     )
     dataProducts: Optional[List[basic.FullyQualifiedEntityName]] = Field(
         None,
         description='List of fully qualified names of data products this entity is part of.',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createCustomProperty.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createCustomProperty.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createCustomProperty.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createDashboard.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createDashboard.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDashboard.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createDashboardDataModel.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createDashboardDataModel.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDashboardDataModel.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createDatabase.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createDatabase.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDatabase.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createDatabaseSchema.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createDatabaseSchema.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDatabaseSchema.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createGlossary.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createGlossary.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createGlossary.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createGlossaryTerm.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createGlossaryTerm.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createGlossaryTerm.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createMlModel.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createMlModel.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createMlModel.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createPipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createQuery.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createQuery.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createQuery.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createSearchIndex.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createSearchIndex.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createSearchIndex.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createStoredProcedure.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createStoredProcedure.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createStoredProcedure.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createTable.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createTable.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createTable.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createTableProfile.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createTableProfile.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createTableProfile.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/createTopic.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/createTopic.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createTopic.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, conint, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/data/loadGlossary.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/data/loadGlossary.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/loadGlossary.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/dataInsight/createDataInsightChart.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/dataInsight/kpi/createKpiRequest.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/docStore/createDocument.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/docStore/createDocument.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/docStore/createDocument.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/domains/createDataProduct.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/domains/createDataProduct.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/domains/createDataProduct.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/domains/createDomain.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/domains/createDomain.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/domains/createDomain.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/closeTask.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/schedule.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 # generated by datamodel-codegen:
-#   filename:  api/feed/closeTask.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  type/schedule.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic
+from . import basic
 
 
-class CloseTaskRequest(BaseModel):
+class Schedule(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    comment: str = Field(
-        ..., description='The closing comment explaining why the task is being closed.'
+    startDate: Optional[basic.DateTime] = Field(
+        None, description='Start date and time of the schedule.'
     )
-    testCaseFQN: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='Fully qualified name of the test case.'
+    repeatFrequency: Optional[basic.Duration] = Field(
+        None,
+        description="Repeat frequency in ISO 8601 duration format. Example - 'P23DT23H'.",
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/createPost.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/createPost.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/createPost.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 
 class CreatePostRequest(BaseModel):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/createSuggestion.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/createSuggestion.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/createSuggestion.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/createThread.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/createThread.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/createThread.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/resolveTask.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/resolveTask.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/resolveTask.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/feed/threadCount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/threadCount.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/threadCount.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/openMetadataServerVersion.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/openMetadataServerVersion.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/openMetadataServerVersion.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/policies/createPolicy.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/policies/createPolicy.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/policies/createPolicy.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createDashboardService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createDashboardService.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createDashboardService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createDatabaseService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createDatabaseService.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createDatabaseService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createMessagingService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createMessagingService.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createMessagingService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createMetadataService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createMetadataService.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createMetadataService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createMlModelService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createMlModelService.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createMlModelService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createPipelineService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createPipelineService.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createPipelineService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createSearchService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createSearchService.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createSearchService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/createStorageService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/createStorageService.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createStorageService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/ingestionPipelines/createIngestionPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/setOwner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/setOwner.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/setOwner.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/createPersona.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/createPersona.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createPersona.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/createRole.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/createRole.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createRole.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/createTeam.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/createTeam.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createTeam.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/teams/createUser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/teams/createUser.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createUser.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createCustomMetric.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createCustomMetric.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createCustomMetric.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createLogicalTestCases.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createLogicalTestCases.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createLogicalTestCases.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createTestCase.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createTestCase.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestCase.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createTestCaseResolutionStatus.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createTestCaseResolutionStatus.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestCaseResolutionStatus.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createTestDefinition.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createTestDefinition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestDefinition.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/api/tests/createTestSuite.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/tests/createTestSuite.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestSuite.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/changePasswordRequest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/changePasswordRequest.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/changePasswordRequest.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/emailVerificationToken.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/emailVerificationToken.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/emailVerificationToken.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/jwtAuth.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/jwtAuth.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/jwtAuth.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/logoutRequest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/logoutRequest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/logoutRequest.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/passwordResetRequest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/passwordResetRequest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/passwordResetRequest.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/passwordResetToken.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/passwordResetToken.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/passwordResetToken.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/personalAccessToken.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/personalAccessToken.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/personalAccessToken.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/refreshToken.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/refreshToken.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/refreshToken.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/registrationRequest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/registrationRequest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/registrationRequest.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field, constr
 
 from ..type import basic
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/auth/ssoAuth.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/auth/ssoAuth.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/ssoAuth.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/appsPrivateConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/appsPrivateConfiguration.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/appsPrivateConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/authConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/authConfig.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/authConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/authenticationConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/authenticationConfiguration.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/authenticationConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/authorizerConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/authorizerConfiguration.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/authorizerConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -38,7 +38,10 @@
     principalDomain: str = Field(..., description='Principal Domain')
     enforcePrincipalDomain: bool = Field(
         ..., description='Enable Enforce Principal Domain'
     )
     enableSecureSocketConnection: bool = Field(
         ..., description='Enable Secure Socket Connection.'
     )
+    useRolesFromProvider: Optional[bool] = Field(
+        False, description='Use Roles from Provider'
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/elasticSearchConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -38,14 +38,17 @@
     truststorePassword: Optional[str] = Field(None, description='Truststore Password')
     connectionTimeoutSecs: int = Field(..., description='Connection Timeout in Seconds')
     socketTimeoutSecs: int = Field(..., description='Socket Timeout in Seconds')
     keepAliveTimeoutSecs: Optional[int] = Field(
         None, description='Keep Alive Timeout in Seconds'
     )
     batchSize: int = Field(..., description='Batch Size for Requests')
+    payLoadSize: Optional[int] = Field(
+        10485760, description='Payload size in bytes depending on elasticsearch config'
+    )
     clusterAlias: Optional[str] = Field(
         None, description='Alias for search indexes to provide segregation of indexes.'
     )
     searchIndexMappingLanguage: SearchIndexMappingLanguage
     searchType: Optional[SearchType] = Field(
         SearchType.elasticsearch,
         description='This enum defines the search Type elastic/open search.',
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/jwtTokenConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/kafkaEventConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapConfiguration.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapTrustStoreConfig/customTrustManagerConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapTrustStoreConfig/hostNameConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapTrustStoreConfig/truststoreConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/loginConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/loginConfiguration.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/loginConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/logoConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/logoConfiguration.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/logoConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/pipelineServiceClientConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/slackAppConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/slackAppConfiguration.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/slackAppConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/testResultNotificationConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/dataInsightChart.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/dataInsightChart.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/dataInsightChart.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/dataInsightChartResult.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/kpi/basic.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/kpi/basic.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/kpi/basic.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/kpi/kpi.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/kpi/kpi.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/kpi/kpi.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/aggregatedUnusedAssetsCount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/aggregatedUnusedAssetsCount.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/aggregatedUnusedAssetsCount.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/aggregatedUnusedAssetsSize.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/aggregatedUnusedAssetsSize.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/aggregatedUnusedAssetsSize.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/aggregatedUsedVsUnusedAssetsCount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/aggregatedUsedVsUnusedAssetsCount.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/aggregatedUsedVsUnusedAssetsCount.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/aggregatedUsedVsUnusedAssetsSize.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/aggregatedUsedVsUnusedAssetsSize.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/aggregatedUsedVsUnusedAssetsSize.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/dailyActiveUsers.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/mostActiveUsers.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/mostViewedEntities.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/pageViewsByEntities.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/percentageOfEntitiesWithDescriptionByType.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/percentageOfEntitiesWithOwnerByType.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/percentageOfServicesWithDescription.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/percentageOfServicesWithDescription.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/percentageOfServicesWithDescription.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/percentageOfServicesWithOwner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/percentageOfServicesWithOwner.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/percentageOfServicesWithOwner.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/totalEntitiesByTier.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/totalEntitiesByType.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/dataInsight/type/unusedAssets.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/dataInsight/type/unusedAssets.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/unusedAssets.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/email/emailRequest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/email/emailRequest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  email/emailRequest.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/email/smtpSettings.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/email/smtpSettings.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  email/smtpSettings.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/app.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/app.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/app.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 from typing import Union  # custom generate import
 
 
 from enum import Enum
 from typing import List, Optional
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/appRunRecord.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/appRunRecord.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/appRunRecord.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/applicationConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/applicationConfig.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/configuration/applicationConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Union
 
 from pydantic import BaseModel
 
-from .external import autoTaggerAppConfig, metaPilotAppConfig
+from .external import automatorAppConfig, metaPilotAppConfig
 from .internal import dataInsightsReportAppConfig, searchIndexingAppConfig
 from .private.external import metaPilotAppPrivateConfig
 
 
 class ApplicationConfigModel(BaseModel):
     pass
 
 
 class PrivateConfig(BaseModel):
     __root__: metaPilotAppPrivateConfig.MetaPilotAppPrivateConfig
 
 
 class AppConfig(BaseModel):
     __root__: Union[
-        autoTaggerAppConfig.AutoTaggerAppConfig,
         metaPilotAppConfig.MetaPilotAppConfig,
+        automatorAppConfig.AutomatorAppConfig,
         dataInsightsReportAppConfig.DataInsightsReportAppConfig,
         searchIndexingAppConfig.SearchIndexingApp,
     ]
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/external/autoTaggerAppConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/common/sslCertValues.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,28 +1,31 @@
 # generated by datamodel-codegen:
-#   filename:  entity/applications/configuration/external/autoTaggerAppConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/common/sslCertValues.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
-from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
-class AutoTaggerAppType(Enum):
-    AutoTagger = 'AutoTagger'
 
-
-class AutoTaggerAppConfig(BaseModel):
+class SslCertificatesByValues(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[AutoTaggerAppType] = Field(
-        AutoTaggerAppType.AutoTagger, description='Service Type', title='Service Type'
+    caCertValue: Optional[CustomSecretStr] = Field(
+        None, description='CA Certificate Value', title='CA Certificate Value'
+    )
+    clientCertValue: Optional[CustomSecretStr] = Field(
+        None, description='Client Certificate Value', title='Client Certificate Value'
+    )
+    privateKeyValue: Optional[CustomSecretStr] = Field(
+        None, description='Private Key Value', title='Private Key Value'
     )
-    confidenceLevel: Optional[int] = Field(
-        80,
-        description='Confidence level for the ML models to apply the PII tagging.',
-        title='Confidence Level',
+    stagingDir: Optional[str] = Field(
+        '/tmp/openmetadata-certs',
+        description='Staging Directory Path',
+        title='Staging Directory Path',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/external/metaPilotAppConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/metaPilotAppConfig.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/configuration/external/metaPilotAppConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/internal/dataInsightsReportAppConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/internal/dataInsightsReportAppConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/configuration/internal/dataInsightsReportAppConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/internal/searchIndexingAppConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/internal/searchIndexingAppConfig.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/configuration/internal/searchIndexingAppConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/configuration/private/external/metaPilotAppPrivateConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/private/external/metaPilotAppPrivateConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/configuration/private/external/metaPilotAppPrivateConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/createAppRequest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/createAppRequest.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/createAppRequest.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/marketplace/appMarketPlaceDefinition.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/marketplace/appMarketPlaceDefinition.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/marketplace/appMarketPlaceDefinition.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/applications/marketplace/createAppMarketPlaceDefinitionReq.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/marketplace/createAppMarketPlaceDefinitionReq.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/applications/marketplace/createAppMarketPlaceDefinitionReq.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/automations/testServiceConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/automations/testServiceConnection.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/automations/testServiceConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/automations/workflow.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/automations/workflow.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/automations/workflow.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/bot.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/bot.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/bot.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/classification/classification.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/classification/classification.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,29 +1,28 @@
 # generated by datamodel-codegen:
 #   filename:  entity/classification/classification.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
 
 from ...type import basic, entityHistory
-from . import tag
 
 
 class Classification(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: Optional[basic.Uuid] = Field(
         None, description='Unique identifier of this entity instance.'
     )
-    name: tag.TagName
+    name: basic.EntityName
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
     displayName: Optional[str] = Field(
         None, description='Display Name that identifies this entity.'
     )
     description: basic.Markdown = Field(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/classification/tag.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/classification/tag.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,34 +1,28 @@
 # generated by datamodel-codegen:
 #   filename:  entity/classification/tag.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import BaseModel, Extra, Field, constr
+from pydantic import BaseModel, Extra, Field
 
 from ...type import basic, entityHistory, entityReference, entityReferenceList
 
 
-class TagName(BaseModel):
-    __root__: constr(
-        regex=r'(?u)^[\w\'\- .&()]+$', min_length=2, max_length=64
-    ) = Field(..., description='Name of the tag.')
-
-
 class Tag(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: Optional[basic.Uuid] = Field(
         None, description='Unique identifier of this entity instance.'
     )
-    name: TagName = Field(..., description='Name of the tag.')
+    name: basic.EntityName = Field(..., description='Name of the tag.')
     displayName: Optional[str] = Field(
         None, description='Display Name that identifies this tag.'
     )
     fullyQualifiedName: Optional[str] = Field(
         None, description='Unique name of the tag of format `Classification.tag1.tag2`.'
     )
     description: basic.Markdown = Field(..., description='Description of the tag.')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/chart.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/chart.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/chart.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/container.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/container.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/container.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
@@ -130,14 +130,17 @@
     extension: Optional[basic.EntityExtension] = Field(
         None,
         description='Entity extension data with custom attributes added to the entity.',
     )
     sourceUrl: Optional[basic.SourceUrl] = Field(
         None, description='Source URL of container.'
     )
+    fullPath: Optional[str] = Field(
+        None, description='Full path of the container/file.'
+    )
     domain: Optional[entityReference.EntityReference] = Field(
         None,
         description='Domain the Container belongs to. When not set, the Container inherits the domain from the storage service it belongs to.',
     )
     dataProducts: Optional[entityReferenceList.EntityReferenceList] = Field(
         None, description='List of data products this entity is part of.'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/dashboard.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/dashboard.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/dashboard.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/dashboardDataModel.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/dashboardDataModel.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/dashboardDataModel.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
@@ -25,15 +25,15 @@
 class DataModelType(Enum):
     TableauDataModel = 'TableauDataModel'
     SupersetDataModel = 'SupersetDataModel'
     MetabaseDataModel = 'MetabaseDataModel'
     LookMlView = 'LookMlView'
     LookMlExplore = 'LookMlExplore'
     PowerBIDataModel = 'PowerBIDataModel'
-    QlikSenseDataModel = 'QlikSenseDataModel'
+    QlikDataModel = 'QlikDataModel'
 
 
 class DashboardDataModel(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/database.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/database.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/database.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/databaseSchema.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/databaseSchema.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/databaseSchema.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/glossary.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/glossary.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/glossary.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/glossaryTerm.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/glossaryTerm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/glossaryTerm.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
@@ -128,7 +128,10 @@
         description='Entity extension data with custom attributes added to the entity.',
     )
     domain: Optional[entityReference.EntityReference] = Field(
         None,
         description='Domain the Glossary Term belongs to. When not set, the Glossary TErm inherits the domain from the Glossary it belongs to.',
     )
     votes: Optional[votes.Votes] = None
+    childrenCount: Optional[int] = Field(
+        None, description='Count of immediate children glossary terms.'
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/metrics.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/metrics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/metrics.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/mlmodel.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/mlmodel.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/mlmodel.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
@@ -49,15 +49,15 @@
     )
 
 
 class MlStore(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    storage: Optional[basic.Href] = Field(
+    storage: Optional[str] = Field(
         None, description='Storage Layer containing the ML Model data.'
     )
     imageRepository: Optional[str] = Field(
         None, description='Container Repository with the ML Model image.'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/pipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/pipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/pipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/query.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/query.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/query.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/report.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/report.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/report.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/searchIndex.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/searchIndex.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/searchIndex.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/storedProcedure.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/storedProcedure.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/storedProcedure.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/table.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/table.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/table.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from datetime import datetime
 from enum import Enum
 from typing import List, Optional, Union
 
@@ -28,14 +28,15 @@
     PERCENTAGE = 'PERCENTAGE'
     ROWS = 'ROWS'
 
 
 class TableType(Enum):
     Regular = 'Regular'
     External = 'External'
+    Dynamic = 'Dynamic'
     View = 'View'
     SecureView = 'SecureView'
     MaterializedView = 'MaterializedView'
     Iceberg = 'Iceberg'
     Local = 'Local'
     Partitioned = 'Partitioned'
     Foreign = 'Foreign'
@@ -154,32 +155,32 @@
 class ColumnName(BaseModel):
     __root__: constr(regex=r'^((?!::).)*$', min_length=1, max_length=256) = Field(
         ...,
         description='Local name (not fully qualified name) of the column. ColumnName is `-` when the column is not named in struct dataType. For example, BigQuery supports struct with unnamed fields.',
     )
 
 
-class IntervalType(Enum):
+class PartitionIntervalTypes(Enum):
     TIME_UNIT = 'TIME-UNIT'
     INTEGER_RANGE = 'INTEGER-RANGE'
     INGESTION_TIME = 'INGESTION-TIME'
     COLUMN_VALUE = 'COLUMN-VALUE'
+    INJECTED = 'INJECTED'
+    ENUM = 'ENUM'
     OTHER = 'OTHER'
 
 
-class TablePartition(BaseModel):
+class PartitionColumnDetails(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    columns: Optional[List[str]] = Field(
+    columnName: Optional[str] = Field(
         None, description='List of column names corresponding to the partition.'
     )
-    intervalType: Optional[IntervalType] = Field(
-        None, description='type of partition interval, example time-unit, integer-range'
-    )
+    intervalType: Optional[PartitionIntervalTypes] = None
     interval: Optional[str] = Field(
         None, description='partition interval , example hourly, daily, monthly.'
     )
 
 
 class JoinedWith(BaseModel):
     class Config:
@@ -349,38 +350,29 @@
         None, description='Column Name of the table to be included.'
     )
     metrics: Optional[List[str]] = Field(
         None, description='Include only following metrics.'
     )
 
 
-class PartitionIntervalType(Enum):
-    TIME_UNIT = 'TIME-UNIT'
-    INTEGER_RANGE = 'INTEGER-RANGE'
-    INGESTION_TIME = 'INGESTION-TIME'
-    COLUMN_VALUE = 'COLUMN-VALUE'
-
-
 class PartitionIntervalUnit(Enum):
     YEAR = 'YEAR'
     MONTH = 'MONTH'
     DAY = 'DAY'
     HOUR = 'HOUR'
 
 
 class PartitionProfilerConfig(BaseModel):
     enablePartitioning: Optional[bool] = Field(
         False, description='whether to use partition'
     )
     partitionColumnName: Optional[str] = Field(
         None, description='name of the column to use for the partition'
     )
-    partitionIntervalType: Optional[PartitionIntervalType] = Field(
-        None, description='type of partition interval'
-    )
+    partitionIntervalType: Optional[PartitionIntervalTypes] = None
     partitionInterval: Optional[int] = Field(
         None, description='The interval to use for the partitioning'
     )
     partitionIntervalUnit: Optional[PartitionIntervalUnit] = Field(
         None, description='unit used for the partition interval'
     )
     partitionValues: Optional[List] = Field(
@@ -391,22 +383,22 @@
     )
     partitionIntegerRangeEnd: Optional[int] = Field(
         None, description='end of the integer range for partitioning'
     )
 
 
 class TableProfilerConfig(BaseModel):
+    profileSampleType: Optional[ProfileSampleType] = ProfileSampleType.PERCENTAGE
     profileSample: Optional[float] = Field(
         None,
-        description='Percentage of data or no. of rows we want to execute the profiler and tests on',
+        description='Percentage of data or no. of rows used to compute the profiler metrics and run data quality tests',
     )
-    profileSampleType: Optional[ProfileSampleType] = ProfileSampleType.PERCENTAGE
     sampleDataCount: Optional[int] = Field(
         50,
-        description='Number of row of sample data to be generated',
+        description="Number of sample rows to ingest when 'Generate Sample Data' is enabled",
         title='Sample Data Rows Count',
     )
     profileQuery: Optional[str] = Field(
         None,
         description="Users' raw SQL query to fetch sample data and profile the table",
     )
     excludeColumns: Optional[List[str]] = Field(
@@ -456,14 +448,23 @@
     avro = 'avro'
     parquet = 'parquet'
     json = 'json'
     json_gz = 'json.gz'
     json_zip = 'json.zip'
 
 
+class TablePartition(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    columns: Optional[List[PartitionColumnDetails]] = Field(
+        None, description='List of column partitions with their type and interval.'
+    )
+
+
 class Column(BaseModel):
     class Config:
         extra = Extra.forbid
 
     name: ColumnName
     displayName: Optional[str] = Field(
         None, description='Display Name that identifies this column name.'
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/data/topic.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/data/topic.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/topic.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, conint, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/docStore/document.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/docStore/document.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/docStore/document.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/domains/dataProduct.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/domains/dataProduct.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/domains/dataProduct.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/domains/domain.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/domains/domain.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/domains/domain.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/events/webhook.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/events/webhook.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/events/webhook.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/feed/suggestion.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/suggestion.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/feed/suggestion.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/feed/thread.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/thread.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,19 +1,47 @@
 # generated by datamodel-codegen:
 #   filename:  entity/feed/thread.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import List, Optional
+from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
 from ...type import basic, entityReferenceList, reaction
+from . import description, entityInfo, owner, tag, testCaseResult
+
+
+class GeneratedBy(Enum):
+    user = 'user'
+    system = 'system'
+
+
+class CardStyle(Enum):
+    default = 'default'
+    logicalTestCaseAdded = 'logicalTestCaseAdded'
+    entityCreated = 'entityCreated'
+    entityDeleted = 'entityDeleted'
+    entitySoftDeleted = 'entitySoftDeleted'
+    description = 'description'
+    tags = 'tags'
+    owner = 'owner'
+    testCaseResult = 'testCaseResult'
+    customProperties = 'customProperties'
+    assets = 'assets'
+    domain = 'domain'
+
+
+class FieldOperation(Enum):
+    added = 'added'
+    updated = 'updated'
+    deleted = 'deleted'
+    none = 'none'
 
 
 class TaskType(Enum):
     RequestDescription = 'RequestDescription'
     UpdateDescription = 'UpdateDescription'
     RequestTag = 'RequestTag'
     UpdateTag = 'UpdateTag'
@@ -106,14 +134,35 @@
         ..., alias='from', description='Name of the User posting the message.'
     )
     reactions: Optional[reaction.ReactionList] = Field(
         None, description='Reactions for the post.'
     )
 
 
+class FeedInfo(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    headerMessage: Optional[str] = Field(
+        None, description='Header message for the feed.'
+    )
+    fieldName: Optional[str] = Field(
+        None, description='Field Name message for the feed.'
+    )
+    entitySpecificInfo: Optional[
+        Union[
+            description.DescriptionFeedInfo,
+            owner.OwnerFeedInfo,
+            testCaseResult.TestCaseResultFeedInfo,
+            tag.TagFeedInfo,
+            entityInfo.EntityInfo,
+        ]
+    ] = None
+
+
 class Thread(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
         ..., description='Unique identifier that identifies an entity instance.'
     )
@@ -129,14 +178,34 @@
         ...,
         description='Data asset about which this thread is created for with format <#E::{entities}::{entityName}::{field}::{fieldValue}.',
     )
     entityId: Optional[basic.Uuid] = Field(
         None,
         description='Entity Id of the entity in `about` that the thread belongs to.',
     )
+    entityType: Optional[str] = Field(
+        None, description='Entity Type the thread is about.'
+    )
+    entityUrlLink: Optional[str] = Field(
+        None, description='Link to the entity in `about` that the thread belongs to.'
+    )
+    generatedBy: Optional[GeneratedBy] = Field(
+        GeneratedBy.user, description='User or team that generated the thread.'
+    )
+    cardStyle: Optional[CardStyle] = Field(
+        CardStyle.default, description='Card style for the thread.'
+    )
+    fieldOperation: Optional[FieldOperation] = Field(
+        FieldOperation.updated,
+        description='Operation on thread, whether the field was added, or updated or deleted.',
+    )
+    feedInfo: Optional[FeedInfo] = Field(
+        None,
+        description='Entity Id of the entity in `about` that the thread belongs to.',
+    )
     addressedTo: Optional[basic.EntityLink] = Field(
         None,
         description='User or team this thread is addressed to in format <#E::{entities}::{entityName}::{field}::{fieldValue}.',
     )
     createdBy: Optional[str] = Field(None, description='User who created the thread.')
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/accessControl/resourceDescriptor.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -17,14 +17,15 @@
     ViewAll = 'ViewAll'
     ViewBasic = 'ViewBasic'
     ViewUsage = 'ViewUsage'
     ViewTests = 'ViewTests'
     ViewQueries = 'ViewQueries'
     ViewDataProfile = 'ViewDataProfile'
     ViewSampleData = 'ViewSampleData'
+    ViewTestCaseFailedRowsSample = 'ViewTestCaseFailedRowsSample'
     EditAll = 'EditAll'
     EditCustomFields = 'EditCustomFields'
     EditDataProfile = 'EditDataProfile'
     EditDescription = 'EditDescription'
     EditDisplayName = 'EditDisplayName'
     EditLineage = 'EditLineage'
     EditPolicy = 'EditPolicy'
@@ -39,14 +40,15 @@
     EditTier = 'EditTier'
     EditTests = 'EditTests'
     EditUsage = 'EditUsage'
     EditUsers = 'EditUsers'
     EditLifeCycle = 'EditLifeCycle'
     EditKnowledgePanel = 'EditKnowledgePanel'
     EditPage = 'EditPage'
+    DeleteTestCaseFailedRowsSample = 'DeleteTestCaseFailedRowsSample'
 
 
 class ResourceDescriptor(BaseModel):
     class Config:
         extra = Extra.forbid
 
     name: Optional[str] = Field(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/accessControl/resourcePermission.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/accessControl/rule.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/accessControl/rule.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/accessControl/rule.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/filters.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/filters.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/filters.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, List
 
 from pydantic import BaseModel, Field
 
-from ..classification import tag
+from ...type import basic
 
 
 class Filters(BaseModel):
     __root__: Any = Field(..., title='Filters')
 
 
 class Prefix(BaseModel):
@@ -20,10 +20,10 @@
 
 
 class Regex(BaseModel):
     __root__: str = Field(..., description='Regex that matches the entity.')
 
 
 class Tags(BaseModel):
-    __root__: List[tag.TagName] = Field(
+    __root__: List[basic.EntityName] = Field(
         ..., description='Set of tags to match on (OR among all tags).'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/policies/policy.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/policies/policy.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/policy.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/common/sslCertPaths.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/common/sslCertPaths.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/common/sslCertPaths.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/common/sslCertValues.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/common/sslCertValues.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  security/ssl/validateSSLClientConfig.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 
-class SslCertificatesByValues(BaseModel):
+class ValidateSslClientConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    caCertValue: Optional[CustomSecretStr] = Field(
-        None, description='CA Certificate Value', title='CA Certificate Value'
+    caCertificate: Optional[CustomSecretStr] = Field(
+        None,
+        description='The CA certificate used for SSL validation.',
+        title='CA Certificate',
     )
-    clientCertValue: Optional[CustomSecretStr] = Field(
-        None, description='Client Certificate Value', title='Client Certificate Value'
+    sslCertificate: Optional[CustomSecretStr] = Field(
+        None,
+        description='The SSL certificate used for client authentication.',
+        title='SSL Certificate',
     )
-    privateKeyValue: Optional[CustomSecretStr] = Field(
-        None, description='Private Key Value', title='Private Key Value'
-    )
-    stagingDir: Optional[str] = Field(
-        '/tmp/openmetadata-certs',
-        description='Staging Directory Path',
-        title='Staging Directory Path',
+    sslKey: Optional[CustomSecretStr] = Field(
+        None,
+        description='The private key associated with the SSL certificate.',
+        title='SSL Key',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/common/sslConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/common/sslConfig.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/common/sslConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/connectionBasicType.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/customDashboardConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/domoDashboardConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/lightdashConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/lightdashConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/lightdashConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/lookerConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/metabaseConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/modeConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/mstrConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/mstrConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/mstrConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,27 +1,49 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/powerBIConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import List, Optional
+from typing import List, Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
+from .powerbi import azureConfig, gcsConfig, s3Config
 
 
 class PowerBiType(Enum):
     PowerBI = 'PowerBI'
 
 
+class PbitFileConfigType(Enum):
+    local = 'local'
+
+
+class LocalConfig(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    pbitFileConfigType: Optional[PbitFileConfigType] = Field(
+        PbitFileConfigType.local, description='pbit File Configuration type'
+    )
+    path: Optional[str] = Field(
+        None, description='Directory path for the pbit files', title='Path'
+    )
+    pbitFilesExtractDir: Optional[str] = Field(
+        '/tmp/pbitFiles',
+        description='Path of the folder where the .pbit files will be unzipped and datamodel schema will be extracted',
+        title='pbit Files Extraction Directory',
+    )
+
+
 class PowerBIConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: Optional[PowerBiType] = Field(
         PowerBiType.PowerBI, description='Service Type', title='Service Type'
     )
@@ -51,10 +73,19 @@
         title='Pagination Entity Per Page',
     )
     useAdminApis: Optional[bool] = Field(
         True,
         description='Fetch the PowerBI metadata using admin APIs',
         title='Use PowerBI Admin APIs',
     )
+    pbitFilesSource: Optional[
+        Union[
+            LocalConfig, azureConfig.AzureConfig, gcsConfig.GCSConfig, s3Config.S3Config
+        ]
+    ] = Field(
+        None,
+        description='Source to get the .pbit files to extract lineage information',
+        title='PowerBI .pbit Files Source',
+    )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/qlikSenseConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/persona.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,72 +1,47 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/dashboard/qlikSenseConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/teams/persona.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
-from enum import Enum
-from typing import Optional, Union
+from typing import Optional
 
-from pydantic import AnyUrl, BaseModel, Extra, Field
+from pydantic import BaseModel, Extra, Field
 
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
+from ...type import basic, entityHistory, entityReferenceList
 
-from .. import connectionBasicType
-
-
-class QlikSenseType(Enum):
-    QlikSense = 'QlikSense'
 
+class Team(BaseModel):
+    class Config:
+        extra = Extra.forbid
 
-class QlikCertificatePath(BaseModel):
-    clientCertificate: str = Field(
-        ..., description='Client Certificate', title='Client Certificate Path'
+    id: basic.Uuid
+    name: basic.EntityName = Field(
+        ..., description="A unique name of Persona. Example 'data engineer'"
     )
-    clientKeyCertificate: str = Field(
-        ..., description='Client Key Certificate.', title='Client Key Certificate'
+    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
+        None, description='FullyQualifiedName same as `name`.'
     )
-    rootCertificate: str = Field(
-        ..., description='Root Certificate.', title='Root Certificate'
+    displayName: Optional[str] = Field(
+        None, description="Name used for display purposes. Example 'Data Steward'."
     )
-
-
-class QlikCertificateValues(BaseModel):
-    clientCertificateData: CustomSecretStr = Field(
-        ..., description='Client Certificate', title='Client Certificate Value'
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of the persona.'
     )
-    clientKeyCertificateData: CustomSecretStr = Field(
-        ..., description='Client Key Certificate.', title='Client Key Certificate Value'
+    updatedAt: Optional[basic.Timestamp] = Field(
+        None,
+        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
-    rootCertificateData: CustomSecretStr = Field(
-        ..., description='Root Certificate.', title='Root Certificate Value'
+    updatedBy: Optional[str] = Field(None, description='User who made the update.')
+    href: Optional[basic.Href] = Field(
+        None, description='Link to the resource corresponding to this entity.'
     )
-    stagingDir: str = Field(
-        ..., description='Staging Directory Path', title='Staging Directory Path'
+    users: Optional[entityReferenceList.EntityReferenceList] = Field(
+        None, description='Users that are assigned a persona.'
     )
-
-
-class QlikSenseConnection(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    type: Optional[QlikSenseType] = Field(
-        QlikSenseType.QlikSense, description='Service Type', title='Service Type'
+    version: Optional[entityHistory.EntityVersion] = Field(
+        None, description='Metadata version of the entity.'
     )
-    displayUrl: Optional[AnyUrl] = Field(
-        None,
-        description='Qlik Sense Base URL, used for genrating dashboard & chat url',
-        title='Qlik Sense Base URL',
+    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
+        None, description='Change that lead to this version of the entity.'
     )
-    hostPort: AnyUrl = Field(
-        ...,
-        description='URL for the Qlik instance.',
-        title='Qlik Engine JSON API Websocket URL',
-    )
-    certificates: Union[QlikCertificateValues, QlikCertificatePath]
-    userDirectory: Optional[str] = Field(
-        None, description='User Directory.', title='User Directory'
-    )
-    userId: Optional[str] = Field(None, description='User ID.', title='User ID')
-    supportsMetadataExtraction: Optional[
-        connectionBasicType.SupportsMetadataExtraction
-    ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/quickSightConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/redashConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/supersetConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/tableauConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/athenaConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/azureSQLConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/bigQueryConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/bigTableConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/bigTableConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/bigTableConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/clickhouseConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/azureConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/common/azureConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/datalake/s3Config.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ......security.credentials import azureCredentials
+from ......security.credentials import awsCredentials
 
 
-class AzureConfigurationSource(BaseModel):
+class S3Config(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    azureConfig: Optional[azureCredentials.AzureCredentials] = Field(
-        None, title='Azure Credentials Configuration'
+    securityConfig: Optional[awsCredentials.AWSCredentials] = Field(
+        None, title='DataLake S3 Security Config'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/basicAuth.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/basicAuth.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/common/basicAuth.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/iamAuthConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/iamAuthConfig.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/common/iamAuthConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/common/jwtAuth.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/jwtAuth.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/common/jwtAuth.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/couchbaseConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/couchbaseConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/couchbaseConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/customDatabaseConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/databricksConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/datalake/azureConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/datalake/gcsConfig.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ......security.credentials import azureCredentials
+from ......security.credentials import gcpCredentials
 
 
-class AzureConfig(BaseModel):
+class GCSConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    securityConfig: Optional[azureCredentials.AzureCredentials] = Field(
-        None, title='Azure Datalake Config Source'
+    securityConfig: Optional[gcpCredentials.GCPCredentials] = Field(
+        None, title='DataLake GCS Security Config'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataGCSConfig.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,20 +1,24 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/datalake/gcsConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  metadataIngestion/storage/storageMetadataGCSConfig.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ......security.credentials import gcpCredentials
+from ...security.credentials import gcpCredentials
+from . import storageBucketDetails
 
 
-class GCSConfig(BaseModel):
+class StorageMetadataGcsConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
     securityConfig: Optional[gcpCredentials.GCPCredentials] = Field(
-        None, title='DataLake GCS Security Config'
+        None, title='GCS Security Config'
+    )
+    prefixConfig: storageBucketDetails.StorageMetadataBucketDetails = Field(
+        ..., title='Storage Metadata Prefix Config'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/apiAuth.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,20 +1,27 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/datalake/s3Config.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/search/elasticSearch/apiAuth.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ......security.credentials import awsCredentials
+from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 
-class S3Config(BaseModel):
+class ApiKeyAuthentication(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    securityConfig: Optional[awsCredentials.AWSCredentials] = Field(
-        None, title='DataLake S3 Security Config'
+    apiKey: Optional[CustomSecretStr] = Field(
+        None,
+        description='Elastic Search API Key for API Authentication',
+        title='API Key',
+    )
+    apiKeyId: Optional[str] = Field(
+        None,
+        description='Elastic Search API Key ID for API Authentication',
+        title='API Key ID',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/datalakeConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/db2Connection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/deltaLakeConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/domoDatabaseConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/dorisConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,77 +1,64 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/dorisConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/mariaDBConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class DorisType(Enum):
-    Doris = 'Doris'
+class MariaDBType(Enum):
+    MariaDB = 'MariaDB'
 
 
-class DorisScheme(Enum):
-    doris = 'doris'
+class MariaDBScheme(Enum):
+    mysql_pymysql = 'mysql+pymysql'
 
 
-class DorisConnection(BaseModel):
+class MariaDBConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[DorisType] = Field(
-        DorisType.Doris, description='Service Type', title='Service Type'
+    type: Optional[MariaDBType] = Field(
+        MariaDBType.MariaDB, description='Service Type', title='Service Type'
     )
-    scheme: Optional[DorisScheme] = Field(
-        DorisScheme.doris,
+    scheme: Optional[MariaDBScheme] = Field(
+        MariaDBScheme.mysql_pymysql,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
     username: str = Field(
         ...,
-        description='Username to connect to Doris. This user should have privileges to read all the metadata in Doris.',
+        description='Username to connect to MariaDB. This user should have privileges to read all the metadata in MariaDB.',
         title='Username',
     )
     password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to Doris.', title='Password'
+        None, description='Password to connect to MariaDB.', title='Password'
     )
     hostPort: str = Field(
-        ..., description='Host and port of the Doris service.', title='Host and Port'
+        ..., description='Host and port of the MariaDB service.', title='Host and Port'
     )
     databaseName: Optional[str] = Field(
         None,
         description='Optional name to give to the database in OpenMetadata. If left blank, we will use default as the database name.',
         title='Database Name',
     )
     databaseSchema: Optional[str] = Field(
         None,
         description='Database Schema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single schema. When left blank, OpenMetadata Ingestion attempts to scan all the schemas.',
         title='Database Schema',
     )
-    sslCA: Optional[str] = Field(
-        None, description='Provide the path to ssl ca file', title='SSL CA'
-    )
-    sslCert: Optional[str] = Field(
-        None,
-        description='Provide the path to ssl client certificate file (ssl_cert)',
-        title='SSL Client Certificate File',
-    )
-    sslKey: Optional[str] = Field(
-        None,
-        description='Provide the path to ssl client certificate file (ssl_key)',
-        title='SSL Client Key File',
-    )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
@@ -80,7 +67,10 @@
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
     supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
         None, title='Supports Query Comment'
     )
+    sampleDataStorageConfig: Optional[
+        connectionBasicType.SampleDataStorageConfig
+    ] = Field(None, title='Storage Config for Sample Data')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/druidConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/dynamoDBConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -37,7 +37,10 @@
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
+    supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
+        None, title='Supports Profiler'
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/glueConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/greenplumConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,90 +1,82 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/greenplumConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/redshiftConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional, Union
+from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from metadata.ingestion.models.custom_pydantic import CustomSecretStr
+
 from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
-from .common import basicAuth, iamAuthConfig
-
-
-class SslMode(Enum):
-    disable = 'disable'
-    allow = 'allow'
-    prefer = 'prefer'
-    require = 'require'
-    verify_ca = 'verify-ca'
-    verify_full = 'verify-full'
 
 
-class GreenplumType(Enum):
-    Greenplum = 'Greenplum'
+class RedshiftType(Enum):
+    Redshift = 'Redshift'
 
 
-class GreenplumScheme(Enum):
-    postgresql_psycopg2 = 'postgresql+psycopg2'
+class RedshiftScheme(Enum):
+    redshift_psycopg2 = 'redshift+psycopg2'
 
 
-class GreenplumConnection(BaseModel):
+class RedshiftConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[GreenplumType] = Field(
-        GreenplumType.Greenplum, description='Service Type', title='Service Type'
+    type: Optional[RedshiftType] = Field(
+        RedshiftType.Redshift, description='Service Type', title='Service Type'
     )
-    scheme: Optional[GreenplumScheme] = Field(
-        GreenplumScheme.postgresql_psycopg2,
+    scheme: Optional[RedshiftScheme] = Field(
+        RedshiftScheme.redshift_psycopg2,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
     username: str = Field(
         ...,
-        description='Username to connect to Greenplum. This user should have privileges to read all the metadata in Greenplum.',
+        description='Username to connect to Redshift. This user should have privileges to read all the metadata in Redshift.',
         title='Username',
     )
-    authType: Optional[
-        Union[basicAuth.BasicAuth, iamAuthConfig.IamAuthConfigurationSource]
-    ] = Field(
-        None, description='Choose Auth Config Type.', title='Auth Configuration Type'
+    password: Optional[CustomSecretStr] = Field(
+        None, description='Password to connect to Redshift.', title='Password'
     )
     hostPort: str = Field(
-        ..., description='Host and port of the source service.', title='Host and Port'
+        ..., description='Host and port of the Redshift service.', title='Host and Port'
     )
     database: str = Field(
         ...,
-        description='Database of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single database. When left blank, OpenMetadata Ingestion attempts to scan all the databases.',
+        description='Initial Redshift database to connect to. If you want to ingest all databases, set ingestAllDatabases to true.',
         title='Database',
     )
-    sslMode: Optional[SslMode] = Field(
-        SslMode.disable,
-        description='SSL Mode to connect to Greenplum database.',
-        title='SSL Mode',
-    )
-    sslConfig: Optional[verifySSLConfig.SslConfig] = None
     ingestAllDatabases: Optional[bool] = Field(
         False,
-        description='Ingest data from all databases in Greenplum. You can use databaseFilterPattern on top of this.',
+        description='Ingest data from all databases in Redshift. You can use databaseFilterPattern on top of this.',
         title='Ingest All Databases',
     )
+    sslMode: Optional[verifySSLConfig.SslMode] = verifySSLConfig.SslMode.disable
+    sslConfig: Optional[verifySSLConfig.SslConfig] = None
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
+    supportsUsageExtraction: Optional[
+        connectionBasicType.SupportsUsageExtraction
+    ] = None
+    supportsLineageExtraction: Optional[
+        connectionBasicType.SupportsLineageExtraction
+    ] = None
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
     supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
         None, title='Supports Database'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/hiveConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/dynamoDbCatalogConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/dynamoDbCatalogConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/iceberg/dynamoDbCatalogConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from ......security.credentials import awsCredentials
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/hiveCatalogConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/hiveCatalogConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/iceberg/hiveCatalogConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/icebergCatalog.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/icebergCatalog.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/iceberg/icebergCatalog.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/icebergFileSystem.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/icebergFileSystem.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/iceberg/icebergFileSystem.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/iceberg/restCatalogConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/iceberg/restCatalogConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/iceberg/restCatalogConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/icebergConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/icebergConnection.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/icebergConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/impalaConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/impalaConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/impalaConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,76 +1,85 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/mariaDBConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/prestoConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class MariaDBType(Enum):
-    MariaDB = 'MariaDB'
+class PrestoType(Enum):
+    Presto = 'Presto'
 
 
-class MariaDBScheme(Enum):
-    mysql_pymysql = 'mysql+pymysql'
+class PrestoScheme(Enum):
+    presto = 'presto'
 
 
-class MariaDBConnection(BaseModel):
+class PrestoConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[MariaDBType] = Field(
-        MariaDBType.MariaDB, description='Service Type', title='Service Type'
+    type: Optional[PrestoType] = Field(
+        PrestoType.Presto, description='Service Type', title='Service Type'
     )
-    scheme: Optional[MariaDBScheme] = Field(
-        MariaDBScheme.mysql_pymysql,
+    scheme: Optional[PrestoScheme] = Field(
+        PrestoScheme.presto,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
     username: str = Field(
         ...,
-        description='Username to connect to MariaDB. This user should have privileges to read all the metadata in MariaDB.',
+        description='Username to connect to Presto. This user should have privileges to read all the metadata in Postgres.',
         title='Username',
     )
     password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to MariaDB.', title='Password'
+        None, description='Password to connect to Presto.', title='Password'
     )
     hostPort: str = Field(
-        ..., description='Host and port of the MariaDB service.', title='Host and Port'
-    )
-    databaseName: Optional[str] = Field(
-        None,
-        description='Optional name to give to the database in OpenMetadata. If left blank, we will use default as the database name.',
-        title='Database Name',
+        ..., description='Host and port of the Presto service.', title='Host and Port'
     )
     databaseSchema: Optional[str] = Field(
         None,
         description='Database Schema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single schema. When left blank, OpenMetadata Ingestion attempts to scan all the schemas.',
         title='Database Schema',
     )
+    catalog: Optional[str] = Field(None, description='Presto catalog', title='Catalog')
+    protocol: Optional[str] = Field(
+        None,
+        description='Protocol ( Connection Argument ) to connect to Presto.',
+        title='Protocol',
+    )
+    verify: Optional[str] = Field(
+        None,
+        description='Verify ( Connection Argument for SSL ) to connect to Presto.',
+        title='Verify',
+    )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
+    supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
+        None, title='Supports Database'
+    )
     supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
         None, title='Supports Query Comment'
     )
     sampleDataStorageConfig: Optional[
         connectionBasicType.SampleDataStorageConfig
     ] = Field(None, title='Storage Config for Sample Data')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/mongoDBConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/dorisConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,59 +1,77 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/mongoDBConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/dorisConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
+from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
 
 
-class MongoDBType(Enum):
-    MongoDB = 'MongoDB'
+class DorisType(Enum):
+    Doris = 'Doris'
 
 
-class MongoDBScheme(Enum):
-    mongodb = 'mongodb'
-    mongodb_srv = 'mongodb+srv'
+class DorisScheme(Enum):
+    doris = 'doris'
 
 
-class MongoDBConnection(BaseModel):
+class DorisConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[MongoDBType] = Field(
-        MongoDBType.MongoDB, description='Service Type', title='Service Type'
+    type: Optional[DorisType] = Field(
+        DorisType.Doris, description='Service Type', title='Service Type'
     )
-    scheme: Optional[MongoDBScheme] = Field(
-        MongoDBScheme.mongodb,
-        description='Mongo connection scheme options.',
+    scheme: Optional[DorisScheme] = Field(
+        DorisScheme.doris,
+        description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
-    username: Optional[str] = Field(
-        None,
-        description='Username to connect to MongoDB. This user should have privileges to read all the metadata in MongoDB.',
+    username: str = Field(
+        ...,
+        description='Username to connect to Doris. This user should have privileges to read all the metadata in Doris.',
         title='Username',
     )
     password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to MongoDB.', title='Password'
+        None, description='Password to connect to Doris.', title='Password'
     )
     hostPort: str = Field(
-        ..., description='Host and port of the MongoDB service.', title='Host and Port'
-    )
-    connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
-        None, title='Connection Options'
+        ..., description='Host and port of the Doris service.', title='Host and Port'
     )
     databaseName: Optional[str] = Field(
         None,
         description='Optional name to give to the database in OpenMetadata. If left blank, we will use default as the database name.',
         title='Database Name',
     )
+    databaseSchema: Optional[str] = Field(
+        None,
+        description='Database Schema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single schema. When left blank, OpenMetadata Ingestion attempts to scan all the schemas.',
+        title='Database Schema',
+    )
+    sslConfig: Optional[verifySSLConfig.SslConfig] = Field(
+        None, description='SSL Configuration details.', title='SSL'
+    )
+    connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
+        None, title='Connection Options'
+    )
+    connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
+        None, title='Connection Arguments'
+    )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
+    supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
+    supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
+        None, title='Supports Profiler'
+    )
+    supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
+        None, title='Supports Query Comment'
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/mssqlConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,94 +1,99 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/mysqlConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/postgresConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
+from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
 from .common import azureConfig, basicAuth, iamAuthConfig
 
 
-class MySQLType(Enum):
-    Mysql = 'Mysql'
+class PostgresType(Enum):
+    Postgres = 'Postgres'
 
 
-class MySQLScheme(Enum):
-    mysql_pymysql = 'mysql+pymysql'
+class PostgresScheme(Enum):
+    postgresql_psycopg2 = 'postgresql+psycopg2'
+    pgspider_psycopg2 = 'pgspider+psycopg2'
 
 
-class MysqlConnection(BaseModel):
+class PostgresConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[MySQLType] = Field(
-        MySQLType.Mysql, description='Service Type', title='Service Type'
+    type: Optional[PostgresType] = Field(
+        PostgresType.Postgres, description='Service Type', title='Service Type'
     )
-    scheme: Optional[MySQLScheme] = Field(
-        MySQLScheme.mysql_pymysql,
+    scheme: Optional[PostgresScheme] = Field(
+        PostgresScheme.postgresql_psycopg2,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
     username: str = Field(
         ...,
-        description='Username to connect to MySQL. This user should have privileges to read all the metadata in Mysql.',
+        description='Username to connect to Postgres. This user should have privileges to read all the metadata in Postgres.',
         title='Username',
     )
     authType: Optional[
         Union[
             basicAuth.BasicAuth,
             iamAuthConfig.IamAuthConfigurationSource,
             azureConfig.AzureConfigurationSource,
         ]
     ] = Field(
         None, description='Choose Auth Config Type.', title='Auth Configuration Type'
     )
     hostPort: str = Field(
-        ..., description='Host and port of the MySQL service.', title='Host and Port'
+        ..., description='Host and port of the source service.', title='Host and Port'
     )
-    databaseName: Optional[str] = Field(
-        None,
-        description='Optional name to give to the database in OpenMetadata. If left blank, we will use default as the database name.',
-        title='Database Name',
-    )
-    databaseSchema: Optional[str] = Field(
-        None,
-        description='Database Schema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single schema. When left blank, OpenMetadata Ingestion attempts to scan all the schemas.',
-        title='Database Schema',
-    )
-    sslCA: Optional[str] = Field(
-        None, description='Provide the path to ssl ca file', title='SSL CA'
-    )
-    sslCert: Optional[str] = Field(
-        None,
-        description='Provide the path to ssl client certificate file (ssl_cert)',
-        title='SSL Client Certificate File',
-    )
-    sslKey: Optional[str] = Field(
-        None,
-        description='Provide the path to ssl client certificate file (ssl_key)',
-        title='SSL Client Key File',
+    database: str = Field(
+        ...,
+        description='Database of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single database. When left blank, OpenMetadata Ingestion attempts to scan all the databases.',
+        title='Database',
+    )
+    ingestAllDatabases: Optional[bool] = Field(
+        False,
+        description='Ingest data from all databases in Postgres. You can use databaseFilterPattern on top of this.',
+        title='Ingest All Databases',
+    )
+    sslMode: Optional[verifySSLConfig.SslMode] = verifySSLConfig.SslMode.disable
+    sslConfig: Optional[verifySSLConfig.SslConfig] = None
+    classificationName: Optional[str] = Field(
+        'PostgresPolicyTags',
+        description='Custom OpenMetadata Classification name for Postgres policy tags.',
+        title='Classification Name',
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
+    supportsUsageExtraction: Optional[
+        connectionBasicType.SupportsUsageExtraction
+    ] = None
+    supportsLineageExtraction: Optional[
+        connectionBasicType.SupportsLineageExtraction
+    ] = None
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
+    supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
+        None, title='Supports Database'
+    )
     supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
         None, title='Supports Query Comment'
     )
     sampleDataStorageConfig: Optional[
         connectionBasicType.SampleDataStorageConfig
     ] = Field(None, title='Storage Config for Sample Data')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/oracleConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/pinotDBConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/greenplumConnection.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,106 +1,77 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/postgresConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/greenplumConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
 from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
-from .common import azureConfig, basicAuth, iamAuthConfig
+from .common import basicAuth, iamAuthConfig
 
 
-class SslMode(Enum):
-    disable = 'disable'
-    allow = 'allow'
-    prefer = 'prefer'
-    require = 'require'
-    verify_ca = 'verify-ca'
-    verify_full = 'verify-full'
+class GreenplumType(Enum):
+    Greenplum = 'Greenplum'
 
 
-class PostgresType(Enum):
-    Postgres = 'Postgres'
-
-
-class PostgresScheme(Enum):
+class GreenplumScheme(Enum):
     postgresql_psycopg2 = 'postgresql+psycopg2'
-    pgspider_psycopg2 = 'pgspider+psycopg2'
 
 
-class PostgresConnection(BaseModel):
+class GreenplumConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[PostgresType] = Field(
-        PostgresType.Postgres, description='Service Type', title='Service Type'
+    type: Optional[GreenplumType] = Field(
+        GreenplumType.Greenplum, description='Service Type', title='Service Type'
     )
-    scheme: Optional[PostgresScheme] = Field(
-        PostgresScheme.postgresql_psycopg2,
+    scheme: Optional[GreenplumScheme] = Field(
+        GreenplumScheme.postgresql_psycopg2,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
     username: str = Field(
         ...,
-        description='Username to connect to Postgres. This user should have privileges to read all the metadata in Postgres.',
+        description='Username to connect to Greenplum. This user should have privileges to read all the metadata in Greenplum.',
         title='Username',
     )
     authType: Optional[
-        Union[
-            basicAuth.BasicAuth,
-            iamAuthConfig.IamAuthConfigurationSource,
-            azureConfig.AzureConfigurationSource,
-        ]
+        Union[basicAuth.BasicAuth, iamAuthConfig.IamAuthConfigurationSource]
     ] = Field(
         None, description='Choose Auth Config Type.', title='Auth Configuration Type'
     )
     hostPort: str = Field(
         ..., description='Host and port of the source service.', title='Host and Port'
     )
     database: str = Field(
         ...,
         description='Database of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single database. When left blank, OpenMetadata Ingestion attempts to scan all the databases.',
         title='Database',
     )
+    sslMode: Optional[verifySSLConfig.SslMode] = verifySSLConfig.SslMode.disable
+    sslConfig: Optional[verifySSLConfig.SslConfig] = None
     ingestAllDatabases: Optional[bool] = Field(
         False,
-        description='Ingest data from all databases in Postgres. You can use databaseFilterPattern on top of this.',
+        description='Ingest data from all databases in Greenplum. You can use databaseFilterPattern on top of this.',
         title='Ingest All Databases',
     )
-    sslMode: Optional[SslMode] = Field(
-        SslMode.disable,
-        description='SSL Mode to connect to postgres database.',
-        title='SSL Mode',
-    )
-    sslConfig: Optional[verifySSLConfig.SslConfig] = None
-    classificationName: Optional[str] = Field(
-        'PostgresPolicyTags',
-        description='Custom OpenMetadata Classification name for Postgres policy tags.',
-        title='Classification Name',
-    )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
-    supportsUsageExtraction: Optional[
-        connectionBasicType.SupportsUsageExtraction
-    ] = None
-    supportsLineageExtraction: Optional[
-        connectionBasicType.SupportsLineageExtraction
-    ] = None
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
     supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
         None, title='Supports Database'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,69 +1,86 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/prestoConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/trinoConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Dict, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
-
 from .. import connectionBasicType
+from .common import basicAuth, jwtAuth
 
 
-class PrestoType(Enum):
-    Presto = 'Presto'
+class TrinoType(Enum):
+    Trino = 'Trino'
 
 
-class PrestoScheme(Enum):
-    presto = 'presto'
+class TrinoScheme(Enum):
+    trino = 'trino'
 
 
-class PrestoConnection(BaseModel):
+class TrinoConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[PrestoType] = Field(
-        PrestoType.Presto, description='Service Type', title='Service Type'
+    type: Optional[TrinoType] = Field(
+        TrinoType.Trino, description='Service Type', title='Service Type'
     )
-    scheme: Optional[PrestoScheme] = Field(
-        PrestoScheme.presto,
+    scheme: Optional[TrinoScheme] = Field(
+        TrinoScheme.trino,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
     username: str = Field(
         ...,
-        description='Username to connect to Presto. This user should have privileges to read all the metadata in Postgres.',
+        description='Username to connect to Trino. This user should have privileges to read all the metadata in Trino.',
         title='Username',
     )
-    password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to Presto.', title='Password'
+    authType: Optional[Union[basicAuth.BasicAuth, jwtAuth.JwtAuth]] = Field(
+        None, description='Choose Auth Config Type.', title='Auth Configuration Type'
     )
     hostPort: str = Field(
-        ..., description='Host and port of the Presto service.', title='Host and Port'
+        ..., description='Host and port of the Trino service.', title='Host and Port'
+    )
+    catalog: Optional[str] = Field(
+        None, description='Catalog of the data source.', title='Catalog'
     )
     databaseSchema: Optional[str] = Field(
         None,
-        description='Database Schema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single schema. When left blank, OpenMetadata Ingestion attempts to scan all the schemas.',
-        title='Database Schema',
+        description='databaseSchema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single databaseSchema. When left blank, OpenMetadata Ingestion attempts to scan all the databaseSchema.',
+        title='databaseSchema',
+    )
+    proxies: Optional[Dict[str, str]] = Field(
+        None,
+        description='Proxies for the connection to Trino data source',
+        title='Proxies',
+    )
+    verify: Optional[str] = Field(
+        None,
+        description='Verify ( Connection Argument for SSL ) to connect to Trino.',
+        title='Verify',
     )
-    catalog: Optional[str] = Field(None, description='Presto catalog', title='Catalog')
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
+    supportsUsageExtraction: Optional[
+        connectionBasicType.SupportsUsageExtraction
+    ] = Field(None, title='Supports Usage Extraction')
+    supportsLineageExtraction: Optional[
+        connectionBasicType.SupportsLineageExtraction
+    ] = Field(None, title='Supports Lineage Extraction')
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
     supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
         None, title='Supports Database'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/alationConnection.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,101 +1,99 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/redshiftConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/metadata/alationConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Any, Dict, Optional, Union
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import AnyUrl, BaseModel, Extra, Field
 
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
-
-from .....security.ssl import verifySSLConfig
+from .....security.credentials import apiAccessTokenAuth, basicAuth
 from .. import connectionBasicType
+from ..database import mysqlConnection, postgresConnection
 
 
-class SslMode(Enum):
-    disable = 'disable'
-    allow = 'allow'
-    prefer = 'prefer'
-    require = 'require'
-    verify_ca = 'verify-ca'
-    verify_full = 'verify-full'
-
-
-class RedshiftType(Enum):
-    Redshift = 'Redshift'
+class AlationType(Enum):
+    Alation = 'Alation'
 
 
-class RedshiftScheme(Enum):
-    redshift_psycopg2 = 'redshift+psycopg2'
-
-
-class RedshiftConnection(BaseModel):
+class AlationConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[RedshiftType] = Field(
-        RedshiftType.Redshift, description='Service Type', title='Service Type'
-    )
-    scheme: Optional[RedshiftScheme] = Field(
-        RedshiftScheme.redshift_psycopg2,
-        description='SQLAlchemy driver scheme options.',
-        title='Connection Scheme',
+    type: Optional[AlationType] = Field(AlationType.Alation, description='Service Type')
+    hostPort: AnyUrl = Field(
+        ..., description='Host and port of the Alation service.', title='Host and Port'
     )
-    username: str = Field(
+    authType: Union[basicAuth.BasicAuth, apiAccessTokenAuth.ApiAccessTokenAuth] = Field(
         ...,
-        description='Username to connect to Redshift. This user should have privileges to read all the metadata in Redshift.',
-        title='Username',
+        description='Types of methods used to authenticate to the alation instance',
+        title='Authentication type for Alation',
     )
-    password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to Redshift.', title='Password'
+    connection: Optional[
+        Union[
+            postgresConnection.PostgresConnection,
+            mysqlConnection.MysqlConnection,
+            Dict[str, Any],
+        ]
+    ] = Field(
+        None,
+        description='Choose between mysql and postgres connection for alation database',
+        title='Alation Database Connection',
+    )
+    projectName: Optional[str] = Field(
+        'AlationAPI',
+        description='Project name to create the refreshToken. Can be anything',
+        title='Project Name',
+    )
+    paginationLimit: Optional[int] = Field(
+        10,
+        description='Pagination limit used for Alation APIs pagination',
+        title='Pagination Limit',
     )
-    hostPort: str = Field(
-        ..., description='Host and port of the Redshift service.', title='Host and Port'
-    )
-    database: str = Field(
-        ...,
-        description='Initial Redshift database to connect to. If you want to ingest all databases, set ingestAllDatabases to true.',
-        title='Database',
-    )
-    ingestAllDatabases: Optional[bool] = Field(
+    includeUndeployedDatasources: Optional[bool] = Field(
         False,
-        description='Ingest data from all databases in Redshift. You can use databaseFilterPattern on top of this.',
-        title='Ingest All Databases',
+        description='Specifies if undeployed datasources should be included while ingesting.',
+        title='Include Undeployed Datasources',
     )
-    sslMode: Optional[SslMode] = Field(
-        SslMode.disable,
-        description='SSL Mode to connect to redshift database.',
-        title='SSL Mode',
-    )
-    sslConfig: Optional[verifySSLConfig.SslConfig] = None
-    connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
-        None, title='Connection Options'
+    includeHiddenDatasources: Optional[bool] = Field(
+        False,
+        description='Specifies if hidden datasources should be included while ingesting.',
+        title='Include Hidden Datasources',
     )
-    connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
-        None, title='Connection Arguments'
+    ingestDatasources: Optional[bool] = Field(
+        True,
+        description='Specifies if Datasources are to be ingested while running the ingestion job.',
+        title='Ingest Datasources',
+    )
+    ingestUsersAndGroups: Optional[bool] = Field(
+        True,
+        description='Specifies if Users and Groups are to be ingested while running the ingestion job.',
+        title='Ingest Users and Groups',
+    )
+    ingestDomains: Optional[bool] = Field(
+        True,
+        description='Specifies if Domains are to be ingested while running the ingestion job.',
+        title='Ingest Domains',
+    )
+    ingestKnowledgeArticles: Optional[bool] = Field(
+        True,
+        description='Specifies if Knowledge Articles are to be ingested while running the ingestion job.',
+        title='Ingest Knowledge Articles',
+    )
+    ingestDashboards: Optional[bool] = Field(
+        True,
+        description='Specifies if Dashboards are to be ingested while running the ingestion job.',
+        title='Ingest Dashboards',
+    )
+    alationTagClassificationName: Optional[str] = Field(
+        'alationTags',
+        description='Custom OpenMetadata Classification name for alation tags.',
+        title='Alation Tags Classification Name',
     )
+    connectionOptions: Optional[connectionBasicType.ConnectionOptions] = None
+    connectionArguments: Optional[connectionBasicType.ConnectionArguments] = None
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
-    ] = Field(None, title='Supports Metadata Extraction')
-    supportsUsageExtraction: Optional[
-        connectionBasicType.SupportsUsageExtraction
-    ] = None
-    supportsLineageExtraction: Optional[
-        connectionBasicType.SupportsLineageExtraction
     ] = None
-    supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
-    supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
-        None, title='Supports Profiler'
-    )
-    supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
-        None, title='Supports Database'
-    )
-    supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
-        None, title='Supports Query Comment'
-    )
-    sampleDataStorageConfig: Optional[
-        connectionBasicType.SampleDataStorageConfig
-    ] = Field(None, title='Storage Config for Sample Data')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/salesforceConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sapHana/sapHanaHDBConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sapHana/sapHanaHDBConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/sapHana/sapHanaHDBConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sapHana/sapHanaSQLConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sapHana/sapHanaSQLConnection.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/sapHana/sapHanaSQLConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sapHanaConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sapHanaConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/sapHanaConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sasConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sasConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/sasConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/singleStoreConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/snowflakeConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/sqliteConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,87 +1,85 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/trinoConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/mysqlConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Dict, Optional, Union
+from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
+from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
-from .common import basicAuth, jwtAuth
+from .common import azureConfig, basicAuth, iamAuthConfig
 
 
-class TrinoType(Enum):
-    Trino = 'Trino'
+class MySQLType(Enum):
+    Mysql = 'Mysql'
 
 
-class TrinoScheme(Enum):
-    trino = 'trino'
+class MySQLScheme(Enum):
+    mysql_pymysql = 'mysql+pymysql'
 
 
-class TrinoConnection(BaseModel):
+class MysqlConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[TrinoType] = Field(
-        TrinoType.Trino, description='Service Type', title='Service Type'
+    type: Optional[MySQLType] = Field(
+        MySQLType.Mysql, description='Service Type', title='Service Type'
     )
-    scheme: Optional[TrinoScheme] = Field(
-        TrinoScheme.trino,
+    scheme: Optional[MySQLScheme] = Field(
+        MySQLScheme.mysql_pymysql,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
     username: str = Field(
         ...,
-        description='Username to connect to Trino. This user should have privileges to read all the metadata in Trino.',
+        description='Username to connect to MySQL. This user should have privileges to read all the metadata in Mysql.',
         title='Username',
     )
-    authType: Optional[Union[basicAuth.BasicAuth, jwtAuth.JwtAuth]] = Field(
+    authType: Optional[
+        Union[
+            basicAuth.BasicAuth,
+            iamAuthConfig.IamAuthConfigurationSource,
+            azureConfig.AzureConfigurationSource,
+        ]
+    ] = Field(
         None, description='Choose Auth Config Type.', title='Auth Configuration Type'
     )
     hostPort: str = Field(
-        ..., description='Host and port of the Trino service.', title='Host and Port'
+        ..., description='Host and port of the MySQL service.', title='Host and Port'
     )
-    catalog: Optional[str] = Field(
-        None, description='Catalog of the data source.', title='Catalog'
+    databaseName: Optional[str] = Field(
+        None,
+        description='Optional name to give to the database in OpenMetadata. If left blank, we will use default as the database name.',
+        title='Database Name',
     )
     databaseSchema: Optional[str] = Field(
         None,
-        description='databaseSchema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single databaseSchema. When left blank, OpenMetadata Ingestion attempts to scan all the databaseSchema.',
-        title='databaseSchema',
+        description='Database Schema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single schema. When left blank, OpenMetadata Ingestion attempts to scan all the schemas.',
+        title='Database Schema',
     )
-    proxies: Optional[Dict[str, str]] = Field(
-        None,
-        description='Proxies for the connection to Trino data source',
-        title='Proxies',
+    sslConfig: Optional[verifySSLConfig.SslConfig] = Field(
+        None, description='SSL Configuration details.', title='SSL'
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
-    supportsUsageExtraction: Optional[
-        connectionBasicType.SupportsUsageExtraction
-    ] = Field(None, title='Supports Usage Extraction')
-    supportsLineageExtraction: Optional[
-        connectionBasicType.SupportsLineageExtraction
-    ] = Field(None, title='Supports Lineage Extraction')
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
-    supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
-        None, title='Supports Database'
-    )
     supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
         None, title='Supports Query Comment'
     )
     sampleDataStorageConfig: Optional[
         connectionBasicType.SampleDataStorageConfig
     ] = Field(None, title='Storage Config for Sample Data')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/unityCatalogConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/unityCatalogConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/unityCatalogConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/verticaConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/customMessagingConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/kafkaConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
+from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
 from . import saslMechanismType
 
 
 class SecurityProtocol(Enum):
     PLAINTEXT = 'PLAINTEXT'
     SASL_PLAINTEXT = 'SASL_PLAINTEXT'
@@ -74,10 +75,15 @@
         title='Consumer Config',
     )
     schemaRegistryConfig: Optional[Dict[str, Any]] = Field(
         {},
         description='Confluent Kafka Schema Registry Config. From https://docs.confluent.io/5.5.1/clients/confluent-kafka-python/index.html#confluent_kafka.schema_registry.SchemaRegistryClient',
         title='Schema Registry Config',
     )
+    schemaRegistrySSL: Optional[verifySSLConfig.SslConfig] = Field(
+        None,
+        description='Schema Registry SSL Config. Configuration for enabling SSL for the Schema Registry connection.',
+        title='Schema Registry SSL',
+    )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/kinesisConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/pulsarConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/redpandaConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/alationConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/mlmodelService.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,95 +1,102 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/metadata/alationConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/mlmodelService.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional, Union
+from typing import List, Optional, Union
 
-from pydantic import AnyUrl, BaseModel, Extra, Field
+from pydantic import BaseModel, Extra, Field
 
-from .....security.credentials import apiAccessTokenAuth, basicAuth
-from .. import connectionBasicType
-from ..database import mysqlConnection, postgresConnection
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
+from .connections import testConnectionResult
+from .connections.mlmodel import (
+    customMlModelConnection,
+    mlflowConnection,
+    sageMakerConnection,
+    sklearnConnection,
+)
 
 
-class AlationType(Enum):
-    Alation = 'Alation'
+class MlModelServiceType(Enum):
+    Mlflow = 'Mlflow'
+    Sklearn = 'Sklearn'
+    CustomMlModel = 'CustomMlModel'
+    SageMaker = 'SageMaker'
 
 
-class AlationConnection(BaseModel):
+class MlModelConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[AlationType] = Field(AlationType.Alation, description='Service Type')
-    hostPort: AnyUrl = Field(
-        ..., description='Host and port of the Alation service.', title='Host and Port'
-    )
-    authType: Union[basicAuth.BasicAuth, apiAccessTokenAuth.ApiAccessTokenAuth] = Field(
-        ...,
-        description='Types of methods used to authenticate to the alation instance',
-        title='Authentication type for Alation',
-    )
-    connection: Optional[
-        Union[postgresConnection.PostgresConnection, mysqlConnection.MysqlConnection]
-    ] = Field(
+    config: Optional[
+        Union[
+            mlflowConnection.MlflowConnection,
+            sklearnConnection.SklearnConnection,
+            customMlModelConnection.CustomMlModelConnection,
+            sageMakerConnection.SageMakerConnection,
+        ]
+    ] = None
+
+
+class MlModelService(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    id: basic.Uuid = Field(
+        ..., description='Unique identifier of this pipeline service instance.'
+    )
+    name: basic.EntityName = Field(
+        ..., description='Name that identifies this pipeline service.'
+    )
+    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
+        None, description='FullyQualifiedName same as `name`.'
+    )
+    serviceType: MlModelServiceType = Field(
+        ..., description='Type of pipeline service such as Airflow or Prefect...'
+    )
+    description: Optional[str] = Field(
+        None, description='Description of a pipeline service instance.'
+    )
+    displayName: Optional[str] = Field(
         None,
-        description='Choose between mysql and postgres connection for alation database',
-        title='Alation Database Connection',
+        description='Display Name that identifies this pipeline service. It could be title or label from the source services.',
+    )
+    version: Optional[entityHistory.EntityVersion] = Field(
+        None, description='Metadata version of the entity.'
+    )
+    updatedAt: Optional[basic.Timestamp] = Field(
+        None,
+        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
+    )
+    updatedBy: Optional[str] = Field(None, description='User who made the update.')
+    pipelines: Optional[entityReferenceList.EntityReferenceList] = Field(
+        None,
+        description='References to pipelines deployed for this pipeline service to extract metadata',
+    )
+    connection: Optional[MlModelConnection] = None
+    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
+        None, description='Last test connection results for this service'
+    )
+    tags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='Tags for this MlModel Service.'
+    )
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this pipeline service.'
+    )
+    href: Optional[basic.Href] = Field(
+        None, description='Link to the resource corresponding to this pipeline service.'
+    )
+    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
+        None, description='Change that lead to this version of the entity.'
+    )
+    deleted: Optional[bool] = Field(
+        False, description='When `true` indicates the entity has been soft deleted.'
+    )
+    dataProducts: Optional[entityReferenceList.EntityReferenceList] = Field(
+        None, description='List of data products this entity is part of.'
+    )
+    domain: Optional[entityReference.EntityReference] = Field(
+        None, description='Domain the MLModel service belongs to.'
     )
-    projectName: Optional[str] = Field(
-        'AlationAPI',
-        description='Project name to create the refreshToken. Can be anything',
-        title='Project Name',
-    )
-    paginationLimit: Optional[int] = Field(
-        10,
-        description='Pagination limit used for Alation APIs pagination',
-        title='Pagination Limit',
-    )
-    includeUndeployedDatasources: Optional[bool] = Field(
-        False,
-        description='Specifies if undeployed datasources should be included while ingesting.',
-        title='Include Undeployed Datasources',
-    )
-    includeHiddenDatasources: Optional[bool] = Field(
-        False,
-        description='Specifies if hidden datasources should be included while ingesting.',
-        title='Include Hidden Datasources',
-    )
-    ingestDatasources: Optional[bool] = Field(
-        True,
-        description='Specifies if Datasources are to be ingested while running the ingestion job.',
-        title='Ingest Datasources',
-    )
-    ingestUsersAndGroups: Optional[bool] = Field(
-        True,
-        description='Specifies if Users and Groups are to be ingested while running the ingestion job.',
-        title='Ingest Users and Groups',
-    )
-    ingestDomains: Optional[bool] = Field(
-        True,
-        description='Specifies if Domains are to be ingested while running the ingestion job.',
-        title='Ingest Domains',
-    )
-    ingestKnowledgeArticles: Optional[bool] = Field(
-        True,
-        description='Specifies if Knowledge Articles are to be ingested while running the ingestion job.',
-        title='Ingest Knowledge Articles',
-    )
-    ingestDashboards: Optional[bool] = Field(
-        True,
-        description='Specifies if Dashboards are to be ingested while running the ingestion job.',
-        title='Ingest Dashboards',
-    )
-    alationTagClassificationName: Optional[str] = Field(
-        'alationTags',
-        description='Custom OpenMetadata Classification name for alation tags.',
-        title='Alation Tags Classification Name',
-    )
-    connectionOptions: Optional[connectionBasicType.ConnectionOptions] = None
-    connectionArguments: Optional[connectionBasicType.ConnectionArguments] = None
-    supportsMetadataExtraction: Optional[
-        connectionBasicType.SupportsMetadataExtraction
-    ] = None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/amundsenConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/atlasConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/metadataESConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/openMetadataConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/customMlModelConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,38 +1,37 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/mlmodel/mlflowConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/pipeline/airbyteConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import AnyUrl, BaseModel, Extra, Field
+
+from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class MlflowType(Enum):
-    Mlflow = 'Mlflow'
+class AirbyteType(Enum):
+    Airbyte = 'Airbyte'
 
 
-class MlflowConnection(BaseModel):
+class AirbyteConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[MlflowType] = Field(
-        MlflowType.Mlflow, description='Service Type', title='Service Type'
+    type: Optional[AirbyteType] = Field(
+        AirbyteType.Airbyte, description='Service Type', title='Service Type'
     )
-    trackingUri: str = Field(
-        ...,
-        description='Mlflow Experiment tracking URI. E.g., http://localhost:5000',
-        title='Tracking URI',
+    hostPort: AnyUrl = Field(..., description='Pipeline Service Management/UI URL.')
+    username: Optional[str] = Field(
+        None, description='Username to connect to Airbyte.', title='Username'
     )
-    registryUri: str = Field(
-        ...,
-        description='Mlflow Model registry backend. E.g., mysql+pymysql://mlflow:password@localhost:3307/experiments',
-        title='Registry URI',
+    password: Optional[CustomSecretStr] = Field(
+        None, description='Password to connect to Airbyte.', title='Password'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/sageMakerConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/sklearnConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,37 +1,47 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/pipeline/airbyteConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/pipeline/domoPipelineConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class AirbyteType(Enum):
-    Airbyte = 'Airbyte'
+class DomoPipelineType(Enum):
+    DomoPipeline = 'DomoPipeline'
 
 
-class AirbyteConnection(BaseModel):
+class DomoPipelineConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[AirbyteType] = Field(
-        AirbyteType.Airbyte, description='Service Type', title='Service Type'
+    type: Optional[DomoPipelineType] = Field(
+        DomoPipelineType.DomoPipeline, description='Service Type', title='Service Type'
     )
-    hostPort: AnyUrl = Field(..., description='Pipeline Service Management/UI URL.')
-    username: Optional[str] = Field(
-        None, description='Username to connect to Airbyte.', title='Username'
+    clientId: str = Field(..., description='Client ID for DOMO', title='Client ID')
+    secretToken: CustomSecretStr = Field(
+        ..., description='Secret token to connect to DOMO', title='Secret Token'
     )
-    password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to Airbyte.', title='Password'
+    accessToken: Optional[str] = Field(
+        None, description='Access token to connect to DOMO', title='Access Token'
+    )
+    apiHost: Optional[str] = Field(
+        'api.domo.com',
+        description='API Host to connect to DOMO instance',
+        title='API Host',
+    )
+    instanceDomain: AnyUrl = Field(
+        ...,
+        description='URL of your Domo instance, e.g., https://openmetadata.domo.com',
+        title='Instance Domain',
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/airflowConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/backendConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/customPipelineConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/elasticSearchConnection.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,39 +1,51 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/pipeline/dagsterConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/search/elasticSearchConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
-
 from .. import connectionBasicType
+from ..common import sslConfig
+from .elasticSearch import apiAuth, basicAuth
 
 
-class DagsterType(Enum):
-    Dagster = 'Dagster'
+class ElasticSearchType(Enum):
+    ElasticSearch = 'ElasticSearch'
 
 
-class DagsterConnection(BaseModel):
+class ElasticsearchConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[DagsterType] = Field(
-        DagsterType.Dagster, description='Service Type', title='Service Type'
+    type: Optional[ElasticSearchType] = Field(
+        ElasticSearchType.ElasticSearch,
+        description='ElasticSearch Type',
+        title='ElasticSearch Type',
+    )
+    hostPort: Optional[AnyUrl] = Field(
+        None,
+        description='Host and port of the ElasticSearch service.',
+        title='Host and Port',
+    )
+    authType: Optional[
+        Union[basicAuth.BasicAuthentication, apiAuth.ApiKeyAuthentication]
+    ] = Field(
+        None, description='Choose Auth Config Type.', title='Auth Configuration Type'
     )
-    host: AnyUrl = Field(..., description='URL to the Dagster instance', title='Host')
-    token: Optional[CustomSecretStr] = Field(
-        None, description='To Connect to Dagster Cloud', title='Token'
+    sslConfig: Optional[sslConfig.SslConfig] = Field(None, title='SSL Config')
+    connectionTimeoutSecs: Optional[int] = Field(
+        30,
+        description='Connection Timeout in Seconds',
+        title='Connection Timeout in Seconds',
     )
-    timeout: Optional[int] = Field(
-        '1000',
-        description='Connection Time Limit Between OM and Dagster Graphql API in second',
-        title='Time Out',
+    connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
+        None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/databricksPipelineConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,47 +1,46 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/pipeline/domoPipelineConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/pipeline/fivetranConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class DomoPipelineType(Enum):
-    DomoPipeline = 'DomoPipeline'
+class FivetranType(Enum):
+    Fivetran = 'Fivetran'
 
 
-class DomoPipelineConnection(BaseModel):
+class FivetranConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[DomoPipelineType] = Field(
-        DomoPipelineType.DomoPipeline, description='Service Type', title='Service Type'
+    type: Optional[FivetranType] = Field(
+        FivetranType.Fivetran, description='Service Type', title='Service Type'
     )
-    clientId: str = Field(..., description='Client ID for DOMO', title='Client ID')
-    secretToken: CustomSecretStr = Field(
-        ..., description='Secret token to connect to DOMO', title='Secret Token'
+    apiKey: str = Field(
+        ..., description='Fivetran API Secret.', title='Fivetran API Key'
     )
-    accessToken: Optional[str] = Field(
-        None, description='Access token to connect to DOMO', title='Access Token'
+    hostPort: Optional[AnyUrl] = Field(
+        'https://api.fivetran.com',
+        description='Pipeline Service Management/UI URI.',
+        title='Host And Port',
     )
-    apiHost: Optional[str] = Field(
-        'api.domo.com',
-        description='API Host to connect to DOMO instance',
-        title='API Host',
+    apiSecret: CustomSecretStr = Field(
+        ..., description='Fivetran API Secret.', title='Fivetran API Secret'
     )
-    instanceDomain: AnyUrl = Field(
-        ...,
-        description='URL of your Domo instance, e.g., https://openmetadata.domo.com',
-        title='Instance Domain',
+    limit: Optional[int] = Field(
+        1000,
+        description='Fivetran API Limit For Pagination.',
+        title='Fivetran API Limit',
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/dashboard/qlikCloudConnection.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,46 +1,38 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/pipeline/fivetranConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/dashboard/qlikCloudConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class FivetranType(Enum):
-    Fivetran = 'Fivetran'
+class QlikCloudType(Enum):
+    QlikCloud = 'QlikCloud'
 
 
-class FivetranConnection(BaseModel):
+class QlikCloudConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[FivetranType] = Field(
-        FivetranType.Fivetran, description='Service Type', title='Service Type'
+    type: Optional[QlikCloudType] = Field(
+        QlikCloudType.QlikCloud, description='Service Type', title='Service Type'
     )
-    apiKey: str = Field(
-        ..., description='Fivetran API Secret.', title='Fivetran API Key'
+    token: CustomSecretStr = Field(
+        ..., description='token to connect to Qlik Cloud.', title='token'
     )
-    hostPort: Optional[AnyUrl] = Field(
-        'https://api.fivetran.com',
-        description='Pipeline Service Management/UI URI.',
-        title='Host And Port',
-    )
-    apiSecret: CustomSecretStr = Field(
-        ..., description='Fivetran API Secret.', title='Fivetran API Secret'
-    )
-    limit: Optional[int] = Field(
-        1000,
-        description='Fivetran API Limit For Pagination.',
-        title='Fivetran API Limit',
+    hostPort: AnyUrl = Field(
+        ...,
+        description='Host and Port of the Qlik Cloud instance.',
+        title='Host and Port',
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/gluePipelineConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/nifiConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/sparkConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/sparkConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/sparkConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/pipeline/splineConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/splineConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/splineConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/customSearchConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/customSearchConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/search/customSearchConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/apiAuth.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/ui/knowledgePanel.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,27 +1,24 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/search/elasticSearch/apiAuth.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  system/ui/knowledgePanel.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from enum import Enum
+from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
+class EntityType(Enum):
+    KnowledgePanel = 'KnowledgePanel'
 
-class ApiKeyAuthentication(BaseModel):
+
+class Team(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    apiKey: Optional[CustomSecretStr] = Field(
-        None,
-        description='Elastic Search API Key for API Authentication',
-        title='API Key',
-    )
-    apiKeyId: Optional[str] = Field(
-        None,
-        description='Elastic Search API Key ID for API Authentication',
-        title='API Key ID',
+    entityType: EntityType = Field(..., description='Entity Type.')
+    configuration: Optional[Dict[str, Any]] = Field(
+        None, description='Configuration for the Knowledge Panel.'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/basicAuth.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/elasticSearch/basicAuth.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/search/elasticSearch/basicAuth.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/elasticSearchConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/adlsConnection.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,51 +1,38 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/search/elasticSearchConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/storage/adlsConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional, Union
+from typing import Optional
 
-from pydantic import AnyUrl, BaseModel, Extra, Field
+from pydantic import BaseModel, Extra, Field
 
+from .....security.credentials import azureCredentials
 from .. import connectionBasicType
-from ..common import sslConfig
-from .elasticSearch import apiAuth, basicAuth
 
 
-class ElasticSearchType(Enum):
-    ElasticSearch = 'ElasticSearch'
+class AzureType(Enum):
+    ADLS = 'ADLS'
 
 
-class ElasticsearchConnection(BaseModel):
+class AdlsConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[ElasticSearchType] = Field(
-        ElasticSearchType.ElasticSearch,
-        description='ElasticSearch Type',
-        title='ElasticSearch Type',
+    type: Optional[AzureType] = Field(
+        AzureType.ADLS, description='Service Type', title='Service Type'
     )
-    hostPort: Optional[AnyUrl] = Field(
-        None,
-        description='Host and port of the ElasticSearch service.',
-        title='Host and Port',
+    credentials: azureCredentials.AzureCredentials = Field(
+        ..., description='Azure Credentials', title='Azure Credentials'
     )
-    authType: Optional[
-        Union[basicAuth.BasicAuthentication, apiAuth.ApiKeyAuthentication]
-    ] = Field(
-        None, description='Choose Auth Config Type.', title='Auth Configuration Type'
-    )
-    sslConfig: Optional[sslConfig.SslConfig] = Field(None, title='SSL Config')
-    connectionTimeoutSecs: Optional[int] = Field(
-        30,
-        description='Connection Timeout in Seconds',
-        title='Connection Timeout in Seconds',
+    connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
+        None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/search/openSearchConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/search/openSearchConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/search/openSearchConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/serviceConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/serviceConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/serviceConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/storage/adlsConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/storage/adlsConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/storage/gcsConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from .....security.credentials import azureCredentials
+from .....security.credentials import gcpCredentials
 from .. import connectionBasicType
 
 
-class AzureType(Enum):
-    ADLS = 'ADLS'
+class GcsType(Enum):
+    GCS = 'GCS'
 
 
-class AdlsConnection(BaseModel):
+class GcsConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[AzureType] = Field(
-        AzureType.ADLS, description='Service Type', title='Service Type'
+    type: Optional[GcsType] = Field(
+        GcsType.GCS, description='Service Type', title='Service Type'
     )
-    credentials: azureCredentials.AzureCredentials = Field(
-        ..., description='Azure Credentials', title='Azure Credentials'
+    credentials: gcpCredentials.GCPCredentials = Field(
+        ..., description='GCP Credentials', title='GCP Credentials'
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/storage/customStorageConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/customStorageConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/storage/customStorageConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/storage/gcsConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/storage/s3Connection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from .....security.credentials import gcpCredentials
+from .....security.credentials import awsCredentials
 from .. import connectionBasicType
 
 
-class GcsType(Enum):
-    GCS = 'GCS'
+class S3Type(Enum):
+    S3 = 'S3'
 
 
-class GcsConnection(BaseModel):
+class S3Connection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[GcsType] = Field(
-        GcsType.GCS, description='Service Type', title='Service Type'
+    type: Optional[S3Type] = Field(
+        S3Type.S3, description='Service Type', title='Service Type'
     )
-    credentials: gcpCredentials.GCPCredentials = Field(
-        ..., description='GCP Credentials', title='GCP Credentials'
+    awsConfig: awsCredentials.AWSCredentials = Field(
+        ..., title='AWS Credentials Configuration'
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/testConnectionDefinition.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/testConnectionResult.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/dashboardService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/dashboardService.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/dashboardService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -16,14 +16,15 @@
     domoDashboardConnection,
     lightdashConnection,
     lookerConnection,
     metabaseConnection,
     modeConnection,
     mstrConnection,
     powerBIConnection,
+    qlikCloudConnection,
     qlikSenseConnection,
     quickSightConnection,
     redashConnection,
     supersetConnection,
     tableauConnection,
 )
 
@@ -38,14 +39,15 @@
     Mode = 'Mode'
     CustomDashboard = 'CustomDashboard'
     DomoDashboard = 'DomoDashboard'
     QuickSight = 'QuickSight'
     QlikSense = 'QlikSense'
     Lightdash = 'Lightdash'
     Mstr = 'Mstr'
+    QlikCloud = 'QlikCloud'
 
 
 class DashboardConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     config: Optional[
@@ -59,14 +61,15 @@
             modeConnection.ModeConnection,
             customDashboardConnection.CustomDashboardConnection,
             domoDashboardConnection.DomoDashboardConnection,
             quickSightConnection.QuickSightConnection,
             qlikSenseConnection.QlikSenseConnection,
             lightdashConnection.LightdashConnection,
             mstrConnection.MstrConnection,
+            qlikCloudConnection.QlikCloudConnection,
         ]
     ] = None
 
 
 class DashboardService(BaseModel):
     class Config:
         extra = Extra.forbid
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/databaseService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/databaseService.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/databaseService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/ingestionPipelines/ingestionPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/ingestionPipelines/pipelineServiceClientResponse.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/ingestionPipelines/pipelineServiceClientResponse.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/ingestionPipelines/pipelineServiceClientResponse.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/ingestionPipelines/status.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/ingestionPipelines/status.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/ingestionPipelines/status.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/messagingService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/messagingService.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/messagingService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/metadataService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/metadataService.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/metadataService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/mlmodelService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/storageService.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,102 +1,101 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/mlmodelService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/storageService.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
 from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from .connections import testConnectionResult
-from .connections.mlmodel import (
-    customMlModelConnection,
-    mlflowConnection,
-    sageMakerConnection,
-    sklearnConnection,
+from .connections.storage import (
+    adlsConnection,
+    customStorageConnection,
+    gcsConnection,
+    s3Connection,
 )
 
 
-class MlModelServiceType(Enum):
-    Mlflow = 'Mlflow'
-    Sklearn = 'Sklearn'
-    CustomMlModel = 'CustomMlModel'
-    SageMaker = 'SageMaker'
+class StorageServiceType(Enum):
+    S3 = 'S3'
+    ADLS = 'ADLS'
+    GCS = 'GCS'
+    CustomStorage = 'CustomStorage'
 
 
-class MlModelConnection(BaseModel):
+class StorageConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     config: Optional[
         Union[
-            mlflowConnection.MlflowConnection,
-            sklearnConnection.SklearnConnection,
-            customMlModelConnection.CustomMlModelConnection,
-            sageMakerConnection.SageMakerConnection,
+            s3Connection.S3Connection,
+            adlsConnection.AdlsConnection,
+            gcsConnection.GcsConnection,
+            customStorageConnection.CustomStorageConnection,
         ]
     ] = None
 
 
-class MlModelService(BaseModel):
+class StorageService(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
-        ..., description='Unique identifier of this pipeline service instance.'
+        ..., description='Unique identifier of this storage service instance.'
     )
     name: basic.EntityName = Field(
-        ..., description='Name that identifies this pipeline service.'
+        ..., description='Name that identifies this storage service.'
     )
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
-    serviceType: MlModelServiceType = Field(
-        ..., description='Type of pipeline service such as Airflow or Prefect...'
-    )
-    description: Optional[str] = Field(
-        None, description='Description of a pipeline service instance.'
-    )
     displayName: Optional[str] = Field(
-        None,
-        description='Display Name that identifies this pipeline service. It could be title or label from the source services.',
+        None, description='Display Name that identifies this storage service.'
     )
-    version: Optional[entityHistory.EntityVersion] = Field(
-        None, description='Metadata version of the entity.'
+    serviceType: StorageServiceType = Field(
+        ..., description='Type of storage service such as S3, GCS, AZURE...'
     )
-    updatedAt: Optional[basic.Timestamp] = Field(
-        None,
-        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of a storage service instance.'
     )
-    updatedBy: Optional[str] = Field(None, description='User who made the update.')
+    connection: Optional[StorageConnection] = None
     pipelines: Optional[entityReferenceList.EntityReferenceList] = Field(
         None,
-        description='References to pipelines deployed for this pipeline service to extract metadata',
+        description='References to pipelines deployed for this storage service to extract metadata, usage, lineage etc..',
     )
-    connection: Optional[MlModelConnection] = None
     testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
         None, description='Last test connection results for this service'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this MlModel Service.'
+        None, description='Tags for this storage Service.'
     )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this pipeline service.'
+    version: Optional[entityHistory.EntityVersion] = Field(
+        None, description='Metadata version of the entity.'
     )
+    updatedAt: Optional[basic.Timestamp] = Field(
+        None,
+        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
+    )
+    updatedBy: Optional[str] = Field(None, description='User who made the update.')
     href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this pipeline service.'
+        None, description='Link to the resource corresponding to this storage service.'
+    )
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this storage service.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
     dataProducts: Optional[entityReferenceList.EntityReferenceList] = Field(
         None, description='List of data products this entity is part of.'
     )
     domain: Optional[entityReference.EntityReference] = Field(
-        None, description='Domain the MLModel service belongs to.'
+        None, description='Domain the Storage service belongs to.'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/pipelineService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/pipelineService.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/pipelineService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -16,15 +16,17 @@
     airflowConnection,
     customPipelineConnection,
     dagsterConnection,
     databricksPipelineConnection,
     domoPipelineConnection,
     fivetranConnection,
     gluePipelineConnection,
+    kafkaConnectConnection,
     nifiConnection,
+    openLineageConnection,
     sparkConnection,
     splineConnection,
 )
 
 
 class PipelineServiceType(Enum):
     Airflow = 'Airflow'
@@ -34,14 +36,16 @@
     Dagster = 'Dagster'
     Nifi = 'Nifi'
     DomoPipeline = 'DomoPipeline'
     CustomPipeline = 'CustomPipeline'
     DatabricksPipeline = 'DatabricksPipeline'
     Spline = 'Spline'
     Spark = 'Spark'
+    OpenLineage = 'OpenLineage'
+    KafkaConnect = 'KafkaConnect'
 
 
 class PipelineConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     config: Optional[
@@ -53,14 +57,16 @@
             dagsterConnection.DagsterConnection,
             nifiConnection.NifiConnection,
             domoPipelineConnection.DomoPipelineConnection,
             customPipelineConnection.CustomPipelineConnection,
             databricksPipelineConnection.DatabricksPipelineConnection,
             splineConnection.SplineConnection,
             sparkConnection.SparkConnection,
+            openLineageConnection.OpenLineageConnection,
+            kafkaConnectConnection.KafkaConnectConnection,
         ]
     ] = None
 
 
 class PipelineService(BaseModel):
     class Config:
         extra = Extra.forbid
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/searchService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/searchService.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/searchService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/services/storageService.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/testSuite.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,101 +1,102 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/storageService.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  tests/testSuite.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import List, Optional, Union
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
-from .connections import testConnectionResult
-from .connections.storage import (
-    adlsConnection,
-    customStorageConnection,
-    gcsConnection,
-    s3Connection,
-)
-
-
-class StorageServiceType(Enum):
-    S3 = 'S3'
-    ADLS = 'ADLS'
-    GCS = 'GCS'
-    CustomStorage = 'CustomStorage'
+from ..api.tests import createTestSuite
+from ..entity.services.connections import testConnectionResult
+from ..type import basic, entityHistory, entityReference, entityReferenceList
+from . import basic as basic_1
 
 
-class StorageConnection(BaseModel):
-    class Config:
-        extra = Extra.forbid
+class ServiceType(Enum):
+    TestSuite = 'TestSuite'
+
+
+class TestSuiteConnection(BaseModel):
+    config: None = None
+
 
-    config: Optional[
-        Union[
-            s3Connection.S3Connection,
-            adlsConnection.AdlsConnection,
-            gcsConnection.GcsConnection,
-            customStorageConnection.CustomStorageConnection,
-        ]
-    ] = None
+class ResultSummary(BaseModel):
+    testCaseName: Optional[basic.FullyQualifiedEntityName] = Field(
+        None, description='Name of the test case.'
+    )
+    status: Optional[basic_1.TestCaseStatus] = Field(
+        None, description='Status of the test case.'
+    )
+    timestamp: Optional[basic.Timestamp] = Field(
+        None, description='Timestamp of the test case execution.'
+    )
 
 
-class StorageService(BaseModel):
+class TestSuite(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: basic.Uuid = Field(
-        ..., description='Unique identifier of this storage service instance.'
+    id: Optional[basic.Uuid] = Field(
+        None, description='Unique identifier of this test suite instance.'
     )
-    name: basic.EntityName = Field(
-        ..., description='Name that identifies this storage service.'
-    )
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='FullyQualifiedName same as `name`.'
+    name: createTestSuite.TestSuiteEntityName = Field(
+        ..., description='Name that identifies this test suite.'
     )
     displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this storage service.'
+        None, description='Display Name that identifies this test suite.'
     )
-    serviceType: StorageServiceType = Field(
-        ..., description='Type of storage service such as S3, GCS, AZURE...'
+    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
+        None, description='FullyQualifiedName same as `name`.'
     )
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of a storage service instance.'
+        None, description='Description of the test suite.'
     )
-    connection: Optional[StorageConnection] = None
+    tests: Optional[List[entityReference.EntityReference]] = None
+    connection: Optional[TestSuiteConnection] = None
+    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = None
     pipelines: Optional[entityReferenceList.EntityReferenceList] = Field(
         None,
-        description='References to pipelines deployed for this storage service to extract metadata, usage, lineage etc..',
+        description='References to pipelines deployed for this database service to extract metadata, usage, lineage etc..',
     )
-    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
-        None, description='Last test connection results for this service'
+    serviceType: Optional[ServiceType] = Field(
+        ServiceType.TestSuite,
+        description='Type of database service such as MySQL, BigQuery, Snowflake, Redshift, Postgres...',
     )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this storage Service.'
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this TestCase definition.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this storage service.'
-    )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this storage service.'
+        None, description='Link to the resource corresponding to this entity.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
-    dataProducts: Optional[entityReferenceList.EntityReferenceList] = Field(
-        None, description='List of data products this entity is part of.'
+    executable: Optional[bool] = Field(
+        False,
+        description='Indicates if the test suite is executable. Set on the backend.',
+    )
+    executableEntityReference: Optional[entityReference.EntityReference] = Field(
+        None,
+        description='Entity reference the test suite is executed against. Only applicable if the test suite is executable.',
+    )
+    summary: Optional[basic_1.TestSummary] = Field(
+        None,
+        description='Summary of the previous day test cases execution for this test suite.',
     )
-    domain: Optional[entityReference.EntityReference] = Field(
-        None, description='Domain the Storage service belongs to.'
+    testCaseResultSummary: Optional[List[ResultSummary]] = Field(
+        None, description='Summary of test case execution'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/persona.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/tableQuery.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,47 +1,56 @@
 # generated by datamodel-codegen:
-#   filename:  entity/teams/persona.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  type/tableQuery.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReferenceList
+from . import basic
 
 
-class Team(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    id: basic.Uuid
-    name: basic.EntityName = Field(
-        ..., description="A unique name of Persona. Example 'data engineer'"
+class TableQuery(BaseModel):
+    query: str = Field(..., description='SQL query')
+    query_type: Optional[str] = Field(None, description='SQL query type')
+    exclude_usage: Optional[bool] = Field(
+        None,
+        description='Flag to check if query is to be excluded while processing usage',
     )
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='FullyQualifiedName same as `name`.'
+    userName: Optional[str] = Field(
+        None, description='Name of the user that executed the SQL query'
     )
-    displayName: Optional[str] = Field(
-        None, description="Name used for display purposes. Example 'Data Steward'."
+    startTime: Optional[str] = Field(
+        None, description='Start time of execution of SQL query'
     )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of the persona.'
+    endTime: Optional[str] = Field(
+        None, description='End time of execution of SQL query'
     )
-    updatedAt: Optional[basic.Timestamp] = Field(
-        None,
-        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
+    analysisDate: Optional[basic.DateTime] = Field(
+        None, description='Date of execution of SQL query'
+    )
+    aborted: Optional[bool] = Field(
+        None, description='Flag to check if query was aborted during execution'
     )
-    updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this entity.'
+    serviceName: str = Field(
+        ..., description='Name that identifies this database service.'
     )
-    users: Optional[entityReferenceList.EntityReferenceList] = Field(
-        None, description='Users that are assigned a persona.'
+    databaseName: Optional[str] = Field(
+        None, description='Database associated with the table in the query'
     )
-    version: Optional[entityHistory.EntityVersion] = Field(
-        None, description='Metadata version of the entity.'
+    databaseSchema: Optional[str] = Field(
+        None, description='Database schema of the associated with query'
     )
-    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
-        None, description='Change that lead to this version of the entity.'
+    duration: Optional[float] = Field(
+        None, description='How long did the query took to run in milliseconds.'
+    )
+
+
+class TableQueries(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    queries: Optional[List[TableQuery]] = Field(
+        None, description='Date of execution of SQL query'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/role.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/role.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/teams/role.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/team.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/team.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/teams/team.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/teamHierarchy.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/teamHierarchy.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/teams/teamHierarchy.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/teams/user.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/teams/user.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/teams/user.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/type.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/type.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/type.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/utils/entitiesCount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/utils/entitiesCount.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/utils/entitiesCount.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/utils/servicesCount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/utils/servicesCount.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/utils/servicesCount.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/entity/utils/supersetApiConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/utils/supersetApiConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/utils/supersetApiConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/alertMetrics.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/alertMetrics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/alertMetrics.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/api/createEventSubscription.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/api/createEventSubscription.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/api/createEventSubscription.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/emailAlertConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/emailAlertConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/emailAlertConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/eventFilterRule.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/eventFilterRule.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/eventFilterRule.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -19,14 +19,19 @@
 
 
 class Effect(Enum):
     include = 'include'
     exclude = 'exclude'
 
 
+class PrefixCondition(Enum):
+    AND = 'AND'
+    OR = 'OR'
+
+
 class EventFilterRule(BaseModel):
     class Config:
         extra = Extra.forbid
 
     name: Optional[str] = Field(None, description='Name of this Event Filter.')
     displayName: Optional[str] = Field(None, description='Display Name of the Filter.')
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
@@ -41,7 +46,11 @@
         ...,
         description='Expression in SpEL used for matching of a `Rule` based on entity, resource, and environmental attributes.',
     )
     arguments: Optional[List[str]] = Field(
         None, description='Arguments to the Condition.'
     )
     inputType: Optional[InputType] = None
+    prefixCondition: Optional[PrefixCondition] = Field(
+        PrefixCondition.AND,
+        description='Prefix Condition to be applied to the Condition.',
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/eventSubscription.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/eventSubscription.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/eventSubscription.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -24,14 +24,18 @@
 
 class ArgumentsInput(BaseModel):
     class Config:
         extra = Extra.forbid
 
     name: Optional[str] = Field(None, description='Name of the filter')
     effect: Optional[eventFilterRule.Effect] = eventFilterRule.Effect.include
+    prefixCondition: Optional[eventFilterRule.PrefixCondition] = Field(
+        eventFilterRule.PrefixCondition.AND,
+        description='Prefix Condition for the filter.',
+    )
     arguments: Optional[List[Argument]] = Field(None, description='Arguments List')
 
 
 class AlertFilteringInput(BaseModel):
     class Config:
         extra = Extra.forbid
 
@@ -62,15 +66,15 @@
     Owners = 'Owners'
     Mentions = 'Mentions'
     Followers = 'Followers'
     External = 'External'
 
 
 class SubscriptionType(Enum):
-    Generic = 'Generic'
+    Webhook = 'Webhook'
     Slack = 'Slack'
     MsTeams = 'MsTeams'
     GChat = 'GChat'
     Email = 'Email'
     ActivityFeed = 'ActivityFeed'
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/eventSubscriptionOffset.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/feed/tag.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 # generated by datamodel-codegen:
-#   filename:  events/eventSubscriptionOffset.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/feed/tag.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ..type import basic
+from ...type import tagLabel
 
 
-class EventSubscriptionOffset(BaseModel):
+class TagFeedInfo(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    offset: int = Field(..., description='Name of this Event Filter.')
-    timestamp: Optional[basic.Timestamp] = Field(
-        None, description='Update time of the job status.'
+    previousTags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='List of previous tags.'
+    )
+    updatedTags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='List of updated tags.'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/failedEvent.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/failedEvent.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/failedEvent.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/filterResourceDescriptor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/filterResourceDescriptor.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/filterResourceDescriptor.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/subscriptionResourceDescriptor.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/application.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/application.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/application.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/applicationPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/applicationPipeline.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/applicationPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,32 +1,45 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dashboardServiceMetadataPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from ..type import filterPattern
 
 
+class LineageInformation(BaseModel):
+    dbServiceNames: Optional[List[str]] = Field(
+        None,
+        description='List of Database Service Names for creation of lineage',
+        title='Database Service Names List',
+    )
+
+
 class DashboardMetadataConfigType(Enum):
     DashboardMetadata = 'DashboardMetadata'
 
 
 class DashboardServiceMetadataPipeline(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: Optional[DashboardMetadataConfigType] = Field(
         DashboardMetadataConfigType.DashboardMetadata, description='Pipeline type'
     )
+    lineageInformation: Optional[LineageInformation] = Field(
+        None,
+        description='Details required to generate Lineage',
+        title='Lineage Information',
+    )
     dashboardFilterPattern: Optional[filterPattern.FilterPattern] = Field(
         None,
         description='Regex to exclude or include dashboards that matches the pattern.',
         title='Dashboard Filter Pattern',
     )
     chartFilterPattern: Optional[filterPattern.FilterPattern] = Field(
         None,
@@ -39,19 +52,14 @@
         title='Data Model Filter Pattern',
     )
     projectFilterPattern: Optional[filterPattern.FilterPattern] = Field(
         None,
         description='Regex to exclude or include projects that matches the pattern.',
         title='Project Filter Pattern',
     )
-    dbServiceNames: Optional[List[str]] = Field(
-        None,
-        description='List of Database Service Names for creation of lineage',
-        title='Database Service Names List',
-    )
     includeOwners: Optional[bool] = Field(
         False,
         description='Enabling a flag will replace the current owner with a new owner from the source during metadata ingestion, if the current owner is null. It is recommended to keep the flag enabled to obtain the owner information during the first metadata ingestion.',
         title='Include Current Owners',
     )
     markDeletedDashboards: Optional[bool] = Field(
         True,
@@ -69,7 +77,12 @@
         title='Include Tags',
     )
     includeDataModels: Optional[bool] = Field(
         True,
         description='Optional configuration to toggle the ingestion of data models.',
         title='Include Data Models',
     )
+    includeDraftDashboard: Optional[bool] = Field(
+        True,
+        description='Optional Configuration to include/exclude draft dashboards. By default it will include draft dashboards',
+        title='Include Draft Dashboards',
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceMetadataPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -12,14 +12,35 @@
 from ..type import filterPattern
 
 
 class DatabaseMetadataConfigType(Enum):
     DatabaseMetadata = 'DatabaseMetadata'
 
 
+class Incremental(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    enabled: bool = Field(
+        ...,
+        description='If True, enables Metadata Extraction to be incremental',
+        title='Enabled',
+    )
+    lookbackDays: Optional[int] = Field(
+        7,
+        description='Number os days to search back for a successful pipeline run. The timestamp of the last found successful pipeline run will be used as a base to search for updated entities.',
+        title='Successful Pipeline Run Lookback Days',
+    )
+    safetyMarginDays: Optional[int] = Field(
+        1,
+        description='Number of days to add to the last successful pipeline run timestamp to search for updated entities.',
+        title='Safety Margin Days',
+    )
+
+
 class DatabaseServiceMetadataPipeline(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: Optional[DatabaseMetadataConfigType] = Field(
         DatabaseMetadataConfigType.DatabaseMetadata, description='Pipeline type'
     )
@@ -84,7 +105,17 @@
         title='Table Filter Pattern',
     )
     databaseFilterPattern: Optional[filterPattern.FilterPattern] = Field(
         None,
         description='Regex to only fetch databases that matches the pattern.',
         title='Database Filter Pattern',
     )
+    threads: Optional[int] = Field(
+        1,
+        description='Number of Threads to use in order to paralellize Table ingestion.',
+        title='Number of Threads',
+    )
+    incremental: Optional[Incremental] = Field(
+        None,
+        description='Use incremental Metadata extraction after the first execution. This is commonly done by getting the changes from Audit tables on the supporting databases.',
+        title='Incremental Metadata Extraction Configuration',
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceProfilerPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -40,53 +40,53 @@
         title='Database Filter Pattern',
     )
     includeViews: Optional[bool] = Field(
         True,
         description='Optional configuration to turn off fetching metadata for views.',
         title='Include Views',
     )
+    useFqnForFiltering: Optional[bool] = Field(
+        False,
+        description='Regex will be applied on fully qualified name (e.g service_name.db_name.schema_name.table_name) instead of raw name (e.g. table_name)',
+        title='Use FQN For Filtering',
+    )
+    generateSampleData: Optional[bool] = Field(
+        True,
+        description='Option to turn on/off generating sample data. If enabled, profiler will ingest sample data for each table.',
+        title='Generate Sample Data',
+    )
+    computeMetrics: Optional[bool] = Field(
+        True,
+        description='Option to turn on/off computing profiler metrics.',
+        title='Compute Metrics',
+    )
     processPiiSensitive: Optional[bool] = Field(
         False,
         description='Optional configuration to automatically tag columns that might contain sensitive information',
         title='Auto Tag PII',
     )
     confidence: Optional[float] = Field(
         80,
-        description='Set the Confidence value for which you want the column to be marked',
-        title='Confidence',
-    )
-    profileSample: Optional[float] = Field(
-        None,
-        description='Percentage of data or no. of rows we want to execute the profiler and tests on',
-        title='Profile Sample',
+        description='Set the Confidence value for which you want the column to be tagged as PII. Confidence value ranges from 0 to 100. A higher number will yield less false positives but more false negatives. A lower number will yield more false positives but less false negatives.',
+        title='PII Inference Confidence Level',
     )
     profileSampleType: Optional[table.ProfileSampleType] = Field(
         table.ProfileSampleType.PERCENTAGE, title='Profile Sample Type'
     )
-    generateSampleData: Optional[bool] = Field(
-        True,
-        description='Option to turn on/off generating sample data.',
-        title='Generate Sample Data',
-    )
-    computeMetrics: Optional[bool] = Field(
-        True,
-        description='Option to turn on/off computing profiler metrics.',
-        title='Compute Metrics',
+    profileSample: Optional[float] = Field(
+        None,
+        description='Percentage of data or no. of rows used to compute the profiler metrics and run data quality tests',
+        title='Profile Sample',
     )
     sampleDataCount: Optional[int] = Field(
         50,
-        description='Number of row of sample data to be generated',
+        description="Number of sample rows to ingest when 'Generate Sample Data' is enabled",
         title='Sample Data Rows Count',
     )
     threadCount: Optional[float] = Field(
         5,
         description='Number of threads to use during metric computations',
         title='Thread Count',
     )
     timeoutSeconds: Optional[int] = Field(
         43200, description='Profiler Timeout in Seconds', title='Timeout (in sec.)'
     )
-    useFqnForFiltering: Optional[bool] = Field(
-        False,
-        description='Regex will be applied on fully qualified name (e.g service_name.db_name.schema_name.table_name) instead of raw name (e.g. table_name)',
-        title='Use FQN For Filtering',
-    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceQueryLineagePipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceQueryUsagePipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtAzureConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtAzureConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtAzureConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtBucketDetails.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtCloudConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/dbtconfig/dbtGCSConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  metadataIngestion/dbtconfig/dbtS3Config.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...security.credentials import gcpCredentials
+from ...security.credentials import awsCredentials
 
 
 class DbtConfigType(Enum):
-    gcs = 'gcs'
+    s3 = 's3'
 
 
 class DbtPrefixConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
     dbtBucketName: Optional[str] = Field(
@@ -28,20 +28,20 @@
     dbtObjectPrefix: Optional[str] = Field(
         None,
         description='Path of the folder where the dbt files are stored',
         title='DBT Object Prefix',
     )
 
 
-class DbtGcsConfig(BaseModel):
+class DbtS3Config(BaseModel):
     class Config:
         extra = Extra.forbid
 
     dbtConfigType: DbtConfigType = Field(..., description='dbt Configuration type')
-    dbtSecurityConfig: gcpCredentials.GCPCredentials = Field(
-        ..., title='DBT GCS Security Config'
+    dbtSecurityConfig: awsCredentials.AWSCredentials = Field(
+        ..., title='DBT S3 Security Config'
     )
     dbtPrefixConfig: Optional[DbtPrefixConfig] = Field(
         None,
         description='Details of the bucket where the dbt files are stored',
         title='DBT Prefix Config',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtHttpConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtLocalConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/dbtconfig/dbtS3Config.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  metadataIngestion/dbtconfig/dbtGCSConfig.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...security.credentials import awsCredentials
+from ...security.credentials import gcpCredentials
 
 
 class DbtConfigType(Enum):
-    s3 = 's3'
+    gcs = 'gcs'
 
 
 class DbtPrefixConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
     dbtBucketName: Optional[str] = Field(
@@ -28,20 +28,20 @@
     dbtObjectPrefix: Optional[str] = Field(
         None,
         description='Path of the folder where the dbt files are stored',
         title='DBT Object Prefix',
     )
 
 
-class DbtS3Config(BaseModel):
+class DbtGcsConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
     dbtConfigType: DbtConfigType = Field(..., description='dbt Configuration type')
-    dbtSecurityConfig: awsCredentials.AWSCredentials = Field(
-        ..., title='DBT S3 Security Config'
+    dbtSecurityConfig: gcpCredentials.GCPCredentials = Field(
+        ..., title='DBT GCS Security Config'
     )
     dbtPrefixConfig: Optional[DbtPrefixConfig] = Field(
         None,
         description='Details of the bucket where the dbt files are stored',
         title='DBT Prefix Config',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/messagingServiceMetadataPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/metadataToElasticSearchPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/mlmodelServiceMetadataPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/pipeline/kafkaConnectConnection.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,53 +1,61 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/pipelineServiceMetadataPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/pipeline/kafkaConnectConnection.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import List, Optional
+from typing import Optional
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import AnyUrl, BaseModel, Extra, Field
 
-from ..type import filterPattern
+from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 
-class PipelineMetadataConfigType(Enum):
-    PipelineMetadata = 'PipelineMetadata'
+class KafkaConnectType(Enum):
+    KafkaConnect = 'KafkaConnect'
 
 
-class PipelineServiceMetadataPipeline(BaseModel):
+class BasicAuthentication(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[PipelineMetadataConfigType] = Field(
-        PipelineMetadataConfigType.PipelineMetadata, description='Pipeline type'
+    username: Optional[str] = Field(
+        None,
+        description='KafkaConnect user to authenticate to the API.',
+        title='Username',
     )
-    includeLineage: Optional[bool] = Field(
-        True,
-        description='Optional configuration to turn off fetching lineage from pipelines.',
-        title='Include Lineage',
+    password: Optional[CustomSecretStr] = Field(
+        None,
+        description='KafkaConnect password to authenticate to the API.',
+        title='Password',
     )
-    includeOwners: Optional[bool] = Field(
-        True,
-        description="Set the 'Include Owners' toggle to control whether to include owners to the ingested entity if the owner email matches with a user stored in the OM server as part of metadata ingestion. If the ingested entity already exists and has an owner, the owner will not be overwritten.",
-        title='Include Owners',
+
+
+class KafkaConnectConnection(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    type: Optional[KafkaConnectType] = Field(
+        KafkaConnectType.KafkaConnect, description='Service Type', title='Service Type'
     )
-    pipelineFilterPattern: Optional[filterPattern.FilterPattern] = Field(
-        None, description='Regex exclude pipelines.', title='Pipeline Filter Pattern'
+    hostPort: AnyUrl = Field(
+        ...,
+        description='KafkaConnect Service Management/UI URI.',
+        title='Host And Port',
     )
-    dbServiceNames: Optional[List[str]] = Field(
+    KafkaConnectConfig: Optional[BasicAuthentication] = Field(
         None,
-        description='List of Database Service Names for creation of lineage',
-        title='Database Service Names List',
+        description='We support username/password or No Authentication',
+        title='KafkaConnect Credentials Configuration',
     )
-    markDeletedPipelines: Optional[bool] = Field(
+    verifySSL: Optional[bool] = Field(
         True,
-        description='Optional configuration to soft delete Pipelines in OpenMetadata if the source Pipelines are deleted. Also, if the Pipeline is deleted, all the associated entities like lineage, etc., with that Pipeline will be deleted',
-        title='Mark Deleted Pipeline',
+        description='Boolean marking if we need to verify the SSL certs for KafkaConnect REST API. True by default.',
+        title='Verify SSL',
     )
-    includeTags: Optional[bool] = Field(
-        True,
-        description='Optional configuration to toggle the tags ingestion.',
-        title='Include Tags',
+    messagingServiceName: Optional[str] = Field(
+        None,
+        description='Name of the Kafka Messaging Service associated with this KafkaConnect Pipeline Service. e.g. local_kafka',
+        title='Kafka Service Name',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/searchServiceMetadataPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/searchServiceMetadataPipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/searchServiceMetadataPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/storage/containerMetadataConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/manifestMetadataConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/manifestMetadataConfig.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/storage/manifestMetadataConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageBucketDetails.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageBucketDetails.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/storage/storageBucketDetails.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataADLSConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataADLSConfig.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/storage/storageMetadataADLSConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataGCSConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataS3Config.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/storage/storageMetadataGCSConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  metadataIngestion/storage/storageMetadataS3Config.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...security.credentials import gcpCredentials
+from ...security.credentials import awsCredentials
 from . import storageBucketDetails
 
 
-class StorageMetadataGcsConfig(BaseModel):
+class StorageMetadataS3Config(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    securityConfig: Optional[gcpCredentials.GCPCredentials] = Field(
-        None, title='GCS Security Config'
+    securityConfig: Optional[awsCredentials.AWSCredentials] = Field(
+        None, title='S3 Security Config'
     )
     prefixConfig: storageBucketDetails.StorageMetadataBucketDetails = Field(
         ..., title='Storage Metadata Prefix Config'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataHttpConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataLocalConfig.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/storage/storageMetadataHttpConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  metadataIngestion/storage/storageMetadataLocalConfig.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 
-class StorageMetadataHttpConfig(BaseModel):
+class StorageMetadataLocalConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    manifestHttpPath: str = Field(
+    manifestFilePath: str = Field(
         ...,
-        description='Storage Metadata manifest http file path to extract locations to ingest from.',
-        title='Storage Metadata Manifest HTTP Path',
+        description='Storage Metadata manifest file path to extract locations to ingest from.',
+        title='Storage Metadata Manifest File Path',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storage/storageMetadataS3Config.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/services/connections/database/common/azureConfig.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,24 +1,20 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/storage/storageMetadataS3Config.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/services/connections/database/common/azureConfig.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...security.credentials import awsCredentials
-from . import storageBucketDetails
+from ......security.credentials import azureCredentials
 
 
-class StorageMetadataS3Config(BaseModel):
+class AzureConfigurationSource(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    securityConfig: Optional[awsCredentials.AWSCredentials] = Field(
-        None, title='S3 Security Config'
-    )
-    prefixConfig: storageBucketDetails.StorageMetadataBucketDetails = Field(
-        ..., title='Storage Metadata Prefix Config'
+    azureConfig: Optional[azureCredentials.AzureCredentials] = Field(
+        None, title='Azure Credentials Configuration'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/storageServiceMetadataPipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/testSuitePipeline.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/metadataIngestion/workflow.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/metadataIngestion/workflow.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/workflow.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/auth0SSOClientConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/azureSSOClientConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/azureSSOClientConfig.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/azureSSOClientConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/googleSSOClientConfig.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,20 +1,24 @@
 # generated by datamodel-codegen:
-#   filename:  security/client/customOidcSSOClientConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  security/client/googleSSOClientConfig.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
+from typing import Optional
+
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 
-class CustomOIDCSSOClientConfig(BaseModel):
+class GoogleSSOClientConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    clientId: str = Field(..., description='Custom OIDC Client ID.')
     secretKey: CustomSecretStr = Field(
-        ..., description='Custom OIDC Client Secret Key.'
+        ..., description='Google SSO client secret key path or contents.'
+    )
+    audience: Optional[str] = Field(
+        'https://www.googleapis.com/oauth2/v4/token',
+        description='Google SSO audience URL',
     )
-    tokenEndpoint: str = Field(..., description='Custom OIDC token endpoint.')
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/oidcClientConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/oidcClientConfig.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/oidcClientConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/oktaSSOClientConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/client/samlSSOClientConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/client/samlSSOClientConfig.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/samlSSOClientConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/accessTokenAuth.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/accessTokenAuth.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/accessTokenAuth.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/awsCredentials.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/awsCredentials.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/awsCredentials.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/azureCredentials.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/azureCredentials.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/azureCredentials.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/basicAuth.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/basicAuth.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/basicAuth.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/bitbucketCredentials.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/bitbucketCredentials.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/bitbucketCredentials.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/gcpCredentials.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/gcpCredentials.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/gcpCredentials.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/gcpExternalAccount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/gcpExternalAccount.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/gcpExternalAccount.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Dict, Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/gcpValues.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/gcpValues.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/gcpValues.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/gitCredentials.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/gitCredentials.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/gitCredentials.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/credentials/githubCredentials.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/credentials/githubCredentials.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/githubCredentials.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/secrets/secretsManagerConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/securityConfiguration.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/securityConfiguration.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/securityConfiguration.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/security/ssl/verifySSLConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/security/ssl/verifySSLConfig.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/ssl/verifySSLConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any
 
 from pydantic import BaseModel, Extra, Field
@@ -25,14 +25,23 @@
 
 class VerifySSL(Enum):
     no_ssl = 'no-ssl'
     ignore = 'ignore'
     validate = 'validate'
 
 
+class SslMode(Enum):
+    disable = 'disable'
+    allow = 'allow'
+    prefer = 'prefer'
+    require = 'require'
+    verify_ca = 'verify-ca'
+    verify_full = 'verify-full'
+
+
 class SslConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
     __root__: validateSSLClientConfig.ValidateSslClientConfig = Field(
         ..., description='Client SSL configuration', title='SSL Config'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/settings/settings.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/settings/settings.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  settings/settings.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Field
@@ -13,14 +13,15 @@
     authenticationConfiguration,
     authorizerConfiguration,
     elasticSearchConfiguration,
     eventHandlerConfiguration,
     fernetConfiguration,
     jwtTokenConfiguration,
     pipelineServiceClientConfiguration,
+    profilerConfiguration,
     slackAppConfiguration,
     taskNotificationConfiguration,
 )
 from ..email import smtpSettings
 
 
 class SettingType(Enum):
@@ -32,19 +33,20 @@
     airflowConfiguration = 'airflowConfiguration'
     fernetConfiguration = 'fernetConfiguration'
     slackEventPublishers = 'slackEventPublishers'
     secretsManagerConfiguration = 'secretsManagerConfiguration'
     sandboxModeEnabled = 'sandboxModeEnabled'
     slackChat = 'slackChat'
     emailConfiguration = 'emailConfiguration'
-    customLogoConfiguration = 'customLogoConfiguration'
+    customUiThemePreference = 'customUiThemePreference'
     loginConfiguration = 'loginConfiguration'
     slackAppConfiguration = 'slackAppConfiguration'
     slackBot = 'slackBot'
     slackInstaller = 'slackInstaller'
+    profilerConfiguration = 'profilerConfiguration'
 
 
 class Settings(BaseModel):
     config_type: SettingType = Field(
         ..., description='Unique identifier that identifies an entity instance.'
     )
     config_value: Optional[
@@ -55,9 +57,10 @@
             elasticSearchConfiguration.ElasticSearchConfiguration,
             eventHandlerConfiguration.EventHandlerConfiguration,
             fernetConfiguration.FernetConfiguration,
             jwtTokenConfiguration.JWTTokenConfiguration,
             taskNotificationConfiguration.TaskNotificationConfiguration,
             smtpSettings.SmtpSettings,
             slackAppConfiguration.SlackAppConfiguration,
+            profilerConfiguration.ProfilerConfiguration,
         ]
     ] = None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/eventPublisherJob.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/eventPublisherJob.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  system/eventPublisherJob.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/indexingError.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/indexingError.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  system/indexingError.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/ui/knowledgePanel.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/filterPattern.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 # generated by datamodel-codegen:
-#   filename:  system/ui/knowledgePanel.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  type/filterPattern.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
-from enum import Enum
-from typing import Any, Dict, Optional
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
 
-class EntityType(Enum):
-    KnowledgePanel = 'KnowledgePanel'
+class FilterPatternModel(BaseModel):
+    pass
 
 
-class Team(BaseModel):
+class FilterPattern(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    entityType: EntityType = Field(..., description='Entity Type.')
-    configuration: Optional[Dict[str, Any]] = Field(
-        None, description='Configuration for the Knowledge Panel.'
+    includes: Optional[List[str]] = Field(
+        [],
+        description='List of strings/regex patterns to match and include only database entities that match.',
+    )
+    excludes: Optional[List[str]] = Field(
+        [],
+        description='List of strings/regex patterns to match and exclude only database entities that match.',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/ui/page.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/ui/page.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  system/ui/page.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/system/validationResponse.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/system/validationResponse.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  system/validationResponse.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/basic.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/basic.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  tests/basic.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Field
@@ -16,28 +16,14 @@
     __root__: Any = Field(
         ...,
         description='This schema defines basic types that are used by other test schemas.',
         title='Basic',
     )
 
 
-class TestSummary(BaseModel):
-    success: Optional[int] = Field(
-        None, description='Number of test cases that passed.'
-    )
-    failed: Optional[int] = Field(None, description='Number of test cases that failed.')
-    aborted: Optional[int] = Field(
-        None, description='Number of test cases that aborted.'
-    )
-    queued: Optional[int] = Field(
-        None, description='Number of test cases that are queued for execution.'
-    )
-    total: Optional[int] = Field(None, description='Total number of test cases.')
-
-
 class TestResultValue(BaseModel):
     name: Optional[str] = Field(None, description='name of the value')
     value: Optional[str] = Field(None, description='test result value')
 
 
 class TestCaseStatus(Enum):
     Success = 'Success'
@@ -48,14 +34,44 @@
 
 class TestSuiteExecutionFrequency(Enum):
     Hourly = 'Hourly'
     Daily = 'Daily'
     Weekly = 'Weekly'
 
 
+class ColumnTestSummaryDefinition(BaseModel):
+    success: Optional[int] = Field(
+        None, description='Number of test cases that passed.'
+    )
+    failed: Optional[int] = Field(None, description='Number of test cases that failed.')
+    aborted: Optional[int] = Field(
+        None, description='Number of test cases that aborted.'
+    )
+    queued: Optional[int] = Field(
+        None, description='Number of test cases that are queued for execution.'
+    )
+    total: Optional[int] = Field(None, description='Total number of test cases.')
+    entityLink: Optional[basic.EntityLink] = None
+
+
+class TestSummary(BaseModel):
+    success: Optional[int] = Field(
+        None, description='Number of test cases that passed.'
+    )
+    failed: Optional[int] = Field(None, description='Number of test cases that failed.')
+    aborted: Optional[int] = Field(
+        None, description='Number of test cases that aborted.'
+    )
+    queued: Optional[int] = Field(
+        None, description='Number of test cases that are queued for execution.'
+    )
+    total: Optional[int] = Field(None, description='Total number of test cases.')
+    columnTestSummary: Optional[List[ColumnTestSummaryDefinition]] = None
+
+
 class TestCaseResult(BaseModel):
     timestamp: Optional[basic.Timestamp] = Field(
         None, description='Data one which test case result is taken.'
     )
     testCaseStatus: Optional[TestCaseStatus] = Field(
         None, description='Status of Test Case run.'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/customMetric.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/customMetric.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  tests/customMetric.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/resolved.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/resolved.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  tests/resolved.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/testCase.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/testCase.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 # generated by datamodel-codegen:
 #   filename:  tests/testCase.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from ..entity.data import table
 from ..type import basic, entityHistory, entityReference
 from . import basic as basic_1
 from . import testSuite
 
 
 class TestCaseParameterValue(BaseModel):
     name: Optional[str] = Field(
@@ -75,7 +76,17 @@
     computePassedFailedRowCount: Optional[bool] = Field(
         False, description='Compute the passed and failed row count for the test case.'
     )
     incidentId: Optional[basic.Uuid] = Field(
         None,
         description='Reference to an ongoing Incident ID (stateId) for this test case.',
     )
+    failedRowsSample: Optional[table.TableData] = Field(
+        None, description='Sample of failed rows for this test case.'
+    )
+    inspectionQuery: Optional[basic.SqlQuery] = Field(
+        None, description='SQL query to retrieve the failed rows for this test case.'
+    )
+    domain: Optional[entityReference.EntityReference] = Field(
+        None,
+        description='Domain the test case belongs to. When not set, the test case inherits the domain from the table it belongs to.',
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/testCaseResolutionStatus.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/testCaseResolutionStatus.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  tests/testCaseResolutionStatus.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/testDefinition.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/tests/testDefinition.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  tests/testDefinition.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -40,14 +40,31 @@
 
 
 class EntityType(Enum):
     TABLE = 'TABLE'
     COLUMN = 'COLUMN'
 
 
+class Rule(Enum):
+    EQUALS = 'EQUALS'
+    NOT_EQUALS = 'NOT_EQUALS'
+    GREATER_THAN_OR_EQUALS = 'GREATER_THAN_OR_EQUALS'
+    LESS_THAN_OR_EQUALS = 'LESS_THAN_OR_EQUALS'
+
+
+class ValidationRule(BaseModel):
+    parameterField: Optional[str] = Field(
+        None, description='Name of the parameter to validate against.'
+    )
+    rule: Optional[Rule] = Field(
+        None,
+        description='This enum defines the type to use for a parameter validation rule.',
+    )
+
+
 class TestCaseParameterDefinition(BaseModel):
     name: Optional[str] = Field(None, description='name of the parameter.')
     displayName: Optional[str] = Field(
         None, description='Display Name that identifies this parameter name.'
     )
     dataType: Optional[TestDataType] = Field(
         None, description='Data type of the parameter (int, date etc.).'
@@ -55,14 +72,17 @@
     description: Optional[basic.Markdown] = Field(
         None, description='Description of the parameter.'
     )
     required: Optional[bool] = Field(False, description='Is this parameter required.')
     optionValues: Optional[List] = Field(
         [], description='List of values that can be passed for this parameter.'
     )
+    validationRule: Optional[ValidationRule] = Field(
+        None, description='Validation for the test parameter value.'
+    )
 
 
 class TestDefinition(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: Optional[basic.Uuid] = Field(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/tests/testSuite.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityLineage.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,102 +1,102 @@
 # generated by datamodel-codegen:
-#   filename:  tests/testSuite.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  type/entityLineage.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ..api.tests import createTestSuite
-from ..entity.services.connections import testConnectionResult
-from ..type import basic, entityHistory, entityReference, entityReferenceList
-from . import basic as basic_1
+from . import basic, entityReference
 
 
-class ServiceType(Enum):
-    TestSuite = 'TestSuite'
+class ColumnLineage(BaseModel):
+    fromColumns: Optional[List[basic.FullyQualifiedEntityName]] = Field(
+        None,
+        description='One or more source columns identified by fully qualified column name used by transformation function to create destination column.',
+    )
+    toColumn: Optional[basic.FullyQualifiedEntityName] = Field(
+        None,
+        description='Destination column identified by fully qualified column name created by the transformation of source columns.',
+    )
+    function: Optional[basic.SqlFunction] = Field(
+        None,
+        description='Transformation function applied to source columns to create destination column. That is `function(fromColumns) -> toColumn`.',
+    )
 
 
-class TestSuiteConnection(BaseModel):
-    config: None = None
+class Source(Enum):
+    Manual = 'Manual'
+    ViewLineage = 'ViewLineage'
+    QueryLineage = 'QueryLineage'
+    PipelineLineage = 'PipelineLineage'
+    DashboardLineage = 'DashboardLineage'
+    DbtLineage = 'DbtLineage'
+    SparkLineage = 'SparkLineage'
+    OpenLineage = 'OpenLineage'
+    ExternalTableLineage = 'ExternalTableLineage'
 
 
-class ResultSummary(BaseModel):
-    testCaseName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='Name of the test case.'
+class LineageDetails(BaseModel):
+    sqlQuery: Optional[basic.SqlQuery] = Field(
+        None, description='SQL used for transformation.'
+    )
+    columnsLineage: Optional[List[ColumnLineage]] = Field(
+        None,
+        description='Lineage information of how upstream columns were combined to get downstream column.',
     )
-    status: Optional[basic_1.TestCaseStatus] = Field(
-        None, description='Status of the test case.'
+    pipeline: Optional[entityReference.EntityReference] = Field(
+        None, description='Pipeline where the sqlQuery is periodically run.'
     )
-    timestamp: Optional[basic.Timestamp] = Field(
-        None, description='Timestamp of the test case execution.'
+    description: Optional[str] = Field(None, description='description of lineage')
+    source: Optional[Source] = Field(
+        Source.Manual, description='Lineage type describes how a lineage was created.'
     )
 
 
-class TestSuite(BaseModel):
+class Edge(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier of this test suite instance.'
-    )
-    name: createTestSuite.TestSuiteEntityName = Field(
-        ..., description='Name that identifies this test suite.'
-    )
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this test suite.'
-    )
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='FullyQualifiedName same as `name`.'
-    )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of the test suite.'
-    )
-    tests: Optional[List[entityReference.EntityReference]] = None
-    connection: Optional[TestSuiteConnection] = None
-    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = None
-    pipelines: Optional[entityReferenceList.EntityReferenceList] = Field(
-        None,
-        description='References to pipelines deployed for this database service to extract metadata, usage, lineage etc..',
-    )
-    serviceType: Optional[ServiceType] = Field(
-        ServiceType.TestSuite,
-        description='Type of database service such as MySQL, BigQuery, Snowflake, Redshift, Postgres...',
+    fromEntity: basic.Uuid = Field(
+        ..., description='From entity that is upstream of lineage edge.'
     )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this TestCase definition.'
+    toEntity: basic.Uuid = Field(
+        ..., description='To entity that is downstream of lineage edge.'
     )
-    version: Optional[entityHistory.EntityVersion] = Field(
-        None, description='Metadata version of the entity.'
-    )
-    updatedAt: Optional[basic.Timestamp] = Field(
+    description: Optional[basic.Markdown] = None
+    lineageDetails: Optional[LineageDetails] = Field(
         None,
-        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
-    )
-    updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this entity.'
-    )
-    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
-        None, description='Change that lead to this version of the entity.'
-    )
-    deleted: Optional[bool] = Field(
-        False, description='When `true` indicates the entity has been soft deleted.'
+        description='Optional lineageDetails provided only for table to table lineage edge.',
     )
-    executable: Optional[bool] = Field(
-        False,
-        description='Indicates if the test suite is executable. Set on the backend.',
+
+
+class EntitiesEdge(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    fromEntity: entityReference.EntityReference = Field(
+        ..., description='From entity that is upstream of lineage edge.'
     )
-    executableEntityReference: Optional[entityReference.EntityReference] = Field(
-        None,
-        description='Entity reference the test suite is executed against. Only applicable if the test suite is executable.',
+    toEntity: entityReference.EntityReference = Field(
+        ..., description='To entity that is downstream of lineage edge.'
     )
-    summary: Optional[basic_1.TestSummary] = Field(
+    description: Optional[basic.Markdown] = None
+    lineageDetails: Optional[LineageDetails] = Field(
         None,
-        description='Summary of the previous day test cases execution for this test suite.',
+        description='Optional lineageDetails provided only for table to table lineage edge.',
     )
-    testCaseResultSummary: Optional[List[ResultSummary]] = Field(
-        None, description='Summary of test case execution'
+
+
+class EntityLineage(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    entity: entityReference.EntityReference = Field(
+        ..., description='Primary entity for which this lineage graph is created.'
     )
+    nodes: Optional[List[entityReference.EntityReference]] = None
+    upstreamEdges: Optional[List[Edge]] = None
+    downstreamEdges: Optional[List[Edge]] = None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/auditLog.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/auditLog.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/auditLog.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/basic.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/basic.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/basic.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from datetime import date, datetime, time
 from enum import Enum
 from typing import Any, Dict, List, Optional
 from uuid import UUID
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/bulkOperationResult.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/bulkOperationResult.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/bulkOperationResult.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/changeEvent.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/changeEvent.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/changeEvent.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/changeEventType.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/changeEventType.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/changeEventType.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 
 
 class EventType(Enum):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/collectionDescriptor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/collectionDescriptor.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/collectionDescriptor.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/csvDocumentation.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/csvDocumentation.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/csvDocumentation.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/csvFile.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/csvFile.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/csvFile.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/csvImportResult.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/csvImportResult.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/csvImportResult.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/customProperty.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/customProperty.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 # generated by datamodel-codegen:
 #   filename:  type/customProperty.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
-from typing import Optional, Union
+from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
 from . import basic, entityReference
 from .customProperties import enumConfig
 
 
 class Format(BaseModel):
     __root__: str = Field(
         ..., description='Applies to date interval, date, time format.'
     )
 
 
 class EntityTypes(BaseModel):
-    __root__: str = Field(
+    __root__: List[str] = Field(
         ...,
         description='Applies to Entity References. Entity Types can be used to restrict what type of entities can be configured for a entity reference.',
     )
 
 
 class CustomPropertyConfig(BaseModel):
     class Config:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/databaseConnectionConfig.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/databaseConnectionConfig.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/databaseConnectionConfig.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityHistory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityHistory.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityHistory.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityReference.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityReference.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityReference.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityReferenceList.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityReferenceList.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityReferenceList.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityRelationship.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityRelationship.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityRelationship.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/entityUsage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityUsage.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityUsage.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/filterPattern.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/entity/applications/configuration/external/automator/addTierAction.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,27 +1,31 @@
 # generated by datamodel-codegen:
-#   filename:  type/filterPattern.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  entity/applications/configuration/external/automator/addTierAction.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
-from typing import List, Optional
+from enum import Enum
+from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from ......type import tagLabel
 
-class FilterPatternModel(BaseModel):
-    pass
 
+class AddTierActionType(Enum):
+    AddTierAction = 'AddTierAction'
 
-class FilterPattern(BaseModel):
+
+class AddTierAction(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    includes: Optional[List[str]] = Field(
-        [],
-        description='List of strings/regex patterns to match and include only database entities that match.',
+    type: AddTierActionType = Field(
+        ..., description='Application Type', title='Application Type'
     )
-    excludes: Optional[List[str]] = Field(
-        [],
-        description='List of strings/regex patterns to match and exclude only database entities that match.',
+    tier: tagLabel.TagLabel = Field(..., description='tier to apply')
+    overwriteMetadata: Optional[bool] = Field(
+        False,
+        description='Update the tier even if it is defined in the asset. By default, we will only apply the tier to assets without tier.',
+        title='Overwrite Metadata',
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/function.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/function.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/function.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/jdbcConnection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/jdbcConnection.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/jdbcConnection.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 
 class DriverClass(BaseModel):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/lifeCycle.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/lifeCycle.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/lifeCycle.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/paging.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/paging.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/paging.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -20,10 +20,13 @@
     after: Optional[str] = Field(
         None,
         description='After cursor used for getting the next page (see API pagination for details).',
     )
     offset: Optional[int] = Field(
         None, description='Offset used in case of offset based pagination.'
     )
+    limit: Optional[int] = Field(
+        None, description='Limit used in case of offset based pagination.'
+    )
     total: int = Field(
         ..., description='Total number of entries available to page through.'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/profile.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/profile.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/profile.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/queryParserData.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/queryParserData.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/queryParserData.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/reaction.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/reaction.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/reaction.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/schedule.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/api/feed/closeTask.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,24 +1,23 @@
 # generated by datamodel-codegen:
-#   filename:  type/schedule.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  api/feed/closeTask.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from . import basic
+from ...type import basic
 
 
-class Schedule(BaseModel):
+class CloseTaskRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    startDate: Optional[basic.DateTime] = Field(
-        None, description='Start date and time of the schedule.'
+    comment: str = Field(
+        ..., description='The closing comment explaining why the task is being closed.'
     )
-    repeatFrequency: Optional[basic.Duration] = Field(
-        None,
-        description="Repeat frequency in ISO 8601 duration format. Example - 'P23DT23H'.",
+    testCaseFQN: Optional[basic.FullyQualifiedEntityName] = Field(
+        None, description='Fully qualified name of the test case.'
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/schema.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/schema.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/schema.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/tableQuery.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/entityHierarchy.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,56 +1,39 @@
 # generated by datamodel-codegen:
-#   filename:  type/tableQuery.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   filename:  type/entityHierarchy.json
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from . import basic
 
 
-class TableQuery(BaseModel):
-    query: str = Field(..., description='SQL query')
-    query_type: Optional[str] = Field(None, description='SQL query type')
-    exclude_usage: Optional[bool] = Field(
-        None,
-        description='Flag to check if query is to be excluded while processing usage',
-    )
-    userName: Optional[str] = Field(
-        None, description='Name of the user that executed the SQL query'
-    )
-    startTime: Optional[str] = Field(
-        None, description='Start time of execution of SQL query'
-    )
-    endTime: Optional[str] = Field(
-        None, description='End time of execution of SQL query'
-    )
-    analysisDate: Optional[basic.DateTime] = Field(
-        None, description='Date of execution of SQL query'
+class EntityHierarchy(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    id: basic.Uuid = Field(
+        ..., description='Unique identifier of an entity hierarchy instance.'
     )
-    aborted: Optional[bool] = Field(
-        None, description='Flag to check if query was aborted during execution'
+    name: basic.EntityName = Field(
+        ..., description='Preferred name for the entity hierarchy.'
     )
-    serviceName: str = Field(
-        ..., description='Name that identifies this database service.'
+    displayName: Optional[str] = Field(
+        None, description='Display name that identifies this hierarchy.'
     )
-    databaseName: Optional[str] = Field(
-        None, description='Database associated with the table in the query'
+    description: basic.Markdown = Field(
+        ..., description='Description of the entity hierarchy.'
     )
-    databaseSchema: Optional[str] = Field(
-        None, description='Database schema of the associated with query'
+    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
+        None,
+        description='A unique name that identifies an entity within the hierarchy. It captures name hierarchy in the form of `rootEntity.childEntity`.',
     )
-    duration: Optional[float] = Field(
-        None, description='How long did the query took to run in milliseconds.'
+    children: Optional[List[EntityHierarchy]] = Field(
+        None, description='Other entities that are children of this entity.'
     )
 
 
-class TableQueries(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    queries: Optional[List[TableQuery]] = Field(
-        None, description='Date of execution of SQL query'
-    )
+EntityHierarchy.update_forward_refs()
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/tableUsageCount.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/tableUsageCount.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/tableUsageCount.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/tagLabel.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/tagLabel.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/tagLabel.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/usageDetails.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/usageDetails.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/usageDetails.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat, conint
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/generated/schema/type/votes.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/generated/schema/type/votes.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/votes.json
-#   timestamp: 2024-05-07T11:23:54+00:00
+#   timestamp: 2024-05-02T07:41:57+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/great_expectations/action.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/great_expectations/action.py`

 * *Files 0% similar despite different names*

```diff
@@ -122,15 +122,14 @@
     ):
         """main function to implement great expectation hook
 
         Args:
             validation_result_suite: result suite returned when checkpoint is ran
             validation_result_suite_identifier: type of result suite
             data_asset:
-            payload:
             expectation_suite_identifier: type of expectation suite
             checkpoint_identifier: identifier for the checkpoint
         """
 
         check_point_spec = self._get_checkpoint_batch_spec(data_asset)
         table_entity = None
         if isinstance(check_point_spec, SqlAlchemyDatasourceBatchSpec):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/great_expectations/utils/ometa_config_handler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/great_expectations/utils/ometa_config_handler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/closeable.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/closeable.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/common.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/common.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/delete.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/delete.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Delete methods
 """
 import traceback
-from typing import Dict, Iterable, Optional, Type
+from typing import Dict, Iterable, List, Optional, Type
 
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
 )
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.models.delete_entity import DeleteEntity
 from metadata.ingestion.ometa.ometa_api import OpenMetadata, T
@@ -51,10 +51,42 @@
                     )
                 )
     except Exception as exc:
         yield Either(
             left=StackTraceError(
                 name="Delete Entity",
                 error=f"Error deleting {entity_type.__class__}: {exc}",
+                stackTrace=traceback.format_exc(),
+            )
+        )
+
+
+def delete_entity_by_name(
+    metadata: OpenMetadata,
+    entity_type: Type[T],
+    entity_names: List[str],
+    mark_deleted_entity: bool = True,
+) -> Iterable[Either[DeleteEntity]]:
+    """
+    Method to delete the entites contained on a given list
+    :param metadata: OMeta client
+    :param entity_type: Pydantic Entity model
+    :param entity_names: List of FullyQualifiedNames of the entities to be deleted
+    :param mark_deleted_entity: Option to mark the entity as deleted or not
+    """
+    try:
+        for entity_name in entity_names:
+            entity = metadata.get_by_name(entity=entity_type, fqn=entity_name)
+            if entity:
+                yield Either(
+                    right=DeleteEntity(
+                        entity=entity, mark_deleted_entities=mark_deleted_entity
+                    )
+                )
+    except Exception as exc:
+        yield Either(
+            left=StackTraceError(
+                name="Delete Entity",
+                error=f"Error deleting {entity_type.__class__}: {exc}",
                 stackTrace=traceback.format_exc(),
             )
         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/status.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/status.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/step.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/step.py`

 * *Files 1% similar despite different names*

```diff
@@ -42,15 +42,20 @@
     status: Status
 
     def __init__(self):
         self.status = Status()
 
     @classmethod
     @abstractmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata) -> "Step":
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ) -> "Step":
         pass
 
     def get_status(self) -> Status:
         return self.status
 
     @property
     @abstractmethod
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/steps.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/steps.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/api/topology_runner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/api/topology_runner.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,16 +8,19 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Mixin to be used by service sources to dynamically
 generate the _run based on their topology.
 """
+import math
+import time
 import traceback
 from collections import defaultdict
+from concurrent.futures import ThreadPoolExecutor
 from functools import singledispatchmethod
 from typing import Any, Generic, Iterable, List, Type, TypeVar
 
 from pydantic import BaseModel
 
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.database import Database
@@ -25,22 +28,24 @@
 from metadata.generated.schema.entity.data.storedProcedure import StoredProcedure
 from metadata.ingestion.api.models import Either, Entity
 from metadata.ingestion.models.custom_properties import OMetaCustomProperties
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.patch_request import PatchRequest
 from metadata.ingestion.models.topology import (
     NodeStage,
+    Queue,
     ServiceTopology,
-    TopologyContext,
+    TopologyContextManager,
     TopologyNode,
     get_topology_node,
     get_topology_root,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.ometa.utils import model_str
+from metadata.utils.execution_time_tracker import ExecutionTimeTrackerContextMap
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.source_hash import generate_source_hash
 
 logger = ingestion_logger()
 
 C = TypeVar("C", bound=BaseModel)
 
@@ -55,19 +60,86 @@
 class TopologyRunnerMixin(Generic[C]):
     """
     Prepares the _run function
     dynamically based on the source topology
     """
 
     topology: ServiceTopology
-    context: TopologyContext
+    context: TopologyContextManager
     metadata: OpenMetadata
 
     # The cache will have the shape {`child_stage.type_`: {`name`: `hash`}}
     cache = defaultdict(dict)
+    queue = Queue()
+
+    def _multithread_process_node(
+        self, node: TopologyNode, threads: int
+    ) -> Iterable[Entity]:
+        """Multithread Processing of a Node"""
+        node_producer = getattr(self, node.producer)
+        child_nodes = self._get_child_nodes(node)
+
+        node_entities = list(node_producer() or [])
+        node_entities_length = len(node_entities)
+
+        if node_entities_length == 0:
+            return
+        else:
+            chunksize = int(math.ceil(node_entities_length / threads))
+            chunks = [
+                node_entities[i : i + chunksize]
+                for i in range(0, node_entities_length, chunksize)
+            ]
+
+            thread_pool = ThreadPoolExecutor(max_workers=threads)
+
+            futures = [
+                thread_pool.submit(
+                    self._multithread_process_entity,
+                    node,
+                    chunk,
+                    child_nodes,
+                    self.context.get_current_thread_id(),
+                )
+                for chunk in chunks
+            ]
+
+            while True:
+                if self.queue.has_tasks():
+                    yield from self.queue.process()
+
+                else:
+                    if not futures:
+                        break
+
+                    for i, future in enumerate(futures):
+                        if future.done():
+                            future.result()
+                            futures.pop(i)
+
+                time.sleep(0.01)
+
+    def _process_node(self, node: TopologyNode) -> Iterable[Entity]:
+        """Processing of a Node in a single thread."""
+        node_producer = getattr(self, node.producer)
+        child_nodes = self._get_child_nodes(node)
+
+        for node_entity in node_producer() or []:
+            for stage in node.stages:
+                yield from self._process_stage(
+                    stage=stage, node_entity=node_entity, child_nodes=child_nodes
+                )
+
+            # Once we are done processing all the stages,
+            for stage in node.stages:
+                if stage.clear_context:
+                    self.context.get().clear_stage(stage=stage)
+
+            # process all children from the node being run
+            yield from self.process_nodes(child_nodes)
 
     def process_nodes(self, nodes: List[TopologyNode]) -> Iterable[Entity]:
         """
         Given a list of nodes, either roots or children,
         yield from its producers and process the children.
 
         The execution tree is created in a depth-first fashion.
@@ -85,38 +157,61 @@
           3. Do nothing - if the fingerprints are the same.
 
         The fingerprint is stored in the db in the field `sourceHash` in each entity.
 
         :param nodes: Topology Nodes to process
         :return: recursively build the execution tree
         """
+
         for node in nodes:
             logger.debug(f"Processing node {node}")
-            node_producer = getattr(self, node.producer)
-            child_nodes = self._get_child_nodes(node)
 
             # Each node producer will give us a list of entities that we need
             # to process. Each of the internal stages will sink result to OM API.
             # E.g., in the DB topology, at the Table TopologyNode, the node_entity
             # will be each `table`
-            for node_entity in node_producer() or []:
-                for stage in node.stages:
-                    yield from self._process_stage(
-                        stage=stage, node_entity=node_entity, child_nodes=child_nodes
-                    )
+            if node.threads and self.context.threads > 1:
+                yield from self._multithread_process_node(node, self.context.threads)
+            else:
+                yield from self._process_node(node)
 
-                # Once we are done processing all the stages,
-                for stage in node.stages:
-                    if stage.clear_context:
-                        self.context.clear_stage(stage=stage)
+            yield from self._run_node_post_process(node=node)
 
-                # process all children from the node being run
-                yield from self.process_nodes(child_nodes)
+    def _multithread_process_entity(
+        self,
+        node: TopologyNode,
+        node_entities: List[Any],
+        child_nodes: List[TopologyNode],
+        parent_thread_id: int,
+    ):
+        """Multithread processing of a Node Entity"""
+        # Generates a new context based on the parent thread.
+        self.context.copy_from(parent_thread_id)
+        ExecutionTimeTrackerContextMap().copy_from_parent(parent_thread_id)
+
+        for node_entity in node_entities:
+            # For each stage, we get all the stage results and one by one yield them by adding them to the Queue.
+            for stage in node.stages:
+                for stage_result in self._process_stage(
+                    stage=stage, node_entity=node_entity, child_nodes=child_nodes
+                ):
+                    self.queue.put(stage_result)
+
+            # After all the stages are done, we clear the context if needed.
+            for stage in node.stages:
+                if stage.clear_context:
+                    self.context.get().clear_stage(stage=stage)
 
-            yield from self._run_node_post_process(node=node)
+            # If the Entity has child nodes that need processing we proceed to processing them with the same logic as above.
+
+            for child_result in self.process_nodes(child_nodes):
+                self.queue.put(child_result)
+
+        # Finally we pop the context and finish the thread
+        self.context.pop()
 
     def _get_child_nodes(self, node: TopologyNode) -> List[TopologyNode]:
         """Compute children nodes if any"""
         return (
             [get_topology_node(child, self.topology) for child in node.children]
             if node.children
             else []
@@ -154,15 +249,15 @@
         If the node has post_process steps, iterate over them and yield the result
         """
         if node.post_process:
             logger.debug(f"Post processing node {node}")
             for process in node.post_process:
                 try:
                     node_post_process = getattr(self, process)
-                    for entity_request in node_post_process():
+                    for entity_request in node_post_process() or []:
                         yield entity_request
                 except Exception as exc:
                     logger.debug(traceback.format_exc())
                     logger.warning(
                         f"Could not run Post Process `{process}` due to [{exc}]"
                     )
 
@@ -173,17 +268,17 @@
         Method to call the API to fill the entities cache.
 
         The cache will be part of the context
         """
         for child_node in child_nodes or []:
             for child_stage in child_node.stages or []:
                 if child_stage.use_cache:
-                    entity_fqn = self.context.fqn_from_stage(
+                    entity_fqn = self.context.get().fqn_from_stage(
                         stage=stage,
-                        entity_name=self.context.__dict__[stage.context],
+                        entity_name=self.context.get().__dict__[stage.context],
                     )
 
                     self.get_fqn_source_hash_dict(
                         parent_type=stage.type_,
                         child_type=child_stage.type_,
                         entity_fqn=entity_fqn,
                     )
@@ -247,15 +342,17 @@
         Handle the process of yielding the request and validating
         that everything was properly updated.
 
         The default implementation is based on a get_by_name validation
         """
         entity = None
         entity_name = model_str(right.name)
-        entity_fqn = self.context.fqn_from_stage(stage=stage, entity_name=entity_name)
+        entity_fqn = self.context.get().fqn_from_stage(
+            stage=stage, entity_name=entity_name
+        )
 
         # If we don't want to write data in OM, we'll return what we fetch from the API.
         # This will be applicable for service entities since we do not want to overwrite the data
         same_fingerprint = False
         if not stage.overwrite and not self._is_force_overwrite_enabled():
             entity = self.metadata.get_by_name(
                 entity=stage.type_,
@@ -321,15 +418,15 @@
                 # Safe access to Entity Request name
                 raise MissingExpectedEntityAckException(
                     f"Missing ack back from [{stage.type_.__name__}: {entity_fqn}] - "
                     "Possible causes are changes in the server Fernet key or mismatched JSON Schemas "
                     "for the service connection."
                 )
 
-        self.context.update_context_name(stage=stage, right=right)
+        self.context.get().update_context_name(stage=stage, right=right)
 
     @yield_and_update_context.register
     def _(
         self,
         right: AddLineageRequest,
         stage: NodeStage,
         entity_request: Either[C],
@@ -337,15 +434,15 @@
         """
         Lineage Implementation for the context information.
 
         There is no simple (efficient) validation to make sure that this specific
         lineage has been properly drawn. We'll skip the process for now.
         """
         yield entity_request
-        self.context.update_context_name(stage=stage, right=right.edge.fromEntity)
+        self.context.get().update_context_name(stage=stage, right=right.edge.fromEntity)
 
     @yield_and_update_context.register
     def _(
         self,
         right: OMetaTagAndClassification,
         stage: NodeStage,
         entity_request: Either[C],
@@ -355,27 +452,27 @@
 
         We need the full OMetaTagAndClassification in the context
         to build the TagLabels during the ingestion. We need to bundle
         both CreateClassificationRequest and CreateTagRequest.
         """
         yield entity_request
 
-        self.context.update_context_value(stage=stage, value=right)
+        self.context.get().update_context_value(stage=stage, value=right)
 
     @yield_and_update_context.register
     def _(
         self,
         right: OMetaCustomProperties,
         stage: NodeStage,
         entity_request: Either[C],
     ) -> Iterable[Either[Entity]]:
         """Custom Property implementation for the context information"""
         yield entity_request
 
-        self.context.update_context_value(stage=stage, value=right)
+        self.context.get().update_context_value(stage=stage, value=right)
 
     def sink_request(
         self, stage: NodeStage, entity_request: Either[C]
     ) -> Iterable[Either[Entity]]:
         """
         Validate that the entity was properly updated or retry if
         ack_sink is flagged.
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/bulksink/metadata_usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/bulksink/metadata_usage.py`

 * *Files 2% similar despite different names*

```diff
@@ -89,15 +89,20 @@
         self.today = datetime.today().strftime("%Y-%m-%d")
 
     @property
     def name(self) -> str:
         return "OpenMetadata"
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         config = MetadataUsageSinkConfig.parse_obj(config_dict)
         return cls(config, metadata)
 
     def __populate_table_usage_map(
         self, table_entity: Table, table_usage: TableUsageCount
     ) -> None:
         """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/builders.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/builders.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/headers.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/headers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/secrets.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/secrets.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/session.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/session.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/connections/test_connections.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/connections/test_connections.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/lineage/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/lineage/models.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,15 +8,17 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Models related to lineage parsing
 """
 from enum import Enum
-from typing import Dict
+from typing import Dict, List, Optional
+
+from pydantic import BaseModel, Extra, Field
 
 from metadata.generated.schema.entity.services.connections.database.athenaConnection import (
     AthenaType,
 )
 from metadata.generated.schema.entity.services.connections.database.azureSQLConnection import (
     AzureSQLType,
 )
@@ -58,14 +60,15 @@
 )
 from metadata.generated.schema.entity.services.connections.database.snowflakeConnection import (
     SnowflakeType,
 )
 from metadata.generated.schema.entity.services.connections.database.sqliteConnection import (
     SQLiteType,
 )
+from metadata.utils.singleton import Singleton
 
 
 class Dialect(Enum):
     """
     Supported dialects by sqllineage
     """
 
@@ -122,7 +125,37 @@
         """
         Returns dialect for a given connection_type
         Args:
             connection_type: the connection type as string
         Returns: a dialect
         """
         return MAP_CONNECTION_TYPE_DIALECT.get(connection_type, Dialect.ANSI)
+
+
+class QueryParsingError(BaseModel):
+    """
+    Represents an error that occurs during query parsing.
+
+    Attributes:
+        query (str): The query text of the failed query.
+        error (str): The error message of the failed query.
+    """
+
+    class Config:
+        extra = Extra.forbid
+
+    query: str = Field(..., description="query text of the failed query")
+    error: Optional[str] = Field(..., description="error message of the failed query")
+
+
+class QueryParsingFailures(metaclass=Singleton):
+    """Tracks the Queries that failed to parse."""
+
+    def __init__(self):
+        """Initializes the list of parsing failures."""
+        self._query_list: List[QueryParsingError] = []
+
+    def add(self, parsing_error: QueryParsingError):
+        self._query_list.append(parsing_error)
+
+    def __iter__(self):
+        return iter(self._query_list)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/lineage/parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/lineage/parser.py`

 * *Files 4% similar despite different names*

```diff
@@ -63,14 +63,16 @@
     def __init__(
         self,
         query: str,
         dialect: Dialect = Dialect.ANSI,
         timeout_seconds: int = LINEAGE_PARSING_TIMEOUT,
     ):
         self.query = query
+        self.query_parsing_success = True
+        self.query_parsing_failure_reason = None
         self._clean_query = self.clean_raw_query(query)
         self.parser = self._evaluate_best_parser(
             self._clean_query, dialect=dialect, timeout_seconds=timeout_seconds
         )
 
     @cached_property
     def involved_tables(self) -> Optional[List[Table]]:
@@ -396,17 +398,16 @@
         # We remove queries of the type 'COPY table FROM path' since they are data manipulation statement and do not
         # provide value for user
         if insensitive_match(clean_query, "^COPY.*"):
             return None
 
         return clean_query.strip()
 
-    @staticmethod
     def _evaluate_best_parser(
-        query: str, dialect: Dialect, timeout_seconds: int
+        self, query: str, dialect: Dialect, timeout_seconds: int
     ) -> Optional[LineageRunner]:
         if query is None:
             return None
 
         @timeout(seconds=timeout_seconds)
         def get_sqlfluff_lineage_runner(qry: str, dlct: str) -> LineageRunner:
             lr_dialect = LineageRunner(qry, dialect=dlct)
@@ -420,23 +421,27 @@
                 set(lr_sqlfluff.source_tables).union(
                     set(lr_sqlfluff.target_tables).union(
                         set(lr_sqlfluff.intermediate_tables)
                     )
                 )
             )
         except TimeoutError:
-            logger.debug(
-                f"Lineage with SqlFluff failed for the [{dialect.value}] query: [{query}]: "
+            self.query_parsing_success = False
+            self.query_parsing_failure_reason = (
+                f"Lineage with SqlFluff failed for the [{dialect.value}]. "
                 f"Parser has been running for more than {timeout_seconds} seconds."
             )
+            logger.debug(f"{self.query_parsing_failure_reason}] query: [{query}]")
             lr_sqlfluff = None
         except Exception:
-            logger.debug(
-                f"Lineage with SqlFluff failed for the [{dialect.value}] query: [{query}]"
+            self.query_parsing_success = False
+            self.query_parsing_failure_reason = (
+                f"Lineage with SqlFluff failed for the [{dialect.value}]"
             )
+            logger.debug(f"{self.query_parsing_failure_reason} query: [{query}]")
             lr_sqlfluff = None
 
         lr_sqlparser = LineageRunner(query)
         try:
             sqlparser_count = len(lr_sqlparser.get_column_lineage()) + len(
                 set(lr_sqlparser.source_tables).union(
                     set(lr_sqlparser.target_tables).union(
@@ -447,18 +452,20 @@
         except Exception:
             # if both runner have failed we return the usual one
             return lr_sqlfluff if lr_sqlfluff else lr_sqlparser
 
         if lr_sqlfluff:
             # if sqlparser retrieve more lineage info that sqlfluff
             if sqlparser_count > sqlfluff_count:
-                logger.debug(
+                self.query_parsing_success = False
+                self.query_parsing_failure_reason = (
                     "Lineage computed with SqlFluff did not perform as expected "
-                    f"for the [{dialect.value}] query: [{query}]"
+                    f"for the [{dialect.value}]"
                 )
+                logger.debug(f"{self.query_parsing_failure_reason} query: [{query}]")
                 return lr_sqlparser
             return lr_sqlfluff
         return lr_sqlparser
 
     @staticmethod
     def clean_table_name(table: Table) -> Table:
         """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/lineage/sql_lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/lineage/sql_lineage.py`

 * *Files 3% similar despite different names*

```diff
@@ -23,24 +23,27 @@
     ColumnLineage,
     EntitiesEdge,
     LineageDetails,
 )
 from metadata.generated.schema.type.entityLineage import Source as LineageSource
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.models import Either
-from metadata.ingestion.lineage.models import Dialect
+from metadata.ingestion.lineage.models import (
+    Dialect,
+    QueryParsingError,
+    QueryParsingFailures,
+)
 from metadata.ingestion.lineage.parser import LINEAGE_PARSING_TIMEOUT, LineageParser
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.utils import fqn
 from metadata.utils.fqn import build_es_fqn_search_string
 from metadata.utils.logger import utils_logger
-from metadata.utils.lru_cache import LRUCache
+from metadata.utils.lru_cache import LRU_CACHE_SIZE, LRUCache
 
 logger = utils_logger()
-LRU_CACHE_SIZE = 4096
 DEFAULT_SCHEMA_NAME = "<default>"
 
 
 def get_column_fqn(table_entity: Table, column: str) -> Optional[str]:
     """
     Get fqn of column if exist in table entity
     """
@@ -361,14 +364,15 @@
             else:
                 lineage_map[str(target.parent)] = {
                     str(parent): [(target.raw_name, source.raw_name)]
                 }
     return lineage_map
 
 
+# pylint: disable=too-many-locals
 def get_lineage_by_query(
     metadata: OpenMetadata,
     service_name: str,
     database_name: Optional[str],
     schema_name: Optional[str],
     query: str,
     dialect: Dialect,
@@ -376,14 +380,15 @@
     lineage_source: LineageSource = LineageSource.QueryLineage,
 ) -> Iterable[Either[AddLineageRequest]]:
     """
     This method parses the query to get source, target and intermediate table names to create lineage,
     and returns True if target table is found to create lineage otherwise returns False.
     """
     column_lineage = {}
+    query_parsing_failures = QueryParsingFailures()
 
     try:
         logger.debug(f"Running lineage with query: {query}")
         lineage_parser = LineageParser(query, dialect, timeout_seconds=timeout_seconds)
 
         raw_column_lineage = lineage_parser.column_lineage
         column_lineage.update(populate_column_lineage_map(raw_column_lineage))
@@ -423,14 +428,20 @@
                         service_name=service_name,
                         database_name=database_name,
                         schema_name=schema_name,
                         query=query,
                         column_lineage_map=column_lineage,
                         lineage_source=lineage_source,
                     )
+        if not lineage_parser.query_parsing_success:
+            query_parsing_failures.add(
+                QueryParsingError(
+                    query=query, error=lineage_parser.query_parsing_failure_reason
+                )
+            )
     except Exception as exc:
         yield Either(
             left=StackTraceError(
                 name="Lineage",
                 error=f"Ingesting lineage failed for service [{service_name}]: {exc}",
                 stackTrace=traceback.format_exc(),
             )
@@ -446,14 +457,15 @@
     query: str,
     dialect: Dialect,
     timeout_seconds: int = LINEAGE_PARSING_TIMEOUT,
     lineage_source: LineageSource = LineageSource.QueryLineage,
 ) -> Iterable[Either[AddLineageRequest]]:
     """Get lineage from table entity"""
     column_lineage = {}
+    query_parsing_failures = QueryParsingFailures()
 
     try:
         logger.debug(f"Getting lineage via table entity using query: {query}")
         lineage_parser = LineageParser(query, dialect, timeout_seconds=timeout_seconds)
         to_table_name = table_entity.name.__root__
 
         for from_table_name in lineage_parser.source_tables:
@@ -464,14 +476,20 @@
                 service_name=service_name,
                 database_name=database_name,
                 schema_name=schema_name,
                 query=query,
                 column_lineage_map=column_lineage,
                 lineage_source=lineage_source,
             ) or []
+        if not lineage_parser.query_parsing_success:
+            query_parsing_failures.add(
+                QueryParsingError(
+                    query=query, error=lineage_parser.query_parsing_failure_reason
+                )
+            )
     except Exception as exc:  # pylint: disable=broad-except
         Either(
             left=StackTraceError(
                 name="Lineage",
                 error=f"Failed to create view lineage for database [{database_name}] and table [{table_entity}]: {exc}",
                 stackTrace=traceback.format_exc(),
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/custom_properties.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/custom_properties.py`

 * *Files 6% similar despite different names*

```diff
@@ -33,14 +33,15 @@
     DURATION = "duration"
     EMAIL = "email"
     NUMBER = "number"
     SQLQUERY = "sqlQuery"
     TIME = "time"
     TIMEINTERVAL = "timeInterval"
     TIMESTAMP = "timestamp"
+    ENUM = "enum"
 
 
 class OMetaCustomProperties(BaseModel):
     entity_type: Type[T]
     createCustomPropertyRequest: CreateCustomPropertyRequest
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/custom_pydantic.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/custom_pydantic.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/custom_types.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/custom_types.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/data_insight.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/data_insight.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/delete_entity.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/delete_entity.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/encoders.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/encoders.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/lf_tags_model.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/lf_tags_model.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/life_cycle.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/life_cycle.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,16 +7,19 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Custom models for life cycle
 """
+from typing import Type
+
 from pydantic import BaseModel
 
 from metadata.generated.schema.type.lifeCycle import LifeCycle
 from metadata.ingestion.api.models import Entity
 
 
 class OMetaLifeCycleData(BaseModel):
-    entity: Entity
+    entity: Type[Entity]
+    entity_fqn: str
     life_cycle: LifeCycle
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/ometa_classification.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/ometa_classification.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/ometa_topic_data.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/ometa_topic_data.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/pipeline_status.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/pipeline_status.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/profile_data.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/profile_data.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/search_index_data.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/search_index_data.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/table_metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/table_metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/tests_data.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/tests_data.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/topology.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/topology.py`

 * *Files 22% similar despite different names*

```diff
@@ -7,16 +7,18 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Defines the topology for ingesting sources
 """
+import queue
+import threading
 from functools import singledispatchmethod
-from typing import Any, Generic, List, Optional, Type, TypeVar
+from typing import Any, Dict, Generic, List, Optional, Type, TypeVar
 
 from pydantic import BaseModel, Extra, Field, create_model
 
 from metadata.generated.schema.api.data.createStoredProcedure import (
     CreateStoredProcedureRequest,
 )
 from metadata.generated.schema.entity.data.storedProcedure import StoredProcedure
@@ -111,14 +113,18 @@
             "Each stage accepts the producer results as an argument"
         ),
     )
     children: Optional[List[str]] = Field(None, description="Nodes to execute next")
     post_process: Optional[List[str]] = Field(
         None, description="Method to be run after the node has been fully processed"
     )
+    threads: bool = Field(
+        False,
+        description="Flag that defines if a node is open to MultiThreading processing.",
+    )
 
 
 class ServiceTopology(BaseModel):
     """
     Bounds all service topologies
     """
 
@@ -241,14 +247,88 @@
             service_name=self.__dict__["database_service"],
             database_name=self.__dict__["database"],
             schema_name=self.__dict__["database_schema"],
             procedure_name=right.name.__root__,
         )
 
 
+class TopologyContextManager:
+    """Manages the Context held for different threads."""
+
+    def __init__(self, topology: ServiceTopology):
+        # Due to our code strucutre, the first time the ContextManager is called will be within the MainThread.
+        # We can leverage this to guarantee we keep track of the MainThread ID.
+        self.main_thread = self.get_current_thread_id()
+        self.contexts: Dict[int, TopologyContext] = {
+            self.main_thread: TopologyContext.create(topology)
+        }
+
+        # Starts with the Multithreading disabled
+        self.threads = 0
+
+    def set_threads(self, threads: Optional[int]):
+        self.threads = threads or 0
+
+    def get_current_thread_id(self):
+        return threading.get_ident()
+
+    def get_global(self) -> TopologyContext:
+        return self.contexts[self.main_thread]
+
+    def get(self, thread_id: Optional[int] = None) -> TopologyContext:
+        """Returns the TopologyContext of a given thread."""
+        if thread_id:
+            return self.contexts[thread_id]
+
+        thread_id = self.get_current_thread_id()
+
+        return self.contexts[thread_id]
+
+    def pop(self, thread_id: Optional[int] = None):
+        """Cleans the TopologyContext of a given thread in order to lower the Memory Profile."""
+        if not thread_id:
+            self.contexts.pop(self.get_current_thread_id())
+        else:
+            self.contexts.pop(thread_id)
+
+    def copy_from(self, parent_thread_id: int):
+        """Copies the TopologyContext from a given Thread to the new thread TopologyContext."""
+        thread_id = self.get_current_thread_id()
+
+        # If it does not exist yet, copies the Parent Context in order to have all context gathered until this point.
+        self.contexts.setdefault(
+            thread_id, self.contexts[parent_thread_id].copy(deep=True)
+        )
+
+
+class Queue:
+    """Small Queue wrapper"""
+
+    def __init__(self):
+        self._queue = queue.Queue()
+
+    def has_tasks(self) -> bool:
+        """Checks that the Queue is not Empty."""
+        return not self._queue.empty()
+
+    def process(self) -> Any:
+        """Yields all the items currently on the Queue."""
+        while True:
+            try:
+                item = self._queue.get_nowait()
+                yield item
+                self._queue.task_done()
+            except queue.Empty:
+                break
+
+    def put(self, item: Any):
+        """Puts new item in the Queue."""
+        self._queue.put(item)
+
+
 def get_topology_nodes(topology: ServiceTopology) -> List[TopologyNode]:
     """
     Fetch all nodes from a ServiceTopology
     :param topology: ServiceTopology
     :return: List of nodes
     """
     return [value for key, value in topology.__dict__.items()]
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/models/user.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/models/user.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/auth_provider.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/auth_provider.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/client.py`

 * *Files 1% similar despite different names*

```diff
@@ -197,14 +197,17 @@
                     "sleep %s seconds and retrying %s %s more time(s)...",
                     retry_wait,
                     url,
                     retry,
                 )
                 time.sleep(retry_wait)
                 retry -= 1
+                if retry == 0:
+                    logger.error(f"No more retries left for {url}")
+                    traceback.format_exc()
         return None
 
     def _one_request(self, method: str, url: URL, opts: dict, retry: int):
         """
         Perform one request, possibly raising RetryException in the case
         the response is 429. Otherwise, if error text contain "code" string,
         then it decodes to json object and returns APIError.
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/client_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/client_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/credentials.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/credentials.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/custom_property_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/custom_property_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/es_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/es_mixin.py`

 * *Files 21% similar despite different names*

```diff
@@ -12,24 +12,21 @@
 Mixin class containing Lineage specific methods
 
 To be used by OpenMetadata class
 """
 import functools
 import json
 import traceback
-from typing import Generic, List, Optional, Set, Type, TypeVar
+from typing import Generic, Iterable, List, Optional, Set, Type, TypeVar
 
 from pydantic import BaseModel
 from requests.utils import quote
 
-from metadata.generated.schema.api.createEventPublisherJob import (
-    CreateEventPublisherJob,
-)
+from metadata.generated.schema.entity.data.container import Container
 from metadata.generated.schema.entity.data.query import Query
-from metadata.generated.schema.system.eventPublisherJob import EventPublisherResult
 from metadata.ingestion.ometa.client import REST, APIError
 from metadata.utils.elasticsearch import ES_INDEX_MAP
 from metadata.utils.logger import ometa_logger
 
 logger = ometa_logger()
 
 T = TypeVar("T", bound=BaseModel)
@@ -41,15 +38,15 @@
 
     To be inherited by OpenMetadata
     """
 
     client: REST
 
     fqdn_search = (
-        "/search/fieldQuery?fieldName=fullyQualifiedName&fieldValue={fqn}&from={from_}"
+        "/search/fieldQuery?fieldName={field_name}&fieldValue={field_value}&from={from_}"
         "&size={size}&index={index}"
     )
 
     @functools.lru_cache(maxsize=512)
     def _search_es_entity(
         self,
         entity_type: Type[T],
@@ -75,15 +72,15 @@
                     fields=fields,
                 )
                 for hit in response["hits"]["hits"]
             ] or None
 
         return None
 
-    def _get_entity_from_es(
+    def get_entity_from_es(
         self, entity: Type[T], query_string: str, fields: Optional[list] = None
     ) -> Optional[T]:
         """Fetch an entity instance from ES"""
 
         try:
             entity_list = self._search_es_entity(
                 entity_type=entity, query_string=query_string, fields=fields
@@ -92,34 +89,117 @@
                 return instance
         except Exception as err:
             logger.debug(traceback.format_exc())
             logger.warning(f"Could not get {entity.__name__} info from ES due to {err}")
 
         return None
 
+    def yield_entities_from_es(
+        self, entity: Type[T], query_string: str, fields: Optional[list] = None
+    ) -> Iterable[T]:
+        """Fetch an entity instance from ES"""
+
+        try:
+            entity_list = self._search_es_entity(
+                entity_type=entity, query_string=query_string, fields=fields
+            )
+            for instance in entity_list or []:
+                yield instance
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Could not get {entity.__name__} info from ES due to {err}")
+
+        return None
+
     def es_search_from_fqn(
         self,
         entity_type: Type[T],
         fqn_search_string: str,
         from_count: int = 0,
         size: int = 10,
         fields: Optional[str] = None,
     ) -> Optional[List[T]]:
         """
-        Given a service_name and some filters, search for entities using ES
+        Given a service name and filters, search for entities using Elasticsearch.
 
-        :param entity_type: Entity to look for
-        :param fqn_search_string: string used to search by FQN. E.g., service.*.schema.table
-        :param from_count: Records to expect
-        :param size: Number of records
-        :param fields: Comma separated list of fields to be returned
-        :return: List of entities
+        Args:
+            entity_type (Type[T]): The type of entity to look for.
+            fqn_search_string (str): The string used to search by fully qualified name (FQN).
+                Example: "service.*.schema.table".
+            from_count (int): The starting index of the search results.
+            size (int): The maximum number of records to return.
+            fields (Optional[str]): Comma-separated list of fields to be returned.
+
+        Returns:
+            Optional[List[T]]: A list of entities that match the search criteria, or None if no entities are found.
+        """
+        return self._es_search_entity(
+            entity_type=entity_type,
+            field_value=fqn_search_string,
+            field_name="fullyQualifiedName",
+            from_count=from_count,
+            size=size,
+            fields=fields,
+        )
+
+    def es_search_container_by_path(
+        self,
+        full_path: str,
+        from_count: int = 0,
+        size: int = 10,
+        fields: Optional[str] = None,
+    ) -> Optional[List[Container]]:
+        """
+        Given a service name and filters, search for containers using Elasticsearch.
+
+        Args:
+            entity_type (Type[T]): The type of entity to look for.
+            full_path (str): The string used to search by full path.
+            from_count (int): The starting index of the search results.
+            size (int): The maximum number of records to return.
+            fields (Optional[str]): Comma-separated list of fields to be returned.
+
+        Returns:
+            Optional[List[Container]]: A list of containers that match the search criteria, or None if no entities are found.
+        """
+        return self._es_search_entity(
+            entity_type=Container,
+            field_value=full_path,
+            field_name="fullPath",
+            from_count=from_count,
+            size=size,
+            fields=fields,
+        )
+
+    def _es_search_entity(
+        self,
+        entity_type: Type[T],
+        field_value: str,
+        field_name: str,
+        from_count: int = 0,
+        size: int = 10,
+        fields: Optional[str] = None,
+    ) -> Optional[List[T]]:
+        """
+        Search for entities using Elasticsearch.
+
+        Args:
+            entity_type (Type[T]): The type of entity to look for.
+            field_value (str): The value to search for in the specified field.
+            field_name (str): The name of the field to search in.
+            from_count (int, optional): The starting index of the search results. Defaults to 0.
+            size (int, optional): The maximum number of search results to return. Defaults to 10.
+            fields (Optional[str], optional): Comma-separated list of fields to be returned. Defaults to None.
+
+        Returns:
+            Optional[List[T]]: A list of entities that match the search criteria, or None if no entities are found.
         """
         query_string = self.fqdn_search.format(
-            fqn=fqn_search_string,
+            field_name=field_name,
+            field_value=field_value,
             from_=from_count,
             size=size,
             index=ES_INDEX_MAP[entity_type.__name__],  # Fail if not exists
         )
 
         try:
             response = self._search_es_entity(
@@ -134,41 +214,14 @@
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(
                 f"Elasticsearch search failed for query [{query_string}]: {exc}"
             )
         return None
 
-    def reindex_es(
-        self,
-        config: CreateEventPublisherJob,
-    ) -> Optional[EventPublisherResult]:
-        """
-        Method to trigger elasticsearch reindex
-        """
-        try:
-            resp = self.client.post(path="/search/reindex", data=config.json())
-            return EventPublisherResult(**resp)
-        except APIError as err:
-            logger.debug(traceback.format_exc())
-            logger.debug(f"Failed to trigger es reindex job due to {err}")
-            return None
-
-    def get_reindex_job_status(self, job_id: str) -> Optional[EventPublisherResult]:
-        """
-        Method to fetch the elasticsearch reindex job status
-        """
-        try:
-            resp = self.client.get(path=f"/search/reindex/{job_id}")
-            return EventPublisherResult(**resp)
-        except APIError as err:
-            logger.debug(traceback.format_exc())
-            logger.debug(f"Failed to fetch reindex job status due to {err}")
-            return None
-
     @staticmethod
     def get_query_with_lineage_filter(service_name: str) -> str:
         query_lineage_filter = {
             "query": {
                 "bool": {
                     "must": [
                         {"term": {"processedLineage": True}},
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/patch_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/patch_mixin.py`

 * *Files 2% similar despite different names*

```diff
@@ -230,36 +230,39 @@
         destination = table.copy(deep=True)
         destination.tableConstraints = constraints
 
         return self.patch(entity=Table, source=table, destination=destination)
 
     def patch_test_case_definition(
         self,
-        source: TestCase,
+        test_case: TestCase,
         entity_link: str,
         test_case_parameter_values: Optional[List[TestCaseParameterValue]] = None,
+        compute_passed_failed_row_count: Optional[bool] = False,
     ) -> Optional[TestCase]:
         """Given a test case and a test case definition JSON PATCH the test case
 
         Args
             test_case: test case object
             test_case_definition: test case definition to add
         """
         source: TestCase = self._fetch_entity_if_exists(
-            entity=TestCase, entity_id=source.id, fields=["testDefinition", "testSuite"]
+            entity=TestCase, entity_id=test_case.id, fields=["testDefinition", "testSuite"]  # type: ignore
         )  # type: ignore
 
         if not source:
             return None
 
         destination = source.copy(deep=True)
 
         destination.entityLink = EntityLink(__root__=entity_link)
         if test_case_parameter_values:
             destination.parameterValues = test_case_parameter_values
+        if compute_passed_failed_row_count != source.computePassedFailedRowCount:
+            destination.computePassedFailedRowCount = compute_passed_failed_row_count
 
         return self.patch(entity=TestCase, source=source, destination=destination)
 
     def patch_tags(
         self,
         entity: Type[T],
         source: T,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/query_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/query_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/search_index_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/search_index_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/server_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/server_mixin.py`

 * *Files 23% similar despite different names*

```diff
@@ -9,16 +9,21 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Mixin class containing Server and client specific methods
 
 To be used by OpenMetadata class
 """
+from typing import Optional
+
 from metadata.__version__ import get_client_version, get_server_version_from_string
+from metadata.generated.schema.settings.settings import Settings, SettingType
+from metadata.ingestion.models.encoders import show_secrets_encoder
 from metadata.ingestion.ometa.client import REST
+from metadata.ingestion.ometa.routes import ROUTES
 from metadata.utils.logger import ometa_logger
 
 logger = ometa_logger()
 
 
 class VersionMismatchException(Exception):
     """
@@ -69,7 +74,50 @@
 
         # Server version will be 0.13.2, vs 0.13.2.X from the client.
         # If the server version is contained in the client version, then we're good to go
         if server_version not in client_version:
             raise VersionMismatchException(
                 f"Server version is {server_version} vs. Client version {client_version}. Both should match."
             )
+
+    def create_or_update_settings(self, settings: Settings) -> Settings:
+        """Create of update setting
+
+        Args:
+            setting (Settings): setting to update or create
+
+        Returns:
+            Settings
+        """
+        data = settings.json(encoder=show_secrets_encoder)
+        response = self.client.put(ROUTES.get(Settings.__name__), data)
+        return Settings.parse_obj(response)
+
+    def get_settings_by_name(self, setting_type: SettingType) -> Optional[Settings]:
+        """Get setting by name
+
+        Args:
+            setting (Settings): setting to update or create
+
+        Returns:
+            Settings
+        """
+        response = self.client.get(
+            f"{ROUTES.get(Settings.__name__)}/{setting_type.value}"
+        )
+        if not response:
+            return None
+        return Settings.parse_obj(response)
+
+    def get_profiler_config_settings(self) -> Optional[Settings]:
+        """Get profiler config setting
+
+        Args:
+            setting (Settings): setting to update or create
+
+        Returns:
+            Settings
+        """
+        response = self.client.get("/system/settings/profilerConfiguration")
+        if not response:
+            return None
+        return Settings.parse_obj(response)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/service_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/service_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/suggestions_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/suggestions_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/table_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/table_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/tests_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/tests_mixin.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 #  limitations under the License.
 """
 Mixin class containing Tests specific methods
 
 To be used by OpenMetadata class
 """
 
+import traceback
 from datetime import datetime, timezone
 from typing import List, Optional, Type, Union
 from urllib.parse import quote
 from uuid import UUID
 
 from metadata.generated.schema.api.tests.createLogicalTestCases import (
     CreateLogicalTestCases,
@@ -26,15 +27,15 @@
 from metadata.generated.schema.api.tests.createTestCaseResolutionStatus import (
     CreateTestCaseResolutionStatus,
 )
 from metadata.generated.schema.api.tests.createTestDefinition import (
     CreateTestDefinitionRequest,
 )
 from metadata.generated.schema.api.tests.createTestSuite import CreateTestSuiteRequest
-from metadata.generated.schema.entity.data.table import Table
+from metadata.generated.schema.entity.data.table import Table, TableData
 from metadata.generated.schema.tests.basic import TestCaseResult
 from metadata.generated.schema.tests.testCase import TestCase, TestCaseParameterValue
 from metadata.generated.schema.tests.testCaseResolutionStatus import (
     TestCaseResolutionStatus,
 )
 from metadata.generated.schema.tests.testDefinition import (
     EntityType,
@@ -319,7 +320,60 @@
         Returns:
             TestCaseResolutionStatus
         """
         path = self.get_suffix(TestCase) + "/testCaseIncidentStatus"
         response = self.client.post(path, data=data.json(encoder=show_secrets_encoder))
 
         return TestCaseResolutionStatus(**response)
+
+    def ingest_failed_rows_sample(
+        self, test_case: TestCase, failed_rows: TableData
+    ) -> Optional[TableData]:
+        """
+        PUT sample failed data for a test case.
+
+        :param test_case: The test case that failed
+        :param failed_rows: Data to add
+        """
+        resp = None
+        try:
+            resp = self.client.put(
+                f"{self.get_suffix(TestCase)}/{test_case.id.__root__}/failedRowsSample",
+                data=failed_rows.json(),
+            )
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Error trying to PUT sample data for {test_case.fullyQualifiedName.__root__}: {exc}"
+            )
+
+        if resp:
+            try:
+                return TableData(**resp["failedRowsSample"])
+            except UnicodeError as err:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Unicode Error parsing the sample data response from {test_case.fullyQualifiedName.__root__}: "
+                    f"{err}"
+                )
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Error trying to parse sample data results from {test_case.fullyQualifiedName.__root__}: {exc}"
+                )
+
+        return None
+
+    def ingest_inspection_query(
+        self, test_case: TestCase, inspection_query: str
+    ) -> Optional[TestCase]:
+        """
+        PUT inspection query for a test case.
+
+        :param test_case: The test case that failed
+        :param inspection_query: SQL query to inspect the failed rows
+        """
+        resp = self.client.put(
+            f"{self.get_suffix(TestCase)}/{test_case.id.__root__}/inspectionQuery",
+            data=inspection_query,
+        )
+        return TestCase(**resp)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/topic_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/topic_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/user_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/user_mixin.py`

 * *Files 1% similar despite different names*

```diff
@@ -74,15 +74,15 @@
             size: number of records
             fields: Optional field list to pass to ES request
         """
         if email:
             query_string = self.email_search_query_es(entity=entity).format(
                 email=email, from_=from_count, size=size
             )
-            return self._get_entity_from_es(
+            return self.get_entity_from_es(
                 entity=entity, query_string=query_string, fields=fields
             )
 
         return None
 
     def _search_by_name(
         self,
@@ -101,15 +101,15 @@
             size: number of records
             fields: Optional field list to pass to ES request
         """
         if name:
             query_string = self.name_search_query_es(entity=entity).format(
                 name=name, from_=from_count, size=size
             )
-            return self._get_entity_from_es(
+            return self.get_entity_from_es(
                 entity=entity, query_string=query_string, fields=fields
             )
 
         return None
 
     @lru_cache(maxsize=None)
     def get_reference_by_email(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/mixins/version_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/mixins/version_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/ometa_api.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/ometa_api.py`

 * *Files 1% similar despite different names*

```diff
@@ -401,15 +401,20 @@
 
         if skip_on_failure:
             entities = []
             for elmt in resp["data"]:
                 try:
                     entities.append(entity(**elmt))
                 except Exception as exc:
-                    logger.error(f"Error creating entity. Failed with exception {exc}")
+                    logger.error(
+                        f"Error creating entity [{entity.__name__}]. Failed with exception {exc}"
+                    )
+                    logger.debug(
+                        f"Can't create [{entity.__name__}] from [{elmt}]. Skipping."
+                    )
                     continue
         else:
             entities = [entity(**elmt) for elmt in resp["data"]]
 
         total = resp["paging"]["total"]
         after = resp["paging"]["after"] if "after" in resp["paging"] else None
         return EntityList(entities=entities, total=total, after=after)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/routes.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/routes.py`

 * *Files 2% similar despite different names*

```diff
@@ -140,14 +140,15 @@
 from metadata.generated.schema.entity.services.mlmodelService import MlModelService
 from metadata.generated.schema.entity.services.pipelineService import PipelineService
 from metadata.generated.schema.entity.services.searchService import SearchService
 from metadata.generated.schema.entity.services.storageService import StorageService
 from metadata.generated.schema.entity.teams.role import Role
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.generated.schema.entity.teams.user import AuthenticationMechanism, User
+from metadata.generated.schema.settings.settings import Settings
 from metadata.generated.schema.tests.testCase import TestCase
 from metadata.generated.schema.tests.testDefinition import TestDefinition
 from metadata.generated.schema.tests.testSuite import TestSuite
 
 ROUTES = {
     MlModel.__name__: "/mlmodels",
     CreateMlModelRequest.__name__: "/mlmodels",
@@ -243,8 +244,10 @@
     Suggestion.__name__: "/suggestions",
     CreateSuggestionRequest.__name__: "/suggestions",
     # Apps
     App.__name__: "/apps",
     CreateAppRequest.__name__: "/apps",
     AppMarketPlaceDefinition.__name__: "/apps/marketplace",
     CreateAppMarketPlaceDefinitionRequest.__name__: "/apps/marketplace",
+    # Settings
+    Settings.__name__: "/system/settings",
 }
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/ometa/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/ometa/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -53,14 +53,16 @@
         return class_name.replace("testdefinition", "testDefinition")
     if "testsuite" in class_name:
         return class_name.replace("testsuite", "testSuite")
     if "databaseschema" in class_name:
         return class_name.replace("databaseschema", "databaseSchema")
     if "searchindex" in class_name:
         return class_name.replace("searchindex", "searchIndex")
+    if "dashboarddatamodel" in class_name:
+        return class_name.replace("dashboarddatamodel", "dashboardDataModel")
 
     return class_name
 
 
 def model_str(arg: Any) -> str:
     """
     Default model stringifying method.
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/processor/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/processor/query_parser.py`

 * *Files 2% similar despite different names*

```diff
@@ -88,15 +88,21 @@
         self.connection_type = connection_type
 
     @property
     def name(self) -> str:
         return "Query Parser"
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata, **kwargs):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+        **kwargs,
+    ):
         config = ConfigModel.parse_obj(config_dict)
         connection_type = kwargs.pop("connection_type", "")
         return cls(config, metadata, connection_type)
 
     def _run(self, record: TableQueries) -> Optional[Either[QueryParserData]]:
         if record and record.queries:
             data = []
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/sink/file.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/sink/file.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Sink that will store metadata in a file.
 Useful for local testing without having OM up.
 """
 import pathlib
+from typing import Optional
 
 from metadata.config.common import ConfigModel
 from metadata.ingestion.api.common import Entity
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import Sink
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.utils.constants import UTF_8
@@ -45,15 +46,17 @@
         fpath = pathlib.Path(self.config.filename)
         # pylint: disable=consider-using-with
         self.file = fpath.open("w", encoding=UTF_8)
         self.file.write("[\n")
         self.wrote_something = False
 
     @classmethod
-    def create(cls, config_dict: dict, _: OpenMetadata):
+    def create(
+        cls, config_dict: dict, _: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = FileSinkConfig.parse_obj(config_dict)
         return cls(config)
 
     def _run(self, record: Entity, *_, **__) -> Either[str]:
         if self.wrote_something:
             self.file.write(",\n")
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/sink/metadata_rest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/sink/metadata_rest.py`

 * *Files 5% similar despite different names*

```diff
@@ -113,15 +113,20 @@
         self.wrote_something = False
         self.charts_dict = {}
         self.metadata = metadata
         self.role_entities = {}
         self.team_entities = {}
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         config = MetadataRestSinkConfig.parse_obj(config_dict)
         return cls(config, metadata)
 
     @property
     def name(self) -> str:
         return "OpenMetadata"
 
@@ -237,15 +242,15 @@
         """PUT Classification and Tag to OM API"""
         self.metadata.create_or_update(record.classification_request)
         tag = self.metadata.create_or_update(record.tag_request)
         return Either(right=tag)
 
     @_run_dispatch.register
     def write_lineage(self, add_lineage: AddLineageRequest) -> Either[Dict[str, Any]]:
-        created_lineage = self.metadata.add_lineage(add_lineage)
+        created_lineage = self.metadata.add_lineage(add_lineage, check_patch=True)
         return Either(right=created_lineage["entity"]["fullyQualifiedName"])
 
     def _create_role(self, create_role: CreateRoleRequest) -> Optional[Role]:
         """
         Internal helper method for write_user
         """
         try:
@@ -495,18 +500,27 @@
         return Either(right=record.entity)
 
     @_run_dispatch.register
     def write_life_cycle_data(self, record: OMetaLifeCycleData) -> Either[Entity]:
         """
         Ingest the life cycle data
         """
-        self.metadata.patch_life_cycle(
-            entity=record.entity, life_cycle=record.life_cycle
+
+        entity = self.metadata.get_by_name(entity=record.entity, fqn=record.entity_fqn)
+
+        if entity:
+            self.metadata.patch_life_cycle(entity=entity, life_cycle=record.life_cycle)
+            return Either(right=entity)
+
+        return Either(
+            left=StackTraceError(
+                name=record.entity_fqn,
+                error=f"Entity of type '{record.entity}' with name '{record.entity_fqn}' not found.",
+            )
         )
-        return Either(right=record)
 
     @_run_dispatch.register
     def write_profiler_response(self, record: ProfilerResponse) -> Either[Table]:
         """Cleanup "`" character in columns and ingest"""
         column_profile = record.profile.columnProfile
         for column in column_profile:
             column.name = column.name.replace("`", "")
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/connections.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/connections.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/dashboard_service.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/dashboard_service.py`

 * *Files 3% similar despite different names*

```diff
@@ -36,29 +36,34 @@
 )
 from metadata.generated.schema.metadataIngestion.dashboardServiceMetadataPipeline import (
     DashboardServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.type.entityLineage import EntitiesEdge, LineageDetails
+from metadata.generated.schema.type.entityLineage import (
+    ColumnLineage,
+    EntitiesEdge,
+    LineageDetails,
+)
 from metadata.generated.schema.type.entityLineage import Source as LineageSource
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.generated.schema.type.usageRequest import UsageRequest
 from metadata.ingestion.api.delete import delete_entity_from_source
 from metadata.ingestion.api.models import Either, Entity
 from metadata.ingestion.api.steps import Source
 from metadata.ingestion.api.topology_runner import C, TopologyRunnerMixin
+from metadata.ingestion.lineage.sql_lineage import get_column_fqn
 from metadata.ingestion.models.delete_entity import DeleteEntity
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.patch_request import PatchRequest
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
-    TopologyContext,
+    TopologyContextManager,
     TopologyNode,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_dashboard, filter_by_project
 from metadata.utils.logger import ingestion_logger
@@ -190,15 +195,15 @@
     source_config: DashboardServiceMetadataPipeline
     config: WorkflowSource
     metadata: OpenMetadata
     # Big union of types we want to fetch dynamically
     service_connection: DashboardConnection.__fields__["config"].type_
 
     topology = DashboardServiceTopology()
-    context = TopologyContext.create(topology)
+    context = TopologyContextManager(topology)
     dashboard_source_state: Set = set()
     datamodel_source_state: Set = set()
 
     def __init__(
         self,
         config: WorkflowSource,
         metadata: OpenMetadata,
@@ -284,57 +289,69 @@
     def yield_datamodel_dashboard_lineage(
         self,
     ) -> Iterable[Either[AddLineageRequest]]:
         """
         Returns:
             Lineage request between Data Models and Dashboards
         """
-        if hasattr(self.context, "dataModels") and self.context.dataModels:
-            for datamodel in self.context.dataModels:
+        if hasattr(self.context.get(), "dataModels") and self.context.get().dataModels:
+            for datamodel in self.context.get().dataModels:
                 try:
                     datamodel_fqn = fqn.build(
                         metadata=self.metadata,
                         entity_type=DashboardDataModel,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         data_model_name=datamodel,
                     )
                     datamodel_entity = self.metadata.get_by_name(
                         entity=DashboardDataModel, fqn=datamodel_fqn
                     )
 
                     dashboard_fqn = fqn.build(
                         self.metadata,
                         entity_type=Dashboard,
-                        service_name=self.context.dashboard_service,
-                        dashboard_name=self.context.dashboard,
+                        service_name=self.context.get().dashboard_service,
+                        dashboard_name=self.context.get().dashboard,
                     )
                     dashboard_entity = self.metadata.get_by_name(
                         entity=Dashboard, fqn=dashboard_fqn
                     )
                     yield self._get_add_lineage_request(
                         to_entity=dashboard_entity, from_entity=datamodel_entity
                     )
                 except Exception as err:
                     logger.debug(traceback.format_exc())
                     logger.error(
                         f"Error to yield dashboard lineage details for data model name [{datamodel.name}]: {err}"
                     )
 
+    def get_db_service_names(self) -> List[str]:
+        """
+        Get the list of db service names
+        """
+        return (
+            self.source_config.lineageInformation.dbServiceNames or []
+            if self.source_config.lineageInformation
+            else []
+        )
+
     def yield_dashboard_lineage(
         self, dashboard_details: Any
     ) -> Iterable[Either[AddLineageRequest]]:
         """
         Yields lineage if config is enabled.
 
         We will look for the data in all the services
         we have informed.
         """
         yield from self.yield_datamodel_dashboard_lineage() or []
 
-        for db_service_name in self.source_config.dbServiceNames or []:
+        db_service_names = self.get_db_service_names()
+
+        for db_service_name in db_service_names or []:
             yield from self.yield_dashboard_lineage_details(
                 dashboard_details, db_service_name
             ) or []
 
     def yield_bulk_tags(
         self, *args, **kwargs
     ) -> Iterable[Either[OMetaTagAndClassification]]:
@@ -376,29 +393,29 @@
         if self.source_config.markDeletedDashboards:
             logger.info("Mark Deleted Dashboards set to True")
             yield from delete_entity_from_source(
                 metadata=self.metadata,
                 entity_type=Dashboard,
                 entity_source_state=self.dashboard_source_state,
                 mark_deleted_entity=self.source_config.markDeletedDashboards,
-                params={"service": self.context.dashboard_service},
+                params={"service": self.context.get().dashboard_service},
             )
 
     def mark_datamodels_as_deleted(self) -> Iterable[Either[DeleteEntity]]:
         """
         Method to mark the datamodels as deleted
         """
         if self.source_config.markDeletedDataModels:
             logger.info("Mark Deleted Datamodels set to True")
             yield from delete_entity_from_source(
                 metadata=self.metadata,
                 entity_type=DashboardDataModel,
                 entity_source_state=self.datamodel_source_state,
                 mark_deleted_entity=self.source_config.markDeletedDataModels,
-                params={"service": self.context.dashboard_service},
+                params={"service": self.context.get().dashboard_service},
             )
 
     def get_owner_ref(  # pylint: disable=unused-argument, useless-return
         self, dashboard_details
     ) -> Optional[EntityReference]:
         """
         Method to process the dashboard owners
@@ -436,36 +453,52 @@
 
         self.datamodel_source_state.add(datamodel_fqn)
 
     @staticmethod
     def _get_add_lineage_request(
         to_entity: Union[Dashboard, DashboardDataModel],
         from_entity: Union[Table, DashboardDataModel, Dashboard],
+        column_lineage: List[ColumnLineage] = None,
     ) -> Optional[Either[AddLineageRequest]]:
         if from_entity and to_entity:
             return Either(
                 right=AddLineageRequest(
                     edge=EntitiesEdge(
                         fromEntity=EntityReference(
                             id=from_entity.id.__root__,
                             type=LINEAGE_MAP[type(from_entity)],
                         ),
                         toEntity=EntityReference(
                             id=to_entity.id.__root__,
                             type=LINEAGE_MAP[type(to_entity)],
                         ),
                         lineageDetails=LineageDetails(
-                            source=LineageSource.DashboardLineage
+                            source=LineageSource.DashboardLineage,
+                            columnsLineage=column_lineage,
                         ),
                     )
                 )
             )
 
         return None
 
+    @staticmethod
+    def _get_data_model_column_fqn(
+        data_model_entity: DashboardDataModel, column: str
+    ) -> Optional[str]:
+        """
+        Get fqn of column if exist in table entity
+        """
+        if not data_model_entity:
+            return None
+        for tbl_column in data_model_entity.columns:
+            if tbl_column.displayName.lower() == column.lower():
+                return tbl_column.fullyQualifiedName.__root__
+        return None
+
     def get_dashboard(self) -> Any:
         """
         Method to iterate through dashboard lists filter dashboards & yield dashboard details
         """
         for dashboard in self.get_dashboards_list():
             dashboard_name = self.get_dashboard_name(dashboard)
             if filter_by_dashboard(
@@ -476,23 +509,23 @@
                     dashboard_name,
                     "Dashboard Filtered Out",
                 )
                 continue
 
             try:
                 dashboard_details = self.get_dashboard_details(dashboard)
-                self.context.project_name = (  # pylint: disable=assignment-from-none
+                self.context.get().project_name = (  # pylint: disable=E1128
                     self.get_project_name(dashboard_details=dashboard_details)
                 )
                 if filter_by_project(
                     self.source_config.projectFilterPattern,
-                    self.context.project_name,
+                    self.context.get().project_name,
                 ):
                     self.status.filter(
-                        self.context.project_name,
+                        self.context.get().project_name,
                         "Project / Workspace Filtered Out",
                     )
                     continue
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.warning(
                     f"Cannot extract dashboard details from {dashboard}: {exc}"
@@ -515,15 +548,15 @@
 
         Read the context
         :param stage: Topology node being processed
         :param entity_request: Request sent to the sink
         :return: Entity FQN derived from context
         """
         context_names = [
-            self.context.__dict__[dependency]
+            self.context.get().__dict__[dependency]
             for dependency in stage.consumer or []  # root nodes do not have consumers
         ]
 
         if isinstance(stage.type_, DashboardDataModel):
             context_names.append("model")
 
         return fqn._build(  # pylint: disable=protected-access
@@ -593,7 +626,33 @@
                         EntityReference(
                             id=datamodel_entity.id.__root__,
                             type=LINEAGE_MAP[type(datamodel_entity)],
                         )
                     )
             patch_request.new_entity.dataModels = datamodel_entity_ref_list
         return patch_request
+
+    def _get_column_lineage(
+        self,
+        om_table: Table,
+        data_model_entity: DashboardDataModel,
+        columns_list: List[str],
+    ) -> List[ColumnLineage]:
+        """
+        Get the column lineage from the fields
+        """
+        try:
+            column_lineage = []
+            for field in columns_list or []:
+                from_column = get_column_fqn(table_entity=om_table, column=field)
+                to_column = self._get_data_model_column_fqn(
+                    data_model_entity=data_model_entity,
+                    column=field,
+                )
+                if from_column and to_column:
+                    column_lineage.append(
+                        ColumnLineage(fromColumns=[from_column], toColumn=to_column)
+                    )
+            return column_lineage
+        except Exception as exc:
+            logger.debug(f"Error to get column lineage: {exc}")
+            logger.debug(traceback.format_exc())
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/domodashboard/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/domodashboard/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -57,15 +57,17 @@
     Dashboard metadata from Domo's metadata db
     """
 
     config: WorkflowSource
     metadata_config: OpenMetadataConnection
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: DomoDashboardConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DomoDashboardConnection):
             raise InvalidSourceException(
                 f"Expected DomoDashboardConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -120,20 +122,20 @@
                 sourceUrl=dashboard_url,
                 displayName=dashboard_details.name,
                 description=dashboard_details.description,
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         chart_name=chart,
                     )
-                    for chart in self.context.charts or []
+                    for chart in self.context.get().charts or []
                 ],
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 owner=self.get_owner_ref(dashboard_details=dashboard_details),
             )
             yield Either(right=dashboard_request)
             self.register_record(dashboard_request=dashboard_request)
         except KeyError as err:
             yield Either(
                 left=StackTraceError(
@@ -215,15 +217,15 @@
                 if chart.name:
                     yield Either(
                         right=CreateChartRequest(
                             name=chart_id,
                             description=chart.description,
                             displayName=chart.name,
                             sourceUrl=chart_url,
-                            service=self.context.dashboard_service,
+                            service=self.context.get().dashboard_service,
                             chartType=get_standard_chart_type(chart.metadata.chartType),
                         )
                     )
             except Exception as exc:
                 name = chart.name if chart else ""
                 yield Either(
                     left=StackTraceError(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,15 +46,17 @@
     Lightdash Source Class
     """
 
     config: WorkflowSource
     metadata_config: OpenMetadataConnection
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: LightdashConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, LightdashConnection):
             raise InvalidSourceException(
                 f"Expected LightdashConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -107,20 +109,20 @@
                 sourceUrl=dashboard_url,
                 displayName=dashboard_details.name,
                 description=dashboard_details.description,
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         chart_name=chart,
                     )
-                    for chart in self.context.charts or []
+                    for chart in self.context.get().charts or []
                 ],
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 owner=self.get_owner_ref(dashboard_details=dashboard_details),
             )
             yield dashboard_request
             self.register_record(dashboard_request=dashboard_request)
         except Exception as exc:  # pylint: disable=broad-except
             logger.debug(traceback.format_exc())
             logger.warning(
@@ -148,15 +150,15 @@
                     self.status.filter(chart.name, "Chart Pattern not allowed")
                     continue
                 yield CreateChartRequest(
                     name=chart.uuid,
                     displayName=chart.name,
                     description=chart.description,
                     sourceUrl=chart_url,
-                    service=self.context.dashboard_service,
+                    service=self.context.get().dashboard_service,
                 )
                 self.status.scanned(chart.name)
             except Exception as exc:  # pylint: disable=broad-except
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Error creating chart [{chart}]: {exc}")
 
     def yield_dashboard_lineage_details(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/lightdash/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/lightdash/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/bulk_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/bulk_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/columns.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/columns.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/links.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/links.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/metadata.py`

 * *Files 3% similar despite different names*

```diff
@@ -170,15 +170,20 @@
         self._main_lookml_repo: Optional[LookMLRepo] = None
         self._main__lookml_manifest: Optional[LookMLManifest] = None
         self._view_data_model: Optional[DashboardDataModel] = None
 
         self._added_lineage: Optional[Dict] = {}
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata) -> "LookerSource":
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ) -> "LookerSource":
         config = WorkflowSource.parse_obj(config_dict)
         connection: LookerConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, LookerConnection):
             raise InvalidSourceException(
                 f"Expected LookerConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -363,15 +368,15 @@
                         f"Error fetching LookML Explore [{explore_nav.name}] in model [{lookml_model.name}] - {err}"
                     )
 
     def _build_data_model(self, data_model_name):
         fqn_datamodel = fqn.build(
             self.metadata,
             DashboardDataModel,
-            service_name=self.context.dashboard_service,
+            service_name=self.context.get().dashboard_service,
             data_model_name=data_model_name,
         )
 
         _datamodel = self.metadata.get_by_name(
             entity=DashboardDataModel,
             fqn=fqn_datamodel,
             fields=["*"],
@@ -392,34 +397,34 @@
             ):
                 self.status.filter(datamodel_name, "Data model filtered out.")
             else:
                 explore_datamodel = CreateDashboardDataModelRequest(
                     name=datamodel_name,
                     displayName=model.name,
                     description=model.description,
-                    service=self.context.dashboard_service,
+                    service=self.context.get().dashboard_service,
                     dataModelType=DataModelType.LookMlExplore.value,
                     serviceType=DashboardServiceType.Looker.value,
                     columns=get_columns_from_model(model),
                     sql=self._get_explore_sql(model),
                     # In Looker, you need to create Explores and Views within a Project
                     project=model.project_name,
                 )
                 yield Either(right=explore_datamodel)
                 self.register_record_datamodel(datamodel_request=explore_datamodel)
 
                 # build datamodel by our hand since ack_sink=False
-                self.context.dataModel = self._build_data_model(datamodel_name)
-                self._view_data_model = copy.deepcopy(self.context.dataModel)
+                self.context.get().dataModel = self._build_data_model(datamodel_name)
+                self._view_data_model = copy.deepcopy(self.context.get().dataModel)
 
                 # Maybe use the project_name as key too?
                 # Save the explores for when we create the lineage with the dashboards and views
                 self._explores_cache[
                     explore_datamodel.name.__root__
-                ] = self.context.dataModel  # This is the newly created explore
+                ] = self.context.get().dataModel  # This is the newly created explore
 
                 # We can get VIEWs from the JOINs to know the dependencies
                 # We will only try and fetch if we have the credentials
                 if self.repository_credentials:
                     for view in model.joins:
                         if filter_by_datamodel(
                             self.source_config.dataModelFilterPattern, view.name
@@ -490,15 +495,15 @@
             view: Optional[LookMlView] = project_parser.find_view(view_name=view_name)
 
             if view:
                 data_model_request = CreateDashboardDataModelRequest(
                     name=build_datamodel_name(explore.model_name, view.name),
                     displayName=view.name,
                     description=view.description,
-                    service=self.context.dashboard_service,
+                    service=self.context.get().dashboard_service,
                     dataModelType=DataModelType.LookMlView.value,
                     serviceType=DashboardServiceType.Looker.value,
                     columns=get_columns_from_model(view),
                     sql=project_parser.parsed_files.get(Includes(view.source_file)),
                     # In Looker, you need to create Explores and Views within a Project
                     project=explore.project_name,
                 )
@@ -537,30 +542,32 @@
 
             else:
                 logger.info(
                     f"Could not find model for explore [{explore.model_name}: {explore.name}] in the cache"
                     " while processing view lineage."
                 )
 
+            db_service_names = self.get_db_service_names()
+
             if view.sql_table_name:
                 source_table_name = self._clean_table_name(view.sql_table_name)
 
                 # View to the source is only there if we are informing the dbServiceNames
-                for db_service_name in self.source_config.dbServiceNames or []:
+                for db_service_name in db_service_names or []:
                     yield self.build_lineage_request(
                         source=source_table_name,
                         db_service_name=db_service_name,
                         to_entity=self._view_data_model,
                     )
 
             elif view.derived_table:
                 sql_query = view.derived_table.sql
                 if not sql_query:
                     return
-                for db_service_name in self.source_config.dbServiceNames or []:
+                for db_service_name in db_service_names or []:
                     db_service = self.metadata.get_by_name(
                         DatabaseService, db_service_name
                     )
 
                     lineage_parser = LineageParser(
                         sql_query,
                         ConnectionTypeDialectMapper.dialect_of(
@@ -650,24 +657,24 @@
             name=clean_dashboard_name(dashboard_details.id),
             displayName=dashboard_details.title,
             description=dashboard_details.description or None,
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
-                    service_name=self.context.dashboard_service,
+                    service_name=self.context.get().dashboard_service,
                     chart_name=chart,
                 )
-                for chart in self.context.charts or []
+                for chart in self.context.get().charts or []
             ],
             # Dashboards are created from the UI directly. They are not linked to a project
             # like LookML assets, but rather just organised in folders.
             project=self.get_project_name(dashboard_details),
             sourceUrl=f"{clean_uri(self.service_connection.hostPort)}/dashboards/{dashboard_details.id}",
-            service=self.context.dashboard_service,
+            service=self.context.get().dashboard_service,
             owner=self.get_owner_ref(dashboard_details=dashboard_details),
         )
         yield Either(right=dashboard_request)
         self.register_record(dashboard_request=dashboard_request)
 
     def get_project_name(self, dashboard_details: LookerDashboard) -> Optional[str]:
         """
@@ -728,15 +735,15 @@
         Get the dashboard model from cache or API
         """
         return self._explores_cache.get(explore_name) or self.metadata.get_by_name(
             entity=DashboardDataModel,
             fqn=fqn.build(
                 self.metadata,
                 entity_type=DashboardDataModel,
-                service_name=self.context.dashboard_service,
+                service_name=self.context.get().dashboard_service,
                 data_model_name=explore_name,
             ),
         )
 
     def yield_dashboard_lineage_details(
         self, dashboard_details: LookerDashboard, _: str
     ) -> Iterable[Either[AddLineageRequest]]:
@@ -753,16 +760,16 @@
             source_explore_list = self.get_dashboard_sources(dashboard_details)
             for explore_name in source_explore_list:
                 cached_explore = self.get_explore(explore_name)
                 if cached_explore:
                     dashboard_fqn = fqn.build(
                         self.metadata,
                         entity_type=Dashboard,
-                        service_name=self.context.dashboard_service,
-                        dashboard_name=self.context.dashboard,
+                        service_name=self.context.get().dashboard_service,
+                        dashboard_name=self.context.get().dashboard,
                     )
                     dashboard_entity = self.metadata.get_by_name(
                         entity=Dashboard, fqn=dashboard_fqn
                     )
                     yield Either(
                         right=AddLineageRequest(
                             edge=EntitiesEdge(
@@ -865,15 +872,15 @@
                         name=chart.id,
                         displayName=chart.title or chart.id,
                         description=self.build_chart_description(chart) or None,
                         chartType=get_standard_chart_type(chart.type).value,
                         sourceUrl=chart.query.share_url
                         if chart.query is not None
                         else f"{clean_uri(self.service_connection.hostPort)}/merge?mid={chart.merge_result_id}",
-                        service=self.context.dashboard_service,
+                        service=self.context.get().dashboard_service,
                     )
                 )
 
             except Exception as exc:
                 yield Either(
                     left=StackTraceError(
                         name=chart.id,
@@ -934,21 +941,21 @@
                 "percentileRank": 0.0
             }
         },
         :param dashboard_details: Looker Dashboard
         :return: UsageRequest, if not computed
         """
 
-        dashboard_name = self.context.dashboard
+        dashboard_name = self.context.get().dashboard
 
         try:
             dashboard_fqn = fqn.build(
                 metadata=self.metadata,
                 entity_type=Dashboard,
-                service_name=self.context.dashboard_service,
+                service_name=self.context.get().dashboard_service,
                 dashboard_name=dashboard_name,
             )
 
             dashboard: Dashboard = self.metadata.get_by_name(
                 entity=Dashboard,
                 fqn=dashboard_fqn,
                 fields=["usageSummary"],
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/looker/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/looker/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/client.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,165 +1,208 @@
-#  Copyright 2021 Collate
+#  Copyright 2023 Collate
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-REST Auth & Client for Metabase
+REST Auth & Client for Mstr
 """
-import json
 import traceback
 from typing import List, Optional
 
 import requests
+from mstr.requests import MSTRRESTSession
 
-from metadata.generated.schema.entity.services.connections.dashboard.metabaseConnection import (
-    MetabaseConnection,
+from metadata.generated.schema.entity.services.connections.dashboard.mstrConnection import (
+    MstrConnection,
 )
 from metadata.ingestion.connections.test_connections import SourceConnectionException
-from metadata.ingestion.ometa.client import REST, ClientConfig
-from metadata.ingestion.source.dashboard.metabase.models import (
-    MetabaseCollection,
-    MetabaseCollectionList,
-    MetabaseDashboard,
-    MetabaseDashboardDetails,
-    MetabaseDashboardList,
-    MetabaseDatabase,
-    MetabaseTable,
+from metadata.ingestion.source.dashboard.mstr.models import (
+    MstrDashboard,
+    MstrDashboardDetails,
+    MstrDashboardList,
+    MstrProject,
+    MstrProjectList,
+    MstrSearchResult,
+    MstrSearchResultList,
 )
-from metadata.utils.constants import AUTHORIZATION_HEADER, NO_ACCESS_TOKEN
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
-USERNAME_HEADER = "username"
-PASSWORD_HEADER = "password"
-SESSION_HEADERS = {"Content-Type": "application/json", "Accept": "*/*"}
-DEFAULT_TIMEOUT = 30
-METABASE_SESSION_HEADER = "X-Metabase-Session"
-API_VERSION = "api"
+API_VERSION = "MicroStrategyLibrary/api"
 
 
-class MetabaseClient:
+class MSTRClient:
     """
     Client Handling API communication with Metabase
     """
 
-    def _get_metabase_session(self) -> str:
-        try:
-            params = {USERNAME_HEADER: self.config.username}
-            if self.config.password:
-                params[PASSWORD_HEADER] = self.config.password.get_secret_value()
-            self.resp = requests.post(
-                f"{self.config.hostPort}/{API_VERSION}/session/",
-                data=json.dumps(params),
-                headers=SESSION_HEADERS,
-                timeout=DEFAULT_TIMEOUT,
+    def _get_base_url(self, path=None):
+        if not path:
+            return f"{self.config.hostPort}/{API_VERSION}/"
+        return f"{self.config.hostPort}/{API_VERSION}/{path}"
+
+    def _get_mstr_session(self) -> MSTRRESTSession:
+        try:
+            session = MSTRRESTSession(base_url=self._get_base_url())
+            session.login(
+                username=self.config.username,
+                password=self.config.password.get_secret_value(),
             )
-            return self.resp.json()["id"]
+            return session
 
         except KeyError as exe:
-            msg = "Failed to fetch metabase session, please validate credentials"
+            msg = "Failed to fetch mstr session, please validate credentials"
             raise SourceConnectionException(msg) from exe
 
         except Exception as exc:
             msg = f"Unknown error in connection: {exc}."
             raise SourceConnectionException(msg) from exc
 
     def __init__(
         self,
-        config: MetabaseConnection,
+        config: MstrConnection,
     ):
         self.config = config
-        session_token = self._get_metabase_session()
-        client_config: ClientConfig = ClientConfig(
-            base_url=self.config.hostPort,
-            api_version=API_VERSION,
-            auth_header=AUTHORIZATION_HEADER,
-            auth_token=lambda: (NO_ACCESS_TOKEN, 0),
-            extra_headers={METABASE_SESSION_HEADER: session_token},
-        )
-        self.client = REST(client_config)
-
-    def get_dashboards_list(self) -> List[MetabaseDashboard]:
-        """
-        Get List of all dashboards
-        """
-        try:
-            resp_dashboards = self.client.get("/dashboard")
-            if resp_dashboards:
-                dashboard_list = MetabaseDashboardList(dashboards=resp_dashboards)
-                return dashboard_list.dashboards
-        except Exception:
-            logger.debug(traceback.format_exc())
-            logger.warning("Failed to fetch the dashboard list")
-        return []
+        self.session = self._get_mstr_session()
+
+    def is_project_name(self) -> bool:
+        return bool(self.config.projectName)
 
-    def get_collections_list(self) -> List[MetabaseCollection]:
+    def get_projects_list(self) -> List[MstrProject]:
         """
-        Get List of all collections
+        Get List of all projects
         """
         try:
-            resp_collections = self.client.get("/collection")
-            if resp_collections:
-                collection_list = MetabaseCollectionList(collections=resp_collections)
-                return collection_list.collections
-        except Exception:
+            resp_projects = self.session.get(
+                url=self._get_base_url("projects"), params={"include_auth": True}
+            )
+
+            if not resp_projects.ok:
+                raise requests.ConnectionError()
+
+            project_list = MstrProjectList(projects=resp_projects.json())
+            return project_list.projects
+
+        except Exception as exc:
             logger.debug(traceback.format_exc())
-            logger.warning("Failed to fetch the collections list")
+            logger.warning(f"Failed to fetch the project list due to [{exc}]")
+
         return []
 
-    def get_dashboard_details(
-        self, dashboard_id: str
-    ) -> Optional[MetabaseDashboardDetails]:
+    def get_project_by_name(self) -> Optional[MstrProject]:
         """
-        Get Dashboard Details
+        Get Project By Name
         """
-        if not dashboard_id:
-            return None  # don't call api if dashboard_id is None
         try:
-            resp_dashboard = self.client.get(f"/dashboard/{dashboard_id}")
-            if resp_dashboard:
-                # Small hack needed to support Metabase versions older than 0.48
-                # https://www.metabase.com/releases/metabase-48#fyi--breaking-changes
-                if "ordered_cards" in resp_dashboard:
-                    resp_dashboard["dashcards"] = resp_dashboard["ordered_cards"]
-                return MetabaseDashboardDetails(**resp_dashboard)
+            resp_projects = self.session.get(
+                url=self._get_base_url(f"projects/{self.config.projectName}"),
+                params={"include_auth": True},
+            )
+
+            if not resp_projects.ok:
+                raise requests.ConnectionError()
+
+            project = MstrProject(**resp_projects.json())
+            return project
+
         except Exception:
             logger.debug(traceback.format_exc())
-            logger.warning(f"Failed to fetch the dashboard with id: {dashboard_id}")
+            logger.warning("Failed to fetch the project list")
+
         return None
 
-    def get_database(self, database_id: str) -> Optional[MetabaseDatabase]:
+    def get_search_results_list(
+        self, project_id, object_type
+    ) -> List[MstrSearchResult]:
+        """
+        Get Search Results
+        """
+        try:
+            resp_results = self.session.get(
+                url=self._get_base_url("searches/results"),
+                params={
+                    "include_auth": True,
+                    "project_id": project_id,
+                    "type": object_type,
+                    "getAncestors": False,
+                    "offset": 0,
+                    "limit": -1,
+                    "certifiedStatus": "ALL",
+                    "isCrossCluster": False,
+                    "result.hidden": False,
+                },
+            )
+
+            if not resp_results.ok:
+                raise requests.ConnectionError()
+
+            results = []
+            for resp_result in resp_results.json()["result"]:
+                results.append(resp_result)
+
+            results_list = MstrSearchResultList(results=results)
+            return results_list.results
+
+        except Exception:
+            logger.debug(traceback.format_exc())
+            logger.warning("Failed to fetch the Search Result list")
+
+        return []
+
+    def get_dashboards_list(self, project_id, project_name) -> List[MstrDashboard]:
         """
-        Get Database using database ID
+        Get Dashboard
         """
-        if not database_id:
-            return None  # don't call api if database_id is None
         try:
-            resp_database = self.client.get(f"/database/{database_id}")
-            if resp_database:
-                return MetabaseDatabase(**resp_database)
+            results = self.get_search_results_list(
+                project_id=project_id, object_type=55
+            )
+
+            dashboards = []
+            for result in results:
+                dashboards.append(
+                    MstrDashboard(projectName=project_name, **result.dict())
+                )
+
+            dashboards_list = MstrDashboardList(dashboards=dashboards)
+            return dashboards_list.dashboards
+
         except Exception:
             logger.debug(traceback.format_exc())
-            logger.warning(f"Failed to fetch the database with id: {database_id}")
-        return None
+            logger.warning("Failed to fetch the dashboard list")
+
+        return []
 
-    def get_table(self, table_id: str) -> Optional[MetabaseTable]:
+    def get_dashboard_details(
+        self, project_id, project_name, dashboard_id
+    ) -> Optional[MstrDashboardDetails]:
         """
-        Get Table using table ID
+        Get Dashboard Details
         """
-        if not table_id:
-            return None  # don't call api if table_id is None
         try:
-            resp_table = self.client.get(f"/table/{table_id}")
-            if resp_table:
-                return MetabaseTable(**resp_table)
+            resp_dashboard = self.session.get(
+                url=self._get_base_url(f"v2/dossiers/{dashboard_id}/definition"),
+                params={
+                    "include_auth": True,
+                },
+                headers={"X-MSTR-ProjectID": project_id},
+            )
+
+            if not resp_dashboard.ok:
+                raise requests.ConnectionError()
+
+            return MstrDashboardDetails(
+                projectId=project_id, projectName=project_name, **resp_dashboard.json()
+            )
+
         except Exception:
             logger.debug(traceback.format_exc())
-            logger.warning(f"Failed to fetch the table with id: {table_id}")
+            logger.warning(f"Failed to fetch the dashboard with id: {dashboard_id}")
+
         return None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/metabase/connection.py`

 * *Files 4% similar despite different names*

```diff
@@ -40,15 +40,16 @@
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
     def custom_executor():
-        return client.get_dashboards_list()
+        collections = client.get_collections_list_test_conn()
+        return client.get_dashboards_list_test_conn(collections)
 
     test_fn = {"GetDashboards": custom_executor}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/metabase/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -63,15 +63,17 @@
     Metabase Source Class
     """
 
     config: WorkflowSource
     metadata_config: OpenMetadataConnection
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: MetabaseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, MetabaseConnection):
             raise InvalidSourceException(
                 f"Expected MetabaseConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -88,15 +90,15 @@
         self.collections = self.client.get_collections_list()
         return super().prepare()
 
     def get_dashboards_list(self) -> Optional[List[MetabaseDashboard]]:
         """
         Get List of all dashboards
         """
-        return self.client.get_dashboards_list()
+        return self.client.get_dashboards_list(self.collections)
 
     def get_dashboard_name(self, dashboard: MetabaseDashboard) -> str:
         """
         Get Dashboard Name
         """
         return dashboard.name
 
@@ -140,25 +142,25 @@
                 f"{replace_special_with(raw=dashboard_details.name.lower(), replacement='-')}"
             )
             dashboard_request = CreateDashboardRequest(
                 name=dashboard_details.id,
                 sourceUrl=dashboard_url,
                 displayName=dashboard_details.name,
                 description=dashboard_details.description,
-                project=self.context.project_name,
+                project=self.context.get().project_name,
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         chart_name=chart,
                     )
-                    for chart in self.context.charts or []
+                    for chart in self.context.get().charts or []
                 ],
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 owner=self.get_owner_ref(dashboard_details=dashboard_details),
             )
             yield Either(right=dashboard_request)
             self.register_record(dashboard_request=dashboard_request)
         except Exception as exc:  # pylint: disable=broad-except
             yield Either(
                 left=StackTraceError(
@@ -196,15 +198,15 @@
                 yield Either(
                     right=CreateChartRequest(
                         name=chart_details.id,
                         displayName=chart_details.name,
                         description=chart_details.description,
                         chartType=get_standard_chart_type(chart_details.display).value,
                         sourceUrl=chart_url,
-                        service=self.context.dashboard_service,
+                        service=self.context.get().dashboard_service,
                     )
                 )
             except Exception as exc:  # pylint: disable=broad-except
                 yield Either(
                     left=StackTraceError(
                         name="Chart",
                         error=f"Error creating chart [{chart}]: {exc}",
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/metabase/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/metabase/models.py`

 * *Files 10% similar despite different names*

```diff
@@ -33,15 +33,15 @@
     """
 
     name: str
     id: str
 
 
 class MetabaseDashboardList(BaseModel):
-    dashboards: Optional[List[MetabaseDashboard]]
+    data: Optional[List[MetabaseDashboard]]
 
 
 class MetabaseCollectionList(BaseModel):
     collections: Optional[List[MetabaseCollection]]
 
 
 class Native(BaseModel):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mode/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mode/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mode/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mode/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mode/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mode/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,15 +55,17 @@
         metadata: OpenMetadata,
     ):
         super().__init__(config, metadata)
         self.workspace_name = config.serviceConnection.__root__.config.workspaceName
         self.data_sources = self.client.get_all_data_sources(self.workspace_name)
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: ModeConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, ModeConnection):
             raise InvalidSourceException(
                 f"Expected ModeConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -99,20 +101,20 @@
             sourceUrl=dashboard_url,
             displayName=dashboard_details.get(client.NAME),
             description=dashboard_details.get(client.DESCRIPTION),
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
-                    service_name=self.context.dashboard_service,
+                    service_name=self.context.get().dashboard_service,
                     chart_name=chart,
                 )
-                for chart in self.context.charts or []
+                for chart in self.context.get().charts or []
             ],
-            service=self.context.dashboard_service,
+            service=self.context.get().dashboard_service,
             owner=self.get_owner_ref(dashboard_details=dashboard_details),
         )
         yield Either(right=dashboard_request)
         self.register_record(dashboard_request=dashboard_request)
 
     def yield_dashboard_lineage_details(
         self, dashboard_details: dict, db_service_name: str
@@ -199,15 +201,15 @@
                     )
                     yield Either(
                         right=CreateChartRequest(
                             name=chart.get(client.TOKEN),
                             displayName=chart_name,
                             chartType=ChartType.Other,
                             sourceUrl=chart_url,
-                            service=self.context.dashboard_service,
+                            service=self.context.get().dashboard_service,
                         )
                     )
                 except Exception as exc:
                     name = chart_name if chart_name else ""
                     yield Either(
                         left=StackTraceError(
                             name=name,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mstr/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mstr/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,20 @@
 
 class MstrSource(DashboardServiceSource):
     """
     MSTR Source Class
     """
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: MstrConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, MstrConnection):
             raise InvalidSourceException(
                 f"Expected MstrConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -106,20 +111,20 @@
                 displayName=dashboard_details.name,
                 sourceUrl=dashboard_url,
                 project=dashboard_details.projectName,
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         chart_name=chart,
                     )
-                    for chart in self.context.charts or []
+                    for chart in self.context.get().charts or []
                 ],
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 owner=self.get_owner_ref(dashboard_details=dashboard_details),
             )
             yield Either(right=dashboard_request)
             self.register_record(dashboard_request=dashboard_request)
         except Exception as exc:
             yield Either(
                 left=StackTraceError(
@@ -165,15 +170,15 @@
                 yield Either(
                     right=CreateChartRequest(
                         name=f"{page.key}{chart.key}",
                         displayName=chart.name,
                         chartType=get_standard_chart_type(
                             chart.visualizationType
                         ).value,
-                        service=self.context.dashboard_service,
+                        service=self.context.get().dashboard_service,
                     )
                 )
             except Exception as exc:
                 yield Either(
                     left=StackTraceError(
                         name="Chart",
                         error=f"Error creating chart [{chart}]: {exc}",
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/mstr/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/mstr/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/client.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,20 +14,22 @@
 import json
 import math
 import traceback
 from time import sleep
 from typing import List, Optional, Tuple
 
 import msal
+from pydantic import BaseModel
 
 from metadata.generated.schema.entity.services.connections.dashboard.powerBIConnection import (
     PowerBIConnection,
 )
 from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.client import REST, ClientConfig
+from metadata.ingestion.source.dashboard.powerbi.file_client import PowerBiFileClient
 from metadata.ingestion.source.dashboard.powerbi.models import (
     DashboardsResponse,
     Dataset,
     DatasetResponse,
     Group,
     GroupsResponse,
     PowerBIDashboard,
@@ -41,15 +43,14 @@
     Workspaces,
     WorkSpaceScanResponse,
 )
 from metadata.utils.logger import utils_logger
 
 logger = utils_logger()
 
-
 # Similar inner methods with mode client. That's fine.
 # pylint: disable=duplicate-code
 class PowerBiApiClient:
     """
     REST Auth & Client for PowerBi
     """
 
@@ -191,16 +192,17 @@
         Returns:
             List[PowerBiTable]
         """
         try:
             response_data = self.client.get(
                 f"/myorg/groups/{group_id}/datasets/{dataset_id}/tables"
             )
-            response = TablesResponse(**response_data)
-            return response.value
+            if response_data:
+                response = TablesResponse(**response_data)
+                return response.value
         except Exception as exc:  # pylint: disable=broad-except
             logger.debug(traceback.format_exc())
             logger.warning(f"Error fetching dataset tables: {exc}")
 
         return None
 
     def fetch_all_workspaces(self) -> Optional[List[Group]]:
@@ -317,7 +319,15 @@
             if poll == max_poll:
                 break
             logger.info(f"Sleeping for {min_sleep_time} seconds")
             sleep(min_sleep_time)
             poll += 1
 
         return False
+
+
+class PowerBiClient(BaseModel):
+    class Config:
+        arbitrary_types_allowed = True
+
+    api_client: PowerBiApiClient
+    file_client: Optional[PowerBiFileClient]
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/mlflow/connection.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,44 +10,48 @@
 #  limitations under the License.
 
 """
 Source connection handler
 """
 from typing import Optional
 
+from mlflow.tracking import MlflowClient
+
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.dashboard.powerBIConnection import (
-    PowerBIConnection,
+from metadata.generated.schema.entity.services.connections.mlmodel.mlflowConnection import (
+    MlflowConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.dashboard.powerbi.client import PowerBiApiClient
 
 
-def get_connection(connection: PowerBIConnection) -> PowerBiApiClient:
+def get_connection(connection: MlflowConnection) -> MlflowClient:
     """
     Create connection
     """
-    return PowerBiApiClient(connection)
+    return MlflowClient(
+        tracking_uri=connection.trackingUri,
+        registry_uri=connection.registryUri,
+    )
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: PowerBiApiClient,
-    service_connection: PowerBIConnection,
+    client: MlflowClient,
+    service_connection: MlflowConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"GetDashboards": client.fetch_dashboards}
+    test_fn = {"GetModels": client.search_registered_models}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/metadata.py`

 * *Files 10% similar despite different names*

```diff
@@ -40,14 +40,15 @@
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.ometa.utils import model_str
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
 from metadata.ingestion.source.dashboard.powerbi.models import (
     Dataset,
     Group,
     PowerBIDashboard,
     PowerBIReport,
     PowerBiTable,
@@ -78,24 +79,29 @@
         metadata: OpenMetadata,
     ):
         super().__init__(config, metadata)
         self.pagination_entity_per_page = min(
             100, self.service_connection.pagination_entity_per_page
         )
         self.workspace_data = []
+        self.datamodel_file_mappings = []
 
     def prepare(self):
         if self.service_connection.useAdminApis:
             groups = self.get_admin_workspace_data()
         else:
             groups = self.get_org_workspace_data()
         if groups:
             self.workspace_data = self.get_filtered_workspaces(groups)
         return super().prepare()
 
+    def close(self):
+        self.metadata.close()
+        self.client.file_client.delete_tmp_files()
+
     def get_filtered_workspaces(self, groups: List[Group]) -> List[Group]:
         """
         Method to get the workspaces filtered by project filter pattern
         """
         filtered_groups = []
         for group in groups:
             if filter_by_project(
@@ -110,54 +116,54 @@
             filtered_groups.append(group)
         return filtered_groups
 
     def get_org_workspace_data(self) -> Optional[List[Group]]:
         """
         fetch all the group workspace ids
         """
-        groups = self.client.fetch_all_workspaces()
+        groups = self.client.api_client.fetch_all_workspaces()
         for group in groups:
             # add the dashboards to the groups
             group.dashboards.extend(
-                self.client.fetch_all_org_dashboards(group_id=group.id) or []
+                self.client.api_client.fetch_all_org_dashboards(group_id=group.id) or []
             )
             for dashboard in group.dashboards:
                 # add the tiles to the dashboards
                 dashboard.tiles.extend(
-                    self.client.fetch_all_org_tiles(
+                    self.client.api_client.fetch_all_org_tiles(
                         group_id=group.id, dashboard_id=dashboard.id
                     )
                     or []
                 )
 
             # add the reports to the groups
             group.reports.extend(
-                self.client.fetch_all_org_reports(group_id=group.id) or []
+                self.client.api_client.fetch_all_org_reports(group_id=group.id) or []
             )
 
             # add the datasets to the groups
             group.datasets.extend(
-                self.client.fetch_all_org_datasets(group_id=group.id) or []
+                self.client.api_client.fetch_all_org_datasets(group_id=group.id) or []
             )
             for dataset in group.datasets:
                 # add the tables to the datasets
                 dataset.tables.extend(
-                    self.client.fetch_dataset_tables(
+                    self.client.api_client.fetch_dataset_tables(
                         group_id=group.id, dataset_id=dataset.id
                     )
                     or []
                 )
         return groups
 
     def get_admin_workspace_data(self) -> Optional[List[Group]]:
         """
         fetch all the workspace ids
         """
         groups = []
-        workspaces = self.client.fetch_all_workspaces()
+        workspaces = self.client.api_client.fetch_all_workspaces()
         if workspaces:
             workspace_id_list = [workspace.id for workspace in workspaces]
 
             # Start the scan of the available workspaces for dashboard metadata
             workspace_paginated_list = [
                 workspace_id_list[i : i + self.pagination_entity_per_page]
                 for i in range(
@@ -165,24 +171,24 @@
                 )
             ]
             count = 1
             for workspace_ids_chunk in workspace_paginated_list:
                 logger.info(
                     f"Scanning {count}/{len(workspace_paginated_list)} set of workspaces"
                 )
-                workspace_scan = self.client.initiate_workspace_scan(
+                workspace_scan = self.client.api_client.initiate_workspace_scan(
                     workspace_ids_chunk
                 )
 
                 # Keep polling the scan status endpoint to check if scan is succeeded
-                workspace_scan_status = self.client.wait_for_scan_complete(
+                workspace_scan_status = self.client.api_client.wait_for_scan_complete(
                     scan_id=workspace_scan.id
                 )
                 if workspace_scan_status:
-                    response = self.client.fetch_workspace_scan_result(
+                    response = self.client.api_client.fetch_workspace_scan_result(
                         scan_id=workspace_scan.id
                     )
                     groups.extend(
                         [
                             active_workspace
                             for active_workspace in response.workspaces
                             if active_workspace.state == "Active"
@@ -192,29 +198,31 @@
                     logger.error("Error in fetching dashboards and charts")
                 count += 1
         else:
             logger.error("Unable to fetch any PowerBI workspaces")
         return groups or None
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: PowerBIConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, PowerBIConnection):
             raise InvalidSourceException(
                 f"Expected PowerBIConnection, but got {connection}"
             )
         return cls(config, metadata)
 
     def get_dashboard(self) -> Any:
         """
         Method to iterate through dashboard lists filter dashboards & yield dashboard details
         """
         for workspace in self.workspace_data:
-            self.context.workspace = workspace
+            self.context.get().workspace = workspace
             for dashboard in self.get_dashboards_list():
                 try:
                     dashboard_details = self.get_dashboard_details(dashboard)
                 except Exception as exc:
                     logger.debug(traceback.format_exc())
                     logger.warning(
                         f"Cannot extract dashboard details from {dashboard}: {exc}"
@@ -235,15 +243,18 @@
 
     def get_dashboards_list(
         self,
     ) -> Optional[List[Union[PowerBIDashboard, PowerBIReport]]]:
         """
         Get List of all dashboards
         """
-        return self.context.workspace.reports + self.context.workspace.dashboards
+        return (
+            self.context.get().workspace.reports
+            + self.context.get().workspace.dashboards
+        )
 
     def get_dashboard_name(
         self, dashboard: Union[PowerBIDashboard, PowerBIReport]
     ) -> str:
         """
         Get Dashboard Name
         """
@@ -316,15 +327,15 @@
         Method to fetch DataModels in bulk
         """
         try:
             data_model_request = CreateDashboardDataModelRequest(
                 name=dataset.id,
                 displayName=dataset.name,
                 description=dataset.description,
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 dataModelType=DataModelType.PowerBIDataModel.value,
                 serviceType=DashboardServiceType.PowerBI.value,
                 columns=self._get_column_info(dataset),
                 project=self._fetch_dataset_workspace(dataset_id=dataset.id),
             )
             yield Either(right=data_model_request)
             self.register_record_datamodel(datamodel_request=data_model_request)
@@ -391,43 +402,43 @@
         Method to Get Dashboard Entity, Dashboard Charts & Lineage
         """
         try:
             if isinstance(dashboard_details, PowerBIDashboard):
                 dashboard_request = CreateDashboardRequest(
                     name=dashboard_details.id,
                     sourceUrl=self._get_dashboard_url(
-                        workspace_id=self.context.workspace.id,
+                        workspace_id=self.context.get().workspace.id,
                         dashboard_id=dashboard_details.id,
                     ),
                     project=self.get_project_name(dashboard_details=dashboard_details),
                     displayName=dashboard_details.displayName,
                     dashboardType=DashboardType.Dashboard,
                     charts=[
                         fqn.build(
                             self.metadata,
                             entity_type=Chart,
-                            service_name=self.context.dashboard_service,
+                            service_name=self.context.get().dashboard_service,
                             chart_name=chart,
                         )
-                        for chart in self.context.charts or []
+                        for chart in self.context.get().charts or []
                     ],
-                    service=self.context.dashboard_service,
+                    service=self.context.get().dashboard_service,
                     owner=self.get_owner_ref(dashboard_details=dashboard_details),
                 )
             else:
                 dashboard_request = CreateDashboardRequest(
                     name=dashboard_details.id,
                     dashboardType=DashboardType.Report,
                     sourceUrl=self._get_report_url(
-                        workspace_id=self.context.workspace.id,
+                        workspace_id=self.context.get().workspace.id,
                         dashboard_id=dashboard_details.id,
                     ),
                     project=self.get_project_name(dashboard_details=dashboard_details),
                     displayName=dashboard_details.name,
-                    service=self.context.dashboard_service,
+                    service=self.context.get().dashboard_service,
                     owner=self.get_owner_ref(dashboard_details=dashboard_details),
                 )
             yield Either(right=dashboard_request)
             self.register_record(dashboard_request=dashboard_request)
         except Exception as exc:  # pylint: disable=broad-except
             yield Either(
                 left=StackTraceError(
@@ -518,63 +529,130 @@
 
                     # create the lineage between table and datamodel
                     yield from self.create_table_datamodel_lineage(
                         db_service_name=db_service_name,
                         tables=dataset.tables,
                         datamodel_entity=datamodel_entity,
                     )
+
+                    # create the lineage between table and datamodel using the pbit files
+                    if self.client.file_client:
+                        yield from self.create_table_datamodel_lineage_from_files(
+                            db_service_name=db_service_name,
+                            datamodel_entity=datamodel_entity,
+                        )
         except Exception as exc:  # pylint: disable=broad-except
             yield Either(
                 left=StackTraceError(
                     name=f"{db_service_name} Report Lineage",
                     error=(
                         "Error to yield datamodel and report lineage details for DB "
                         f"service name [{db_service_name}]: {exc}"
                     ),
                     stackTrace=traceback.format_exc(),
                 )
             )
 
+    def _get_table_and_datamodel_lineage(
+        self,
+        db_service_name: str,
+        table: PowerBiTable,
+        datamodel_entity: DashboardDataModel,
+    ) -> Optional[Iterable[Either[AddLineageRequest]]]:
+        """
+        Method to create lineage between table and datamodels
+        """
+        try:
+            table_fqn = fqn.build(
+                self.metadata,
+                entity_type=Table,
+                service_name=db_service_name,
+                database_name=None,
+                schema_name=None,
+                table_name=table.name,
+            )
+            table_entity = self.metadata.get_by_name(
+                entity=Table,
+                fqn=table_fqn,
+            )
+
+            if table_entity and datamodel_entity:
+                return self._get_add_lineage_request(
+                    to_entity=datamodel_entity, from_entity=table_entity
+                )
+        except Exception as exc:  # pylint: disable=broad-except
+            return Either(
+                left=StackTraceError(
+                    name="DataModel Lineage for pbit files",
+                    error=(
+                        "Error to yield datamodel lineage details using pbit files for"
+                        f"datamodel [{datamodel_entity.name}]: {exc}"
+                    ),
+                    stackTrace=traceback.format_exc(),
+                )
+            )
+        return None
+
+    def create_table_datamodel_lineage_from_files(
+        self,
+        db_service_name: str,
+        datamodel_entity: Optional[DashboardDataModel],
+    ) -> Iterable[Either[AddLineageRequest]]:
+        """
+        Method to create lineage between table and datamodels using pbit files
+        """
+        try:
+            # check if the datamodel_file_mappings is populated or not
+            # if not, then populate the datamodel_file_mappings and process the lineage
+            if not self.datamodel_file_mappings:
+                self.datamodel_file_mappings = (
+                    self.client.file_client.get_data_model_schema_mappings()
+                )
+
+            # search which file contains the datamodel and for the given datamodel_entity
+            datamodel_file_list = []
+            for datamodel_schema in self.datamodel_file_mappings or []:
+                for connections in (
+                    datamodel_schema.connectionFile.RemoteArtifacts or []
+                ):
+                    if connections.DatasetId == model_str(datamodel_entity.name):
+                        datamodel_file_list.append(datamodel_schema)
+
+            for datamodel_schema_file in datamodel_file_list:
+                for table in datamodel_schema_file.tables or []:
+                    yield self._get_table_and_datamodel_lineage(
+                        db_service_name=db_service_name,
+                        table=table,
+                        datamodel_entity=datamodel_entity,
+                    )
+        except Exception as exc:  # pylint: disable=broad-except
+            yield Either(
+                left=StackTraceError(
+                    name="DataModel Lineage",
+                    error=(
+                        "Error to yield datamodel lineage details for DB "
+                        f"service name [{db_service_name}]: {exc}"
+                    ),
+                    stackTrace=traceback.format_exc(),
+                )
+            )
+
     def create_table_datamodel_lineage(
         self,
         db_service_name: str,
         tables: Optional[List[PowerBiTable]],
         datamodel_entity: Optional[DashboardDataModel],
     ) -> Iterable[Either[CreateDashboardRequest]]:
         """Method to create lineage between table and datamodels"""
         for table in tables or []:
-            try:
-                table_fqn = fqn.build(
-                    self.metadata,
-                    entity_type=Table,
-                    service_name=db_service_name,
-                    database_name=None,
-                    schema_name=None,
-                    table_name=table.name,
-                )
-                table_entity = self.metadata.get_by_name(
-                    entity=Table,
-                    fqn=table_fqn,
-                )
-
-                if table_entity and datamodel_entity:
-                    yield self._get_add_lineage_request(
-                        to_entity=datamodel_entity, from_entity=table_entity
-                    )
-            except Exception as exc:  # pylint: disable=broad-except
-                yield Either(
-                    left=StackTraceError(
-                        name="DataModel Lineage",
-                        error=(
-                            "Error to yield datamodel lineage details for DB "
-                            f"service name [{db_service_name}]: {exc}"
-                        ),
-                        stackTrace=traceback.format_exc(),
-                    )
-                )
+            yield self._get_table_and_datamodel_lineage(
+                db_service_name=db_service_name,
+                table=table,
+                datamodel_entity=datamodel_entity,
+            )
 
     def yield_dashboard_lineage_details(
         self,
         dashboard_details: Union[PowerBIDashboard, PowerBIReport],
         db_service_name: str,
     ) -> Iterable[Either[AddLineageRequest]]:
         """
@@ -626,18 +704,18 @@
                     yield Either(
                         right=CreateChartRequest(
                             name=chart.id,
                             displayName=chart_display_name,
                             chartType=ChartType.Other.value,
                             sourceUrl=self._get_chart_url(
                                 report_id=chart.reportId,
-                                workspace_id=self.context.workspace.id,
+                                workspace_id=self.context.get().workspace.id,
                                 dashboard_id=dashboard_details.id,
                             ),
-                            service=self.context.dashboard_service,
+                            service=self.context.get().dashboard_service,
                         )
                     )
                 except Exception as exc:
                     yield Either(
                         left=StackTraceError(
                             name=chart.title,
                             error=f"Error creating chart [{chart.title}]: {exc}",
@@ -701,14 +779,14 @@
         return None
 
     def get_project_name(self, dashboard_details: Any) -> Optional[str]:
         """
         Get the project / workspace / folder / collection name of the dashboard
         """
         try:
-            return str(self.context.workspace.name)
+            return str(self.context.get().workspace.name)
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(
                 f"Error fetching project name for {dashboard_details.id}: {exc}"
             )
         return None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/powerbi/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/models.py`

 * *Files 10% similar despite different names*

```diff
@@ -188,7 +188,33 @@
 class PowerBiToken(BaseModel):
     """
     PowerBI Token Model
     """
 
     expires_in: Optional[int]
     access_token: Optional[str]
+
+
+class RemoteArtifacts(BaseModel):
+    """
+    PowerBI RemoteArtifacts Model
+    """
+
+    DatasetId: str
+    ReportId: str
+
+
+class ConnectionFile(BaseModel):
+    """
+    PowerBi Connection File Model
+    """
+
+    RemoteArtifacts: Optional[List[RemoteArtifacts]]
+
+
+class DataModelSchema(BaseModel):
+    """
+    PowerBi Data Model Schema Model
+    """
+
+    tables: Optional[List[PowerBiTable]]
+    connectionFile: Optional[ConnectionFile]
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/client.py`

 * *Files 7% similar despite different names*

```diff
@@ -36,16 +36,17 @@
     QlikDashboardResult,
     QlikDataModelResult,
     QlikSheet,
     QlikSheetResult,
     QlikTable,
 )
 from metadata.utils.constants import UTF_8
-from metadata.utils.helpers import clean_uri, delete_dir_content, init_staging_dir
+from metadata.utils.helpers import clean_uri
 from metadata.utils.logger import ingestion_logger
+from metadata.utils.ssl_manager import SSLManager
 
 logger = ingestion_logger()
 
 QLIK_USER_HEADER = "X-Qlik-User"
 
 
 class QlikSenseClient:
@@ -68,43 +69,25 @@
 
     def _get_ssl_context(self) -> Optional[dict]:
         if isinstance(self.config.certificates, QlikCertificatePath):
             context = {
                 "ca_certs": self.config.certificates.rootCertificate,
                 "certfile": self.config.certificates.clientCertificate,
                 "keyfile": self.config.certificates.clientKeyCertificate,
+                "check_hostname": self.config.validateHostName,
             }
             return context
 
-        init_staging_dir(self.config.certificates.stagingDir)
-        root_path = Path(self.config.certificates.stagingDir, "root.pem")
-        client_path = Path(self.config.certificates.stagingDir, "client.pem")
-        client_key_path = Path(self.config.certificates.stagingDir, "client_key.pem")
-
-        self.write_data_to_file(
-            root_path, self.config.certificates.rootCertificateData.get_secret_value()
-        )
-
-        self.write_data_to_file(
-            client_path,
-            self.config.certificates.clientCertificateData.get_secret_value(),
+        self.ssl_manager = SSLManager(
+            ca=self.config.certificates.sslConfig.__root__.caCertificate,
+            cert=self.config.certificates.sslConfig.__root__.sslCertificate,
+            key=self.config.certificates.sslConfig.__root__.sslKey,
         )
 
-        self.write_data_to_file(
-            client_key_path,
-            self.config.certificates.clientKeyCertificateData.get_secret_value(),
-        )
-
-        context = {
-            "ca_certs": root_path,
-            "certfile": client_path,
-            "keyfile": client_key_path,
-        }
-
-        return context
+        return self.ssl_manager.setup_ssl(self.config)
 
     def connect_websocket(self, app_id: str = None) -> None:
         """
         Method to initialise websocket connection
         """
         # pylint: disable=import-outside-toplevel
         import ssl
@@ -129,15 +112,15 @@
             self.get_dashboards_list(create_new_socket=False)
 
     def close_websocket(self) -> None:
         if self.socket_connection:
             self.socket_connection.close()
 
         if isinstance(self.config.certificates, QlikCertificateValues):
-            delete_dir_content(self.config.certificates.stagingDir)
+            self.ssl_manager.cleanup_temp_files()
 
     def __init__(
         self,
         config: QlikSenseConnection,
     ) -> None:
         self.config = config
         self.socket_connection = None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/constants.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/constants.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,17 +27,14 @@
 from metadata.generated.schema.entity.data.table import Column, DataType, Table
 from metadata.generated.schema.entity.services.connections.dashboard.qlikSenseConnection import (
     QlikSenseConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
-from metadata.generated.schema.entity.services.dashboardService import (
-    DashboardServiceType,
-)
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -62,15 +59,17 @@
     """Qlik Sense Source Class"""
 
     config: WorkflowSource
     client: QlikSenseClient
     metadata_config: OpenMetadataConnection
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: QlikSenseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, QlikSenseConnection):
             raise InvalidSourceException(
                 f"Expected QlikSenseConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -81,17 +80,26 @@
         metadata: OpenMetadata,
     ):
         super().__init__(config, metadata)
         self.collections: List[QlikDashboard] = []
         # Data models will be cleared up for each dashboard
         self.data_models: List[QlikTable] = []
 
+    def filter_draft_dashboard(self, dashboard: QlikDashboard) -> bool:
+        # When only published(non-draft) dashboards are allowed, filter dashboard based on "published" flag from QlikDashboardMeta(qMeta)
+        return (not self.source_config.includeDraftDashboard) and (
+            not dashboard.qMeta.published
+        )
+
     def get_dashboards_list(self) -> Iterable[QlikDashboard]:
         """Get List of all dashboards"""
         for dashboard in self.client.get_dashboards_list():
+            if self.filter_draft_dashboard(dashboard):
+                # Skip unpublished dashboards
+                continue
             # create app specific websocket
             self.client.connect_websocket(dashboard.qDocId)
             # clean data models for next iteration
             self.data_models = []
             yield dashboard
 
     def get_dashboard_name(self, dashboard: QlikDashboard) -> str:
@@ -122,20 +130,20 @@
                 sourceUrl=dashboard_url,
                 displayName=dashboard_details.qDocName,
                 description=dashboard_details.qMeta.description,
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         chart_name=chart,
                     )
-                    for chart in self.context.charts or []
+                    for chart in self.context.get().charts or []
                 ],
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 owner=self.get_owner_ref(dashboard_details=dashboard_details),
             )
             yield Either(right=dashboard_request)
             self.register_record(dashboard_request=dashboard_request)
         except Exception as exc:  # pylint: disable=broad-except
             yield Either(
                 left=StackTraceError(
@@ -169,15 +177,15 @@
                 yield Either(
                     right=CreateChartRequest(
                         name=chart.qInfo.qId,
                         displayName=chart.qMeta.title,
                         description=chart.qMeta.description,
                         chartType=ChartType.Other,
                         sourceUrl=chart_url,
-                        service=self.context.dashboard_service,
+                        service=self.context.get().dashboard_service,
                     )
                 )
             except Exception as exc:  # pylint: disable=broad-except
                 yield Either(
                     left=StackTraceError(
                         name=dashboard_details.qDocName,
                         error=f"Error creating chart [{chart}]: {exc}",
@@ -211,21 +219,20 @@
                         data_model.tableName if data_model.tableName else data_model.id
                     )
                     if filter_by_datamodel(
                         self.source_config.dataModelFilterPattern, data_model_name
                     ):
                         self.status.filter(data_model_name, "Data model filtered out.")
                         continue
-
                     data_model_request = CreateDashboardDataModelRequest(
                         name=data_model.id,
                         displayName=data_model_name,
-                        service=self.context.dashboard_service,
-                        dataModelType=DataModelType.QlikSenseDataModel.value,
-                        serviceType=DashboardServiceType.QlikSense.value,
+                        service=self.context.get().dashboard_service,
+                        dataModelType=DataModelType.QlikDataModel.value,
+                        serviceType=self.service_connection.type.value,
                         columns=self.get_column_info(data_model),
                     )
                     yield Either(right=data_model_request)
                     self.register_record_datamodel(datamodel_request=data_model_request)
                 except Exception as exc:
                     name = (
                         data_model.tableName if data_model.tableName else data_model.id
@@ -234,20 +241,20 @@
                         left=StackTraceError(
                             name=name,
                             error=f"Error yielding Data Model [{name}]: {exc}",
                             stackTrace=traceback.format_exc(),
                         )
                     )
 
-    def _get_datamodel(self, datamodel: QlikTable):
+    def _get_datamodel(self, datamodel_id: str):
         datamodel_fqn = fqn.build(
             self.metadata,
             entity_type=DashboardDataModel,
-            service_name=self.context.dashboard_service,
-            data_model_name=datamodel.id,
+            service_name=self.context.get().dashboard_service,
+            data_model_name=datamodel_id,
         )
         if datamodel_fqn:
             return self.metadata.get_by_name(
                 entity=DashboardDataModel,
                 fqn=datamodel_fqn,
             )
         return None
@@ -297,22 +304,28 @@
     ) -> Iterable[Either[AddLineageRequest]]:
         """Get lineage method"""
         db_service_entity = self.metadata.get_by_name(
             entity=DatabaseService, fqn=db_service_name
         )
         for datamodel in self.data_models or []:
             try:
-                data_model_entity = self._get_datamodel(datamodel=datamodel)
+                data_model_entity = self._get_datamodel(datamodel=datamodel.id)
                 if data_model_entity:
                     om_table = self._get_database_table(
                         db_service_entity, datamodel=datamodel
                     )
                     if om_table:
+                        columns_list = [col.name for col in datamodel.fields]
+                        column_lineage = self._get_column_lineage(
+                            om_table, data_model_entity, columns_list
+                        )
                         yield self._get_add_lineage_request(
-                            to_entity=data_model_entity, from_entity=om_table
+                            to_entity=data_model_entity,
+                            from_entity=om_table,
+                            column_lineage=column_lineage,
                         )
             except Exception as err:
                 yield Either(
                     left=StackTraceError(
                         name=f"{dashboard_details.qDocName} Lineage",
                         error=(
                             "Error to yield dashboard lineage details for DB "
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/qliksense/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qliksense/models.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,14 +16,15 @@
 from pydantic import BaseModel
 
 # dashboard models
 
 
 class QlikDashboardMeta(BaseModel):
     description: Optional[str]
+    published: Optional[bool]
 
 
 class QlikDashboard(BaseModel):
     qDocName: str
     qDocId: str
     qTitle: str
     qMeta: Optional[QlikDashboardMeta] = QlikDashboardMeta()
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/quicksight/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/quicksight/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/quicksight/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/quicksight/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -62,15 +62,17 @@
         )
         self.default_args = {
             "AwsAccountId": self.aws_account_id,
             "MaxResults": QUICKSIGHT_MAX_RESULTS,
         }
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: QuickSightConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, QuickSightConnection):
             raise InvalidSourceException(
                 f"Expected QuickSightConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -137,20 +139,20 @@
             sourceUrl=self.dashboard_url,
             displayName=dashboard_details["Name"],
             description=dashboard_details["Version"].get("Description"),
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
-                    service_name=self.context.dashboard_service,
+                    service_name=self.context.get().dashboard_service,
                     chart_name=chart,
                 )
-                for chart in self.context.charts or []
+                for chart in self.context.get().charts or []
             ],
-            service=self.context.dashboard_service,
+            service=self.context.get().dashboard_service,
             owner=self.get_owner_ref(dashboard_details=dashboard_details),
         )
         yield Either(right=dashboard_request)
         self.register_record(dashboard_request=dashboard_request)
 
     def yield_dashboard_chart(
         self, dashboard_details: Any
@@ -174,15 +176,15 @@
                 )
                 yield Either(
                     right=CreateChartRequest(
                         name=chart["SheetId"],
                         displayName=chart["Name"],
                         chartType=ChartType.Other.value,
                         sourceUrl=self.dashboard_url,
-                        service=self.context.dashboard_service,
+                        service=self.context.get().dashboard_service,
                     )
                 )
             except Exception as exc:
                 yield Either(
                     left=StackTraceError(
                         name="Chart",
                         error=f"Error creating chart [{chart}]: {exc}",
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/quicksight/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/quicksight/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/redash/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/redash/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/redash/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/redash/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/redash/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/redash/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -64,15 +64,20 @@
         metadata: OpenMetadata,
     ):
         super().__init__(config, metadata)
         self.dashboard_list = []  # We will populate this in `prepare`
         self.tags = []  # To create the tags before yielding final entities
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: RedashConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, RedashConnection):
             raise InvalidSourceException(
                 f"Expected RedashConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -150,20 +155,20 @@
                 name=dashboard_details["id"],
                 displayName=dashboard_details.get("name"),
                 description=dashboard_description,
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         chart_name=chart,
                     )
-                    for chart in self.context.charts or []
+                    for chart in self.context.get().charts or []
                 ],
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 sourceUrl=self.get_dashboard_url(dashboard_details),
                 tags=get_tag_labels(
                     metadata=self.metadata,
                     tags=dashboard_details.get("tags"),
                     classification_name=REDASH_TAG_CATEGORY,
                     include_tags=self.source_config.includeTags,
                 ),
@@ -262,15 +267,15 @@
                         name=widgets["id"],
                         displayName=chart_display_name
                         if visualization and visualization["query"]
                         else "",
                         chartType=get_standard_chart_type(
                             visualization["type"] if visualization else ""
                         ),
-                        service=self.context.dashboard_service,
+                        service=self.context.get().dashboard_service,
                         sourceUrl=self.get_dashboard_url(dashboard_details),
                         description=visualization["description"]
                         if visualization
                         else "",
                     )
                 )
             except Exception as exc:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/api_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/api_source.py`

 * *Files 2% similar despite different names*

```diff
@@ -73,14 +73,19 @@
         current_page = 0
         page_size = 25
         total_dashboards = self.client.fetch_total_dashboards()
         while current_page * page_size <= total_dashboards:
             dashboards = self.client.fetch_dashboards(current_page, page_size)
             current_page += 1
             for dashboard in dashboards.result:
+                if (
+                    not self.source_config.includeDraftDashboard
+                    and not dashboard.published
+                ):
+                    continue
                 yield dashboard
 
     def yield_dashboard(
         self, dashboard_details: DashboardResult
     ) -> Iterable[Either[CreateDashboardRequest]]:
         """
         Method to Get Dashboard Entity
@@ -90,20 +95,20 @@
                 name=dashboard_details.id,
                 displayName=dashboard_details.dashboard_title,
                 sourceUrl=f"{clean_uri(self.service_connection.hostPort)}{dashboard_details.url}",
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         chart_name=chart,
                     )
-                    for chart in self.context.charts or []
+                    for chart in self.context.get().charts or []
                 ],
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 owner=self.get_owner_ref(dashboard_details=dashboard_details),
             )
             yield Either(right=dashboard_request)
             self.register_record(dashboard_request=dashboard_request)
         except Exception as exc:  # pylint: disable=broad-except
             yield Either(
                 left=StackTraceError(
@@ -136,15 +141,15 @@
                     continue
                 chart = CreateChartRequest(
                     name=chart_json.id,
                     displayName=chart_json.slice_name,
                     description=chart_json.description,
                     chartType=get_standard_chart_type(chart_json.viz_type),
                     sourceUrl=f"{clean_uri(self.service_connection.hostPort)}{chart_json.url}",
-                    service=self.context.dashboard_service,
+                    service=self.context.get().dashboard_service,
                 )
                 yield Either(right=chart)
             except Exception as exc:  # pylint: disable=broad-except
                 yield Either(
                     left=StackTraceError(
                         name=chart_json.id,
                         error=f"Error creating chart [{chart_json.id} - {chart_json.slice_name}]: {exc}",
@@ -211,15 +216,15 @@
                         self.status.filter(
                             datasource_json.result.table_name,
                             "Data model filtered out.",
                         )
                     data_model_request = CreateDashboardDataModelRequest(
                         name=datasource_json.id,
                         displayName=datasource_json.result.table_name,
-                        service=self.context.dashboard_service,
+                        service=self.context.get().dashboard_service,
                         columns=self.get_column_info(datasource_json.result.columns),
                         dataModelType=DataModelType.SupersetDataModel.value,
                     )
                     yield Either(right=data_model_request)
                     self.register_record_datamodel(datamodel_request=data_model_request)
                 except Exception as exc:
                     yield Either(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/db_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/db_source.py`

 * *Files 6% similar despite different names*

```diff
@@ -41,14 +41,15 @@
     FetchColumn,
     FetchDashboard,
 )
 from metadata.ingestion.source.dashboard.superset.queries import (
     FETCH_ALL_CHARTS,
     FETCH_COLUMN,
     FETCH_DASHBOARDS,
+    FETCH_PUBLISHED_DASHBOARDS,
 )
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_datamodel
 from metadata.utils.helpers import (
     clean_uri,
     get_database_name_for_lineage,
     get_standard_chart_type,
@@ -96,15 +97,20 @@
             )
         return []
 
     def get_dashboards_list(self) -> Iterable[FetchDashboard]:
         """
         Get List of all dashboards
         """
-        dashboards = self.engine.execute(FETCH_DASHBOARDS)
+        query = (
+            FETCH_DASHBOARDS
+            if self.source_config.includeDraftDashboard
+            else FETCH_PUBLISHED_DASHBOARDS
+        )
+        dashboards = self.engine.execute(query)
         for dashboard in dashboards:
             yield FetchDashboard(**dashboard)
 
     def yield_dashboard(
         self, dashboard_details: FetchDashboard
     ) -> Iterable[Either[CreateDashboardRequest]]:
         """Method to Get Dashboard Entity"""
@@ -113,20 +119,20 @@
                 name=dashboard_details.id,
                 displayName=dashboard_details.dashboard_title,
                 sourceUrl=f"{clean_uri(self.service_connection.hostPort)}/superset/dashboard/{dashboard_details.id}/",
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         chart_name=chart,
                     )
-                    for chart in self.context.charts or []
+                    for chart in self.context.get().charts or []
                 ],
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 owner=self.get_owner_ref(dashboard_details=dashboard_details),
             )
             yield Either(right=dashboard_request)
             self.register_record(dashboard_request=dashboard_request)
         except Exception as exc:
             yield Either(
                 left=StackTraceError(
@@ -165,15 +171,15 @@
                     continue
                 chart = CreateChartRequest(
                     name=chart_json.id,
                     displayName=chart_json.slice_name,
                     description=chart_json.description,
                     chartType=get_standard_chart_type(chart_json.viz_type),
                     sourceUrl=f"{clean_uri(self.service_connection.hostPort)}/explore/?slice_id={chart_json.id}",
-                    service=self.context.dashboard_service,
+                    service=self.context.get().dashboard_service,
                 )
                 yield Either(right=chart)
             except Exception as exc:
                 yield Either(
                     left=StackTraceError(
                         name=chart_json.id,
                         error=f"Error yielding Chart [{chart_json.id} - {chart_json.slice_name}]: {exc}",
@@ -230,15 +236,15 @@
                         chart_json.table_name, "Data model filtered out."
                     )
                 col_names = self.get_column_list(chart_json.table_name)
                 try:
                     data_model_request = CreateDashboardDataModelRequest(
                         name=chart_json.datasource_id,
                         displayName=chart_json.table_name,
-                        service=self.context.dashboard_service,
+                        service=self.context.get().dashboard_service,
                         columns=self.get_column_info(col_names),
                         dataModelType=DataModelType.SupersetDataModel.value,
                     )
                     yield Either(right=data_model_request)
                     self.register_record_datamodel(datamodel_request=data_model_request)
 
                 except Exception as exc:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,14 +7,16 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Superset source module
 """
+from typing import Optional
+
 from metadata.generated.schema.entity.services.connections.dashboard.supersetConnection import (
     SupersetConnection,
 )
 from metadata.generated.schema.entity.utils.supersetApiConnection import (
     SupersetApiConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
@@ -28,15 +30,20 @@
 
 class SupersetSource:
     """
     Superset Source Class
     """
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: SupersetConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SupersetConnection):
             raise InvalidSourceException(
                 f"Expected SupersetConnection, but got {connection}"
             )
         if isinstance(connection.connection, SupersetApiConnection):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/mixin.py`

 * *Files 3% similar despite different names*

```diff
@@ -66,15 +66,20 @@
     service_connection: SupersetConnection
 
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.all_charts = {}
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: SupersetConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SupersetConnection):
             raise InvalidSourceException(
                 f"Expected SupersetConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -173,17 +178,29 @@
                             data_model_name=str(chart_json.datasource_id),
                         )
                         to_entity = self.metadata.get_by_name(
                             entity=DashboardDataModel,
                             fqn=datamodel_fqn,
                         )
 
+                        datasource_json = self.client.fetch_datasource(
+                            chart_json.datasource_id
+                        )
+                        datasource_columns = self.get_column_info(
+                            datasource_json.result.columns
+                        )
+                        columns_list = [col.displayName for col in datasource_columns]
+                        column_lineage = self._get_column_lineage(
+                            from_entity, to_entity, columns_list
+                        )
                         if from_entity and to_entity:
                             yield self._get_add_lineage_request(
-                                to_entity=to_entity, from_entity=from_entity
+                                to_entity=to_entity,
+                                from_entity=from_entity,
+                                column_lineage=column_lineage,
                             )
                     except Exception as exc:
                         yield Either(
                             left=StackTraceError(
                                 name=db_service_name,
                                 error=(
                                     "Error to yield dashboard lineage details for DB "
@@ -198,15 +215,15 @@
     ) -> Optional[DashboardDataModel]:
         """
         Get the datamodel entity for lineage
         """
         datamodel_fqn = fqn.build(
             self.metadata,
             entity_type=DashboardDataModel,
-            service_name=self.context.dashboard_service,
+            service_name=self.context.get().dashboard_service,
             data_model_name=datamodel.id,
         )
         if datamodel_fqn:
             return self.metadata.get_by_name(
                 entity=DashboardDataModel,
                 fqn=datamodel_fqn,
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/models.py`

 * *Files 4% similar despite different names*

```diff
@@ -38,14 +38,15 @@
 class DashboardResult(BaseModel):
     dashboard_title: Optional[str]
     url: Optional[str]
     owners: Optional[List[DashOwner]] = []
     position_json: Optional[str]
     id: Optional[int]
     email: Optional[str]
+    published: Optional[bool]
 
 
 class SupersetDashboardCount(BaseModel):
     count: Optional[int]
     ids: Optional[List[int]] = []
     dashboard_title: Optional[str]
     result: Optional[List[DashboardResult]] = []
@@ -131,14 +132,15 @@
     result: Optional[DatabaseResult] = DatabaseResult()
 
 
 class FetchDashboard(BaseModel):
     id: Optional[int]
     dashboard_title: Optional[str]
     position_json: Optional[str]
+    published: Optional[bool]
     email: Optional[str]
 
 
 class FetchChart(BaseModel):
     id: Optional[int]
     slice_name: Optional[str]
     description: Optional[str]
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/superset/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/superset/queries.py`

 * *Files 22% similar despite different names*

```diff
@@ -14,16 +14,16 @@
 
 
 FETCH_ALL_CHARTS = """
 select 
 	s.id,
 	s.slice_name,
 	s.description,
-    s.datasource_id,
-    s.viz_type,
+    	s.datasource_id,
+    	s.viz_type,
 	t.table_name,
 	t.schema,
 	db.database_name,
     db.sqlalchemy_uri
 from 
 	slices s left join "tables" t 
 on	s.datasource_id  = t.id and s.datasource_type = 'table' 
@@ -33,23 +33,40 @@
 
 
 FETCH_DASHBOARDS = """
 select
 	d.id, 
 	d.dashboard_title, 
 	d.position_json,
+    	d.published,
 	au.email 
 from 
 	dashboards d
 LEFT JOIN
 	ab_user au
 ON
 	d.created_by_fk = au.id
 """
 
+FETCH_PUBLISHED_DASHBOARDS = """
+select
+	d.id, 
+	d.dashboard_title, 
+	d.position_json,
+    	d.published,
+	au.email 
+from 
+	dashboards d
+LEFT JOIN
+	ab_user au
+ON
+	d.created_by_fk = au.id
+where 
+	d.published=true
+"""
 
 FETCH_ALL_CHARTS_TEST = """
 select 
 	s.id
 from 
 	slices s left join "tables" t 
 on	s.datasource_id  = t.id and s.datasource_type = 'table' 
@@ -70,18 +87,18 @@
 	d.created_by_fk = au.id
 LIMIT 1
 """
 
 FETCH_COLUMN = """
 select 
 	tc.id, 
-    table_name ,
-    column_name, 
-    type,
-    tc.description 
+    	table_name ,
+    	column_name, 
+    	type,
+    	tc.description 
 from 
 	table_columns  tc  
 inner join 
 	tables t 
 on 
 	t.id=tc.table_id  
 where
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/__init__.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -43,17 +43,19 @@
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
+from metadata.generated.schema.type.entityLineage import ColumnLineage
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import InvalidSourceException
+from metadata.ingestion.lineage.sql_lineage import get_column_fqn
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
 from metadata.ingestion.source.dashboard.tableau.client import TableauClient
 from metadata.ingestion.source.dashboard.tableau.models import (
     ChartUrl,
     DataSource,
@@ -84,15 +86,20 @@
     """
 
     config: WorkflowSource
     metadata_config: OpenMetadataConnection
     client: TableauClient
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: TableauConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, TableauConnection):
             raise InvalidSourceException(
                 f"Expected TableauConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -180,15 +187,15 @@
                 ):
                     self.status.filter(data_model_name, "Data model filtered out.")
                     continue
                 try:
                     data_model_request = CreateDashboardDataModelRequest(
                         name=data_model.id,
                         displayName=data_model_name,
-                        service=self.context.dashboard_service,
+                        service=self.context.get().dashboard_service,
                         dataModelType=DataModelType.TableauDataModel.value,
                         serviceType=DashboardServiceType.Tableau.value,
                         columns=self.get_column_info(data_model),
                         sql=self._get_datamodel_sql_query(data_model=data_model),
                     )
                     yield Either(right=data_model_request)
                     self.register_record_datamodel(datamodel_request=data_model_request)
@@ -224,49 +231,93 @@
                 displayName=dashboard_details.name,
                 description=dashboard_details.description,
                 project=self.get_project_name(dashboard_details=dashboard_details),
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         chart_name=chart,
                     )
-                    for chart in self.context.charts or []
+                    for chart in self.context.get().charts or []
                 ],
                 dataModels=[
                     fqn.build(
                         self.metadata,
                         entity_type=DashboardDataModel,
-                        service_name=self.context.dashboard_service,
+                        service_name=self.context.get().dashboard_service,
                         data_model_name=data_model,
                     )
-                    for data_model in self.context.dataModels or []
+                    for data_model in self.context.get().dataModels or []
                 ],
                 tags=get_tag_labels(
                     metadata=self.metadata,
                     tags=[tag.label for tag in dashboard_details.tags],
                     classification_name=TABLEAU_TAG_CATEGORY,
                     include_tags=self.source_config.includeTags,
                 ),
                 sourceUrl=dashboard_url,
-                service=self.context.dashboard_service,
+                service=self.context.get().dashboard_service,
                 owner=self.get_owner_ref(dashboard_details=dashboard_details),
             )
             yield Either(right=dashboard_request)
             self.register_record(dashboard_request=dashboard_request)
         except Exception as exc:
             yield Either(
                 left=StackTraceError(
                     name=dashboard_details.id,
                     error=f"Error to yield dashboard for {dashboard_details}: {exc}",
                     stackTrace=traceback.format_exc(),
                 )
             )
 
+    @staticmethod
+    def _get_data_model_column_fqn(
+        data_model_entity: DashboardDataModel, column: str
+    ) -> Optional[str]:
+        """
+        Get fqn of column if exist in table entity
+        """
+        if not data_model_entity:
+            return None
+        for tbl_column in data_model_entity.columns:
+            for child_column in tbl_column.children or []:
+                if column.lower() == child_column.name.__root__.lower():
+                    return child_column.fullyQualifiedName.__root__
+        return None
+
+    def _get_column_lineage(
+        self,
+        upstream_table: UpstreamTable,
+        table_entity: Table,
+        data_model_entity: DashboardDataModel,
+        upstream_col_set: Set[str],
+    ) -> List[ColumnLineage]:
+        """
+        Get the column lineage from the fields
+        """
+        try:
+            column_lineage = []
+            for column in upstream_table.columns or []:
+                if column.id in upstream_col_set:
+                    from_column = get_column_fqn(
+                        table_entity=table_entity, column=column.name
+                    )
+                    to_column = self._get_data_model_column_fqn(
+                        data_model_entity=data_model_entity,
+                        column=column.id,
+                    )
+                    column_lineage.append(
+                        ColumnLineage(fromColumns=[from_column], toColumn=to_column)
+                    )
+            return column_lineage
+        except Exception as exc:
+            logger.debug(f"Error to get column lineage: {exc}")
+            logger.debug(traceback.format_exc())
+
     def yield_dashboard_lineage_details(
         self, dashboard_details: TableauDashboard, db_service_name: str
     ) -> Iterable[Either[AddLineageRequest]]:
         """
         In Tableau, we get the lineage between data models and data sources.
 
         We build a DatabaseTable set from the sheets (data models) columns, and create a lineage request with an OM
@@ -281,20 +332,30 @@
         """
         db_service_entity = self.metadata.get_by_name(
             entity=DatabaseService, fqn=db_service_name
         )
         for datamodel in dashboard_details.dataModels or []:
             try:
                 data_model_entity = self._get_datamodel(datamodel=datamodel)
+                upstream_col_set = {
+                    column.id
+                    for field in datamodel.fields
+                    for column in field.upstreamColumns
+                }
                 if data_model_entity:
                     for table in datamodel.upstreamTables or []:
                         om_table = self._get_database_table(db_service_entity, table)
                         if om_table:
+                            column_lineage = self._get_column_lineage(
+                                table, om_table, data_model_entity, upstream_col_set
+                            )
                             yield self._get_add_lineage_request(
-                                to_entity=data_model_entity, from_entity=om_table
+                                to_entity=data_model_entity,
+                                from_entity=om_table,
+                                column_lineage=column_lineage,
                             )
             except Exception as err:
                 yield Either(
                     left=StackTraceError(
                         name="Lineage",
                         error=(
                             "Error to yield dashboard lineage details for DB "
@@ -336,15 +397,15 @@
                     sourceUrl=chart_url,
                     tags=get_tag_labels(
                         metadata=self.metadata,
                         tags=[tag.label for tag in chart.tags],
                         classification_name=TABLEAU_TAG_CATEGORY,
                         include_tags=self.source_config.includeTags,
                     ),
-                    service=self.context.dashboard_service,
+                    service=self.context.get().dashboard_service,
                 )
                 yield Either(right=chart)
             except Exception as exc:
                 yield Either(
                     left=StackTraceError(
                         name="Chart",
                         error=f"Error to yield dashboard chart [{chart}]: {exc}",
@@ -404,15 +465,15 @@
     def _get_datamodel(self, datamodel: DataSource) -> Optional[DashboardDataModel]:
         """
         Get the datamodel entity for lineage
         """
         datamodel_fqn = fqn.build(
             self.metadata,
             entity_type=DashboardDataModel,
-            service_name=self.context.dashboard_service,
+            service_name=self.context.get().dashboard_service,
             data_model_name=datamodel.id,
         )
         if datamodel_fqn:
             return self.metadata.get_by_name(
                 entity=DashboardDataModel,
                 fqn=datamodel_fqn,
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/dashboard/tableau/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/tableau/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/metadata.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,25 +8,27 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """Athena source module"""
 
 import traceback
-from typing import Iterable, Tuple
+from copy import deepcopy
+from typing import Dict, Iterable, List, Optional, Tuple
 
 from pyathena.sqlalchemy.base import AthenaDialect
 from sqlalchemy import types
 from sqlalchemy.engine import reflection
 from sqlalchemy.engine.reflection import Inspector
 
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.table import (
     Column,
-    IntervalType,
+    PartitionColumnDetails,
+    PartitionIntervalTypes,
     Table,
     TablePartition,
     TableType,
 )
 from metadata.generated.schema.entity.services.connections.database.athenaConnection import (
     AthenaConnection,
 )
@@ -53,14 +55,26 @@
 from metadata.utils.tag_utils import get_ometa_tag_and_classification
 
 logger = ingestion_logger()
 
 ATHENA_TAG = "ATHENA TAG"
 ATHENA_TAG_CLASSIFICATION = "ATHENA TAG CLASSIFICATION"
 
+ATHENA_INTERVAL_TYPE_MAP = {
+    **dict.fromkeys(["enum", "string", "VARCHAR"], PartitionIntervalTypes.COLUMN_VALUE),
+    **dict.fromkeys(
+        ["integer", "bigint", "INTEGER", "BIGINT"], PartitionIntervalTypes.INTEGER_RANGE
+    ),
+    **dict.fromkeys(
+        ["date", "timestamp", "DATE", "DATETIME", "TIMESTAMP"],
+        PartitionIntervalTypes.TIME_UNIT,
+    ),
+    "injected": PartitionIntervalTypes.INJECTED,
+}
+
 
 def _get_column_type(self, type_):
     """
     Function overwritten from AthenaDialect
     to add custom SQA typing.
     """
     type_ = type_.replace(" ", "").lower()
@@ -119,14 +133,37 @@
         col_type = col_map.get(name)
     else:
         logger.warning(f"Did not recognize type '{type_}'")
         col_type = types.NullType
     return col_type(*args)
 
 
+def _get_projection_details(
+    columns: List[Dict], projection_parameters: Dict
+) -> List[Dict]:
+    """Get the projection details for the columns
+
+    Args:
+        columns (List[Dict]): list of columns
+        projection_parameters (Dict): projection parameters
+    """
+    if not projection_parameters:
+        return columns
+
+    columns = deepcopy(columns)
+    for col in columns:
+        projection_details = next(
+            ({k: v} for k, v in projection_parameters.items() if k == col["name"]), None
+        )
+        if projection_details:
+            col["projection_type"] = projection_details[col["name"]]
+
+    return columns
+
+
 @reflection.cache
 def get_columns(self, connection, table_name, schema=None, **kw):
     """
     Method to handle table columns
     """
     metadata = self._get_table(  # pylint: disable=protected-access
         connection, table_name, schema=schema, **kw
@@ -143,14 +180,22 @@
             "is_complex": is_complex_type(c.type),
             "dialect_options": {"awsathena_partition": True},
         }
         for c in metadata.partition_keys
     ]
 
     if kw.get("only_partition_columns"):
+        # Return projected partition information to set partition type in `get_table_partition_details`
+        # projected partition fields are stored in the form of `projection.<field_name>.type` as a table parameter
+        projection_parameters = {
+            key_.split(".")[1]: value_
+            for key_, value_ in metadata.parameters.items()
+            if key_.startswith("projection") and key_.endswith("type")
+        }
+        columns = _get_projection_details(columns, projection_parameters)
         return columns
 
     columns += [
         {
             "name": c.name,
             "type": self._get_column_type(c.type),  # pylint: disable=protected-access
             "nullable": True,
@@ -188,15 +233,17 @@
 class AthenaSource(CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Athena Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: AthenaConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, AthenaConnection):
             raise InvalidSourceException(
                 f"Expected AthenaConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -219,22 +266,42 @@
         return [
             TableNameAndType(name=name, type_=TableType.External)
             for name in self.inspector.get_table_names(schema_name)
         ]
 
     def get_table_partition_details(
         self, table_name: str, schema_name: str, inspector: Inspector
-    ) -> Tuple[bool, TablePartition]:
+    ) -> Tuple[bool, Optional[TablePartition]]:
+        """Get Athena table partition detail
+
+        Args:
+            table_name (str): name of the table
+            schema_name (str): name of the schema
+            inspector (Inspector):
+
+
+        Returns:
+            Tuple[bool, Optional[TablePartition]]:
+        """
         columns = inspector.get_columns(
             table_name=table_name, schema=schema_name, only_partition_columns=True
         )
         if columns:
             partition_details = TablePartition(
-                intervalType=IntervalType.COLUMN_VALUE.value,
-                columns=[column["name"] for column in columns],
+                columns=[
+                    PartitionColumnDetails(
+                        columnName=col["name"],
+                        intervalType=ATHENA_INTERVAL_TYPE_MAP.get(
+                            col.get("projection_type", str(col["type"])),
+                            PartitionIntervalTypes.COLUMN_VALUE,
+                        ),
+                        interval=None,
+                    )
+                    for col in columns
+                ]
             )
             return True, partition_details
         return False, None
 
     def yield_tag(
         self, schema_name: str
     ) -> Iterable[Either[OMetaTagAndClassification]]:
@@ -247,16 +314,16 @@
                     name=schema_name
                 )
                 for tag in tags or []:
                     yield from get_ometa_tag_and_classification(
                         tag_fqn=fqn.build(
                             self.metadata,
                             DatabaseSchema,
-                            service_name=self.context.database_service,
-                            database_name=self.context.database,
+                            service_name=self.context.get().database_service,
+                            database_name=self.context.get().database,
                             schema_name=schema_name,
                         ),
                         tags=tag.TagValues,
                         classification_name=tag.TagKey,
                         tag_description=ATHENA_TAG,
                         classification_description=ATHENA_TAG_CLASSIFICATION,
                     )
@@ -276,27 +343,28 @@
         Method to yield table and column tags
         """
         if self.source_config.includeTags:
             try:
                 table_name, _ = table_name_and_type
                 table_tags = (
                     self.athena_lake_formation_client.get_table_and_column_tags(
-                        schema_name=self.context.database_schema, table_name=table_name
+                        schema_name=self.context.get().database_schema,
+                        table_name=table_name,
                     )
                 )
 
                 # yield the table tags
                 for tag in table_tags.LFTagsOnTable or []:
                     yield from get_ometa_tag_and_classification(
                         tag_fqn=fqn.build(
                             self.metadata,
                             Table,
-                            service_name=self.context.database_service,
-                            database_name=self.context.database,
-                            schema_name=self.context.database_schema,
+                            service_name=self.context.get().database_service,
+                            database_name=self.context.get().database,
+                            schema_name=self.context.get().database_schema,
                             table_name=table_name,
                         ),
                         tags=tag.TagValues,
                         classification_name=tag.TagKey,
                         tag_description=ATHENA_TAG,
                         classification_description=ATHENA_TAG_CLASSIFICATION,
                     )
@@ -304,17 +372,17 @@
                 # yield the column tags
                 for column in table_tags.LFTagsOnColumns or []:
                     for tag in column.LFTags or []:
                         yield from get_ometa_tag_and_classification(
                             tag_fqn=fqn.build(
                                 self.metadata,
                                 Column,
-                                service_name=self.context.database_service,
-                                database_name=self.context.database,
-                                schema_name=self.context.database_schema,
+                                service_name=self.context.get().database_service,
+                                database_name=self.context.get().database,
+                                schema_name=self.context.get().database_schema,
                                 table_name=table_name,
                                 column_name=column.Name,
                             ),
                             tags=tag.TagValues,
                             classification_name=tag.TagKey,
                             tag_description=ATHENA_TAG,
                             classification_description=ATHENA_TAG_CLASSIFICATION,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/query_parser.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 """
 Athena Query parser module
 """
 
 import traceback
 from abc import ABC
 from math import ceil
+from typing import Optional
 
 from metadata.clients.aws_client import AWSClient
 from metadata.generated.schema.entity.services.connections.database.athenaConnection import (
     AthenaConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
@@ -49,15 +50,17 @@
     filters: str
 
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.client = AWSClient(self.service_connection.awsConfig).get_athena_client()
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: AthenaConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, AthenaConnection):
             raise InvalidSourceException(
                 f"Expected AthenaConnection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/athena/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/athena/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,15 +55,17 @@
 class AzuresqlSource(CommonDbSourceService, MultiDBSource):
     """
     Implements the necessary methods to extract
     Database metadata from Azuresql Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: AzureSQLConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, AzureSQLConnection):
             raise InvalidSourceException(
                 f"Expected AzureSQLConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -82,15 +84,15 @@
             self.set_inspector(database_name=configured_db)
             yield configured_db
         else:
             for new_database in self.get_database_names_raw():
                 database_fqn = fqn.build(
                     self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
+                    service_name=self.context.get().database_service,
                     database_name=new_database,
                 )
 
                 if filter_by_database(
                     self.source_config.databaseFilterPattern,
                     database_fqn
                     if self.source_config.useFqnForFiltering
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/query_parser.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 AzureSQL usage module
 """
 from abc import ABC
+from typing import Optional
 
 from metadata.generated.schema.entity.services.connections.database.azureSQLConnection import (
     AzureSQLConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -28,15 +29,17 @@
     """
     AzureSQL base for Usage and Lineage
     """
 
     filters: str
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: AzureSQLConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, AzureSQLConnection):
             raise InvalidSourceException(
                 f"Expected Azuresql Connection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/azuresql/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/azuresql/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/metadata.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,673 +5,728 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Bigquery source module
+Snowflake source module
 """
-import os
+import json
 import traceback
+from datetime import datetime
 from typing import Dict, Iterable, List, Optional, Tuple
 
-from google import auth
-from google.cloud.datacatalog_v1 import PolicyTagManagerClient
+import sqlparse
+from snowflake.sqlalchemy.custom_types import VARIANT
+from snowflake.sqlalchemy.snowdialect import SnowflakeDialect, ischema_names
 from sqlalchemy.engine.reflection import Inspector
-from sqlalchemy.sql.sqltypes import Interval
-from sqlalchemy.types import String
-from sqlalchemy_bigquery import BigQueryDialect, _types
-from sqlalchemy_bigquery._types import _get_sqla_column_type
+from sqlparse.sql import Function, Identifier
 
-from metadata.generated.schema.api.data.createDatabaseSchema import (
-    CreateDatabaseSchemaRequest,
-)
 from metadata.generated.schema.api.data.createStoredProcedure import (
     CreateStoredProcedureRequest,
 )
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.storedProcedure import StoredProcedureCode
 from metadata.generated.schema.entity.data.table import (
-    IntervalType,
+    PartitionColumnDetails,
+    PartitionIntervalTypes,
+    Table,
     TablePartition,
     TableType,
 )
-from metadata.generated.schema.entity.services.connections.database.bigQueryConnection import (
-    BigQueryConnection,
+from metadata.generated.schema.entity.services.connections.database.snowflakeConnection import (
+    SnowflakeConnection,
 )
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.security.credentials.gcpValues import (
-    GcpCredentialsValues,
-)
 from metadata.generated.schema.type.basic import EntityName, SourceUrl
-from metadata.generated.schema.type.tagLabel import TagLabel
+from metadata.ingestion.api.delete import delete_entity_by_name
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.connections import get_test_connection_fn
-from metadata.ingestion.source.database.bigquery.helper import (
-    get_foreign_keys,
-    get_inspector_details,
-    get_pk_constraint,
-)
-from metadata.ingestion.source.database.bigquery.models import (
-    STORED_PROC_LANGUAGE_MAP,
-    BigQueryStoredProcedure,
-)
-from metadata.ingestion.source.database.bigquery.queries import (
-    BIGQUERY_GET_STORED_PROCEDURE_QUERIES,
-    BIGQUERY_GET_STORED_PROCEDURES,
-    BIGQUERY_LIFE_CYCLE_QUERY,
-    BIGQUERY_SCHEMA_DESCRIPTION,
-    BIGQUERY_TABLE_AND_TYPE,
-)
 from metadata.ingestion.source.database.column_type_parser import create_sqlalchemy_type
 from metadata.ingestion.source.database.common_db_source import (
     CommonDbSourceService,
     TableNameAndType,
 )
+from metadata.ingestion.source.database.external_table_lineage_mixin import (
+    ExternalTableLineageMixin,
+)
+from metadata.ingestion.source.database.incremental_metadata_extraction import (
+    IncrementalConfig,
+)
 from metadata.ingestion.source.database.life_cycle_query_mixin import (
     LifeCycleQueryMixin,
 )
 from metadata.ingestion.source.database.multi_db_source import MultiDBSource
+from metadata.ingestion.source.database.snowflake.models import (
+    STORED_PROC_LANGUAGE_MAP,
+    SnowflakeStoredProcedure,
+)
+from metadata.ingestion.source.database.snowflake.queries import (
+    SNOWFLAKE_DESC_STORED_PROCEDURE,
+    SNOWFLAKE_FETCH_ALL_TAGS,
+    SNOWFLAKE_GET_CLUSTER_KEY,
+    SNOWFLAKE_GET_CURRENT_ACCOUNT,
+    SNOWFLAKE_GET_DATABASE_COMMENTS,
+    SNOWFLAKE_GET_DATABASES,
+    SNOWFLAKE_GET_EXTERNAL_LOCATIONS,
+    SNOWFLAKE_GET_ORGANIZATION_NAME,
+    SNOWFLAKE_GET_SCHEMA_COMMENTS,
+    SNOWFLAKE_GET_STORED_PROCEDURE_QUERIES,
+    SNOWFLAKE_GET_STORED_PROCEDURES,
+    SNOWFLAKE_LIFE_CYCLE_QUERY,
+    SNOWFLAKE_SESSION_TAG_QUERY,
+)
+from metadata.ingestion.source.database.snowflake.utils import (
+    _current_database_schema,
+    get_columns,
+    get_foreign_keys,
+    get_pk_constraint,
+    get_schema_columns,
+    get_schema_foreign_keys,
+    get_table_comment,
+    get_table_names,
+    get_table_names_reflection,
+    get_unique_constraints,
+    get_view_definition,
+    get_view_names,
+    get_view_names_reflection,
+    normalize_names,
+)
 from metadata.ingestion.source.database.stored_procedures_mixin import (
     QueryByProcedure,
     StoredProcedureMixin,
 )
 from metadata.utils import fqn
-from metadata.utils.credentials import GOOGLE_CREDENTIALS
 from metadata.utils.filters import filter_by_database
 from metadata.utils.helpers import get_start_and_end
 from metadata.utils.logger import ingestion_logger
-from metadata.utils.sqlalchemy_utils import is_complex_type
-from metadata.utils.tag_utils import get_ometa_tag_and_classification, get_tag_label
-from metadata.utils.tag_utils import get_tag_labels as fetch_tag_labels_om
-
-_bigquery_table_types = {
-    "BASE TABLE": TableType.Regular,
-    "EXTERNAL": TableType.External,
-}
-
-
-class BQJSON(String):
-    """The SQL JSON type."""
-
-    def get_col_spec(self, **kw):  # pylint: disable=unused-argument
-        return "JSON"
+from metadata.utils.sqlalchemy_utils import get_all_table_comments
+from metadata.utils.tag_utils import get_ometa_tag_and_classification
 
+ischema_names["VARIANT"] = VARIANT
+ischema_names["GEOGRAPHY"] = create_sqlalchemy_type("GEOGRAPHY")
+ischema_names["GEOMETRY"] = create_sqlalchemy_type("GEOMETRY")
 
 logger = ingestion_logger()
-# pylint: disable=protected-access
-_types._type_map.update(
-    {
-        "GEOGRAPHY": create_sqlalchemy_type("GEOGRAPHY"),
-        "JSON": BQJSON,
-        "INTERVAL": Interval,
-    }
-)
-
-
-def _array_sys_data_type_repr(col_type):
-    """clean up the repr of the array data type
-
-    Args:
-        col_type (_type_): column type
-    """
-    return (
-        repr(col_type)
-        .replace("(", "<")
-        .replace(")", ">")
-        .replace("=", ":")
-        .replace("<>", "")
-        .lower()
-    )
-
-
-def get_columns(bq_schema):
-    """
-    get_columns method overwritten to include tag details
-    """
-    col_list = []
-    for field in bq_schema:
-        col_type = _get_sqla_column_type(field)
-        col_obj = {
-            "name": field.name,
-            "type": col_type,
-            "nullable": field.mode in ("NULLABLE", "REPEATED"),
-            "comment": field.description,
-            "default": None,
-            "precision": field.precision,
-            "scale": field.scale,
-            "max_length": field.max_length,
-            "system_data_type": _array_sys_data_type_repr(col_type)
-            if str(col_type) == "ARRAY"
-            else str(col_type),
-            "is_complex": is_complex_type(str(col_type)),
-            "policy_tags": None,
-        }
-        try:
-            if field.policy_tags:
-                policy_tag_name = field.policy_tags.names[0]
-                taxonomy_name = (
-                    policy_tag_name.split("/policyTags/")[0] if policy_tag_name else ""
-                )
-                if not taxonomy_name:
-                    raise NotImplementedError(
-                        f"Taxonomy Name not present for {field.name}"
-                    )
-                col_obj["taxonomy"] = (
-                    PolicyTagManagerClient()
-                    .get_taxonomy(name=taxonomy_name)
-                    .display_name
-                )
-                col_obj["policy_tags"] = (
-                    PolicyTagManagerClient()
-                    .get_policy_tag(name=policy_tag_name)
-                    .display_name
-                )
-        except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.warning(f"Skipping Policy Tag: {exc}")
-        col_list.append(col_obj)
-    return col_list
-
 
-_types.get_columns = get_columns
 
+SnowflakeDialect._json_deserializer = json.loads  # pylint: disable=protected-access
+SnowflakeDialect.get_table_names = get_table_names
+SnowflakeDialect.get_view_names = get_view_names
+SnowflakeDialect.get_all_table_comments = get_all_table_comments
+SnowflakeDialect.normalize_name = normalize_names
+SnowflakeDialect.get_table_comment = get_table_comment
+SnowflakeDialect.get_view_definition = get_view_definition
+SnowflakeDialect.get_unique_constraints = get_unique_constraints
+SnowflakeDialect._get_schema_columns = (  # pylint: disable=protected-access
+    get_schema_columns
+)
+Inspector.get_table_names = get_table_names_reflection
+Inspector.get_view_names = get_view_names_reflection
+SnowflakeDialect._current_database_schema = (  # pylint: disable=protected-access
+    _current_database_schema
+)
+SnowflakeDialect.get_pk_constraint = get_pk_constraint
+SnowflakeDialect.get_foreign_keys = get_foreign_keys
+SnowflakeDialect.get_columns = get_columns
+SnowflakeDialect._get_schema_foreign_keys = get_schema_foreign_keys
 
-@staticmethod
-def _build_formatted_table_id(table):
-    """We overide the methid as it returns both schema and table name if dataset_id is None. From our
-    investigation, this method seems to be used only in `_get_table_or_view_names()` of bigquery sqalchemy
-    https://github.com/googleapis/python-bigquery-sqlalchemy/blob/2b1f5c464ad2576e4512a0407bb044da4287c65e/sqlalchemy_bigquery/base.py
-    """
-    return f"{table.table_id}"
-
-
-BigQueryDialect._build_formatted_table_id = (  # pylint: disable=protected-access
-    _build_formatted_table_id
-)
-BigQueryDialect.get_pk_constraint = get_pk_constraint
-BigQueryDialect.get_foreign_keys = get_foreign_keys
 
-
-class BigquerySource(
-    LifeCycleQueryMixin, StoredProcedureMixin, CommonDbSourceService, MultiDBSource
+class SnowflakeSource(
+    LifeCycleQueryMixin,
+    StoredProcedureMixin,
+    ExternalTableLineageMixin,
+    CommonDbSourceService,
+    MultiDBSource,
 ):
     """
     Implements the necessary methods to extract
-    Database metadata from Bigquery Source
+    Database metadata from Snowflake Source
     """
 
-    def __init__(self, config, metadata):
-        # Check if the engine is established before setting project IDs
-        # This ensures that we don't try to set project IDs when there is no engine
-        # as per service connection config, which would result in an error.
-        self.test_connection = lambda: None
+    def __init__(
+        self,
+        config,
+        metadata,
+        pipeline_name,
+        incremental_configuration: IncrementalConfig,
+    ):
         super().__init__(config, metadata)
-        self.client = None
-        # Used to delete temp json file created while initializing bigquery client
-        self.temp_credentials_file_path = []
-        # Upon invoking the set_project_id method, we retrieve a comprehensive
-        # list of all project IDs. Subsequently, after the invokation,
-        # we proceed to test the connections for each of these project IDs
-        self.project_ids = self.set_project_id()
-        self.life_cycle_query = BIGQUERY_LIFE_CYCLE_QUERY
-        self.test_connection = self._test_connection
-        self.test_connection()
+        self.partition_details = {}
+        self.schema_desc_map = {}
+        self.database_desc_map = {}
+        self.external_location_map = {}
+
+        self._account: Optional[str] = None
+        self._org_name: Optional[str] = None
+        self.life_cycle_query = SNOWFLAKE_LIFE_CYCLE_QUERY
+        self.context.get_global().deleted_tables = []
+        self.pipeline_name = pipeline_name
+        self.incremental = incremental_configuration
+
+        if self.incremental.enabled:
+            date = datetime.fromtimestamp(self.incremental.start_timestamp / 1000)
+            logger.info(
+                "Starting Incremental Metadata Extraction.\n\t Considering Table changes from %s",
+                date,
+            )
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: BigQueryConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, BigQueryConnection):
+        connection: SnowflakeConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, SnowflakeConnection):
             raise InvalidSourceException(
-                f"Expected BigQueryConnection, but got {connection}"
+                f"Expected SnowflakeConnection, but got {connection}"
             )
-        return cls(config, metadata)
 
-    @staticmethod
-    def set_project_id() -> List[str]:
-        _, project_ids = auth.default()
-        return project_ids if isinstance(project_ids, list) else [project_ids]
-
-    def _test_connection(self) -> None:
-        for project_id in self.project_ids:
-            inspector_details = get_inspector_details(
-                database_name=project_id, service_connection=self.service_connection
-            )
-            test_connection_fn = get_test_connection_fn(self.service_connection)
-            test_connection_fn(
-                self.metadata, inspector_details.engine, self.service_connection
-            )
-            # GOOGLE_CREDENTIALS may not have been set,
-            # to avoid key error, we use `get` for dict
-            if os.environ.get(GOOGLE_CREDENTIALS):
-                self.temp_credentials_file_path.append(os.environ[GOOGLE_CREDENTIALS])
+        incremental_config = IncrementalConfig.create(
+            config.sourceConfig.config.incremental, pipeline_name, metadata
+        )
+        return cls(config, metadata, pipeline_name, incremental_config)
 
-    def query_table_names_and_types(
-        self, schema_name: str
-    ) -> Iterable[TableNameAndType]:
+    @property
+    def account(self) -> Optional[str]:
         """
-        Connect to the source database to get the table
-        name and type. By default, use the inspector method
-        to get the names and pass the Regular type.
+        Query the account information
+            ref https://docs.snowflake.com/en/sql-reference/functions/current_account_name
+        """
+        if self._account is None:
+            self._account = self._get_current_account()
 
-        This is useful for sources where we need fine-grained
-        logic on how to handle table types, e.g., external, foreign,...
+        return self._account
+
+    @property
+    def org_name(self) -> Optional[str]:
         """
+        Query the Organization information.
+            ref https://docs.snowflake.com/en/sql-reference/functions/current_organization_name
+        """
+        if self._org_name is None:
+            self._org_name = self._get_org_name()
 
-        return [
-            TableNameAndType(
-                name=table_name,
-                type_=_bigquery_table_types.get(table_type, TableType.Regular),
-            )
-            for table_name, table_type in self.engine.execute(
-                BIGQUERY_TABLE_AND_TYPE.format(
-                    project_id=self.client.project, schema_name=schema_name
-                )
-            )
-            or []
-        ]
+        return self._org_name
 
-    def yield_tag(
-        self, schema_name: str
-    ) -> Iterable[Either[OMetaTagAndClassification]]:
+    def set_session_query_tag(self) -> None:
         """
-        Build tag context
-        :param _:
-        :return:
+        Method to set query tag for current session
         """
-        try:
-            # Fetching labels on the databaseSchema ( dataset ) level
-            dataset_obj = self.client.get_dataset(schema_name)
-            if dataset_obj.labels:
-                for key, value in dataset_obj.labels.items():
-                    yield from get_ometa_tag_and_classification(
-                        tags=[value],
-                        classification_name=key,
-                        tag_description="Bigquery Dataset Label",
-                        classification_description="",
-                        include_tags=self.source_config.includeTags,
-                    )
-            # Fetching policy tags on the column level
-            list_project_ids = [self.context.database]
-            if not self.service_connection.taxonomyProjectID:
-                self.service_connection.taxonomyProjectID = []
-            list_project_ids.extend(self.service_connection.taxonomyProjectID)
-            for project_ids in list_project_ids:
-                taxonomies = PolicyTagManagerClient().list_taxonomies(
-                    parent=f"projects/{project_ids}/locations/{self.service_connection.taxonomyLocation}"
-                )
-                for taxonomy in taxonomies:
-                    policy_tags = PolicyTagManagerClient().list_policy_tags(
-                        parent=taxonomy.name
-                    )
-                    yield from get_ometa_tag_and_classification(
-                        tags=[tag.display_name for tag in policy_tags],
-                        classification_name=taxonomy.display_name,
-                        tag_description="Bigquery Policy Tag",
-                        classification_description="",
-                        include_tags=self.source_config.includeTags,
-                    )
-        except Exception as exc:
-            yield Either(
-                left=StackTraceError(
-                    name="Tags and Classifications",
-                    error=f"Skipping Policy Tag ingestion due to: {exc}",
-                    stackTrace=traceback.format_exc(),
+        if self.service_connection.queryTag:
+            self.engine.execute(
+                SNOWFLAKE_SESSION_TAG_QUERY.format(
+                    query_tag=self.service_connection.queryTag
                 )
             )
 
-    def get_schema_description(self, schema_name: str) -> Optional[str]:
-        try:
-            query_resp = self.client.query(
-                BIGQUERY_SCHEMA_DESCRIPTION.format(
-                    project_id=self.client.project,
-                    region=self.service_connection.usageLocation,
-                    schema_name=schema_name,
-                )
-            )
+    def set_partition_details(self) -> None:
+        self.partition_details.clear()
+        results = self.engine.execute(SNOWFLAKE_GET_CLUSTER_KEY).all()
+        for row in results:
+            if row.CLUSTERING_KEY:
+                self.partition_details[
+                    f"{row.TABLE_SCHEMA}.{row.TABLE_NAME}"
+                ] = row.CLUSTERING_KEY
 
-            query_result = [result.schema_description for result in query_resp.result()]
-            return fqn.unquote_name(query_result[0])
-        except IndexError:
-            logger.debug(f"No dataset description found for {schema_name}")
-        except Exception as err:
-            logger.debug(traceback.format_exc())
-            logger.debug(
-                f"Failed to fetch dataset description for [{schema_name}]: {err}"
-            )
-        return ""
+    def set_schema_description_map(self) -> None:
+        self.schema_desc_map.clear()
+        results = self.engine.execute(SNOWFLAKE_GET_SCHEMA_COMMENTS).all()
+        for row in results:
+            self.schema_desc_map[(row.DATABASE_NAME, row.SCHEMA_NAME)] = row.COMMENT
 
-    def yield_database_schema(
-        self, schema_name: str
-    ) -> Iterable[CreateDatabaseSchemaRequest]:
+    def set_database_description_map(self) -> None:
+        self.database_desc_map.clear()
+        if not self.database_desc_map:
+            results = self.engine.execute(SNOWFLAKE_GET_DATABASE_COMMENTS).all()
+            for row in results:
+                self.database_desc_map[row.DATABASE_NAME] = row.COMMENT
+
+    def set_external_location_map(self, database_name: str) -> None:
+        self.external_location_map.clear()
+        results = self.engine.execute(
+            SNOWFLAKE_GET_EXTERNAL_LOCATIONS.format(database_name=database_name)
+        ).all()
+        self.external_location_map = {
+            (row.database_name, row.schema_name, row.name): row.location
+            for row in results
+        }
+
+    def get_schema_description(self, schema_name: str) -> Optional[str]:
         """
-        From topology.
-        Prepare a database schema request and pass it to the sink
+        Method to fetch the schema description
         """
+        return self.schema_desc_map.get((self.context.get().database, schema_name))
 
-        database_schema_request_obj = CreateDatabaseSchemaRequest(
-            name=schema_name,
-            database=fqn.build(
-                metadata=self.metadata,
-                entity_type=Database,
-                service_name=self.context.database_service,
-                database_name=self.context.database,
-            ),
-            description=self.get_schema_description(schema_name),
-            sourceUrl=self.get_source_url(
-                database_name=self.context.database,
-                schema_name=schema_name,
-            ),
-        )
-        if self.source_config.includeTags:
-            dataset_obj = self.client.get_dataset(schema_name)
-            if dataset_obj.labels:
-                database_schema_request_obj.tags = []
-                for label_classification, label_tag_name in dataset_obj.labels.items():
-                    tag_label = get_tag_label(
-                        metadata=self.metadata,
-                        tag_name=label_tag_name,
-                        classification_name=label_classification,
-                    )
-                    if tag_label:
-                        database_schema_request_obj.tags.append(tag_label)
-        yield Either(right=database_schema_request_obj)
-
-    def get_table_obj(self, table_name: str):
-        schema_name = self.context.database_schema
-        database = self.context.database
-        bq_table_fqn = fqn._build(database, schema_name, table_name)
-        return self.client.get_table(bq_table_fqn)
-
-    def yield_table_tags(self, table_name_and_type: Tuple[str, str]):
-        table_name, _ = table_name_and_type
-        table_obj = self.get_table_obj(table_name=table_name)
-        if table_obj.labels:
-            for key, value in table_obj.labels.items():
-                yield from get_ometa_tag_and_classification(
-                    tags=[value],
-                    classification_name=key,
-                    tag_description="Bigquery Table Label",
-                    classification_description="",
-                    include_tags=self.source_config.includeTags,
-                )
-
-    def get_tag_labels(self, table_name: str) -> Optional[List[TagLabel]]:
-        """
-        This will only get executed if the tags context
-        is properly informed
-        """
-        table_tag_labels = super().get_tag_labels(table_name) or []
-        table_obj = self.get_table_obj(table_name=table_name)
-        if table_obj.labels:
-            for key, value in table_obj.labels.items():
-                tag_label = get_tag_label(
-                    metadata=self.metadata,
-                    tag_name=value,
-                    classification_name=key,
-                )
-                if tag_label:
-                    table_tag_labels.append(tag_label)
-        return table_tag_labels
-
-    def get_column_tag_labels(
-        self, table_name: str, column: dict
-    ) -> Optional[List[TagLabel]]:
-        """
-        This will only get executed if the tags context
-        is properly informed
-        """
-        if column.get("policy_tags"):
-            return fetch_tag_labels_om(
-                metadata=self.metadata,
-                tags=[column["policy_tags"]],
-                classification_name=column["taxonomy"],
-                include_tags=self.source_config.includeTags,
-            )
-        return None
-
-    def set_inspector(self, database_name: str):
-        inspector_details = get_inspector_details(
-            database_name=database_name, service_connection=self.service_connection
-        )
-        if os.environ.get(GOOGLE_CREDENTIALS):
-            self.temp_credentials_file_path.append(os.environ[GOOGLE_CREDENTIALS])
-        self.client = inspector_details.client
-        self.engine = inspector_details.engine
-        self.inspector = inspector_details.inspector
+    def get_database_description(self, database_name: str) -> Optional[str]:
+        """
+        Method to fetch the database description
+        """
+        return self.database_desc_map.get(database_name)
 
     def get_configured_database(self) -> Optional[str]:
-        return None
+        return self.service_connection.database
 
     def get_database_names_raw(self) -> Iterable[str]:
-        yield from self.project_ids
+        results = self.connection.execute(SNOWFLAKE_GET_DATABASES)
+        for res in results:
+            row = list(res)
+            yield row[1]
 
     def get_database_names(self) -> Iterable[str]:
-        for project_id in self.project_ids:
-            database_fqn = fqn.build(
-                self.metadata,
-                entity_type=Database,
-                service_name=self.context.database_service,
-                database_name=project_id,
-            )
-            if filter_by_database(
-                self.source_config.databaseFilterPattern,
-                database_fqn if self.source_config.useFqnForFiltering else project_id,
-            ):
-                self.status.filter(database_fqn, "Database Filtered out")
-            else:
+        configured_db = self.config.serviceConnection.__root__.config.database
+        if configured_db:
+            self.set_inspector(configured_db)
+            self.set_session_query_tag()
+            self.set_partition_details()
+            self.set_schema_description_map()
+            self.set_database_description_map()
+            self.set_external_location_map(configured_db)
+            yield configured_db
+        else:
+            for new_database in self.get_database_names_raw():
+                database_fqn = fqn.build(
+                    self.metadata,
+                    entity_type=Database,
+                    service_name=self.context.get().database_service,
+                    database_name=new_database,
+                )
+
+                if filter_by_database(
+                    self.source_config.databaseFilterPattern,
+                    database_fqn
+                    if self.source_config.useFqnForFiltering
+                    else new_database,
+                ):
+                    self.status.filter(database_fqn, "Database Filtered Out")
+                    continue
+
                 try:
-                    self.set_inspector(database_name=project_id)
-                    yield project_id
+                    self.set_inspector(database_name=new_database)
+                    self.set_session_query_tag()
+                    self.set_partition_details()
+                    self.set_schema_description_map()
+                    self.set_database_description_map()
+                    self.set_external_location_map(new_database)
+                    yield new_database
                 except Exception as exc:
                     logger.debug(traceback.format_exc())
-                    logger.error(
-                        f"Error trying to connect to database {project_id}: {exc}"
+                    logger.warning(
+                        f"Error trying to connect to database {new_database}: {exc}"
                     )
 
-    def get_view_definition(
-        self, table_type: str, table_name: str, schema_name: str, inspector: Inspector
-    ) -> Optional[str]:
-        if table_type == TableType.View:
-            try:
-                view_definition = inspector.get_view_definition(
-                    fqn._build(self.context.database, schema_name, table_name)
-                )
-                view_definition = (
-                    "" if view_definition is None else str(view_definition)
-                )
-            except NotImplementedError:
-                logger.warning("View definition not implemented")
-                view_definition = ""
-            return f"CREATE VIEW {schema_name}.{table_name} AS {view_definition}"
+    def __get_identifier_from_function(self, function_token: Function) -> List:
+        identifiers = []
+        for token in function_token.get_parameters():
+            if isinstance(token, Function):
+                # get column names from nested functions
+                identifiers.extend(self.__get_identifier_from_function(token))
+            elif isinstance(token, Identifier):
+                identifiers.append(token.get_real_name())
+        return identifiers
+
+    def parse_column_name_from_expr(self, cluster_key_expr: str) -> Optional[List[str]]:
+        try:
+            parser = sqlparse.parse(cluster_key_expr)
+            if not parser:
+                return []
+            result = []
+            tokens_list = parser[0].tokens
+            for token in tokens_list:
+                if isinstance(token, Function):
+                    result.extend(self.__get_identifier_from_function(token))
+                elif isinstance(token, Identifier):
+                    result.append(token.get_real_name())
+            return result
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Failed to parse cluster key - {err}")
         return None
 
+    def __fix_partition_column_case(
+        self,
+        table_name: str,
+        schema_name: str,
+        inspector: Inspector,
+        partition_columns: Optional[List[str]],
+    ) -> List[str]:
+        if partition_columns:
+            columns = []
+            table_columns = inspector.get_columns(
+                table_name=table_name, schema=schema_name
+            )
+            for pcolumn in partition_columns:
+                for tcolumn in table_columns:
+                    if tcolumn["name"].lower() == pcolumn.lower():
+                        columns.append(tcolumn["name"])
+                        break
+            return columns
+        return []
+
     def get_table_partition_details(
         self, table_name: str, schema_name: str, inspector: Inspector
-    ) -> Tuple[bool, TablePartition]:
-        """
-        check if the table is partitioned table and return the partition details
-        """
-        database = self.context.database
-        table = self.client.get_table(fqn._build(database, schema_name, table_name))
-        if table.time_partitioning is not None:
-            if table.time_partitioning.field:
-                table_partition = TablePartition(
-                    interval=str(table.time_partitioning.type_),
-                    intervalType=IntervalType.TIME_UNIT.value,
-                )
-                table_partition.columns = [table.time_partitioning.field]
-                return True, table_partition
-
-            return True, TablePartition(
-                interval=str(table.time_partitioning.type_),
-                intervalType=IntervalType.INGESTION_TIME.value,
-            )
-        if table.range_partitioning:
-            table_partition = TablePartition(
-                intervalType=IntervalType.INTEGER_RANGE.value,
+    ) -> Tuple[bool, Optional[TablePartition]]:
+        cluster_key = self.partition_details.get(f"{schema_name}.{table_name}")
+        if cluster_key:
+            partition_columns = self.parse_column_name_from_expr(cluster_key)
+            partition_details = TablePartition(
+                columns=[
+                    PartitionColumnDetails(
+                        columnName=column,
+                        intervalType=PartitionIntervalTypes.COLUMN_VALUE,
+                        interval=None,
+                    )
+                    for column in self.__fix_partition_column_case(
+                        table_name, schema_name, inspector, partition_columns
+                    )
+                ]
             )
-            if hasattr(table.range_partitioning, "range_") and hasattr(
-                table.range_partitioning.range_, "interval"
-            ):
-                table_partition.interval = table.range_partitioning.range_.interval
-            if (
-                hasattr(table.range_partitioning, "field")
-                and table.range_partitioning.field
-            ):
-                table_partition.columns = [table.range_partitioning.field]
-            return True, table_partition
+            return True, partition_details
         return False, None
 
-    def clean_raw_data_type(self, raw_data_type):
-        return raw_data_type.replace(", ", ",").replace(" ", ":").lower()
+    def yield_tag(
+        self, schema_name: str
+    ) -> Iterable[Either[OMetaTagAndClassification]]:
+        if self.source_config.includeTags:
+            result = []
+            try:
+                result = self.connection.execute(
+                    SNOWFLAKE_FETCH_ALL_TAGS.format(
+                        database_name=self.context.get().database,
+                        schema_name=schema_name,
+                    )
+                )
+
+            except Exception as exc:
+                try:
+                    logger.debug(traceback.format_exc())
+                    logger.warning(
+                        f"Error fetching tags {exc}. Trying with quoted names"
+                    )
+                    result = self.connection.execute(
+                        SNOWFLAKE_FETCH_ALL_TAGS.format(
+                            database_name=f'"{self.context.get().database}"',
+                            schema_name=f'"{self.context.get().database_schema}"',
+                        )
+                    )
+                except Exception as inner_exc:
+                    yield Either(
+                        left=StackTraceError(
+                            name="Tags and Classifications",
+                            error=f"Failed to fetch tags due to [{inner_exc}]",
+                            stackTrace=traceback.format_exc(),
+                        )
+                    )
 
-    def close(self):
-        super().close()
-        os.environ.pop("GOOGLE_CLOUD_PROJECT", "")
-        if isinstance(
-            self.service_connection.credentials.gcpConfig, GcpCredentialsValues
-        ) and (GOOGLE_CREDENTIALS in os.environ):
-            del os.environ[GOOGLE_CREDENTIALS]
-            for temp_file_path in self.temp_credentials_file_path:
-                if os.path.exists(temp_file_path):
-                    os.remove(temp_file_path)
+            for res in result:
+                row = list(res)
+                fqn_elements = [name for name in row[2:] if name]
+                yield from get_ometa_tag_and_classification(
+                    tag_fqn=fqn._build(  # pylint: disable=protected-access
+                        self.context.get().database_service, *fqn_elements
+                    ),
+                    tags=[row[1]],
+                    classification_name=row[0],
+                    tag_description="SNOWFLAKE TAG VALUE",
+                    classification_description="SNOWFLAKE TAG NAME",
+                )
 
-    def _get_source_url(
-        self,
-        database_name: Optional[str] = None,
-        schema_name: Optional[str] = None,
-        table_name: Optional[str] = None,
-        type_infix: str = "4m3",
-    ) -> Optional[str]:
+    def _get_table_names_and_types(
+        self, schema_name: str, table_type: TableType = TableType.Regular
+    ) -> List[TableNameAndType]:
+        table_type_to_params_map = {
+            TableType.Regular: {},
+            TableType.External: {"external_tables": True},
+            TableType.Transient: {"include_transient_tables": True},
+            TableType.Dynamic: {"dynamic_tables": True},
+        }
+
+        snowflake_tables = self.inspector.get_table_names(
+            schema=schema_name,
+            incremental=self.incremental,
+            **table_type_to_params_map[table_type],
+        )
+
+        self.context.get_global().deleted_tables.extend(
+            [
+                fqn.build(
+                    metadata=self.metadata,
+                    entity_type=Table,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=schema_name,
+                    table_name=table.name,
+                )
+                for table in snowflake_tables.get_deleted()
+            ]
+        )
+
+        return [
+            TableNameAndType(name=table.name, type_=table_type)
+            for table in snowflake_tables.get_not_deleted()
+        ]
+
+    def query_table_names_and_types(
+        self, schema_name: str
+    ) -> Iterable[TableNameAndType]:
         """
-        Method to get the source url for bigquery
+        Connect to the source database to get the table
+        name and type. By default, use the inspector method
+        to get the names and pass the Regular type.
+
+        This is useful for sources where we need fine-grained
+        logic on how to handle table types, e.g., external, foreign,...
         """
+        table_list = self._get_table_names_and_types(schema_name)
+
+        table_list.extend(
+            self._get_table_names_and_types(schema_name, table_type=TableType.External)
+        )
+
+        table_list.extend(
+            self._get_table_names_and_types(schema_name, table_type=TableType.Dynamic)
+        )
+
+        if self.service_connection.includeTransientTables:
+            table_list.extend(
+                self._get_table_names_and_types(
+                    schema_name, table_type=TableType.Transient
+                )
+            )
+
+        return table_list
+
+    def _get_org_name(self) -> Optional[str]:
         try:
-            bigquery_host = "https://console.cloud.google.com/"
-            database_url = f"{bigquery_host}bigquery?project={database_name}"
+            res = self.engine.execute(SNOWFLAKE_GET_ORGANIZATION_NAME).one()
+            if res:
+                return res.NAME
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.debug(f"Failed to fetch Organization name due to: {exc}")
+        return None
 
-            schema_table_url = None
-            if schema_name:
-                schema_table_url = f"&ws=!1m4!1m3!3m2!1s{database_name}!2s{schema_name}"
-            if table_name:
-                schema_table_url = (
-                    f"&ws=!1m5!1m4!{type_infix}!1s{database_name}"
-                    f"!2s{schema_name}!3s{table_name}"
-                )
-            if schema_table_url:
-                return f"{database_url}{schema_table_url}"
-            return database_url
+    def _get_current_account(self) -> Optional[str]:
+        try:
+            res = self.engine.execute(SNOWFLAKE_GET_CURRENT_ACCOUNT).one()
+            if res:
+                return res.ACCOUNT
         except Exception as exc:
             logger.debug(traceback.format_exc())
-            logger.warning(f"Unable to get source url: {exc}")
+            logger.debug(f"Failed to fetch current account due to: {exc}")
         return None
 
+    def _get_source_url_root(
+        self, database_name: Optional[str] = None, schema_name: Optional[str] = None
+    ) -> str:
+        url = (
+            f"https://app.snowflake.com/{self.org_name.lower()}"
+            f"/{self.account.lower()}/#/data/databases/{database_name}"
+        )
+        if schema_name:
+            url = f"{url}/schemas/{schema_name}"
+
+        return url
+
     def get_source_url(
         self,
         database_name: Optional[str] = None,
         schema_name: Optional[str] = None,
         table_name: Optional[str] = None,
         table_type: Optional[TableType] = None,
     ) -> Optional[str]:
-        return self._get_source_url(
-            database_name=database_name,
-            schema_name=schema_name,
-            table_name=table_name,
-            # This infix identifies tables in the URL
-            type_infix="4m3",
+        """
+        Method to get the source url for snowflake
+        """
+        try:
+            if self.account and self.org_name:
+                tab_type = "view" if table_type == TableType.View else "table"
+                url = self._get_source_url_root(
+                    database_name=database_name, schema_name=schema_name
+                )
+                if table_name:
+                    url = f"{url}/{tab_type}/{table_name}"
+                return url
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.error(f"Unable to get source url: {exc}")
+        return None
+
+    def _get_view_names_and_types(
+        self, schema_name: str, materialized_views: bool = False
+    ) -> List[TableNameAndType]:
+        table_type = (
+            TableType.MaterializedView if materialized_views else TableType.View
         )
 
-    def get_stored_procedure_url(
-        self,
-        database_name: Optional[str] = None,
-        schema_name: Optional[str] = None,
-        table_name: Optional[str] = None,
-    ) -> Optional[str]:
-        return self._get_source_url(
-            database_name=database_name,
-            schema_name=schema_name,
-            table_name=table_name,
-            # This infix identifies Stored Procedures in the URL
-            type_infix="6m3",
+        snowflake_views = self.inspector.get_view_names(
+            schema=schema_name,
+            incremental=self.incremental,
+            materialized_views=materialized_views,
         )
 
-    def get_stored_procedures(self) -> Iterable[BigQueryStoredProcedure]:
-        """List BigQuery Stored Procedures"""
+        self.context.get_global().deleted_tables.extend(
+            [
+                fqn.build(
+                    metadata=self.metadata,
+                    entity_type=Table,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=schema_name,
+                    table_name=view.name,
+                )
+                for view in snowflake_views.get_deleted()
+            ]
+        )
+
+        return [
+            TableNameAndType(name=view.name, type_=table_type)
+            for view in snowflake_views.get_not_deleted()
+        ]
+
+    def query_view_names_and_types(
+        self, schema_name: str
+    ) -> Iterable[TableNameAndType]:
+        """
+        Connect to the source database to get the view
+        name and type. By default, use the inspector method
+        to get the names and pass the View type.
+
+        This is useful for sources where we need fine-grained
+        logic on how to handle table types, e.g., material views,...
+        """
+        views = self._get_view_names_and_types(schema_name)
+        views.extend(
+            self._get_view_names_and_types(schema_name, materialized_views=True)
+        )
+
+        return views
+
+    def get_stored_procedures(self) -> Iterable[SnowflakeStoredProcedure]:
+        """List Snowflake stored procedures"""
         if self.source_config.includeStoredProcedures:
             results = self.engine.execute(
-                BIGQUERY_GET_STORED_PROCEDURES.format(
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                SNOWFLAKE_GET_STORED_PROCEDURES.format(
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 )
             ).all()
             for row in results:
-                stored_procedure = BigQueryStoredProcedure.parse_obj(dict(row))
+                stored_procedure = SnowflakeStoredProcedure.parse_obj(dict(row))
+                if stored_procedure.definition is None:
+                    logger.debug(
+                        f"Missing ownership permissions on procedure {stored_procedure.name}."
+                        " Trying to fetch description via DESCRIBE."
+                    )
+                    stored_procedure.definition = self.describe_procedure_definition(
+                        stored_procedure
+                    )
                 yield stored_procedure
 
+    def describe_procedure_definition(
+        self, stored_procedure: SnowflakeStoredProcedure
+    ) -> str:
+        """
+        We can only get the SP definition via the INFORMATION_SCHEMA.PROCEDURES if the
+        user has OWNERSHIP grants, which will not always be the case.
+
+        Then, if the procedure is created with `EXECUTE AS CALLER`, we can still try to
+        get the definition with a DESCRIBE.
+        """
+        res = self.engine.execute(
+            SNOWFLAKE_DESC_STORED_PROCEDURE.format(
+                database_name=self.context.get().database,
+                schema_name=self.context.get().database_schema,
+                procedure_name=stored_procedure.name,
+                procedure_signature=stored_procedure.unquote_signature(),
+            )
+        )
+        return dict(res.all()).get("body", "")
+
     def yield_stored_procedure(
-        self, stored_procedure: BigQueryStoredProcedure
+        self, stored_procedure: SnowflakeStoredProcedure
     ) -> Iterable[Either[CreateStoredProcedureRequest]]:
         """Prepare the stored procedure payload"""
 
         try:
             stored_procedure_request = CreateStoredProcedureRequest(
                 name=EntityName(__root__=stored_procedure.name),
+                description=stored_procedure.comment,
                 storedProcedureCode=StoredProcedureCode(
-                    language=STORED_PROC_LANGUAGE_MAP.get(
-                        stored_procedure.language or "SQL",
-                    ),
+                    language=STORED_PROC_LANGUAGE_MAP.get(stored_procedure.language),
                     code=stored_procedure.definition,
                 ),
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 ),
                 sourceUrl=SourceUrl(
-                    __root__=self.get_stored_procedure_url(
-                        database_name=self.context.database,
-                        schema_name=self.context.database_schema,
-                        # Follow the same building strategy as tables
-                        table_name=stored_procedure.name,
+                    __root__=self._get_source_url_root(
+                        database_name=self.context.get().database,
+                        schema_name=self.context.get().database_schema,
                     )
+                    + f"/procedure/{stored_procedure.name}"
+                    + f"{stored_procedure.signature if stored_procedure.signature else ''}"
                 ),
             )
             yield Either(right=stored_procedure_request)
             self.register_record_stored_proc_request(stored_procedure_request)
+
         except Exception as exc:
             yield Either(
                 left=StackTraceError(
                     name=stored_procedure.name,
                     error=f"Error yielding Stored Procedure [{stored_procedure.name}] due to [{exc}]",
                     stackTrace=traceback.format_exc(),
                 )
             )
 
     def get_stored_procedure_queries_dict(self) -> Dict[str, List[QueryByProcedure]]:
         """
-        Pick the stored procedure name from the context
-        and return the list of associated queries
+        Return the dictionary associating stored procedures to the
+        queries they triggered
         """
         start, _ = get_start_and_end(self.source_config.queryLogDuration)
-        query = BIGQUERY_GET_STORED_PROCEDURE_QUERIES.format(
+        query = SNOWFLAKE_GET_STORED_PROCEDURE_QUERIES.format(
             start_date=start,
-            region=self.service_connection.usageLocation,
         )
+
         queries_dict = self.procedure_queries_dict(
             query=query,
         )
 
         return queries_dict
+
+    def mark_tables_as_deleted(self):
+        """
+        Use the current inspector to mark tables as deleted
+        """
+        if self.incremental.enabled:
+            if not self.context.get().__dict__.get("database"):
+                raise ValueError(
+                    "No Database found in the context. We cannot run the table deletion."
+                )
+
+            if self.source_config.markDeletedTables:
+                logger.info(
+                    f"Mark Deleted Tables set to True. Processing database [{self.context.get().database}]"
+                )
+                yield from delete_entity_by_name(
+                    self.metadata,
+                    entity_type=Table,
+                    entity_names=self.context.get_global().deleted_tables,
+                    mark_deleted_entity=self.source_config.markDeletedTables,
+                )
+        else:
+            yield from super().mark_tables_as_deleted()
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/models.py`

 * *Files 19% similar despite different names*

```diff
@@ -4,33 +4,27 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""
-BigQuery models
-"""
+"""MSSQL models"""
 from typing import Optional
 
 from pydantic import BaseModel, Field
 
 from metadata.generated.schema.entity.data.storedProcedure import Language
-from metadata.utils.logger import ingestion_logger
-
-logger = ingestion_logger()
 
 STORED_PROC_LANGUAGE_MAP = {
     "SQL": Language.SQL,
-    "JAVASCRIPT": Language.JavaScript,
+    "EXTERNAL": Language.External,
 }
 
 
-class BigQueryStoredProcedure(BaseModel):
-    """BigQuery Stored Procedure list query results"""
+class MssqlStoredProcedure(BaseModel):
+    """MSSQL stored procedure list query results"""
 
-    name: str
-    definition: str
-    language: Optional[str] = Field(
-        None, description="Will only be informed for non-SQL routines."
-    )
+    name: str = Field(...)
+    owner: Optional[str] = Field(None)
+    language: str = Field(Language.SQL)
+    definition: str = Field(None)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/queries.py`

 * *Files 14% similar despite different names*

```diff
@@ -34,59 +34,60 @@
   AND query NOT LIKE '/* {{"app": "OpenMetadata", %%}} */%%'
   AND query NOT LIKE '/* {{"app": "dbt", %%}} */%%'
   LIMIT {result_limit}
 """
 )
 
 BIGQUERY_TEST_STATEMENT = textwrap.dedent(
-    """SELECT query FROM `region-{region}`.INFORMATION_SCHEMA.JOBS_BY_PROJECT 
+    """SELECT query FROM `region-{region}`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
     where creation_time > '{creation_date}' limit 1"""
 )
 
 
 BIGQUERY_SCHEMA_DESCRIPTION = textwrap.dedent(
     """
     SELECT option_value as schema_description FROM
-    `{project_id}`.`region-{region}`.INFORMATION_SCHEMA.SCHEMATA_OPTIONS 
-    where schema_name = '{schema_name}' and option_name = 'description' 
+    `{project_id}`.`region-{region}`.INFORMATION_SCHEMA.SCHEMATA_OPTIONS
+    where schema_name = '{schema_name}' and option_name = 'description'
     and option_value is not null
     """
 )
 
 BIGQUERY_TABLE_AND_TYPE = textwrap.dedent(
     """
     select table_name, table_type from `{project_id}`.{schema_name}.INFORMATION_SCHEMA.TABLES where table_type != 'VIEW'
     """
 )
 
 BIGQUERY_TABLE_CONSTRAINTS = textwrap.dedent(
     """
     SELECT * 
     FROM `{project_id}`.{schema_name}.INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE 
-    WHERE table_name = '{table_name}' AND constraint_name LIKE '%pk$';
+    WHERE constraint_name LIKE '%pk$';
     """
 )
 
 BIGQUERY_FOREIGN_CONSTRAINTS = textwrap.dedent(
     """
-    SELECT 
-      c.table_name AS referred_table, 
-      r.table_schema as referred_schema, 
-      r.constraint_name as name, 
+    SELECT
+      c.table_name AS referred_table,
+      r.table_schema as referred_schema,
+      r.constraint_name as name,
       c.column_name as referred_columns,
-      c.column_name as constrained_columns 
+      c.column_name as constrained_columns,
+      r.table_name as table_name
     FROM `{project_id}`.{schema_name}.INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE c 
     JOIN `{project_id}`.{schema_name}.INFORMATION_SCHEMA.TABLE_CONSTRAINTS r ON c.constraint_name = r.constraint_name 
-    WHERE r.constraint_type = 'FOREIGN KEY' AND r.table_name='{table_name}';
+    WHERE r.constraint_type = 'FOREIGN KEY';
     """
 )
 
 BIGQUERY_GET_STORED_PROCEDURES = textwrap.dedent(
     """
-SELECT 
+SELECT
   routine_name as name,
   routine_definition as definition,
   external_language as language
 FROM `{schema_name}`.INFORMATION_SCHEMA.ROUTINES
 WHERE routine_type in ('PROCEDURE', 'TABLE FUNCTION')
   AND routine_catalog = '{database_name}'
   AND routine_schema = '{schema_name}'
@@ -147,15 +148,27 @@
   AND Q.user_name = SP.user_name
 ORDER BY procedure_start_time DESC
 """
 )
 
 BIGQUERY_LIFE_CYCLE_QUERY = textwrap.dedent(
     """
-select 
+select
 table_name as table_name,
 creation_time as created_at
 from `{schema_name}`.INFORMATION_SCHEMA.TABLES
 where table_schema = '{schema_name}'
 and table_catalog = '{database_name}'
 """
 )
+
+BIGQUERY_GET_CHANGED_TABLES_FROM_CLOUD_LOGGING = """
+protoPayload.metadata.@type="type.googleapis.com/google.cloud.audit.BigQueryAuditMetadata"
+AND (
+    protoPayload.methodName = ("google.cloud.bigquery.v2.TableService.UpdateTable" OR "google.cloud.bigquery.v2.TableService.InsertTable" OR "google.cloud.bigquery.v2.TableService.PatchTable" OR "google.cloud.bigquery.v2.TableService.DeleteTable")
+    OR
+    (protoPayload.methodName = "google.cloud.bigquery.v2.JobService.InsertJob" AND (protoPayload.metadata.tableCreation:* OR protoPayload.metadata.tableChange:* OR protoPayload.metadata.tableDeletion:*))
+)
+AND resource.labels.project_id = "{project}"
+AND resource.labels.dataset_id = "{dataset}"
+AND timestamp >= "{start_date}"
+"""
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/query_parser.py`

 * *Files 1% similar despite different names*

```diff
@@ -41,15 +41,17 @@
 
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.project_id = self.set_project_id()
         self.database = self.project_id
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: BigQueryConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, BigQueryConnection):
             raise InvalidSourceException(
                 f"Expected BigQueryConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigquery/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigquery/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -67,15 +67,17 @@
         self.client: MultiProjectClient = self.connection_obj
 
         # ths instances and tables are cached to avoid making redundant requests to the API.
         self.instances: Dict[ProjectId, Dict[InstanceId, Instance]] = {}
         self.tables: Dict[ProjectId, Dict[InstanceId, Dict[TableId, Table]]] = {}
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: BigTableConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, BigTableConnection):
             raise InvalidSourceException(
                 f"Expected BigTableConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -92,15 +94,15 @@
     def get_database_names(self) -> Iterable[str]:
         return self.get_database_names_raw()
 
     def get_database_names_raw(self) -> Iterable[str]:
         yield from self.client.project_ids()
 
     def get_schema_name_list(self) -> List[str]:
-        project_id = self.context.database
+        project_id = self.context.get().database
         try:
             # the first element is a list of instances
             # the second element is another collection (seems empty) and I do not know what is its purpose
             instances, _ = self.client.list_instances(project_id=project_id)
             self.instances[project_id] = {
                 instance.instance_id: instance for instance in instances
             }
@@ -109,15 +111,15 @@
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Failed to list BigTable instances in project {project_id}: {err}"
             )
             raise
 
     def get_table_name_list(self, schema_name: str) -> List[str]:
-        project_id = self.context.database
+        project_id = self.context.get().database
         try:
             instance = self._get_instance(project_id, schema_name)
             if instance is None:
                 raise RuntimeError(f"Instance {project_id}/{schema_name} not found.")
             tables = instance.list_tables()
             for table in tables:
                 self._set_nested(
@@ -142,15 +144,15 @@
                 constraintType=ConstraintType.PRIMARY_KEY, columns=["row_key"]
             )
         ]
 
     def get_table_columns_dict(
         self, schema_name: str, table_name: str
     ) -> Union[List[Dict], Dict]:
-        project_id = self.context.database
+        project_id = self.context.get().database
         try:
             table = self._get_table(project_id, schema_name, table_name)
             if table is None:
                 raise RuntimeError(
                     f"Table {project_id}/{schema_name}/{table_name} not found."
                 )
             column_families = table.list_column_families()
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/bigtable/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/bigtable/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -100,15 +100,17 @@
 class ClickhouseSource(CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Clickhouse Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: ClickhouseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, ClickhouseConnection):
             raise InvalidSourceException(
                 f"Expected ClickhouseConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/query_parser.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 Clickhouse usage module
 """
 
 import ast
 import traceback
 from abc import ABC
 from datetime import datetime
-from typing import List
+from typing import List, Optional
 
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.services.connections.database.clickhouseConnection import (
     ClickhouseConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
@@ -35,15 +35,17 @@
 
 class ClickhouseQueryParserSource(QueryParserSource, ABC):
     """
     Clickhouse base for Usage and Lineage
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: ClickhouseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, ClickhouseConnection):
             raise InvalidSourceException(
                 f"Expected ClickhouseConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/clickhouse/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/clickhouse/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/column_helpers.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/column_helpers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/column_type_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/column_type_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/common_db_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/common_db_source.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #  limitations under the License.
 """
 Generic source to build SQL connectors.
 """
 import traceback
 from abc import ABC
 from copy import deepcopy
-from typing import Any, Iterable, List, Optional, Tuple, Union
+from typing import Any, Iterable, List, Optional, Tuple, Union, cast
 
 from pydantic import BaseModel
 from sqlalchemy.engine import Connection
 from sqlalchemy.engine.base import Engine
 from sqlalchemy.engine.reflection import Inspector
 from sqlalchemy.inspection import inspect
 
@@ -47,28 +47,32 @@
 from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
     DatabaseServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.models import Either
-from metadata.ingestion.lineage.sql_lineage import get_column_fqn
+from metadata.ingestion.connections.session import create_and_bind_thread_safe_session
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.database_service import DatabaseServiceSource
 from metadata.ingestion.source.database.sql_column_handler import SqlColumnHandlerMixin
 from metadata.ingestion.source.database.sqlalchemy_source import SqlAlchemySource
 from metadata.ingestion.source.database.stored_procedures_mixin import QueryByProcedure
 from metadata.ingestion.source.models import TableView
 from metadata.utils import fqn
 from metadata.utils.db_utils import get_view_lineage
-from metadata.utils.execution_time_tracker import calculate_execution_time_generator
+from metadata.utils.execution_time_tracker import (
+    calculate_execution_time,
+    calculate_execution_time_generator,
+)
 from metadata.utils.filters import filter_by_table
 from metadata.utils.logger import ingestion_logger
+from metadata.utils.ssl_manager import SSLManager, check_ssl_and_init
 
 logger = ingestion_logger()
 
 
 class TableNameAndType(BaseModel):
     """
     Helper model for passing down
@@ -98,40 +102,51 @@
         )
 
         self.metadata = metadata
 
         # It will be one of the Unions. We don't know the specific type here.
         self.service_connection = self.config.serviceConnection.__root__.config
 
+        self.ssl_manager = None
+        self.ssl_manager: SSLManager = check_ssl_and_init(self.service_connection)
+        if self.ssl_manager:
+            self.service_connection = self.ssl_manager.setup_ssl(
+                self.service_connection
+            )
+
         self.engine: Engine = get_connection(self.service_connection)
+        self.session = create_and_bind_thread_safe_session(self.engine)
 
         # Flag the connection for the test connection
         self.connection_obj = self.engine
         self.test_connection()
 
-        self._connection = None  # Lazy init as well
+        self._connection_map = {}  # Lazy init as well
+        self._inspector_map = {}
         self.table_constraints = None
         self.database_source_state = set()
-        self.context.table_views = []
-        self.context.table_constrains = []
+        self.context.get_global().table_views = []
+        self.context.get_global().table_constrains = []
+        self.context.set_threads(self.source_config.threads)
         super().__init__()
 
     def set_inspector(self, database_name: str) -> None:
         """
         When sources override `get_database_names`, they will need
         to setup multiple inspectors. They can use this function.
         :param database_name: new database to set
         """
         logger.info(f"Ingesting from database: {database_name}")
 
         new_service_connection = deepcopy(self.service_connection)
         new_service_connection.database = database_name
         self.engine = get_connection(new_service_connection)
-        self.inspector = inspect(self.engine)
-        self._connection = None  # Lazy init as well
+
+        self._connection_map = {}  # Lazy init as well
+        self._inspector_map = {}
 
     def get_database_names(self) -> Iterable[str]:
         """
         Default case with a single database.
 
         It might come informed - or not - from the source.
 
@@ -140,42 +155,41 @@
         """
         custom_database_name = self.service_connection.__dict__.get("databaseName")
 
         database_name = self.service_connection.__dict__.get(
             "database", custom_database_name or "default"
         )
 
-        # By default, set the inspector on the created engine
-        self.inspector = inspect(self.engine)
         yield database_name
 
     def get_database_description(self, database_name: str) -> Optional[str]:
         """
         Method to fetch the database description
         by default there will be no database description
         """
 
     def get_schema_description(self, schema_name: str) -> Optional[str]:
         """
         Method to fetch the schema description
         by default there will be no schema description
         """
 
+    @calculate_execution_time_generator()
     def yield_database(
         self, database_name: str
     ) -> Iterable[Either[CreateDatabaseRequest]]:
         """
         From topology.
         Prepare a database request and pass it to the sink
         """
 
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
                 description=self.get_database_description(database_name),
                 sourceUrl=self.get_source_url(database_name=database_name),
                 tags=self.get_database_tag_labels(database_name=database_name),
             )
         )
 
     def get_raw_database_schema_names(self) -> Iterable[str]:
@@ -187,41 +201,43 @@
 
     def get_database_schema_names(self) -> Iterable[str]:
         """
         return schema names
         """
         yield from self._get_filtered_schema_names()
 
+    @calculate_execution_time_generator()
     def yield_database_schema(
         self, schema_name: str
     ) -> Iterable[Either[CreateDatabaseSchemaRequest]]:
         """
         From topology.
         Prepare a database schema request and pass it to the sink
         """
 
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name,
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                 ),
                 description=self.get_schema_description(schema_name),
                 sourceUrl=self.get_source_url(
-                    database_name=self.context.database,
+                    database_name=self.context.get().database,
                     schema_name=schema_name,
                 ),
                 tags=self.get_schema_tag_labels(schema_name=schema_name),
             )
         )
 
     @staticmethod
+    @calculate_execution_time()
     def get_table_description(
         schema_name: str, table_name: str, inspector: Inspector
     ) -> str:
         description = None
         try:
             table_info: dict = inspector.get_table_comment(table_name, schema_name)
         # Catch any exception without breaking the ingestion
@@ -273,35 +289,37 @@
         Handle table and views.
 
         Fetches them up using the context information and
         the inspector set when preparing the db.
 
         :return: tables or views, depending on config
         """
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
         try:
             if self.source_config.includeTables:
                 for table_and_type in self.query_table_names_and_types(schema_name):
                     table_name = self.standardize_table_name(
                         schema_name, table_and_type.name
                     )
                     table_fqn = fqn.build(
                         self.metadata,
                         entity_type=Table,
-                        service_name=self.context.database_service,
-                        database_name=self.context.database,
-                        schema_name=self.context.database_schema,
+                        service_name=self.context.get().database_service,
+                        database_name=self.context.get().database,
+                        schema_name=self.context.get().database_schema,
                         table_name=table_name,
                         skip_es_search=True,
                     )
                     if filter_by_table(
                         self.source_config.tableFilterPattern,
-                        table_fqn
-                        if self.source_config.useFqnForFiltering
-                        else table_name,
+                        (
+                            table_fqn
+                            if self.source_config.useFqnForFiltering
+                            else table_name
+                        ),
                     ):
                         self.status.filter(
                             table_fqn,
                             "Table Filtered Out",
                         )
                         continue
                     yield table_name, table_and_type.type_
@@ -310,38 +328,41 @@
                 for view_and_type in self.query_view_names_and_types(schema_name):
                     view_name = self.standardize_table_name(
                         schema_name, view_and_type.name
                     )
                     view_fqn = fqn.build(
                         self.metadata,
                         entity_type=Table,
-                        service_name=self.context.database_service,
-                        database_name=self.context.database,
-                        schema_name=self.context.database_schema,
+                        service_name=self.context.get().database_service,
+                        database_name=self.context.get().database,
+                        schema_name=self.context.get().database_schema,
                         table_name=view_name,
                     )
 
                     if filter_by_table(
                         self.source_config.tableFilterPattern,
-                        view_fqn
-                        if self.source_config.useFqnForFiltering
-                        else view_name,
+                        (
+                            view_fqn
+                            if self.source_config.useFqnForFiltering
+                            else view_name
+                        ),
                     ):
                         self.status.filter(
                             view_fqn,
                             "Table Filtered Out",
                         )
                         continue
                     yield view_name, view_and_type.type_
         except Exception as err:
             logger.warning(
                 f"Fetching tables names failed for schema {schema_name} due to - {err}"
             )
             logger.debug(traceback.format_exc())
 
+    @calculate_execution_time()
     def get_view_definition(
         self, table_type: str, table_name: str, schema_name: str, inspector: Inspector
     ) -> Optional[str]:
         if table_type in (TableType.View, TableType.MaterializedView):
             try:
                 view_definition = inspector.get_view_definition(table_name, schema_name)
                 view_definition = (
@@ -395,39 +416,40 @@
         self, stored_procedure: Any
     ) -> Iterable[Either[CreateStoredProcedureRequest]]:
         """Not implemented"""
 
     def get_stored_procedure_queries(self) -> Iterable[QueryByProcedure]:
         """Not Implemented"""
 
+    @calculate_execution_time_generator()
     def yield_procedure_lineage_and_queries(
         self,
     ) -> Iterable[Either[Union[AddLineageRequest, CreateQueryRequest]]]:
         """Not Implemented"""
         yield from []
 
-    @calculate_execution_time_generator(store=False)
+    @calculate_execution_time_generator()
     def yield_table(
         self, table_name_and_type: Tuple[str, str]
     ) -> Iterable[Either[CreateTableRequest]]:
         """
         From topology.
         Prepare a table request and pass it to the sink
         """
         table_name, table_type = table_name_and_type
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
         try:
             (
                 columns,
                 table_constraints,
                 foreign_columns,
             ) = self.get_columns_and_constraints(
                 schema_name=schema_name,
                 table_name=table_name,
-                db_name=self.context.database,
+                db_name=self.context.get().database,
                 inspector=self.inspector,
             )
 
             view_definition = self.get_view_definition(
                 table_type=table_type,
                 table_name=table_name,
                 schema_name=schema_name,
@@ -446,25 +468,25 @@
                 ),
                 columns=columns,
                 tableConstraints=table_constraints,
                 viewDefinition=view_definition,
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                     schema_name=schema_name,
                 ),
                 tags=self.get_tag_labels(
                     table_name=table_name
                 ),  # Pick tags from context info, if any
                 sourceUrl=self.get_source_url(
                     table_name=table_name,
                     schema_name=schema_name,
-                    database_name=self.context.database,
+                    database_name=self.context.get().database,
                     table_type=table_type,
                 ),
                 owner=self.get_owner_ref(table_name=table_name),
             )
 
             is_partitioned, partition_details = self.get_table_partition_details(
                 table_name=table_name, schema_name=schema_name, inspector=self.inspector
@@ -480,61 +502,68 @@
 
             # Flag view as visited
             if table_type == TableType.View or view_definition:
                 table_view = TableView.parse_obj(
                     {
                         "table_name": table_name,
                         "schema_name": schema_name,
-                        "db_name": self.context.database,
+                        "db_name": self.context.get().database,
                         "view_definition": view_definition,
                     }
                 )
-                self.context.table_views.append(table_view)
+                self.context.get_global().table_views.append(table_view)
 
         except Exception as exc:
             error = f"Unexpected exception to yield table [{table_name}]: {exc}"
             yield Either(
                 left=StackTraceError(
                     name=table_name, error=error, stackTrace=traceback.format_exc()
                 )
             )
 
+    @calculate_execution_time_generator()
     def yield_view_lineage(self) -> Iterable[Either[AddLineageRequest]]:
         logger.info("Processing Lineage for Views")
         for view in [
-            v for v in self.context.table_views if v.view_definition is not None
+            v for v in self.context.get().table_views if v.view_definition is not None
         ]:
             yield from get_view_lineage(
                 view=view,
                 metadata=self.metadata,
-                service_name=self.context.database_service,
+                service_name=self.context.get().database_service,
                 connection_type=self.service_connection.type.value,
                 timeout_seconds=self.source_config.queryParsingTimeoutLimit,
             )
 
     def _get_foreign_constraints(self, foreign_columns) -> List[TableConstraint]:
         """
         Search the referred table for foreign constraints
         and get referred column fqn
         """
+        supports_database = hasattr(self.service_connection, "supportsDatabase")
 
         foreign_constraints = []
         for column in foreign_columns:
             referred_column_fqns = []
-            referred_table = fqn.search_table_from_es(
+            if supports_database:
+                database_name = column.get("referred_database")
+            else:
+                database_name = self.context.get().database
+            referred_table_fqn = fqn.build(
                 metadata=self.metadata,
+                entity_type=Table,
                 table_name=column.get("referred_table"),
                 schema_name=column.get("referred_schema"),
-                database_name=None,
-                service_name=self.context.database_service,
+                database_name=database_name,
+                service_name=self.context.get().database_service,
             )
-            if referred_table:
+            if referred_table_fqn:
                 for referred_column in column.get("referred_columns"):
-                    col_fqn = get_column_fqn(
-                        table_entity=referred_table, column=referred_column
+                    col_fqn = fqn._build(
+                        referred_table_fqn, referred_column, quote=False
                     )
                     if col_fqn:
                         referred_column_fqns.append(col_fqn)
             else:
                 # do not build partial foreign constraint. It will updated in next run.
                 continue
             foreign_constraints.append(
@@ -543,14 +572,15 @@
                     columns=column.get("constrained_columns"),
                     referredColumns=referred_column_fqns,
                 )
             )
 
         return foreign_constraints
 
+    @calculate_execution_time()
     def update_table_constraints(
         self, table_constraints, foreign_columns
     ) -> List[TableConstraint]:
         """
         From topology.
         process the table constraints of all tables
         """
@@ -563,22 +593,38 @@
         return table_constraints
 
     @property
     def connection(self) -> Connection:
         """
         Return the SQLAlchemy connection
         """
-        if not self._connection:
-            self._connection = self.engine.connect()
+        thread_id = self.context.get_current_thread_id()
+
+        if not self._connection_map.get(thread_id):
+            self._connection_map[thread_id] = self.engine.connect()
+
+        return self._connection_map[thread_id]
+
+    @property
+    def inspector(self) -> Inspector:
+        thread_id = self.context.get_current_thread_id()
+
+        if not self._inspector_map.get(thread_id):
+            self._inspector_map[thread_id] = inspect(self.connection)
 
-        return self._connection
+        return self._inspector_map[thread_id]
 
     def close(self):
         if self.connection is not None:
             self.connection.close()
+        for connection in self._connection_map.values():
+            connection.close()
+        if hasattr(self, "ssl_manager") and self.ssl_manager:
+            self.ssl_manager = cast(SSLManager, self.ssl_manager)
+            self.ssl_manager.cleanup_temp_files()
         self.engine.dispose()
 
     def fetch_table_tags(
         self,
         table_name: str,
         schema_name: str,
         inspector: Inspector,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/common_nosql_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/common_nosql_source.py`

 * *Files 8% similar despite different names*

```diff
@@ -100,15 +100,15 @@
         From topology.
         Prepare a database request and pass it to the sink
         """
 
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
                 sourceUrl=self.get_source_url(database_name=database_name),
             )
         )
 
     @abstractmethod
     def get_schema_name_list(self) -> List[str]:
         """
@@ -117,16 +117,16 @@
         """
 
     def get_database_schema_names(self) -> Iterable[str]:
         for schema in self.get_schema_name_list():
             schema_fqn = fqn.build(
                 self.metadata,
                 entity_type=DatabaseSchema,
-                service_name=self.context.database_service,
-                database_name=self.context.database,
+                service_name=self.context.get().database_service,
+                database_name=self.context.get().database,
                 schema_name=schema,
             )
 
             if filter_by_schema(
                 self.source_config.schemaFilterPattern,
                 schema_fqn if self.source_config.useFqnForFiltering else schema,
             ):
@@ -145,19 +145,19 @@
 
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name,
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                 ),
                 sourceUrl=self.get_source_url(
-                    database_name=self.context.database,
+                    database_name=self.context.get().database,
                     schema_name=schema_name,
                 ),
             )
         )
 
     @abstractmethod
     def get_table_name_list(self, schema_name: str) -> List[str]:
@@ -171,24 +171,24 @@
         Handle table and views.
 
         Fetches them up using the context information and
         the inspector set when preparing the db.
 
         :return: tables or views, depending on config
         """
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
         if self.source_config.includeTables:
             for collection in self.get_table_name_list(schema_name):
                 table_name = collection
                 table_fqn = fqn.build(
                     self.metadata,
                     entity_type=Table,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                     table_name=table_name,
                 )
                 if filter_by_table(
                     self.source_config.tableFilterPattern,
                     table_fqn if self.source_config.useFqnForFiltering else table_name,
                 ):
                     self.status.filter(
@@ -222,38 +222,38 @@
         """
         From topology.
         Prepare a table request and pass it to the sink
         """
         import pandas as pd  # pylint: disable=import-outside-toplevel
 
         table_name, table_type = table_name_and_type
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
         try:
             data = self.get_table_columns_dict(schema_name, table_name)
             df = pd.DataFrame.from_records(list(data))
             column_parser = DataFrameColumnParser.create(df)
             columns = column_parser.get_columns()
             table_request = CreateTableRequest(
                 name=table_name,
                 tableType=table_type,
                 columns=columns,
                 tableConstraints=self.get_table_constraints(
                     schema_name=schema_name,
                     table_name=table_name,
-                    db_name=self.context.database,
+                    db_name=self.context.get().database,
                 ),
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                     schema_name=schema_name,
                 ),
                 sourceUrl=self.get_source_url(
-                    database_name=self.context.database,
+                    database_name=self.context.get().database,
                     schema_name=schema_name,
                     table_name=table_name,
                     table_type=table_type,
                 ),
             )
 
             yield Either(right=table_request)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/couchbase/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/couchbase/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/couchbase/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/couchbase/metadata.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Couchbase source methods.
 """
 
 import traceback
-from typing import Dict, Iterable, List
+from typing import Dict, Iterable, List, Optional
 
 from metadata.generated.schema.entity.services.connections.database.couchbaseConnection import (
     CouchbaseConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -43,15 +43,17 @@
     """
 
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.couchbase = self.connection_obj
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: CouchbaseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, CouchbaseConnection):
             raise InvalidSourceException(
                 f"Expected CouchbaseConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -70,18 +72,18 @@
 
     def get_schema_name_list(self) -> List[str]:
         """
         Method to get list of schema names available within NoSQL db
         need to be overridden by sources
         """
         try:
-            database_name = self.context.database
+            database_name = self.context.get().database
             bucket = self.couchbase.bucket(database_name)
             collection_manager = bucket.collections()
-            self.context.scope_dict = {
+            self.context.get().scope_dict = {
                 scope.name: scope for scope in collection_manager.get_all_scopes()
             }
             return [scopes.name for scopes in collection_manager.get_all_scopes()]
         except Exception as exp:
             logger.debug(
                 f"Failed to list scope for bucket names [{database_name}]: {exp}"
             )
@@ -89,30 +91,30 @@
         return []
 
     def get_table_name_list(self, schema_name: str) -> List[str]:
         """
         Method to get list of table names available within schema db
         """
         try:
-            scope_object = self.context.scope_dict.get(schema_name)
+            scope_object = self.context.get().scope_dict.get(schema_name)
             return [collection.name for collection in scope_object.collections]
         except Exception as exp:
             logger.debug(
                 f"Failed to list collection names for scope [{schema_name}]: {exp}"
             )
             logger.debug(traceback.format_exc())
         return []
 
     def get_table_columns_dict(self, schema_name: str, table_name: str) -> List[Dict]:
         """
         Method to get actual data available within table
         need to be overridden by sources
         """
         try:
-            database_name = self.context.database
+            database_name = self.context.get().database
             query = COUCHBASE_SQL_STATEMENT.format(table_name=table_name)
             result = self.couchbase.query(query)
             for row in result.rows():
                 if len(row) > 0:
                     query_coln = COUCHBASE_GET_DATA.format(
                         database_name=database_name,
                         schema_name=schema_name,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/couchbase/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/couchbase/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/database_service.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/database_service.py`

 * *Files 2% similar despite different names*

```diff
@@ -58,19 +58,20 @@
 from metadata.ingestion.api.steps import Source
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.life_cycle import OMetaLifeCycleData
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
-    TopologyContext,
+    TopologyContextManager,
     TopologyNode,
 )
 from metadata.ingestion.source.connections import get_test_connection_fn
 from metadata.utils import fqn
+from metadata.utils.execution_time_tracker import calculate_execution_time
 from metadata.utils.filters import filter_by_schema
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.tag_utils import get_tag_label
 
 logger = ingestion_logger()
 
 
@@ -104,15 +105,19 @@
                 cache_entities=True,
             ),
         ],
         children=["database"],
         # Note how we have `yield_view_lineage` and `yield_stored_procedure_lineage`
         # as post_processed. This is because we cannot ensure proper lineage processing
         # until we have finished ingesting all the metadata from the source.
-        post_process=["yield_view_lineage", "yield_procedure_lineage_and_queries"],
+        post_process=[
+            "yield_view_lineage",
+            "yield_procedure_lineage_and_queries",
+            "yield_external_table_lineage",
+        ],
     )
     database = TopologyNode(
         producer="get_database_names",
         stages=[
             NodeStage(
                 type_=OMetaTagAndClassification,
                 context="tags",
@@ -148,14 +153,15 @@
                 consumer=["database_service", "database"],
                 cache_entities=True,
                 use_cache=True,
             ),
         ],
         children=["table", "stored_procedure"],
         post_process=["mark_tables_as_deleted", "mark_stored_procedures_as_deleted"],
+        threads=True,
     )
     table = TopologyNode(
         producer="get_tables_name_and_type",
         stages=[
             NodeStage(
                 type_=OMetaTagAndClassification,
                 context="tags",
@@ -208,15 +214,15 @@
     # Big union of types we want to fetch dynamically
     service_connection: DatabaseConnection.__fields__["config"].type_
 
     # When processing the database, the source will update the inspector if needed
     inspector: Inspector
 
     topology = DatabaseServiceTopology()
-    context = TopologyContext.create(topology)
+    context = TopologyContextManager(topology)
 
     @property
     def name(self) -> str:
         return self.service_connection.type.name
 
     def prepare(self):
         """By default, there is no preparation needed"""
@@ -376,15 +382,15 @@
     def get_tag_by_fqn(self, entity_fqn: str) -> Optional[List[TagLabel]]:
         """
         Pick up the tags registered in the context
         searching by entity FQN
         """
 
         tag_labels = []
-        for tag_and_category in self.context.tags or []:
+        for tag_and_category in self.context.get().tags or []:
             if tag_and_category.fqn and tag_and_category.fqn.__root__ == entity_fqn:
                 tag_label = get_tag_label(
                     metadata=self.metadata,
                     tag_name=tag_and_category.tag_request.name.__root__,
                     classification_name=tag_and_category.classification_request.name.__root__,
                 )
                 if tag_label:
@@ -396,45 +402,46 @@
         Method to get schema tags
         This will only get executed if the tags context
         is properly informed
         """
         database_fqn = fqn.build(
             self.metadata,
             entity_type=Database,
-            service_name=self.context.database_service,
+            service_name=self.context.get().database_service,
             database_name=database_name,
         )
         return self.get_tag_by_fqn(entity_fqn=database_fqn)
 
     def get_schema_tag_labels(self, schema_name: str) -> Optional[List[TagLabel]]:
         """
         Method to get schema tags
         This will only get executed if the tags context
         is properly informed
         """
         schema_fqn = fqn.build(
             self.metadata,
             entity_type=DatabaseSchema,
-            service_name=self.context.database_service,
-            database_name=self.context.database,
+            service_name=self.context.get().database_service,
+            database_name=self.context.get().database,
             schema_name=schema_name,
         )
         return self.get_tag_by_fqn(entity_fqn=schema_fqn)
 
+    @calculate_execution_time()
     def get_tag_labels(self, table_name: str) -> Optional[List[TagLabel]]:
         """
         This will only get executed if the tags context
         is properly informed
         """
         table_fqn = fqn.build(
             self.metadata,
             entity_type=Table,
-            service_name=self.context.database_service,
-            database_name=self.context.database,
-            schema_name=self.context.database_schema,
+            service_name=self.context.get().database_service,
+            database_name=self.context.get().database,
+            schema_name=self.context.get().database_schema,
             table_name=table_name,
             skip_es_search=True,
         )
         return self.get_tag_by_fqn(entity_fqn=table_fqn)
 
     def get_column_tag_labels(
         self, table_name: str, column: dict
@@ -442,32 +449,33 @@
         """
         This will only get executed if the tags context
         is properly informed
         """
         col_fqn = fqn.build(
             self.metadata,
             entity_type=Column,
-            service_name=self.context.database_service,
-            database_name=self.context.database,
-            schema_name=self.context.database_schema,
+            service_name=self.context.get().database_service,
+            database_name=self.context.get().database,
+            schema_name=self.context.get().database_schema,
             table_name=table_name,
             column_name=column["name"],
         )
         return self.get_tag_by_fqn(entity_fqn=col_fqn)
 
+    @calculate_execution_time()
     def register_record(self, table_request: CreateTableRequest) -> None:
         """
         Mark the table record as scanned and update the database_source_state
         """
         table_fqn = fqn.build(
             self.metadata,
             entity_type=Table,
-            service_name=self.context.database_service,
-            database_name=self.context.database,
-            schema_name=self.context.database_schema,
+            service_name=self.context.get().database_service,
+            database_name=self.context.get().database,
+            schema_name=self.context.get().database_schema,
             table_name=table_request.name.__root__,
             skip_es_search=True,
         )
 
         self.database_source_state.add(table_fqn)
 
     def register_record_stored_proc_request(
@@ -475,72 +483,73 @@
     ) -> None:
         """
         Mark the table record as scanned and update the database_source_state
         """
         table_fqn = fqn.build(
             self.metadata,
             entity_type=StoredProcedure,
-            service_name=self.context.database_service,
-            database_name=self.context.database,
-            schema_name=self.context.database_schema,
+            service_name=self.context.get().database_service,
+            database_name=self.context.get().database,
+            schema_name=self.context.get().database_schema,
             procedure_name=stored_proc_request.name.__root__,
         )
 
         self.stored_procedure_source_state.add(table_fqn)
 
     def _get_filtered_schema_names(
         self, return_fqn: bool = False, add_to_status: bool = True
     ) -> Iterable[str]:
         for schema_name in self.get_raw_database_schema_names():
             schema_fqn = fqn.build(
                 self.metadata,
                 entity_type=DatabaseSchema,
-                service_name=self.context.database_service,
-                database_name=self.context.database,
+                service_name=self.context.get().database_service,
+                database_name=self.context.get().database,
                 schema_name=schema_name,
             )
             if filter_by_schema(
                 self.source_config.schemaFilterPattern,
                 schema_fqn if self.source_config.useFqnForFiltering else schema_name,
             ):
                 if add_to_status:
                     self.status.filter(schema_fqn, "Schema Filtered Out")
                 continue
             yield schema_fqn if return_fqn else schema_name
 
+    @calculate_execution_time()
     def get_owner_ref(self, table_name: str) -> Optional[EntityReference]:
         """
         Method to process the table owners
         """
         try:
             if self.source_config.includeOwners:
                 owner_name = self.inspector.get_table_owner(
-                    connection=self.connection,  # pylint: disable=no-member
+                    connection=self.connection,  # pylint: disable=no-member.fetchall()
                     table_name=table_name,
-                    schema=self.context.database_schema,
+                    schema=self.context.get().database_schema,
                 )
                 owner_ref = self.metadata.get_reference_by_name(name=owner_name)
                 return owner_ref
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Error processing owner for table {table_name}: {exc}")
         return None
 
     def mark_tables_as_deleted(self):
         """
         Use the current inspector to mark tables as deleted
         """
-        if not self.context.__dict__.get("database"):
+        if not self.context.get().__dict__.get("database"):
             raise ValueError(
                 "No Database found in the context. We cannot run the table deletion."
             )
 
         if self.source_config.markDeletedTables:
             logger.info(
-                f"Mark Deleted Tables set to True. Processing database [{self.context.database}]"
+                f"Mark Deleted Tables set to True. Processing database [{self.context.get().database}]"
             )
             schema_fqn_list = self._get_filtered_schema_names(
                 return_fqn=True, add_to_status=False
             )
 
             for schema_fqn in schema_fqn_list:
                 yield from delete_entity_from_source(
@@ -553,15 +562,15 @@
 
     def mark_stored_procedures_as_deleted(self):
         """
         Use the current inspector to mark Stored Procedures as deleted
         """
         if self.source_config.markDeletedStoredProcedures:
             logger.info(
-                f"Mark Deleted Stored Procedures Processing database [{self.context.database}]"
+                f"Mark Deleted Stored Procedures Processing database [{self.context.get().database}]"
             )
 
             schema_fqn_list = self._get_filtered_schema_names(
                 return_fqn=True, add_to_status=False
             )
 
             for schema_fqn in schema_fqn_list:
@@ -574,10 +583,15 @@
                 )
 
     def yield_life_cycle_data(self, _) -> Iterable[Either[OMetaLifeCycleData]]:
         """
         Get the life cycle data of the table
         """
 
+    def yield_external_table_lineage(self) -> Iterable[Either[AddLineageRequest]]:
+        """
+        Process external table lineage
+        """
+
     def test_connection(self) -> None:
         test_connection_fn = get_test_connection_fn(self.service_connection)
         test_connection_fn(self.metadata, self.connection_obj, self.service_connection)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/metadata.py`

 * *Files 6% similar despite different names*

```diff
@@ -14,16 +14,16 @@
 import traceback
 from copy import deepcopy
 from typing import Iterable, Optional, Tuple, Union
 
 from pyhive.sqlalchemy_hive import _type_map
 from sqlalchemy import types, util
 from sqlalchemy.engine import reflection
+from sqlalchemy.engine.reflection import Inspector
 from sqlalchemy.exc import DatabaseError
-from sqlalchemy.inspection import inspect
 from sqlalchemy.sql.sqltypes import String
 from sqlalchemy_databricks._dialect import DatabricksDialect
 
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.table import Column, Table, TableType
 from metadata.generated.schema.entity.services.connections.database.databricksConnection import (
@@ -47,14 +47,17 @@
     DATABRICKS_GET_CATALOGS_TAGS,
     DATABRICKS_GET_COLUMN_TAGS,
     DATABRICKS_GET_SCHEMA_TAGS,
     DATABRICKS_GET_TABLE_COMMENTS,
     DATABRICKS_GET_TABLE_TAGS,
     DATABRICKS_VIEW_DEFINITIONS,
 )
+from metadata.ingestion.source.database.external_table_lineage_mixin import (
+    ExternalTableLineageMixin,
+)
 from metadata.ingestion.source.database.multi_db_source import MultiDBSource
 from metadata.utils import fqn
 from metadata.utils.constants import DEFAULT_DATABASE
 from metadata.utils.filters import filter_by_database
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.sqlalchemy_utils import (
     get_all_view_definitions,
@@ -254,40 +257,43 @@
 DatabricksDialect.get_columns = get_columns
 DatabricksDialect.get_schema_names = get_schema_names
 DatabricksDialect.get_view_definition = get_view_definition
 DatabricksDialect.get_all_view_definitions = get_all_view_definitions
 reflection.Inspector.get_schema_names = get_schema_names_reflection
 
 
-class DatabricksSource(CommonDbSourceService, MultiDBSource):
+class DatabricksSource(ExternalTableLineageMixin, CommonDbSourceService, MultiDBSource):
     """
     Implements the necessary methods to extract
     Database metadata from Databricks Source using
     the legacy hive metastore method
     """
 
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.is_older_version = False
         self._init_version()
         self.catalog_tags = {}
         self.schema_tags = {}
         self.table_tags = {}
+        self.external_location_map = {}
         self.column_tags = {}
 
     def _init_version(self):
         try:
             self.connection.execute(DATABRICKS_GET_CATALOGS).fetchone()
             self.is_older_version = False
         except DatabaseError as soe:
             logger.debug(f"Failed to fetch catalogs due to: {soe}")
             self.is_older_version = True
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: DatabricksConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DatabricksConnection):
             raise InvalidSourceException(
                 f"Expected DatabricksConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -299,16 +305,17 @@
         :param database_name: new database to set
         """
         logger.info(f"Ingesting from catalog: {database_name}")
 
         new_service_connection = deepcopy(self.service_connection)
         new_service_connection.catalog = database_name
         self.engine = get_connection(new_service_connection)
-        self.inspector = inspect(self.engine)
-        self._connection = None  # Lazy init as well
+
+        self._connection_map = {}  # Lazy init as well
+        self._inspector_map = {}
 
     def get_configured_database(self) -> Optional[str]:
         return self.service_connection.catalog
 
     def get_database_names_raw(self) -> Iterable[str]:
         if not self.is_older_version:
             results = self.connection.execute(DATABRICKS_GET_CATALOGS)
@@ -418,15 +425,15 @@
             self.populate_tags_cache(database_name=configured_catalog)
             yield configured_catalog
         else:
             for new_catalog in self.get_database_names_raw():
                 database_fqn = fqn.build(
                     self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
+                    service_name=self.context.get().database_service,
                     database_name=new_catalog,
                 )
                 if filter_by_database(
                     self.source_config.databaseFilterPattern,
                     database_fqn
                     if self.source_config.useFqnForFiltering
                     else new_catalog,
@@ -444,15 +451,15 @@
                     )
 
     def get_raw_database_schema_names(self) -> Iterable[str]:
         if self.service_connection.__dict__.get("databaseSchema"):
             yield self.service_connection.databaseSchema
         else:
             for schema_name in self.inspector.get_schema_names(
-                database=self.context.database,
+                database=self.context.get().database,
                 is_old_version=self.is_older_version,
             ):
                 yield schema_name
 
     def yield_database_tag(
         self, database_name: str
     ) -> Iterable[Either[OMetaTagAndClassification]]:
@@ -462,15 +469,15 @@
         try:
             catalog_tags = self.catalog_tags.get(database_name, [])
             for tag_name, tag_value in catalog_tags:
                 yield from get_ometa_tag_and_classification(
                     tag_fqn=fqn.build(
                         self.metadata,
                         Database,
-                        service_name=self.context.database_service,
+                        service_name=self.context.get().database_service,
                         database_name=database_name,
                     ),
                     tags=[tag_value],
                     classification_name=tag_name,
                     tag_description=DATABRICKS_TAG,
                     classification_description=DATABRICKS_TAG_CLASSIFICATION,
                 )
@@ -487,22 +494,24 @@
     def yield_tag(
         self, schema_name: str
     ) -> Iterable[Either[OMetaTagAndClassification]]:
         """
         Method to yield schema tags
         """
         try:
-            schema_tags = self.schema_tags.get((self.context.database, schema_name), [])
+            schema_tags = self.schema_tags.get(
+                (self.context.get().database, schema_name), []
+            )
             for tag_name, tag_value in schema_tags:
                 yield from get_ometa_tag_and_classification(
                     tag_fqn=fqn.build(
                         self.metadata,
                         DatabaseSchema,
-                        service_name=self.context.database_service,
-                        database_name=self.context.database,
+                        service_name=self.context.get().database_service,
+                        database_name=self.context.get().database,
                         schema_name=schema_name,
                     ),
                     tags=[tag_value],
                     classification_name=tag_name,
                     tag_description=DATABRICKS_TAG,
                     classification_description=DATABRICKS_TAG_CLASSIFICATION,
                 )
@@ -518,44 +527,54 @@
 
     def yield_table_tags(
         self, table_name_and_type: Tuple[str, TableType]
     ) -> Iterable[Either[OMetaTagAndClassification]]:
         table_name, _ = table_name_and_type
         try:
             table_tags = self.table_tags.get(
-                (self.context.database, self.context.database_schema, table_name), []
+                (
+                    self.context.get().database,
+                    self.context.get().database_schema,
+                    table_name,
+                ),
+                [],
             )
             for tag_name, tag_value in table_tags:
                 yield from get_ometa_tag_and_classification(
                     tag_fqn=fqn.build(
                         self.metadata,
                         Table,
-                        service_name=self.context.database_service,
-                        database_name=self.context.database,
-                        schema_name=self.context.database_schema,
+                        service_name=self.context.get().database_service,
+                        database_name=self.context.get().database,
+                        schema_name=self.context.get().database_schema,
                         table_name=table_name,
                     ),
                     tags=[tag_value],
                     classification_name=tag_name,
                     tag_description=DATABRICKS_TAG,
                     classification_description=DATABRICKS_TAG_CLASSIFICATION,
                 )
 
             column_tags = self.column_tags.get(
-                (self.context.database, self.context.database_schema, table_name), {}
+                (
+                    self.context.get().database,
+                    self.context.get().database_schema,
+                    table_name,
+                ),
+                {},
             )
             for column_name, tags in column_tags.items():
                 for tag_name, tag_value in tags or []:
                     yield from get_ometa_tag_and_classification(
                         tag_fqn=fqn.build(
                             self.metadata,
                             Column,
-                            service_name=self.context.database_service,
-                            database_name=self.context.database,
-                            schema_name=self.context.database_schema,
+                            service_name=self.context.get().database_service,
+                            database_name=self.context.get().database,
+                            schema_name=self.context.get().database_schema,
                             table_name=table_name,
                             column_name=column_name,
                         ),
                         tags=[tag_value],
                         classification_name=tag_name,
                         tag_description=DATABRICKS_TAG,
                         classification_description=DATABRICKS_TAG_CLASSIFICATION,
@@ -565,7 +584,40 @@
             yield Either(
                 left=StackTraceError(
                     name="Tags and Classifications",
                     error=f"Failed to fetch table/column tags due to [{exc}]",
                     stackTrace=traceback.format_exc(),
                 )
             )
+
+    def get_table_description(
+        self, schema_name: str, table_name: str, inspector: Inspector
+    ) -> str:
+        description = None
+        try:
+            cursor = self.connection.execute(
+                DATABRICKS_GET_TABLE_COMMENTS.format(
+                    schema_name=schema_name,
+                    table_name=table_name,
+                    catalog_name=self.context.get().database,
+                )
+            )
+            for result in list(cursor):
+                data = result.values()
+                if data[0] and data[0].strip() == "Comment":
+                    description = data[1] if data and data[1] else None
+                elif data[0] and data[0].strip() == "Location":
+                    self.external_location_map[
+                        (self.context.get().database, schema_name, table_name)
+                    ] = (
+                        data[1]
+                        if data and data[1] and not data[1].startswith("dbfs")
+                        else None
+                    )
+
+        # Catch any exception without breaking the ingestion
+        except Exception as exc:  # pylint: disable=broad-except
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Table description error for table [{schema_name}.{table_name}]: {exc}"
+            )
+        return description
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/queries.py`

 * *Files 7% similar despite different names*

```diff
@@ -21,15 +21,17 @@
         TABLE_SCHEMA as schema,
         VIEW_DEFINITION as view_def
     from INFORMATION_SCHEMA.VIEWS WHERE VIEW_DEFINITION IS NOT NULL
     """
 )
 
 
-DATABRICKS_GET_TABLE_COMMENTS = "DESCRIBE TABLE EXTENDED {schema_name}.{table_name}"
+DATABRICKS_GET_TABLE_COMMENTS = (
+    "DESCRIBE TABLE EXTENDED {catalog_name}.{schema_name}.{table_name}"
+)
 
 DATABRICKS_GET_CATALOGS = "SHOW CATALOGS"
 
 DATABRICKS_GET_CATALOGS_TAGS = textwrap.dedent(
     """SELECT * FROM {database_name}.information_schema.catalog_tags;"""
 )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/query_parser.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Databricks Query parser module
 """
 from abc import ABC
+from typing import Optional
 
 from metadata.generated.schema.entity.services.connections.database.databricksConnection import (
     DatabricksConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -31,24 +32,30 @@
 class DatabricksQueryParserSource(QueryParserSource, ABC):
     """
     Databricks base for Usage and Lineage
     """
 
     filters: str
 
-    def _init_super(self, config: WorkflowSource, metadata: OpenMetadata):
+    def _init_super(
+        self,
+        config: WorkflowSource,
+        metadata: OpenMetadata,
+    ):
         super().__init__(config, metadata, False)
 
     # pylint: disable=super-init-not-called
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         self._init_super(config=config, metadata=metadata)
         self.client = DatabricksClient(self.service_connection)
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: DatabricksConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DatabricksConnection):
             raise InvalidSourceException(
                 f"Expected DatabricksConnection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/databricks/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/databricks/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/datalake/columns.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/datalake/columns.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/datalake/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/datalake/connection.py`

 * *Files 1% similar despite different names*

```diff
@@ -85,30 +85,29 @@
     set_google_credentials(gcp_credentials=gcs_config.securityConfig)
     gcs_client = storage.Client()
     return gcs_client
 
 
 @get_datalake_client.register
 def _(config: AzureConfig):
-
     try:
         return AzureClient(config.securityConfig).create_blob_client()
     except Exception as exc:
         raise RuntimeError(
             f"Unknown error connecting with {config.securityConfig}: {exc}."
         )
 
 
 def set_gcs_datalake_client(config: GCSConfig, project_id: str):
     gcs_config = deepcopy(config)
     if hasattr(gcs_config.securityConfig, "gcpConfig"):
         gcs_config.securityConfig.gcpConfig.projectId = SingleProjectId.parse_obj(
             project_id
         )
-    return get_datalake_client(config=gcs_config)
+    return get_datalake_client(gcs_config)
 
 
 def get_connection(connection: DatalakeConnection) -> DatalakeClient:
     """
     Create connection.
 
     Returns an AWS, Azure or GCS Clients.
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/datalake/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/datalake/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 
 """
 DataLake connector to fetch metadata from a files stored s3, gcs and Hdfs
 """
 import json
 import os
 import traceback
-from typing import Any, Iterable, Tuple, Union
+from typing import Any, Iterable, Optional, Tuple, Union
 
 from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.data.createDatabaseSchema import (
     CreateDatabaseSchemaRequest,
 )
 from metadata.generated.schema.api.data.createQuery import CreateQueryRequest
 from metadata.generated.schema.api.data.createStoredProcedure import (
@@ -113,15 +113,17 @@
         self.database_source_state = set()
         self.config_source = self.service_connection.configSource
         self.connection_obj = self.connection
         self.test_connection()
         self.reader = get_reader(config_source=self.config_source, client=self.client)
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: DatalakeConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DatalakeConnection):
             raise InvalidSourceException(
                 f"Expected DatalakeConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -144,15 +146,15 @@
                 list,
             ):
                 project_id_list = [project_id_list]
             for project_id in project_id_list:
                 database_fqn = fqn.build(
                     self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
+                    service_name=self.context.get().database_service,
                     database_name=project_id,
                 )
                 if filter_by_database(
                     self.source_config.databaseFilterPattern,
                     database_fqn
                     if self.source_config.useFqnForFiltering
                     else project_id,
@@ -185,31 +187,31 @@
         Prepare a database request and pass it to the sink
         """
         if isinstance(self.config_source, GCSConfig):
             database_name = self.client.project
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
             )
         )
 
     def fetch_gcs_bucket_names(self):
         """
         Fetch Google cloud storage buckets
         """
         try:
             # List all the buckets in the project
             for bucket in self.client.list_buckets():
                 # Build a fully qualified name (FQN) for each bucket
                 schema_fqn = fqn.build(
                     self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                     schema_name=bucket.name,
                 )
 
                 # Check if the bucket matches a certain filter pattern
                 if filter_by_schema(
                     self.config.sourceConfig.config.schemaFilterPattern,
                     schema_fqn
@@ -232,16 +234,16 @@
             )
 
     def fetch_s3_bucket_names(self):
         for bucket in self.client.list_buckets()["Buckets"]:
             schema_fqn = fqn.build(
                 self.metadata,
                 entity_type=DatabaseSchema,
-                service_name=self.context.database_service,
-                database_name=self.context.database,
+                service_name=self.context.get().database_service,
+                database_name=self.context.get().database,
                 schema_name=bucket["Name"],
             )
             if filter_by_schema(
                 self.config.sourceConfig.config.schemaFilterPattern,
                 schema_fqn
                 if self.config.sourceConfig.config.useFqnForFiltering
                 else bucket["Name"],
@@ -280,16 +282,16 @@
             else ""
         )
         schema_names = self.client.list_containers(name_starts_with=prefix)
         for schema in schema_names:
             schema_fqn = fqn.build(
                 self.metadata,
                 entity_type=DatabaseSchema,
-                service_name=self.context.database_service,
-                database_name=self.context.database,
+                service_name=self.context.get().database_service,
+                database_name=self.context.get().database,
                 schema_name=schema["name"],
             )
             if filter_by_schema(
                 self.config.sourceConfig.config.schemaFilterPattern,
                 schema_fqn
                 if self.config.sourceConfig.config.useFqnForFiltering
                 else schema["name"],
@@ -308,16 +310,16 @@
         """
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name,
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                 ),
             )
         )
 
     def get_tables_name_and_type(  # pylint: disable=too-many-branches
         self,
     ) -> Iterable[Tuple[str, TableType]]:
@@ -325,15 +327,15 @@
         Handle table and views.
 
         Fetches them up using the context information and
         the inspector set when preparing the db.
 
         :return: tables or views, depending on config
         """
-        bucket_name = self.context.database_schema
+        bucket_name = self.context.get().database_schema
         prefix = self.service_connection.prefix
         try:
             metadata_config_response = self.reader.read(
                 path=OPENMETADATA_TEMPLATE_FILE_NAME,
                 bucket_name=bucket_name,
                 verbose=False,
             )
@@ -400,15 +402,15 @@
         self, table_name_and_type: Tuple[str, str]
     ) -> Iterable[Either[CreateTableRequest]]:
         """
         From topology.
         Prepare a table request and pass it to the sink
         """
         table_name, table_type, table_extension = table_name_and_type
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
         try:
             table_constraints = None
             data_frame, raw_data = fetch_dataframe(
                 config_source=self.config_source,
                 client=self.client,
                 file_fqn=DatalakeTableSchemaWrapper(
                     key=table_name,
@@ -430,16 +432,16 @@
                     name=table_name,
                     tableType=table_type,
                     columns=columns,
                     tableConstraints=table_constraints if table_constraints else None,
                     databaseSchema=fqn.build(
                         metadata=self.metadata,
                         entity_type=DatabaseSchema,
-                        service_name=self.context.database_service,
-                        database_name=self.context.database,
+                        service_name=self.context.get().database_service,
+                        database_name=self.context.get().database,
                         schema_name=schema_name,
                     ),
                     fileFormat=table_extension.value if table_extension else None,
                 )
                 yield Either(right=table_request)
                 self.register_record(table_request=table_request)
         except Exception as exc:
@@ -482,17 +484,17 @@
         return table
 
     def filter_dl_table(self, table_name: str):
         """Filters Datalake Tables based on filterPattern"""
         table_fqn = fqn.build(
             self.metadata,
             entity_type=Table,
-            service_name=self.context.database_service,
-            database_name=self.context.database,
-            schema_name=self.context.database_schema,
+            service_name=self.context.get().database_service,
+            database_name=self.context.get().database,
+            schema_name=self.context.get().database_schema,
             table_name=table_name,
             skip_es_search=True,
         )
 
         if filter_by_table(
             self.config.sourceConfig.config.tableFilterPattern,
             table_fqn
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/db2/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/db2/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/db2/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/db2/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """Db2 source module"""
 import traceback
+from typing import Optional
 
 from ibm_db_sa.base import DB2Dialect
 from sqlalchemy.engine import reflection
 from sqlalchemy.engine.reflection import Inspector
 from sqlalchemy.engine.row import LegacyRow
 
 from metadata.generated.schema.entity.services.connections.database.db2Connection import (
@@ -43,15 +44,17 @@
 class Db2Source(CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Db2 Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: Db2Connection = config.serviceConnection.__root__.config
         if not isinstance(connection, Db2Connection):
             raise InvalidSourceException(
                 f"Expected Db2Connection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/constants.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/constants.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/dbt_config.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/dbt_config.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/dbt_service.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/dbt_service.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import Source
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
-    TopologyContext,
+    TopologyContextManager,
     TopologyNode,
 )
 from metadata.ingestion.source.database.database_service import DataModelLink
 from metadata.ingestion.source.database.dbt.dbt_config import get_dbt_details
 from metadata.ingestion.source.database.dbt.models import (
     DbtFiles,
     DbtFilteredModel,
@@ -129,15 +129,15 @@
 
 class DbtServiceSource(TopologyRunnerMixin, Source, ABC):
     """
     Class for defining the topology of the DBT source
     """
 
     topology = DbtServiceTopology()
-    context = TopologyContext.create(topology)
+    context = TopologyContextManager(topology)
     source_config: DbtPipeline
 
     @property
     def name(self) -> str:
         return "dbt"
 
     def remove_manifest_non_required_keys(self, manifest_dict: dict):
@@ -158,28 +158,30 @@
                 if key.lower() not in required_manifest_keys
             }
         )
 
     def get_dbt_files(self) -> Iterable[DbtFiles]:
         dbt_files = get_dbt_details(self.source_config.dbtConfigSource)
         for dbt_file in dbt_files:
-            self.context.dbt_file = dbt_file
+            self.context.get().dbt_file = dbt_file
             yield dbt_file
 
     def get_dbt_objects(self) -> Iterable[DbtObjects]:
         self.remove_manifest_non_required_keys(
-            manifest_dict=self.context.dbt_file.dbt_manifest
+            manifest_dict=self.context.get().dbt_file.dbt_manifest
         )
         dbt_objects = DbtObjects(
-            dbt_catalog=parse_catalog(self.context.dbt_file.dbt_catalog)
-            if self.context.dbt_file.dbt_catalog
+            dbt_catalog=parse_catalog(self.context.get().dbt_file.dbt_catalog)
+            if self.context.get().dbt_file.dbt_catalog
             else None,
-            dbt_manifest=parse_manifest(self.context.dbt_file.dbt_manifest),
-            dbt_run_results=parse_run_results(self.context.dbt_file.dbt_run_results)
-            if self.context.dbt_file.dbt_run_results
+            dbt_manifest=parse_manifest(self.context.get().dbt_file.dbt_manifest),
+            dbt_run_results=parse_run_results(
+                self.context.get().dbt_file.dbt_run_results
+            )
+            if self.context.get().dbt_file.dbt_run_results
             else None,
         )
         yield dbt_objects
 
     @abstractmethod
     def validate_dbt_files(self, dbt_files: DbtFiles):
         """
@@ -200,15 +202,15 @@
         Yield the data models
         """
 
     def get_data_model(self) -> Iterable[DataModelLink]:
         """
         Prepare the data models
         """
-        for data_model_link in self.context.data_model_links:
+        for data_model_link in self.context.get().data_model_links:
             yield data_model_link
 
     @abstractmethod
     def create_dbt_lineage(self, data_model_link: DataModelLink) -> AddLineageRequest:
         """
         Method to process DBT lineage from upstream nodes
         """
@@ -227,15 +229,15 @@
         Method to process DBT descriptions using patch APIs
         """
 
     def get_dbt_tests(self) -> dict:
         """
         Prepare the DBT tests
         """
-        for _, dbt_test in self.context.dbt_tests.items():
+        for _, dbt_test in self.context.get().dbt_tests.items():
             yield dbt_test
 
     @abstractmethod
     def create_dbt_tests_definition(
         self, dbt_test: dict
     ) -> CreateTestDefinitionRequest:
         """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/dbt_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/dbt_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/metadata.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.api.tests.createTestCase import CreateTestCaseRequest
 from metadata.generated.schema.api.tests.createTestDefinition import (
     CreateTestDefinitionRequest,
 )
 from metadata.generated.schema.entity.classification.tag import Tag
+from metadata.generated.schema.entity.data.glossaryTerm import GlossaryTerm
 from metadata.generated.schema.entity.data.table import (
     Column,
     DataModel,
     ModelType,
     Table,
 )
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
@@ -79,14 +80,15 @@
     generate_entity_link,
     get_corrected_name,
     get_data_model_path,
     get_dbt_compiled_query,
     get_dbt_model_name,
     get_dbt_raw_query,
 )
+from metadata.ingestion.source.database.dbt.models import DbtMeta
 from metadata.utils import fqn
 from metadata.utils.elasticsearch import get_entity_from_es_result
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.tag_utils import get_ometa_tag_and_classification, get_tag_labels
 from metadata.utils.time_utils import convert_timestamp_to_milliseconds
 
 logger = ingestion_logger()
@@ -111,15 +113,17 @@
         self.tag_classification_name = (
             self.source_config.dbtClassificationName
             if self.source_config.dbtClassificationName
             else "dbtTags"
         )
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         return cls(config, metadata)
 
     def test_connection(self) -> None:
         """
         DBT does not need to connect to any source to process information
         """
@@ -293,19 +297,21 @@
 
     def add_dbt_tests(
         self, key: str, manifest_node, manifest_entities, dbt_objects: DbtObjects
     ) -> None:
         """
         Method to append dbt test cases for later processing
         """
-        self.context.dbt_tests[key] = {DbtCommonEnum.MANIFEST_NODE.value: manifest_node}
-        self.context.dbt_tests[key][
+        self.context.get().dbt_tests[key] = {
+            DbtCommonEnum.MANIFEST_NODE.value: manifest_node
+        }
+        self.context.get().dbt_tests[key][
             DbtCommonEnum.UPSTREAM.value
         ] = self.parse_upstream_nodes(manifest_entities, manifest_node)
-        self.context.dbt_tests[key][DbtCommonEnum.RESULTS.value] = next(
+        self.context.get().dbt_tests[key][DbtCommonEnum.RESULTS.value] = next(
             (
                 item
                 for item in dbt_objects.dbt_run_results.results
                 if item.unique_id == key
             ),
             None,
         )
@@ -324,22 +330,22 @@
                 **dbt_objects.dbt_manifest.nodes,
             }
             if dbt_objects.dbt_catalog:
                 catalog_entities = {
                     **dbt_objects.dbt_catalog.sources,
                     **dbt_objects.dbt_catalog.nodes,
                 }
-            self.context.data_model_links = []
-            self.context.dbt_tests = {}
-            self.context.run_results_generate_time = None
+            self.context.get().data_model_links = []
+            self.context.get().dbt_tests = {}
+            self.context.get().run_results_generate_time = None
             if (
                 dbt_objects.dbt_run_results
                 and dbt_objects.dbt_run_results.metadata.generated_at
             ):
-                self.context.run_results_generate_time = (
+                self.context.get().run_results_generate_time = (
                     dbt_objects.dbt_run_results.metadata.generated_at
                 )
             for key, manifest_node in manifest_entities.items():
                 try:
                     # If the run_results file is passed then only DBT tests will be processed
                     if (
                         dbt_objects.dbt_run_results
@@ -381,23 +387,28 @@
 
                     logger.debug(f"Processing DBT node: {model_name}")
 
                     catalog_node = None
                     if dbt_objects.dbt_catalog:
                         catalog_node = catalog_entities.get(key)
 
-                    dbt_table_tags_list = None
+                    dbt_table_tags_list = []
                     if manifest_node.tags:
                         dbt_table_tags_list = get_tag_labels(
                             metadata=self.metadata,
                             tags=manifest_node.tags,
                             classification_name=self.tag_classification_name,
                             include_tags=self.source_config.includeTags,
                         )
 
+                    if manifest_node.meta:
+                        dbt_table_tags_list.extend(
+                            self.process_dbt_meta(manifest_node.meta) or []
+                        )
+
                     dbt_compiled_query = get_dbt_compiled_query(manifest_node)
                     dbt_raw_query = get_dbt_raw_query(manifest_node)
 
                     # Get the table entity from ES
                     # TODO: Change to get_by_name once the postgres case sensitive calls is fixed
                     table_fqn = fqn.build(
                         self.metadata,
@@ -437,19 +448,19 @@
                                 upstream=self.parse_upstream_nodes(
                                     manifest_entities, manifest_node
                                 ),
                                 owner=self.get_dbt_owner(
                                     manifest_node=manifest_node,
                                     catalog_node=catalog_node,
                                 ),
-                                tags=dbt_table_tags_list,
+                                tags=dbt_table_tags_list or None,
                             ),
                         )
                         yield Either(right=data_model_link)
-                        self.context.data_model_links.append(data_model_link)
+                        self.context.get().data_model_links.append(data_model_link)
                     else:
                         logger.warning(
                             f"Unable to find the table '{table_fqn}' in OpenMetadata"
                             "Please check if the table exists and is ingested in OpenMetadata"
                             "Also name, database, schema of the manifest node matches with the table present "
                             "in OpenMetadata"
                         )
@@ -540,14 +551,42 @@
                 column_name = (
                     catalog_column.name if catalog_column else manifest_column.name
                 )
                 column_description = None
                 if catalog_column and catalog_column.comment:
                     column_description = catalog_column.comment
 
+                dbt_column_tag_list = []
+                dbt_column_tag_list.extend(
+                    get_tag_labels(
+                        metadata=self.metadata,
+                        tags=manifest_column.tags,
+                        classification_name=self.tag_classification_name,
+                        include_tags=self.source_config.includeTags,
+                    )
+                    or []
+                )
+
+                if manifest_column.meta:
+                    dbt_column_meta = DbtMeta(**manifest_column.meta)
+                    logger.debug(f"Processing DBT column glossary: {key}")
+                    if (
+                        dbt_column_meta.openmetadata
+                        and dbt_column_meta.openmetadata.glossary
+                    ):
+                        dbt_column_tag_list.extend(
+                            get_tag_labels(
+                                metadata=self.metadata,
+                                tags=dbt_column_meta.openmetadata.glossary,
+                                include_tags=self.source_config.includeTags,
+                                tag_type=GlossaryTerm,
+                            )
+                            or []
+                        )
+
                 columns.append(
                     Column(
                         name=column_name,
                         description=manifest_column.description
                         if manifest_column.description
                         else column_description,
                         dataType=ColumnTypeParser.get_column_type(
@@ -555,20 +594,15 @@
                             if catalog_column
                             else manifest_column.data_type
                         ),
                         dataLength=1,
                         ordinalPosition=catalog_column.index
                         if catalog_column
                         else None,
-                        tags=get_tag_labels(
-                            metadata=self.metadata,
-                            tags=manifest_column.tags,
-                            classification_name=self.tag_classification_name,
-                            include_tags=self.source_config.includeTags,
-                        ),
+                        tags=dbt_column_tag_list or None,
                     )
                 )
                 logger.debug(f"Successfully processed DBT column: {key}")
             except Exception as exc:  # pylint: disable=broad-except
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Failed to parse DBT column {column_name}: {exc}")
 
@@ -666,14 +700,50 @@
                         f"Failed to parse the query {data_model_link.datamodel.sql.__root__}"
                         f" to capture lineage: {exc}"
                     ),
                     stackTrace=traceback.format_exc(),
                 )
             )
 
+    def process_dbt_meta(self, manifest_meta):
+        """
+        Method to process DBT meta for Tags and GlossaryTerms
+        """
+        dbt_table_tags_list = []
+        try:
+            dbt_meta_info = DbtMeta(**manifest_meta)
+            if dbt_meta_info.openmetadata and dbt_meta_info.openmetadata.glossary:
+                dbt_table_tags_list.extend(
+                    get_tag_labels(
+                        metadata=self.metadata,
+                        tags=dbt_meta_info.openmetadata.glossary,
+                        include_tags=self.source_config.includeTags,
+                        tag_type=GlossaryTerm,
+                    )
+                    or []
+                )
+
+            if dbt_meta_info.openmetadata and dbt_meta_info.openmetadata.tier:
+                tier_fqn = dbt_meta_info.openmetadata.tier
+                dbt_table_tags_list.extend(
+                    get_tag_labels(
+                        metadata=self.metadata,
+                        tags=[tier_fqn.split(fqn.FQN_SEPARATOR)[-1]],
+                        classification_name=tier_fqn.split(fqn.FQN_SEPARATOR)[0],
+                        include_tags=self.source_config.includeTags,
+                    )
+                    or []
+                )
+
+        except Exception as exc:  # pylint: disable=broad-except
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Failed to process meta dbt Tags and GlossaryTerms: {exc}")
+
+        return dbt_table_tags_list or []
+
     def process_dbt_descriptions(self, data_model_link: DataModelLink):
         """
         Method to process DBT descriptions using patch APIs
         """
         table_entity: Table = data_model_link.table_entity
         logger.debug(
             f"Processing DBT Descriptions for: {table_entity.fullyQualifiedName.__root__}"
@@ -843,16 +913,16 @@
                 dbt_test_completed_at = None
                 for dbt_test_timing in dbt_test_timings:
                     if dbt_test_timing.name == "execute":
                         dbt_test_completed_at = dbt_test_timing.completed_at
                 dbt_timestamp = None
                 if dbt_test_completed_at:
                     dbt_timestamp = dbt_test_completed_at
-                elif self.context.run_results_generate_time:
-                    dbt_timestamp = self.context.run_results_generate_time
+                elif self.context.get().run_results_generate_time:
+                    dbt_timestamp = self.context.get().run_results_generate_time
 
                 # check if the timestamp is a str type and convert accordingly
                 if isinstance(dbt_timestamp, str):
                     dbt_timestamp = datetime.strptime(
                         dbt_timestamp, DBT_RUN_RESULT_DATE_FORMAT
                     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dbt/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dbt/models.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Models required for dbt 
 """
 
-from typing import Any, Optional
+from typing import Any, List, Optional
 
 from pydantic import BaseModel
 
 
 class DbtFiles(BaseModel):
     dbt_catalog: Optional[dict]
     dbt_manifest: dict
@@ -29,7 +29,16 @@
     dbt_run_results: Optional[Any]
 
 
 class DbtFilteredModel(BaseModel):
     is_filtered: Optional[bool] = False
     message: Optional[str]
     model_fqn: Optional[str]
+
+
+class DbtMetaGlossaryTier(BaseModel):
+    tier: Optional[str]
+    glossary: Optional[List[str]]
+
+
+class DbtMeta(BaseModel):
+    openmetadata: Optional[DbtMetaGlossaryTier]
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/deltalake/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/deltalake/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/deltalake/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/deltalake/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -113,15 +113,17 @@
         self.table_constraints = None
         self.database_source_state = set()
 
         self.connection_obj = self.spark
         self.test_connection()
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: DeltaLakeConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DeltaLakeConnection):
             raise InvalidSourceException(
                 f"Expected DeltaLakeConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -144,29 +146,29 @@
         """
         From topology.
         Prepare a database request and pass it to the sink
         """
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
             )
         )
 
     def get_database_schema_names(self) -> Iterable[str]:
         """
         return schema names
         """
         schemas = self.spark.catalog.listDatabases()
         for schema in schemas:
             schema_fqn = fqn.build(
                 self.metadata,
                 entity_type=DatabaseSchema,
-                service_name=self.context.database_service,
-                database_name=self.context.database,
+                service_name=self.context.get().database_service,
+                database_name=self.context.get().database,
                 schema_name=schema.name,
             )
             if filter_by_schema(
                 self.config.sourceConfig.config.schemaFilterPattern,
                 schema_fqn
                 if self.config.sourceConfig.config.useFqnForFiltering
                 else schema.name,
@@ -184,39 +186,39 @@
         """
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name,
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                 ),
             )
         )
 
     def get_tables_name_and_type(self) -> Optional[Iterable[Tuple[str, str]]]:
         """
         Handle table and views.
 
         Fetches them up using the context information and
         the inspector set when preparing the db.
 
         :return: tables or views, depending on config
         """
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
         for table in self.spark.catalog.listTables(dbName=schema_name):
             try:
                 table_name = table.name
                 table_fqn = fqn.build(
                     self.metadata,
                     entity_type=Table,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                     table_name=table.name,
                 )
                 if filter_by_table(
                     self.source_config.tableFilterPattern,
                     table_fqn if self.source_config.useFqnForFiltering else table.name,
                 ):
                     self.status.filter(
@@ -230,25 +232,25 @@
                     and table.tableType != SparkTableType.VIEW.value
                 ):
                     # We will skip ingesting any TMP table
                     if table.tableType == SparkTableType.TEMPORARY.value:
                         logger.debug(f"Skipping temporary table {table.name}")
                         continue
 
-                    self.context.table_description = table.description
+                    self.context.get().table_description = table.description
                     yield table_name, TABLE_TYPE_MAP.get(
                         table.tableType, TableType.Regular
                     )
 
                 if (
                     self.source_config.includeViews
                     and table.tableType
                     and table.tableType == SparkTableType.VIEW.value
                 ):
-                    self.context.table_description = table.description
+                    self.context.get().table_description = table.description
                     yield table_name, TableType.View
 
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Unexpected exception for table [{table}]: {exc}")
                 self.status.warnings.append(f"{self.config.serviceName}.{table.name}")
 
@@ -256,34 +258,34 @@
         self, table_name_and_type: Tuple[str, TableType]
     ) -> Iterable[Either[CreateTableRequest]]:
         """
         From topology.
         Prepare a table request and pass it to the sink
         """
         table_name, table_type = table_name_and_type
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
         try:
             columns = self.get_columns(schema_name, table_name)
             view_definition = (
                 self._fetch_view_schema(table_name)
                 if table_type == TableType.View
                 else None
             )
 
             table_request = CreateTableRequest(
                 name=table_name,
                 tableType=table_type,
-                description=self.context.table_description,
+                description=self.context.get().table_description,
                 columns=columns,
                 tableConstraints=None,
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                     schema_name=schema_name,
                 ),
                 viewDefinition=view_definition,
             )
 
             yield Either(right=table_request)
             self.register_record(table_request=table_request)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/domodatabase/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/domodatabase/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/domodatabase/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/domodatabase/metadata.py`

 * *Files 10% similar despite different names*

```diff
@@ -79,15 +79,20 @@
         self.metadata = metadata
         self.service_connection = self.config.serviceConnection.__root__.config
         self.domo_client = get_connection(self.service_connection)
         self.connection_obj = self.domo_client
         self.test_connection()
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: DomoDatabaseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DomoDatabaseConnection):
             raise InvalidSourceException(
                 f"Expected DomoDatabaseConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -98,15 +103,15 @@
 
     def yield_database(
         self, database_name: str
     ) -> Iterable[Either[CreateDatabaseRequest]]:
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
             )
         )
 
     def get_database_schema_names(self) -> Iterable[str]:
         scheme_name = "default"
         yield scheme_name
 
@@ -115,33 +120,33 @@
     ) -> Iterable[Either[CreateDatabaseSchemaRequest]]:
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name,
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                 ),
             )
         )
 
     def get_tables_name_and_type(self) -> Optional[Iterable[Tuple[str, str]]]:
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
         try:
             tables = list(self.domo_client.datasets.list())
             for table in tables:
                 table_id = table["id"]
                 table_id = self.standardize_table_name(schema_name, table_id)
                 table_fqn = fqn.build(
                     self.metadata,
                     entity_type=Table,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                     table_name=table["name"],
                 )
 
                 if filter_by_table(
                     self.config.sourceConfig.config.tableFilterPattern,
                     table_fqn
                     if self.config.sourceConfig.config.useFqnForFiltering
@@ -190,17 +195,17 @@
                 description=table_object.description,
                 columns=columns,
                 owner=self.get_owners(owner=table_object.owner),
                 tableConstraints=table_constraints,
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 ),
                 sourceUrl=self.get_source_url(
                     table_name=table_id,
                 ),
             )
             yield Either(right=table_request)
             self.register_record(table_request=table_request)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/domodatabase/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/domodatabase/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/connection.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,58 +15,64 @@
 from typing import Optional
 
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.dorisConnection import (
-    DorisConnection,
+from metadata.generated.schema.entity.services.connections.database.mssqlConnection import (
+    MssqlConnection,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
     get_connection_url_common,
-    init_empty_connection_options,
-)
-from metadata.ingestion.connections.test_connections import (
-    test_connection_db_schema_sources,
 )
+from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.database.azuresql.connection import (
+    get_connection_url as get_pyodbc_connection_url,
+)
+from metadata.ingestion.source.database.mssql.queries import (
+    MSSQL_GET_DATABASE,
+    MSSQL_TEST_GET_QUERIES,
+)
+
+
+def get_connection_url(connection: MssqlConnection) -> str:
+    if connection.scheme.value == connection.scheme.mssql_pyodbc.value:
+        return get_pyodbc_connection_url(connection)
+    return get_connection_url_common(connection)
 
 
-def get_connection(connection: DorisConnection) -> Engine:
+def get_connection(connection: MssqlConnection) -> Engine:
     """
     Create connection
     """
-    if connection.sslCA or connection.sslCert or connection.sslKey:
-        if not connection.connectionOptions:
-            connection.connectionOptions = init_empty_connection_options()
-        if connection.sslCA:
-            connection.connectionOptions.__root__["ssl_ca"] = connection.sslCA
-        if connection.sslCert:
-            connection.connectionOptions.__root__["ssl_cert"] = connection.sslCert
-        if connection.sslKey:
-            connection.connectionOptions.__root__["ssl_key"] = connection.sslKey
     return create_generic_db_connection(
         connection=connection,
-        get_connection_url_fn=get_connection_url_common,
+        get_connection_url_fn=get_connection_url,
         get_connection_args_fn=get_connection_args_common,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: DorisConnection,
+    service_connection: MssqlConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-    test_connection_db_schema_sources(
+    queries = {
+        "GetQueries": MSSQL_TEST_GET_QUERIES,
+        "GetDatabases": MSSQL_GET_DATABASE,
+    }
+    test_connection_db_common(
         metadata=metadata,
         engine=engine,
         service_connection=service_connection,
         automation_workflow=automation_workflow,
+        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,16 @@
 from pydoris.sqlalchemy.dialect import DorisDialect
 from sqlalchemy import sql
 from sqlalchemy.dialects.mysql.reflection import MySQLTableDefinitionParser
 from sqlalchemy.engine.reflection import Inspector
 
 from metadata.generated.schema.entity.data.table import (
     Column,
-    IntervalType,
+    PartitionColumnDetails,
+    PartitionIntervalTypes,
     TableConstraint,
     TablePartition,
     TableType,
 )
 from metadata.generated.schema.entity.services.connections.database.dorisConnection import (
     DorisConnection,
 )
@@ -45,14 +46,15 @@
 )
 from metadata.ingestion.source.database.doris.utils import (
     get_table_comment,
     get_table_names_and_type,
 )
 from metadata.ingestion.source.database.mysql.utils import parse_column
 from metadata.utils.logger import ingestion_logger
+from metadata.utils.ssl_manager import SSLManager, check_ssl_and_init
 
 MySQLTableDefinitionParser._parse_column = (  # pylint: disable=protected-access
     parse_column
 )
 
 RELKIND_MAP = {
     "Doris": TableType.Regular,
@@ -139,16 +141,26 @@
 
 class DorisSource(CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Mysql Source
     """
 
+    def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
+        self.ssl_manager = None
+        service_connection = config.serviceConnection.__root__.config
+        self.ssl_manager: SSLManager = check_ssl_and_init(service_connection)
+        if self.ssl_manager:
+            service_connection = self.ssl_manager.setup_ssl(service_connection)
+        super().__init__(config, metadata)
+
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         if config.serviceConnection is None:
             raise InvalidSourceException("Missing service connection")
         connection = cast(DorisConnection, config.serviceConnection.__root__.config)
         if not isinstance(connection, DorisConnection):
             raise InvalidSourceException(
                 f"Expected DorisConnection, but got {connection}"
@@ -302,14 +314,21 @@
         try:
             result = self.engine.execute(
                 sql.text(DORIS_PARTITION_DETAILS.format(schema_name, table_name))
             ).all()
 
             if result and result[0].PartitionKey != "":
                 partition_details = TablePartition(
-                    intervalType=IntervalType.TIME_UNIT.value,
-                    columns=result[0].PartitionKey.split(", "),
+                    columns=[
+                        PartitionColumnDetails(
+                            columnName=partition_key,
+                            intervalType=PartitionIntervalTypes.TIME_UNIT,
+                            interval=None,
+                        )
+                        for partition_key in result[0].PartitionKey.split(", ")
+                    ]
                 )
+
                 return True, partition_details
             return False, None
         except Exception:
             return False, None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/doris/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/druid/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/druid/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/druid/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/druid/metadata.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,28 +8,32 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Druid source methods.
 """
 
+from typing import Optional
+
 from metadata.generated.schema.entity.services.connections.database.druidConnection import (
     DruidConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
 
 
 class DruidSource(CommonDbSourceService):
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: DruidConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DruidConnection):
             raise InvalidSourceException(
                 f"Expected DruidConnection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dynamodb/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dynamodb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dynamodb/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dynamodb/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,15 +42,17 @@
     """
 
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.dynamodb = self.connection_obj
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: DynamoDBConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DynamoDBConnection):
             raise InvalidSourceException(
                 f"Expected DynamoDBConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/dynamodb/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/dynamodb/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/extended_sample_data.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/extended_sample_data.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Sample Data source ingestion
 """
 import json
 from collections import namedtuple
-from typing import Iterable
+from typing import Iterable, Optional
 
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.api.data.createDashboardDataModel import (
     CreateDashboardDataModelRequest,
 )
 from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
@@ -190,15 +190,17 @@
         self.dashboard_service = self.metadata.get_service_or_create(
             entity=DashboardService,
             config=WorkflowSource(**self.dashboard_service_json),
         )
         self.db_name = None
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: CustomDatabaseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, CustomDatabaseConnection):
             raise InvalidSourceException(
                 f"Expected CustomDatabaseConnection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/glue/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/glue/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/glue/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/glue/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -77,15 +77,17 @@
         self.service_connection = self.config.serviceConnection.__root__.config
         self.glue = get_connection(self.service_connection)
 
         self.connection_obj = self.glue
         self.test_connection()
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: GlueConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, GlueConnection):
             raise InvalidSourceException(
                 f"Expected GlueConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -93,15 +95,15 @@
     def _get_glue_database_and_schemas(self):
         paginator = self.glue.get_paginator("get_databases")
         paginator_response = paginator.paginate()
         for page in paginator_response:
             yield DatabasePage(**page)
 
     def _get_glue_tables(self):
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
         paginator = self.glue.get_paginator("get_tables")
         paginator_response = paginator.paginate(DatabaseName=schema_name)
         for page in paginator_response:
             yield TablePage(**page)
 
     def get_database_names(self) -> Iterable[str]:
         """
@@ -120,15 +122,15 @@
             database_names = set()
             for page in self._get_glue_database_and_schemas() or []:
                 for schema in page.DatabaseList:
                     try:
                         database_fqn = fqn.build(
                             self.metadata,
                             entity_type=Database,
-                            service_name=self.context.database_service,
+                            service_name=self.context.get().database_service,
                             database_name=schema.CatalogId,
                         )
                         if filter_by_database(
                             self.config.sourceConfig.config.databaseFilterPattern,
                             database_fqn
                             if self.config.sourceConfig.config.useFqnForFiltering
                             else schema.CatalogId,
@@ -158,30 +160,30 @@
         """
         From topology.
         Prepare a database request and pass it to the sink
         """
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
             )
         )
 
     def get_database_schema_names(self) -> Iterable[str]:
         """
         return schema names
         """
         for page in self._get_glue_database_and_schemas() or []:
             for schema in page.DatabaseList:
                 try:
                     schema_fqn = fqn.build(
                         self.metadata,
                         entity_type=DatabaseSchema,
-                        service_name=self.context.database_service,
-                        database_name=self.context.database,
+                        service_name=self.context.get().database_service,
+                        database_name=self.context.get().database,
                         schema_name=schema.Name,
                     )
                     if filter_by_schema(
                         self.config.sourceConfig.config.schemaFilterPattern,
                         schema_fqn
                         if self.config.sourceConfig.config.useFqnForFiltering
                         else schema.Name,
@@ -207,46 +209,46 @@
         """
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name,
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                 ),
                 sourceUrl=self.get_source_url(
-                    database_name=self.context.database,
+                    database_name=self.context.get().database,
                     schema_name=schema_name,
                 ),
             )
         )
 
     def get_tables_name_and_type(self) -> Optional[Iterable[Tuple[str, str]]]:
         """
         Handle table and views.
 
         Fetches them up using the context information and
         the inspector set when preparing the db.
 
         :return: tables or views, depending on config
         """
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
 
         for page in self._get_glue_tables():
             for table in page.TableList:
                 try:
                     table_name = table.Name
                     table_name = self.standardize_table_name(schema_name, table_name)
                     table_fqn = fqn.build(
                         self.metadata,
                         entity_type=Table,
-                        service_name=self.context.database_service,
-                        database_name=self.context.database,
-                        schema_name=self.context.database_schema,
+                        service_name=self.context.get().database_service,
+                        database_name=self.context.get().database,
+                        schema_name=self.context.get().database_schema,
                         table_name=table_name,
                     )
                     if filter_by_table(
                         self.config.sourceConfig.config.tableFilterPattern,
                         table_fqn
                         if self.config.sourceConfig.config.useFqnForFiltering
                         else table_name,
@@ -265,15 +267,15 @@
                         # https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-creating-tables.html
                         table_type = TableType.Iceberg
                     elif table.TableType == "EXTERNAL_TABLE":
                         table_type = TableType.External
                     elif table.TableType == "VIRTUAL_VIEW":
                         table_type = TableType.View
 
-                    self.context.table_data = table
+                    self.context.get().table_data = table
                     yield table_name, table_type
                 except Exception as exc:
                     self.status.failed(
                         StackTraceError(
                             name=table.Name,
                             error=f"Unexpected exception to get table [{table.Name}]: {exc}",
                             stackTrace=traceback.format_exc(),
@@ -284,36 +286,36 @@
         self, table_name_and_type: Tuple[str, str]
     ) -> Iterable[Either[CreateTableRequest]]:
         """
         From topology.
         Prepare a table request and pass it to the sink
         """
         table_name, table_type = table_name_and_type
-        table = self.context.table_data
+        table = self.context.get().table_data
         table_constraints = None
         try:
             columns = self.get_columns(table.StorageDescriptor)
 
             table_request = CreateTableRequest(
                 name=table_name,
                 tableType=table_type,
                 description=table.Description,
                 columns=columns,
                 tableConstraints=table_constraints,
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 ),
                 sourceUrl=self.get_source_url(
                     table_name=table_name,
-                    schema_name=self.context.database_schema,
-                    database_name=self.context.database,
+                    schema_name=self.context.get().database_schema,
+                    database_name=self.context.get().database,
                 ),
             )
             yield Either(right=table_request)
             self.register_record(table_request=table_request)
         except Exception as exc:
             yield Either(
                 left=StackTraceError(
@@ -346,15 +348,15 @@
 
     def get_columns(self, column_data: StorageDetails) -> Optional[Iterable[Column]]:
         # process table regular columns info
         for column in column_data.Columns:
             yield self._get_column_object(column)
 
         # process table regular columns info
-        for column in self.context.table_data.PartitionKeys:
+        for column in self.context.get().table_data.PartitionKeys:
             yield self._get_column_object(column)
 
     def standardize_table_name(self, _: str, table: str) -> str:
         return table[:128]
 
     def yield_view_lineage(self) -> Iterable[Either[AddLineageRequest]]:
         yield from []
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/glue/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/glue/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/connection.py`

 * *Files 12% similar despite different names*

```diff
@@ -18,39 +18,29 @@
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
 from metadata.generated.schema.entity.services.connections.database.greenplumConnection import (
     GreenplumConnection,
-    SslMode,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
     get_connection_url_common,
-    init_empty_connection_arguments,
 )
 from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.database.greenplum.queries import GREENPLUM_GET_DATABASE
 
 
 def get_connection(connection: GreenplumConnection) -> Engine:
     """
     Create connection
     """
-    if connection.sslMode:
-        if not connection.connectionArguments:
-            connection.connectionArguments = init_empty_connection_arguments()
-        connection.connectionArguments.__root__["sslmode"] = connection.sslMode.value
-        if connection.sslMode in (SslMode.verify_ca, SslMode.verify_full):
-            connection.connectionArguments.__root__[
-                "sslrootcert"
-            ] = connection.sslConfig.__root__.certificatePath
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url_common,
         get_connection_args_fn=get_connection_args_common,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/metadata.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,15 +17,16 @@
 
 from sqlalchemy import sql
 from sqlalchemy.dialects.postgresql.base import PGDialect, ischema_names
 from sqlalchemy.engine.reflection import Inspector
 
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.table import (
-    IntervalType,
+    PartitionColumnDetails,
+    PartitionIntervalTypes,
     TablePartition,
     TableType,
 )
 from metadata.generated.schema.entity.services.connections.database.greenplumConnection import (
     GreenplumConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
@@ -62,17 +63,17 @@
 
 TableKey = namedtuple("TableKey", ["schema", "table_name"])
 
 logger = ingestion_logger()
 
 
 INTERVAL_TYPE_MAP = {
-    "list": IntervalType.COLUMN_VALUE.value,
-    "hash": IntervalType.COLUMN_VALUE.value,
-    "range": IntervalType.TIME_UNIT.value,
+    "list": PartitionIntervalTypes.COLUMN_VALUE,
+    "hash": PartitionIntervalTypes.COLUMN_VALUE,
+    "range": PartitionIntervalTypes.TIME_UNIT,
 }
 
 RELKIND_MAP = {
     "r": TableType.Regular,
     "p": TableType.Partitioned,
     "f": TableType.Foreign,
 }
@@ -113,15 +114,20 @@
 class GreenplumSource(CommonDbSourceService, MultiDBSource):
     """
     Implements the necessary methods to extract
     Database metadata from Greenplum Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadataConnection):
+    def create(
+        cls,
+        config_dict,
+        metadata: OpenMetadataConnection,
+        pipeline_name: Optional[str] = None,
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: GreenplumConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, GreenplumConnection):
             raise InvalidSourceException(
                 f"Expected GreenplumConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -159,15 +165,15 @@
             self.set_inspector(database_name=configured_db)
             yield configured_db
         else:
             for new_database in self.get_database_names_raw():
                 database_fqn = fqn.build(
                     self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
+                    service_name=self.context.get().database_service,
                     database_name=new_database,
                 )
 
                 if filter_by_database(
                     self.source_config.databaseFilterPattern,
                     database_fqn
                     if self.source_config.useFqnForFiltering
@@ -183,22 +189,31 @@
                     logger.debug(traceback.format_exc())
                     logger.error(
                         f"Error trying to connect to database {new_database}: {exc}"
                     )
 
     def get_table_partition_details(
         self, table_name: str, schema_name: str, inspector: Inspector
-    ) -> Tuple[bool, TablePartition]:
+    ) -> Tuple[bool, Optional[TablePartition]]:
         result = self.engine.execute(
             GREENPLUM_PARTITION_DETAILS.format(
                 table_name=table_name, schema_name=schema_name
             )
         ).all()
+
         if result:
             partition_details = TablePartition(
-                intervalType=INTERVAL_TYPE_MAP.get(
-                    result[0].partition_strategy, IntervalType.COLUMN_VALUE.value
-                ),
-                columns=[row.column_name for row in result if row.column_name],
+                columns=[
+                    PartitionColumnDetails(
+                        columnName=row.column_name,
+                        intervalType=INTERVAL_TYPE_MAP.get(
+                            result[0].partition_strategy,
+                            PartitionIntervalTypes.COLUMN_VALUE,
+                        ),
+                        interval=None,
+                    )
+                    for row in result
+                    if row.column_name
+                ]
             )
             return True, partition_details
         return False, None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/greenplum/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/greenplum/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,18 +8,17 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Hive source methods.
 """
 
-from typing import Tuple
+from typing import Optional, Tuple
 
 from pyhive.sqlalchemy_hive import HiveDialect
-from sqlalchemy.inspection import inspect
 
 from metadata.generated.schema.entity.services.connections.database.hiveConnection import (
     HiveConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -52,15 +51,17 @@
     Implements the necessary methods to extract
     Database metadata from Hive Source
     """
 
     service_connection: HiveConnection
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: HiveConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, HiveConnection):
             raise InvalidSourceException(
                 f"Expected HiveConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -89,8 +90,9 @@
             else:
                 HiveDialect.get_table_names = get_table_names_older_versions
                 HiveDialect.get_view_names = get_view_names_older_versions
         else:
             self.engine = get_metastore_connection(
                 self.service_connection.metastoreConnection
             )
-        self.inspector = inspect(self.engine)
+        self._connection_map = {}  # Lazy init as well
+        self._inspector_map = {}
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/__init__.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/dialect.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/mysql/dialect.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/__init__.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/dialect.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/metastore_dialects/postgres/dialect.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/hive/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/hive/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/__init__.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/base.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/base.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/dynamodb.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/dynamodb.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/glue.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/glue.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/hive.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/hive.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/catalog/rest.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/catalog/rest.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/fs/__init__.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/fs/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/fs/azure.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/fs/azure.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/fs/base.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/fs/base.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/fs/s3.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/fs/s3.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/helper.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/helper.py`

 * *Files 19% similar despite different names*

```diff
@@ -16,15 +16,20 @@
 from itertools import takewhile
 from typing import Optional, Tuple
 
 import pyiceberg.partitioning
 import pyiceberg.table
 import pyiceberg.types
 
-from metadata.generated.schema.entity.data.table import Column, Constraint, DataType
+from metadata.generated.schema.entity.data.table import (
+    Column,
+    Constraint,
+    DataType,
+    PartitionIntervalTypes,
+)
 
 
 def namespace_to_str(namespace: tuple[str]) -> str:
     """Turns a PyIceberg Namespace into a String.
 
     The PyIceberg namespaces are returned as tuples and we turn them into a String
     concatenating the items with a '.' in between.
@@ -41,21 +46,53 @@
     # We are skipping the first item because it is the schema name.
     return ".".join(table.name()[1:])
 
 
 def get_column_from_partition(
     columns: Tuple[pyiceberg.types.NestedField, ...],
     partition: pyiceberg.partitioning.PartitionField,
-) -> str:
+) -> Optional[str]:
     """Returns the Column Name belonging to a partition."""
     # A Partition in Iceberg has a Source Column to which a Transformation is applied.
     # We need to return the Source Column name.
-    return [
-        column.name for column in columns if column.field_id == partition.source_id
-    ][0]
+    return next(
+        (column.name for column in columns if column.field_id == partition.source_id),
+        None,
+    )
+
+
+def get_column_partition_type(
+    columns: Tuple[pyiceberg.types.NestedField, ...],
+    partition: pyiceberg.partitioning.PartitionField,
+) -> Optional[PartitionIntervalTypes]:
+    """Get the partition type for a given partition column."""
+    iceberg_interval_type_map = {
+        "INT": PartitionIntervalTypes.INTEGER_RANGE,
+        **dict.fromkeys(
+            ["TIME", "DATE", "TIMESTAMP", "TIMESTAMPTZ"],
+            PartitionIntervalTypes.TIME_UNIT,
+        ),
+    }
+
+    data_type = str(
+        next(
+            (
+                column.field_type
+                for column in columns
+                if column.field_id == partition.source_id
+            ),
+            "",
+        )
+    )
+    if not data_type.isalpha():
+        return None
+
+    return iceberg_interval_type_map.get(
+        data_type.upper(), PartitionIntervalTypes.COLUMN_VALUE
+    )
 
 
 def get_owner_from_table(
     table: pyiceberg.table.Table, property_key: str
 ) -> Optional[str]:
     """Retrives the owner information from given Table Property."""
     return table.properties.get(property_key)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/metadata.py`

 * *Files 7% similar despite different names*

```diff
@@ -78,15 +78,17 @@
         self.service_connection = self.config.serviceConnection.__root__.config
         self.iceberg = get_connection(self.service_connection)
 
         self.connection_obj = self.iceberg
         self.test_connection()
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: IcebergConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, IcebergConnection):
             raise InvalidSourceException(
                 f"Expected GlueConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -106,31 +108,31 @@
         Prepare a database request and pass it to the sink.
 
         Also, update the self.inspector value to the current db.
         """
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
             )
         )
 
     def get_database_schema_names(self) -> Iterable[str]:
         """
         Prepares the database schema name to be sent to stage.
         Filtering happens here.
         """
         for namespace in self.iceberg.list_namespaces():
             namespace_name = namespace_to_str(namespace)
             try:
                 schema_fqn = fqn.build(
                     self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                     schema_name=namespace_name,
                 )
                 if filter_by_schema(
                     self.config.sourceConfig.config.schemaFilterPattern,
                     schema_fqn
                     if self.config.sourceConfig.config.useFqnForFiltering
                     else namespace_name,
@@ -156,52 +158,52 @@
         """
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name,
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                 ),
             )
         )
 
     def get_tables_name_and_type(self) -> Optional[Iterable[Tuple[str, str]]]:
         """
         Prepares the table name to be sent to stage.
         Filtering happens here.
         """
-        namespace = self.context.database_schema
+        namespace = self.context.get().database_schema
 
         for table_identifier in self.iceberg.list_tables(namespace):
             try:
                 table = self.iceberg.load_table(table_identifier)
                 table_name = get_table_name_as_str(table)
                 table_fqn = fqn.build(
                     self.metadata,
                     entity_type=Table,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                     table_name=table_name,
                 )
                 if filter_by_table(
                     self.config.sourceConfig.config.tableFilterPattern,
                     table_fqn
                     if self.config.sourceConfig.config.useFqnForFiltering
                     else table_name,
                 ):
                     self.status.filter(
                         table_fqn,
                         "Table Filtered Out",
                     )
                     continue
 
-                self.context.iceberg_table = table
+                self.context.get().iceberg_table = table
                 yield table_name, TableType.Regular
             except pyiceberg.exceptions.NoSuchPropertyException:
                 logger.warning(
                     f"Table [{table_identifier}] does not have the 'table_type' property. Skipped."
                 )
                 continue
             except pyiceberg.exceptions.NoSuchIcebergTableError:
@@ -220,15 +222,15 @@
                         error=f"Unexpected exception to get table [{table_name}]: {exc}",
                         stackTrace=traceback.format_exc(),
                     )
                 )
 
     def get_owner_ref(self, table_name: str) -> Optional[EntityReference]:
         owner = get_owner_from_table(
-            self.context.iceberg_table, self.service_connection.ownershipProperty
+            self.context.get().iceberg_table, self.service_connection.ownershipProperty
         )
         try:
             if owner:
                 owner_reference = self.metadata.get_reference_by_email(owner)
                 return owner_reference
         except Exception as err:
             logger.debug(traceback.format_exc())
@@ -241,15 +243,15 @@
         """
         From topology.
         Prepare a table request and pass it to the sink.
 
         Also, update the self.inspector value to the current db.
         """
         table_name, table_type = table_name_and_type
-        iceberg_table = self.context.iceberg_table
+        iceberg_table = self.context.get().iceberg_table
         try:
             owner = self.get_owner_ref(table_name)
             table = IcebergTable.from_pyiceberg(
                 table_name, table_type, owner, iceberg_table
             )
             table_request = CreateTableRequest(
                 name=table.name,
@@ -257,17 +259,17 @@
                 description=table.description,
                 owner=table.owner,
                 columns=table.columns,
                 tablePartition=table.tablePartition,
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 ),
             )
             yield Either(right=table_request)
             self.register_record(table_request=table_request)
         except Exception as exc:
             yield Either(
                 left=StackTraceError(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/iceberg/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/iceberg/models.py`

 * *Files 16% similar despite different names*

```diff
@@ -16,21 +16,23 @@
 from typing import List, Optional
 
 import pyiceberg.table
 from pydantic import BaseModel
 
 from metadata.generated.schema.entity.data.table import (
     Column,
+    PartitionColumnDetails,
     TablePartition,
     TableType,
 )
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.source.database.iceberg.helper import (
     IcebergColumnParser,
     get_column_from_partition,
+    get_column_partition_type,
 )
 
 
 class IcebergTable(BaseModel):
     name: str
     tableType: TableType
     description: Optional[str]
@@ -53,14 +55,20 @@
             name=name,
             tableType=table_type,
             description=table.properties.get("comment"),
             owner=owner,
             columns=[IcebergColumnParser.parse(column) for column in iceberg_columns],
             tablePartition=TablePartition(
                 columns=[
-                    get_column_from_partition(iceberg_columns, partition)
+                    PartitionColumnDetails(
+                        columnName=get_column_from_partition(
+                            iceberg_columns, partition
+                        ),
+                        intervalType=get_column_partition_type(
+                            iceberg_columns, partition
+                        ),
+                        interval=None,
+                    )
                     for partition in table.spec().fields
-                ],
-                intervalType=None,
-                interval=None,
+                ]
             ),
         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/impala/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/impala/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/impala/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/impala/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Impala source methods.
 """
 
 import re
+from typing import Optional
 
 from impala.sqlalchemy import ImpalaDialect, _impala_type_to_sqlalchemy_type
 from sqlalchemy import types, util
 from sqlalchemy.engine import reflection
 
 from metadata.generated.schema.entity.services.connections.database.impalaConnection import (
     ImpalaConnection,
@@ -176,15 +177,17 @@
 class ImpalaSource(CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Impala Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: ImpalaConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, ImpalaConnection):
             raise InvalidSourceException(
                 f"Expected ImpalaConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/impala/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/impala/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/life_cycle_query_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/life_cycle_query_mixin.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 """
 Mixin class with common Life Cycle logic.
 """
 import traceback
 from collections import defaultdict
 from datetime import datetime
 from functools import lru_cache
-from typing import Dict, Iterable, List, Optional
+from typing import Dict, Iterable, List, Optional, Type
 
 from pydantic import BaseModel, Field
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
@@ -27,15 +27,15 @@
 from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
     DatabaseServiceMetadataPipeline,
 )
 from metadata.generated.schema.type.lifeCycle import AccessDetails, LifeCycle
 from metadata.ingestion.api.models import Either, Entity
 from metadata.ingestion.api.status import Status
 from metadata.ingestion.models.life_cycle import OMetaLifeCycleData
-from metadata.ingestion.models.topology import TopologyContext
+from metadata.ingestion.models.topology import TopologyContextManager
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.utils import fqn
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.time_utils import convert_timestamp_to_milliseconds
 
 logger = ingestion_logger()
 
@@ -53,15 +53,15 @@
 
 
 class LifeCycleQueryMixin:
     """
     Module stores the methods to query the sources and get life cycle information
     """
 
-    context: TopologyContext
+    context: TopologyContextManager
     status: Status
     source_config: DatabaseServiceMetadataPipeline
     engine: Engine
     metadata: OpenMetadata
 
     @lru_cache(
         maxsize=1
@@ -88,66 +88,69 @@
                         error=f"Error trying to get life cycle information due to [{exc}]",
                         stackTrace=traceback.format_exc(),
                     )
                 )
 
         return queries_dict
 
-    def get_life_cycle_data(self, entity: Entity, query: str):
+    def get_life_cycle_data(
+        self, entity: Type[Entity], entity_name: str, entity_fqn: str, query: str
+    ):
         """
         Get the life cycle data
         """
-        if entity:
-            try:
-                life_cycle_data = self.life_cycle_query_dict(query=query).get(
-                    entity.name.__root__
-                )
-                if life_cycle_data:
-                    life_cycle = LifeCycle(
-                        created=AccessDetails(
-                            timestamp=convert_timestamp_to_milliseconds(
-                                life_cycle_data.created_at.timestamp()
-                            )
+        try:
+            life_cycle_data = self.life_cycle_query_dict(query=query).get(entity_name)
+            if life_cycle_data:
+                life_cycle = LifeCycle(
+                    created=AccessDetails(
+                        timestamp=convert_timestamp_to_milliseconds(
+                            life_cycle_data.created_at.timestamp()
                         )
                     )
-                    yield Either(
-                        right=OMetaLifeCycleData(entity=entity, life_cycle=life_cycle)
-                    )
-            except Exception as exc:
+                )
                 yield Either(
-                    left=StackTraceError(
-                        name=entity.name.__root__,
-                        error=f"Unable to get the table life cycle data for table {entity.name.__root__}: {exc}",
-                        stackTrace=traceback.format_exc(),
+                    right=OMetaLifeCycleData(
+                        entity=entity, entity_fqn=entity_fqn, life_cycle=life_cycle
                     )
                 )
+        except Exception as exc:
+            yield Either(
+                left=StackTraceError(
+                    name=entity_name,
+                    error=f"Unable to get the table life cycle data for table {entity_name}: {exc}",
+                    stackTrace=traceback.format_exc(),
+                )
+            )
 
     def yield_life_cycle_data(self, _) -> Iterable[Either[OMetaLifeCycleData]]:
         """
         Get the life cycle data of the table
         """
         try:
             table_fqn = fqn.build(
                 self.metadata,
                 entity_type=Table,
-                service_name=self.context.database_service,
-                database_name=self.context.database,
-                schema_name=self.context.database_schema,
-                table_name=self.context.table,
+                service_name=self.context.get().database_service,
+                database_name=self.context.get().database,
+                schema_name=self.context.get().database_schema,
+                table_name=self.context.get().table,
                 skip_es_search=True,
             )
-            table = self.metadata.get_by_name(entity=Table, fqn=table_fqn)
-            if table:
-                yield from self.get_life_cycle_data(
-                    entity=table,
-                    query=self.life_cycle_query.format(
-                        database_name=table.database.name,
-                        schema_name=table.databaseSchema.name,
-                    ),
-                )
+            # table = self.metadata.get_by_name(entity=Table, fqn=table_fqn)
+            # if table:
+            yield from self.get_life_cycle_data(
+                entity=Table,
+                entity_name=self.context.get().table,
+                entity_fqn=table_fqn,
+                query=self.life_cycle_query.format(
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
+                ),
+            )
         except Exception as exc:
             yield Either(
                 left=StackTraceError(
                     name="lifeCycle",
                     error=f"Error Processing life cycle data: {exc}",
                     stackTrace=traceback.format_exc(),
                 )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/lineage_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/lineage_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mariadb/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mariadb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mariadb/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mariadb/metadata.py`

 * *Files 5% similar despite different names*

```diff
@@ -7,14 +7,16 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 MariaDB source module
 """
+from typing import Optional
+
 from sqlalchemy.dialects.mysql.base import ischema_names
 from sqlalchemy.dialects.mysql.reflection import MySQLTableDefinitionParser
 
 from metadata.generated.schema.entity.services.connections.database.mariaDBConnection import (
     MariaDBConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
@@ -36,15 +38,17 @@
 class MariadbSource(CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Hive Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: MariaDBConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, MariaDBConnection):
             raise InvalidSourceException(
                 f"Expected MariaDBConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mongodb/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mongodb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mongodb/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mongodb/metadata.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 MongoDB source methods.
 """
 
 import traceback
-from typing import Dict, List, Union
+from typing import Dict, List, Optional, Union
 
 from pymongo.errors import OperationFailure
 
 from metadata.generated.schema.entity.services.connections.database.mongoDBConnection import (
     MongoDBConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
@@ -41,15 +41,17 @@
     """
 
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.mongodb = self.connection_obj
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: MongoDBConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, MongoDBConnection):
             raise InvalidSourceException(
                 f"Expected MongoDBConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sqlite/connection.py`

 * *Files 24% similar despite different names*

```diff
@@ -15,64 +15,51 @@
 from typing import Optional
 
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.mssqlConnection import (
-    MssqlConnection,
+from metadata.generated.schema.entity.services.connections.database.sqliteConnection import (
+    SQLiteConnection,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
-    get_connection_url_common,
 )
 from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.azuresql.connection import (
-    get_connection_url as get_pyodbc_connection_url,
-)
-from metadata.ingestion.source.database.mssql.queries import (
-    MSSQL_GET_DATABASE,
-    MSSQL_TEST_GET_QUERIES,
-)
 
 
-def get_connection_url(connection: MssqlConnection) -> str:
-    if connection.scheme.value == connection.scheme.mssql_pyodbc.value:
-        return get_pyodbc_connection_url(connection)
-    return get_connection_url_common(connection)
+def get_connection_url(connection: SQLiteConnection) -> str:
+    database_mode = connection.databaseMode if connection.databaseMode else ":memory:"
+
+    return f"{connection.scheme.value}:///{database_mode}"
 
 
-def get_connection(connection: MssqlConnection) -> Engine:
+def get_connection(connection: SQLiteConnection) -> Engine:
     """
     Create connection
     """
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url,
         get_connection_args_fn=get_connection_args_common,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: MssqlConnection,
+    service_connection: SQLiteConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-    queries = {
-        "GetQueries": MSSQL_TEST_GET_QUERIES,
-        "GetDatabases": MSSQL_GET_DATABASE,
-    }
     test_connection_db_common(
         metadata=metadata,
         engine=engine,
         service_connection=service_connection,
         automation_workflow=automation_workflow,
-        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -91,15 +91,17 @@
 class MssqlSource(StoredProcedureMixin, CommonDbSourceService, MultiDBSource):
     """
     Implements the necessary methods to extract
     Database metadata from MSSQL Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: MssqlConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, MssqlConnection):
             raise InvalidSourceException(
                 f"Expected MssqlConnection, but got {connection}"
             )
@@ -119,15 +121,15 @@
             self.set_inspector(database_name=configured_db)
             yield configured_db
         else:
             for new_database in self.get_database_names_raw():
                 database_fqn = fqn.build(
                     self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
+                    service_name=self.context.get().database_service,
                     database_name=new_database,
                 )
 
                 if filter_by_database(
                     self.source_config.databaseFilterPattern,
                     database_fqn
                     if self.source_config.useFqnForFiltering
@@ -146,16 +148,16 @@
                     )
 
     def get_stored_procedures(self) -> Iterable[MssqlStoredProcedure]:
         """List Snowflake stored procedures"""
         if self.source_config.includeStoredProcedures:
             results = self.engine.execute(
                 MSSQL_GET_STORED_PROCEDURES.format(
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 )
             ).all()
             for row in results:
                 try:
                     stored_procedure = MssqlStoredProcedure.parse_obj(dict(row))
                     yield stored_procedure
                 except Exception as exc:
@@ -180,17 +182,17 @@
                 storedProcedureCode=StoredProcedureCode(
                     language=STORED_PROC_LANGUAGE_MAP.get(stored_procedure.language),
                     code=stored_procedure.definition,
                 ),
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 ),
             )
             yield Either(right=stored_procedure_request)
             self.register_record_stored_proc_request(stored_procedure_request)
 
         except Exception as exc:
             yield Either(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_array.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,27 +4,47 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""MSSQL models"""
-from typing import Optional
 
-from pydantic import BaseModel, Field
+# pylint: disable=abstract-method
 
-from metadata.generated.schema.entity.data.storedProcedure import Language
+"""
+Expand sqlalchemy types to map them to OpenMetadata DataType
+"""
+from sqlalchemy.sql.sqltypes import ARRAY, TypeDecorator
 
-STORED_PROC_LANGUAGE_MAP = {
-    "SQL": Language.SQL,
-    "EXTERNAL": Language.External,
-}
+from metadata.utils.logger import profiler_logger
 
+logger = profiler_logger()
 
-class MssqlStoredProcedure(BaseModel):
-    """MSSQL stored procedure list query results"""
 
-    name: str = Field(...)
-    owner: Optional[str] = Field(None)
-    language: str = Field(Language.SQL)
-    definition: str = Field(None)
+class CustomArray(TypeDecorator):
+    """
+    Convert numpy ndarray to python list
+    """
+
+    impl = ARRAY
+    cache_ok = True
+
+    @property
+    def python_type(self):
+        return list
+
+    def process_result_value(self, value, dialect):
+        """This is executed during result retrieval
+
+        Args:
+            value: database record
+            dialect: database dialect
+        Returns:
+            python list conversion of ndarray
+        """
+        import numpy as np  # pylint: disable=import-outside-toplevel
+
+        if isinstance(value, np.ndarray):
+            return value.tolist()
+
+        return value
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/queries.py`

 * *Files 5% similar despite different names*

```diff
@@ -42,35 +42,35 @@
 )
 
 MSSQL_GET_TABLE_COMMENTS = textwrap.dedent(
     """
 SELECT obj.name AS table_name,
         ep.value AS table_comment,
         s.name AS "schema"
-FROM sys.tables AS obj
+FROM sys.objects AS obj
 LEFT JOIN sys.extended_properties AS ep
-    ON obj.object_id = ep.major_id AND ep.minor_id = 0
+    ON obj.object_id = ep.major_id AND ep.minor_id = 0 AND ep.name = 'MS_Description'
 JOIN sys.schemas AS s
     ON obj.schema_id = s.schema_id
-WHERE ep.name = 'MS_Description'
+WHERE
+    obj.type IN ('U', 'V') /* User tables and views */
 """
 )
 
 MSSQL_ALL_VIEW_DEFINITIONS = textwrap.dedent(
     """
-select
-	definition view_def,
-	views.name view_name,
-	sch.name "schema"
-from sys.sql_modules as mod,
-sys.views as views,
-sys.schemas as sch
- where
-mod.object_id=views.object_id and
-views.schema_id=sch.schema_id
+SELECT
+    definition view_def,
+    views.name view_name,
+    sch.name "schema"
+FROM sys.sql_modules as mod
+INNER JOIN sys.views as views
+    ON mod.object_id = views.object_id
+INNER JOIN sys.schemas as sch
+    ON views.schema_id = sch.schema_id
 """
 )
 
 MSSQL_GET_DATABASE = """
 SELECT name FROM master.sys.databases order by name
 """
 
@@ -187,18 +187,20 @@
         fk_info.ordinal_position
 """
 
 MSSQL_GET_STORED_PROCEDURES = textwrap.dedent(
     """
 SELECT
   ROUTINE_NAME AS name,
-  NULL AS owner,
+  NULL AS owner,            
   ROUTINE_BODY AS language,
-  ROUTINE_DEFINITION AS definition
-FROM INFORMATION_SCHEMA.ROUTINES
+  l.definition AS definition
+FROM INFORMATION_SCHEMA.ROUTINES r
+JOIN sys.procedures p ON p.name = r.ROUTINE_NAME 
+JOIN sys.sql_modules l on l.object_id = p.object_id
  WHERE ROUTINE_TYPE = 'PROCEDURE'
    AND ROUTINE_CATALOG = '{database_name}'
    AND ROUTINE_SCHEMA = '{schema_name}' 
    AND LEFT(ROUTINE_NAME, 3) NOT IN ('sp_', 'xp_', 'ms_')
     """
 )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/pinotdb/metadata.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,40 +4,38 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""
-MSSQL usage module
-"""
-from abc import ABC
+"""PinotDb source module"""
+from typing import Optional
 
-from metadata.generated.schema.entity.services.connections.database.mssqlConnection import (
-    MssqlConnection,
+from metadata.generated.schema.entity.services.connections.database.pinotDBConnection import (
+    PinotDBConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.query_parser_source import QueryParserSource
+from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
 
 
-class MssqlQueryParserSource(QueryParserSource, ABC):
+class PinotdbSource(CommonDbSourceService):
     """
-    MSSQL base for Usage and Lineage
+    Implements the necessary methods to extract
+    Database metadata from Oracle Source
     """
 
-    filters: str
-
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
-        """Create class instance"""
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: MssqlConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, MssqlConnection):
+        connection: PinotDBConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, PinotDBConnection):
             raise InvalidSourceException(
-                f"Expected MssqlConnection, but got {connection}"
+                f"Expected PinotdbConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mssql/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -120,14 +120,15 @@
     extended_properties = Table(
         "extended_properties",
         sqlalchemy_metadata,
         Column("major_id", Integer, primary_key=True),
         Column("minor_id", Integer, primary_key=True),
         Column("name", String, primary_key=True),
         Column("value", String),
+        Column("class_desc", String),
         schema="sys",
     )
     sys_columns = alias(
         Table(
             "columns",
             sqlalchemy_metadata,
             Column("object_id", Integer, primary_key=True),
@@ -177,14 +178,16 @@
             isouter=True,
         )
         .join(
             extended_properties,
             onclause=sql.and_(
                 extended_properties.c.major_id == sys_columns.c.object_id,
                 extended_properties.c.minor_id == sys_columns.c.column_id,
+                extended_properties.c.class_desc == "OBJECT_OR_COLUMN",
+                extended_properties.c.name == "MS_Description",
             ),
             isouter=True,
         )
     )
 
     if self._supports_nvarchar_max:  # pylint: disable=protected-access
         computed_definition = computed_cols.c.definition
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/multi_db_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/multi_db_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mysql/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/nifi/connection.py`

 * *Files 23% similar despite different names*

```diff
@@ -10,78 +10,63 @@
 #  limitations under the License.
 
 """
 Source connection handler
 """
 from typing import Optional
 
-from sqlalchemy.engine import Engine
-
-from metadata.clients.azure_client import AzureClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.common.basicAuth import (
-    BasicAuth,
-)
-from metadata.generated.schema.entity.services.connections.database.mysqlConnection import (
-    MysqlConnection,
-)
-from metadata.ingestion.connections.builders import (
-    create_generic_db_connection,
-    get_connection_args_common,
-    get_connection_url_common,
-    init_empty_connection_options,
-)
-from metadata.ingestion.connections.test_connections import (
-    test_connection_db_schema_sources,
+from metadata.generated.schema.entity.services.connections.pipeline.nifiConnection import (
+    BasicAuthentication,
+    NifiConnection,
 )
+from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.pipeline.nifi.client import NifiClient
 
 
-def get_connection(connection: MysqlConnection) -> Engine:
+def get_connection(connection: NifiConnection) -> NifiClient:
     """
     Create connection
     """
-    if hasattr(connection.authType, "azureConfig"):
-        azure_client = AzureClient(connection.authType.azureConfig).create_client()
-        if not connection.authType.azureConfig.scopes:
-            raise ValueError(
-                "Azure Scopes are missing, please refer https://learn.microsoft.com/en-gb/azure/mysql/flexible-server/how-to-azure-ad#2---retrieve-microsoft-entra-access-token and fetch the resource associated with it, for e.g. https://ossrdbms-aad.database.windows.net/.default"
-            )
-        access_token_obj = azure_client.get_token(
-            *connection.authType.azureConfig.scopes.split(",")
+    if isinstance(connection.nifiConfig, BasicAuthentication):
+        return NifiClient(
+            host_port=connection.hostPort,
+            username=connection.nifiConfig.username,
+            password=connection.nifiConfig.password.get_secret_value()
+            if connection.nifiConfig.password
+            else None,
+            verify=connection.nifiConfig.verifySSL,
         )
-        connection.authType = BasicAuth(password=access_token_obj.token)
-    if connection.sslCA or connection.sslCert or connection.sslKey:
-        if not connection.connectionOptions:
-            connection.connectionOptions = init_empty_connection_options()
-        if connection.sslCA:
-            connection.connectionOptions.__root__["ssl_ca"] = connection.sslCA
-        if connection.sslCert:
-            connection.connectionOptions.__root__["ssl_cert"] = connection.sslCert
-        if connection.sslKey:
-            connection.connectionOptions.__root__["ssl_key"] = connection.sslKey
-
-    return create_generic_db_connection(
-        connection=connection,
-        get_connection_url_fn=get_connection_url_common,
-        get_connection_args_fn=get_connection_args_common,
+
+    return NifiClient(
+        host_port=connection.hostPort,
+        ca_file_path=connection.nifiConfig.certificateAuthorityPath,
+        client_cert_path=connection.nifiConfig.clientCertificatePath,
+        client_key_path=connection.nifiConfig.clientkeyPath,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
-    engine: Engine,
-    service_connection: MysqlConnection,
+    client: NifiClient,
+    service_connection: NifiConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-    test_connection_db_schema_sources(
+
+    def custom_executor():
+        list(client.list_process_groups())
+
+    test_fn = {"GetPipelines": custom_executor}
+
+    test_connection_steps(
         metadata=metadata,
-        engine=engine,
-        service_connection=service_connection,
+        test_fn=test_fn,
+        service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mysql/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mysql/metadata.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """Mysql source module"""
-from typing import cast
+from typing import Optional, cast
 
 from sqlalchemy.dialects.mysql.base import ischema_names
 from sqlalchemy.dialects.mysql.reflection import MySQLTableDefinitionParser
 
 from metadata.generated.schema.entity.services.connections.database.mysqlConnection import (
     MysqlConnection,
 )
@@ -36,15 +36,17 @@
 class MysqlSource(CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Mysql Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection = cast(MysqlConnection, config.serviceConnection.__root__.config)
         if not isinstance(connection, MysqlConnection):
             raise InvalidSourceException(
                 f"Expected MysqlConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/mysql/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mysql/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/connection.py`

 * *Files 0% similar despite different names*

```diff
@@ -34,15 +34,15 @@
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
     get_connection_options_dict,
 )
 from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.oracle.queries import CHECK_ACCESS_TO_DBA
+from metadata.ingestion.source.database.oracle.queries import CHECK_ACCESS_TO_ALL
 from metadata.utils.logger import ingestion_logger
 
 CX_ORACLE_LIB_VERSION = "8.3.0"
 LD_LIB_ENV = "LD_LIBRARY_PATH"
 
 logger = ingestion_logger()
 
@@ -134,15 +134,15 @@
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_conn_queries = {"CheckAccess": CHECK_ACCESS_TO_DBA}
+    test_conn_queries = {"CheckAccess": CHECK_ACCESS_TO_ALL}
 
     test_connection_db_common(
         metadata=metadata,
         engine=engine,
         service_connection=service_connection,
         automation_workflow=automation_workflow,
         queries=test_conn_queries,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -100,15 +100,17 @@
 class OracleSource(StoredProcedureMixin, CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Oracle Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: OracleConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, OracleConnection):
             raise InvalidSourceException(
                 f"Expected OracleConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -175,15 +177,15 @@
         return result_dict
 
     def get_stored_procedures(self) -> Iterable[OracleStoredProcedure]:
         """List Oracle Stored Procedures"""
         if self.source_config.includeStoredProcedures:
             results: FetchProcedureList = self.engine.execute(
                 ORACLE_GET_STORED_PROCEDURES.format(
-                    schema=self.context.database_schema.upper()
+                    schema=self.context.get().database_schema.upper()
                 )
             ).all()
             results = self.process_result(data=results)
             for row in results.items():
                 stored_procedure = OracleStoredProcedure(
                     name=row[0][1], definition=row[1]["text"], owner=row[0][0]
                 )
@@ -203,17 +205,17 @@
                 ),
                 owner=self.metadata.get_reference_by_name(
                     name=stored_procedure.owner.lower()
                 ),
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 ),
             )
             yield Either(right=stored_procedure_request)
             self.register_record_stored_proc_request(stored_procedure_request)
         except Exception as exc:
             yield Either(
                 left=StackTraceError(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/queries.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,62 +16,62 @@
 
 ORACLE_ALL_TABLE_COMMENTS = textwrap.dedent(
     """
 SELECT
 	comments table_comment,
 	LOWER(table_name) "table_name",
 	LOWER(owner) "schema" 	
-FROM dba_tab_comments
+FROM ALL_TAB_COMMENTS
 where comments is not null and owner not in ('SYSTEM', 'SYS')
 """
 )
 
 
 ORACLE_ALL_VIEW_DEFINITIONS = textwrap.dedent(
     """
 SELECT
 LOWER(view_name) AS "view_name",
 LOWER(owner) AS "schema",
 DBMS_METADATA.GET_DDL('VIEW', view_name, owner) AS view_def
-FROM DBA_VIEWS
+FROM ALL_VIEWS
 WHERE owner NOT IN ('SYSTEM', 'SYS')
 UNION ALL
 SELECT
 LOWER(mview_name) AS "view_name",
 LOWER(owner) AS "schema",
 DBMS_METADATA.GET_DDL('MATERIALIZED_VIEW', mview_name, owner) AS view_def
-FROM DBA_MVIEWS
+FROM ALL_MVIEWS
 WHERE owner NOT IN ('SYSTEM', 'SYS')
 """
 )
 
 GET_MATERIALIZED_VIEW_NAMES = textwrap.dedent(
     """
-SELECT mview_name FROM DBA_MVIEWS WHERE owner = :owner
+SELECT mview_name FROM ALL_MVIEWS WHERE owner = :owner
 """
 )
 
 ORACLE_GET_TABLE_NAMES = textwrap.dedent(
     """
-SELECT table_name FROM DBA_TABLES WHERE 
+SELECT table_name FROM ALL_TABLES WHERE 
 {tablespace}
 OWNER = :owner  
 AND IOT_NAME IS NULL 
 AND DURATION IS NULL
 AND TABLE_NAME NOT IN 
-(SELECT mview_name FROM DBA_MVIEWS WHERE owner = :owner)
+(SELECT mview_name FROM ALL_MVIEWS WHERE owner = :owner)
 """
 )
 
 ORACLE_IDENTITY_TYPE = textwrap.dedent(
     """\
 col.default_on_null,
 (
 	SELECT id.generation_type || ',' || id.IDENTITY_OPTIONS
-	FROM DBA_TAB_IDENTITY_COLS{dblink} id
+	FROM ALL_TAB_IDENTITY_COLS{dblink} id
 	WHERE col.table_name = id.table_name
 	AND col.column_name = id.column_name
 	AND col.owner = id.owner
 ) AS identity_options
 """
 )
 
@@ -79,29 +79,29 @@
     """
 SELECT
     OWNER,
     NAME,
     LINE,
     TEXT
 FROM
-    DBA_SOURCE
+    ALL_SOURCE
 WHERE
     type = 'PROCEDURE' and owner = '{schema}'
 """
 )
-CHECK_ACCESS_TO_DBA = "SELECT table_name FROM DBA_TABLES where ROWNUM < 2"
+CHECK_ACCESS_TO_ALL = "SELECT table_name FROM ALL_TABLES where ROWNUM < 2"
 ORACLE_GET_STORED_PROCEDURE_QUERIES = textwrap.dedent(
     """
 WITH SP_HISTORY AS (SELECT
 	sql_text AS query_text,
     TO_TIMESTAMP(FIRST_LOAD_TIME, 'YYYY-MM-DD HH24:MI:SS') AS start_time,
     TO_TIMESTAMP(LAST_LOAD_TIME, 'YYYY-MM-DD HH24:MI:SS') + NUMTODSINTERVAL(ELAPSED_TIME / 1000, 'SECOND') AS end_time,
     PARSING_SCHEMA_NAME as user_name
   FROM gv$sql
-  WHERE sql_text LIKE 'CALL%%'
+  WHERE UPPER(sql_text) LIKE 'CALL%%'
   AND TO_TIMESTAMP(FIRST_LOAD_TIME, 'YYYY-MM-DD HH24:MI:SS') >= TO_TIMESTAMP('{start_date}', 'YYYY-MM-DD HH24:MI:SS')
  ),
  Q_HISTORY AS (SELECT
       sql_text AS query_text,
       CASE 
       	WHEN UPPER(SQL_TEXT) LIKE 'INSERT%' THEN 'INSERT'
       	WHEN UPPER(SQL_TEXT) LIKE 'SELECT%' THEN 'SELECT'
@@ -110,15 +110,15 @@
       TO_TIMESTAMP(FIRST_LOAD_TIME, 'YYYY-MM-DD HH24:MI:SS') AS start_time,
       TO_TIMESTAMP(LAST_LOAD_TIME, 'YYYY-MM-DD HH24:MI:SS') 
       + NUMTODSINTERVAL(ELAPSED_TIME / 1000, 'SECOND') AS end_time,
       PARSING_SCHEMA_NAME AS user_name,
       PARSING_SCHEMA_NAME AS SCHEMA_NAME,
       NULL AS DATABASE_NAME
     FROM gv$sql
-    WHERE sql_text NOT LIKE '%CALL%'
+    WHERE UPPER(sql_text) NOT LIKE '%CALL%'
       AND SQL_FULLTEXT NOT LIKE '/* {{"app": "OpenMetadata", %%}} */%%'
       AND SQL_FULLTEXT NOT LIKE '/* {{"app": "dbt", %%}} */%%'
       AND TO_TIMESTAMP(FIRST_LOAD_TIME, 'YYYY-MM-DD HH24:MI:SS') 
       >= TO_TIMESTAMP('{start_date}', 'YYYY-MM-DD HH24:MI:SS')
 )
 SELECT
   Q.QUERY_TYPE AS QUERY_TYPE,
@@ -148,16 +148,16 @@
             col.data_precision,
             col.data_scale,
             col.nullable,
             col.data_default,
             com.comments,
             col.virtual_column,
             {identity_cols}
-        FROM DBA_TAB_COLS{dblink} col
-        LEFT JOIN all_col_comments{dblink} com
+        FROM ALL_TAB_COLS{dblink} col
+        LEFT JOIN ALL_COL_COMMENTS{dblink} com
         ON col.table_name = com.table_name
         AND col.column_name = com.column_name
         AND col.owner = com.owner
         WHERE col.table_name = CAST(:table_name AS VARCHAR2(128))
         AND col.hidden_column = 'NO'
     """
 )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/query_parser.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Oracle query parsing module
 """
 from abc import ABC
+from typing import Optional
 
 from metadata.generated.schema.entity.services.connections.database.oracleConnection import (
     OracleConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -31,15 +32,17 @@
     """
     Oracle base for usage and lineage
     """
 
     filters: str
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: OracleConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, OracleConnection):
             raise InvalidSourceException(
                 f"Expected OracleConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/oracle/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/oracle/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/pinotdb/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/pinotdb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/pinotdb/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/query_parser.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,35 +4,43 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""PinotDb source module"""
+"""
+Trino usage module
+"""
+from abc import ABC
+from typing import Optional
 
-from metadata.generated.schema.entity.services.connections.database.pinotDBConnection import (
-    PinotDBConnection,
+from metadata.generated.schema.entity.services.connections.database.trinoConnection import (
+    TrinoConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
+from metadata.ingestion.source.database.query_parser_source import QueryParserSource
 
 
-class PinotdbSource(CommonDbSourceService):
+class TrinoQueryParserSource(QueryParserSource, ABC):
     """
-    Implements the necessary methods to extract
-    Database metadata from Oracle Source
+    Trino base for Usage and Lineage
     """
 
+    filters: str
+
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
+        """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: PinotDBConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, PinotDBConnection):
+        connection: TrinoConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, TrinoConnection):
             raise InvalidSourceException(
-                f"Expected PinotdbConnection, but got {connection}"
+                f"Expected TrinoConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mysql/connection.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,96 +8,69 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-
 from typing import Optional
 
 from sqlalchemy.engine import Engine
 
 from metadata.clients.azure_client import AzureClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
 from metadata.generated.schema.entity.services.connections.database.common.basicAuth import (
     BasicAuth,
 )
-from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
-    PostgresConnection,
-    SslMode,
+from metadata.generated.schema.entity.services.connections.database.mysqlConnection import (
+    MysqlConnection,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
     get_connection_url_common,
-    init_empty_connection_arguments,
-)
-from metadata.ingestion.connections.test_connections import test_connection_db_common
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.postgres.queries import (
-    POSTGRES_GET_DATABASE,
-    POSTGRES_TEST_GET_QUERIES,
-    POSTGRES_TEST_GET_TAGS,
 )
-from metadata.ingestion.source.database.postgres.utils import (
-    get_postgres_time_column_name,
+from metadata.ingestion.connections.test_connections import (
+    test_connection_db_schema_sources,
 )
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
 
 
-def get_connection(connection: PostgresConnection) -> Engine:
+def get_connection(connection: MysqlConnection) -> Engine:
     """
     Create connection
     """
-
     if hasattr(connection.authType, "azureConfig"):
         azure_client = AzureClient(connection.authType.azureConfig).create_client()
         if not connection.authType.azureConfig.scopes:
             raise ValueError(
-                "Azure Scopes are missing, please refer https://learn.microsoft.com/en-gb/azure/postgresql/flexible-server/how-to-configure-sign-in-azure-ad-authentication#retrieve-the-microsoft-entra-access-token and fetch the resource associated with it, for e.g. https://ossrdbms-aad.database.windows.net/.default"
+                "Azure Scopes are missing, please refer https://learn.microsoft.com/en-gb/azure/mysql/flexible-server/how-to-azure-ad#2---retrieve-microsoft-entra-access-token and fetch the resource associated with it, for e.g. https://ossrdbms-aad.database.windows.net/.default"
             )
         access_token_obj = azure_client.get_token(
             *connection.authType.azureConfig.scopes.split(",")
         )
         connection.authType = BasicAuth(password=access_token_obj.token)
-    if connection.sslMode:
-        if not connection.connectionArguments:
-            connection.connectionArguments = init_empty_connection_arguments()
-        connection.connectionArguments.__root__["sslmode"] = connection.sslMode.value
-        if connection.sslMode in (SslMode.verify_ca, SslMode.verify_full):
-            connection.connectionArguments.__root__[
-                "sslrootcert"
-            ] = connection.sslConfig.__root__.certificatePath
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url_common,
         get_connection_args_fn=get_connection_args_common,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: PostgresConnection,
+    service_connection: MysqlConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-
-    queries = {
-        "GetQueries": POSTGRES_TEST_GET_QUERIES.format(
-            time_column_name=get_postgres_time_column_name(engine=engine),
-        ),
-        "GetDatabases": POSTGRES_GET_DATABASE,
-        "GetTags": POSTGRES_TEST_GET_TAGS,
-    }
-    test_connection_db_common(
+    test_connection_db_schema_sources(
         metadata=metadata,
         engine=engine,
         service_connection=service_connection,
         automation_workflow=automation_workflow,
-        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/metadata.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,21 +11,23 @@
 """
 Postgres source module
 """
 import traceback
 from collections import namedtuple
 from typing import Iterable, Optional, Tuple
 
+from sqlalchemy import String as SqlAlchemyString
 from sqlalchemy import sql
 from sqlalchemy.dialects.postgresql.base import PGDialect, ischema_names
 from sqlalchemy.engine import Inspector
 
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.table import (
-    IntervalType,
+    PartitionColumnDetails,
+    PartitionIntervalTypes,
     TablePartition,
     TableType,
 )
 from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
     PostgresConnection,
 )
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
@@ -50,14 +52,15 @@
     POSTGRES_GET_TABLE_NAMES,
     POSTGRES_PARTITION_DETAILS,
 )
 from metadata.ingestion.source.database.postgres.utils import (
     get_column_info,
     get_columns,
     get_etable_owner,
+    get_foreign_keys,
     get_table_comment,
     get_table_owner,
     get_view_definition,
 )
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_database
 from metadata.utils.logger import ingestion_logger
@@ -70,17 +73,17 @@
 
 TableKey = namedtuple("TableKey", ["schema", "table_name"])
 
 logger = ingestion_logger()
 
 
 INTERVAL_TYPE_MAP = {
-    "list": IntervalType.COLUMN_VALUE.value,
-    "hash": IntervalType.COLUMN_VALUE.value,
-    "range": IntervalType.TIME_UNIT.value,
+    "list": PartitionIntervalTypes.COLUMN_VALUE,
+    "hash": PartitionIntervalTypes.COLUMN_VALUE,
+    "range": PartitionIntervalTypes.TIME_UNIT,
 }
 
 RELKIND_MAP = {
     "r": TableType.Regular,
     "p": TableType.Partitioned,
     "f": TableType.Foreign,
 }
@@ -91,22 +94,24 @@
 
 ischema_names.update(
     {
         "geometry": GEOMETRY,
         "point": POINT,
         "polygon": POLYGON,
         "box": create_sqlalchemy_type("BOX"),
+        "bpchar": SqlAlchemyString,
         "circle": create_sqlalchemy_type("CIRCLE"),
         "line": create_sqlalchemy_type("LINE"),
         "lseg": create_sqlalchemy_type("LSEG"),
         "path": create_sqlalchemy_type("PATH"),
         "pg_lsn": create_sqlalchemy_type("PG_LSN"),
         "pg_snapshot": create_sqlalchemy_type("PG_SNAPSHOT"),
         "tsquery": create_sqlalchemy_type("TSQUERY"),
         "txid_snapshot": create_sqlalchemy_type("TXID_SNAPSHOT"),
+        "xid": SqlAlchemyString,
         "xml": create_sqlalchemy_type("XML"),
     }
 )
 
 
 PGDialect.get_all_table_comments = get_all_table_comments
 PGDialect.get_table_comment = get_table_comment
@@ -117,23 +122,27 @@
 
 PGDialect.get_all_table_owners = get_all_table_owners
 PGDialect.get_table_owner = get_table_owner
 PGDialect.ischema_names = ischema_names
 
 Inspector.get_table_owner = get_etable_owner
 
+PGDialect.get_foreign_keys = get_foreign_keys
+
 
 class PostgresSource(CommonDbSourceService, MultiDBSource):
     """
     Implements the necessary methods to extract
     Database metadata from Postgres Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: PostgresConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, PostgresConnection):
             raise InvalidSourceException(
                 f"Expected PostgresConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -171,23 +180,25 @@
             self.set_inspector(database_name=configured_db)
             yield configured_db
         else:
             for new_database in self.get_database_names_raw():
                 database_fqn = fqn.build(
                     self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
+                    service_name=self.context.get().database_service,
                     database_name=new_database,
                 )
 
                 if filter_by_database(
                     self.source_config.databaseFilterPattern,
-                    database_fqn
-                    if self.source_config.useFqnForFiltering
-                    else new_database,
+                    (
+                        database_fqn
+                        if self.source_config.useFqnForFiltering
+                        else new_database
+                    ),
                 ):
                     self.status.filter(database_fqn, "Database Filtered Out")
                     continue
 
                 try:
                     self.set_inspector(database_name=new_database)
                     yield new_database
@@ -199,43 +210,51 @@
 
     def get_table_partition_details(
         self, table_name: str, schema_name: str, inspector
     ) -> Tuple[bool, TablePartition]:
         result = self.engine.execute(
             POSTGRES_PARTITION_DETAILS, table_name=table_name, schema_name=schema_name
         ).all()
+
         if result:
             partition_details = TablePartition(
-                intervalType=INTERVAL_TYPE_MAP.get(
-                    result[0].partition_strategy, IntervalType.COLUMN_VALUE.value
-                ),
-                columns=[row.column_name for row in result if row.column_name],
+                columns=[
+                    PartitionColumnDetails(
+                        columnName=row.column_name,
+                        intervalType=INTERVAL_TYPE_MAP.get(
+                            row.partition_strategy, PartitionIntervalTypes.COLUMN_VALUE
+                        ),
+                        interval=None,
+                    )
+                    for row in result
+                    if row.column_name
+                ]
             )
             return True, partition_details
         return False, None
 
     def yield_tag(
         self, schema_name: str
     ) -> Iterable[Either[OMetaTagAndClassification]]:
         """
         Fetch Tags
         """
         try:
             result = self.engine.execute(
                 POSTGRES_GET_ALL_TABLE_PG_POLICY.format(
-                    database_name=self.context.database,
+                    database_name=self.context.get().database,
                     schema_name=schema_name,
                 )
             ).all()
             for res in result:
                 row = list(res)
                 fqn_elements = [name for name in row[2:] if name]
                 yield from get_ometa_tag_and_classification(
                     tag_fqn=fqn._build(  # pylint: disable=protected-access
-                        self.context.database_service, *fqn_elements
+                        self.context.get().database_service, *fqn_elements
                     ),
                     tags=[row[1]],
                     classification_name=self.service_connection.classificationName,
                     tag_description="Postgres Tag Value",
                     classification_description="Postgres Tag Name",
                 )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/pgspider/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/pgspider/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/pgspider/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/pgspider/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/queries.py`

 * *Files 3% similar despite different names*

```diff
@@ -72,15 +72,15 @@
         and col.table_name = par.relname
         and ordinal_position = pt.column_index
      where par.relname=%(table_name)s and  par.relnamespace::regnamespace::text=%(schema_name)s
     """
 )
 
 POSTGRES_GET_ALL_TABLE_PG_POLICY = """
-SELECT object_id, polname, table_catalog , table_schema ,table_name  
+SELECT object_id, polname, table_catalog, table_schema, table_name  
 FROM information_schema.tables AS it
 JOIN (SELECT pc.oid as object_id, pc.relname, pp.*
       FROM pg_policy AS pp
       JOIN pg_class AS pc ON pp.polrelid = pc.oid
       JOIN pg_namespace as pn ON pc.relnamespace = pn.oid) AS ppr ON it.table_name = ppr.relname
 WHERE it.table_schema='{schema_name}' AND it.table_catalog='{database_name}';
 """
@@ -184,7 +184,23 @@
         AND a.attnum > 0 AND NOT a.attisdropped
         ORDER BY a.attnum
     """
 
 POSTGRES_GET_SERVER_VERSION = """
 show server_version
 """
+
+POSTGRES_FETCH_FK = """
+    SELECT r.conname,
+        pg_catalog.pg_get_constraintdef(r.oid, true) as condef,
+        n.nspname as conschema,
+        d.datname AS con_db_name
+    FROM  pg_catalog.pg_constraint r,
+        pg_namespace n,
+        pg_class c
+    JOIN pg_database d ON d.datname = current_database()
+    WHERE r.conrelid = :table AND
+        r.contype = 'f' AND
+        c.oid = confrelid AND
+        n.oid = c.relnamespace
+    ORDER BY 1
+"""
```

#### html2text {}

```diff
@@ -25,15 +25,15 @@
 partnatts, case partstrat when 'l' then 'list' when 'h' then 'hash' when 'r'
 then 'range' end as partition_strategy, unnest(partattrs) column_index from
 pg_partitioned_table) pt join pg_class par on par.oid = pt.partrelid left join
 information_schema.columns col on col.table_schema = par.relnamespace::
 regnamespace::text and col.table_name = par.relname and ordinal_position =
 pt.column_index where par.relname=%(table_name)s and par.relnamespace::
 regnamespace::text=%(schema_name)s """ ) POSTGRES_GET_ALL_TABLE_PG_POLICY = """
-SELECT object_id, polname, table_catalog , table_schema ,table_name FROM
+SELECT object_id, polname, table_catalog, table_schema, table_name FROM
 information_schema.tables AS it JOIN (SELECT pc.oid as object_id, pc.relname,
 pp.* FROM pg_policy AS pp JOIN pg_class AS pc ON pp.polrelid = pc.oid JOIN
 pg_namespace as pn ON pc.relnamespace = pn.oid) AS ppr ON it.table_name =
 ppr.relname WHERE it.table_schema='{schema_name}' AND it.table_catalog='
 {database_name}'; """ POSTGRES_TABLE_COMMENTS = """ SELECT n.nspname as schema,
 c.relname as table_name, pgd.description as table_comment FROM
 pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace n ON n.oid =
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/query_parser.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Postgres Query parser module
 """
 import traceback
 from abc import ABC
-from typing import Iterable
+from typing import Iterable, Optional
 
 from sqlalchemy.engine.base import Engine
 
 from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
     PostgresConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
@@ -49,15 +49,17 @@
         super().__init__(config, metadata)
         # Postgres does not allow retrieval of data older than 7 days
         # Update start and end based on this
         duration = min(self.source_config.queryLogDuration, 6)
         self.start, self.end = get_start_and_end(duration)
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: PostgresConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, PostgresConnection):
             raise InvalidSourceException(
                 f"Expected PostgresConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/postgres/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/postgres/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/metadata.py`

 * *Files 19% similar despite different names*

```diff
@@ -4,419 +4,334 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-#  pylint: disable=protected-access
-
 """
-Postgres SQLAlchemy util methods
+Vertica source implementation.
 """
 import re
 import traceback
-from typing import Dict, Optional, Tuple
+from textwrap import dedent
+from typing import Iterable, Optional
 
-from packaging import version
 from sqlalchemy import sql, util
-from sqlalchemy.dialects.postgresql.base import ENUM
 from sqlalchemy.engine import reflection
 from sqlalchemy.sql import sqltypes
+from sqlalchemy_vertica.base import VerticaDialect, ischema_names
 
-from metadata.ingestion.source.database.postgres.queries import (
-    POSTGRES_COL_IDENTITY,
-    POSTGRES_GET_SERVER_VERSION,
-    POSTGRES_SQL_COLUMNS,
-    POSTGRES_TABLE_COMMENTS,
-    POSTGRES_TABLE_OWNERS,
-    POSTGRES_VIEW_DEFINITIONS,
+from metadata.generated.schema.entity.data.database import Database
+from metadata.generated.schema.entity.services.connections.database.verticaConnection import (
+    VerticaConnection,
+)
+from metadata.generated.schema.metadataIngestion.workflow import (
+    Source as WorkflowSource,
+)
+from metadata.ingestion.api.steps import InvalidSourceException
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.database.column_type_parser import create_sqlalchemy_type
+from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
+from metadata.ingestion.source.database.multi_db_source import MultiDBSource
+from metadata.ingestion.source.database.vertica.queries import (
+    VERTICA_GET_COLUMNS,
+    VERTICA_GET_PRIMARY_KEYS,
+    VERTICA_LIST_DATABASES,
+    VERTICA_SCHEMA_COMMENTS,
+    VERTICA_TABLE_COMMENTS,
+    VERTICA_VIEW_DEFINITION,
 )
-from metadata.utils.logger import utils_logger
+from metadata.utils import fqn
+from metadata.utils.filters import filter_by_database
+from metadata.utils.logger import ingestion_logger
 from metadata.utils.sqlalchemy_utils import (
+    get_all_table_comments,
+    get_schema_descriptions,
     get_table_comment_wrapper,
-    get_table_owner_wrapper,
-    get_view_definition_wrapper,
 )
 
-logger = utils_logger()
-
-OLD_POSTGRES_VERSION = "13.0"
-
-
-def get_etable_owner(
-    self, connection, table_name=None, schema=None
-):  # pylint: disable=unused-argument
-    """Return all owners.
-
-    :param schema: Optional, retrieve names from a non-default schema.
-        For special quoting, use :class:`.quoted_name`.
-
-    """
-
-    with self._operation_context() as conn:
-        return self.dialect.get_table_owner(
-            connection=conn,
-            query=POSTGRES_TABLE_OWNERS,
-            table_name=table_name,
-            schema=schema,
-        )
-
-
-@reflection.cache
-def get_table_owner(
-    self, connection, table_name, schema=None, **kw
-):  # pylint: disable=unused-argument
-    return get_table_owner_wrapper(
-        self,
-        connection=connection,
-        query=POSTGRES_TABLE_OWNERS,
-        table_name=table_name,
-        schema=schema,
-    )
+logger = ingestion_logger()
 
-
-@reflection.cache
-def get_table_comment(
-    self, connection, table_name, schema=None, **kw
-):  # pylint: disable=unused-argument
-    return get_table_comment_wrapper(
-        self,
-        connection,
-        table_name=table_name,
-        schema=schema,
-        query=POSTGRES_TABLE_COMMENTS,
-    )
+ischema_names.update(
+    {
+        "UUID": create_sqlalchemy_type("UUID"),
+        "GEOGRAPHY": create_sqlalchemy_type("GEOGRAPHY"),
+    }
+)
 
 
 @reflection.cache
-def get_columns(  # pylint: disable=too-many-locals
+def get_columns(
     self, connection, table_name, schema=None, **kw
-):
+):  # pylint: disable=too-many-locals,unused-argument
     """
-    Overriding the dialect method to add raw_data_type in response
+    Method to handle column details
     """
-
-    table_oid = self.get_table_oid(
-        connection, table_name, schema, info_cache=kw.get("info_cache")
-    )
-
-    generated = (
-        "a.attgenerated as generated"
-        if self.server_version_info >= (12,)
-        else "NULL as generated"
-    )
-    if self.server_version_info >= (10,):
-        # a.attidentity != '' is required or it will reflect also
-        # serial columns as identity.
-        identity = POSTGRES_COL_IDENTITY
+    if schema is not None:
+        schema_condition = f"lower(table_schema) = '{schema.lower()}'"
     else:
-        identity = "NULL as identity_options"
+        schema_condition = "1"
 
-    sql_col_query = POSTGRES_SQL_COLUMNS.format(
-        generated=generated,
-        identity=identity,
-    )
-    sql_col_query = (
-        sql.text(sql_col_query)
-        .bindparams(sql.bindparam("table_oid", type_=sqltypes.Integer))
-        .columns(attname=sqltypes.Unicode, default=sqltypes.Unicode)
+    sql_query = sql.text(
+        dedent(
+            VERTICA_GET_COLUMNS.format(
+                table=table_name.lower(), schema_condition=schema_condition
+            )
+        )
     )
-    conn = connection.execute(sql_col_query, {"table_oid": table_oid})
-    rows = conn.fetchall()
 
-    # dictionary with (name, ) if default search path or (schema, name)
-    # as keys
-    domains = self._load_domains(connection)
-
-    # dictionary with (name, ) if default search path or (schema, name)
-    # as keys
-    enums = dict(
-        ((rec["name"],), rec) if rec["visible"] else ((rec["schema"], rec["name"]), rec)
-        for rec in self._load_enums(connection, schema="*")
+    spk = sql.text(
+        dedent(
+            VERTICA_GET_PRIMARY_KEYS.format(
+                table=table_name.lower(), schema_condition=schema_condition
+            )
+        )
     )
 
-    # format columns
-    columns = []
+    pk_columns = [x[0] for x in connection.execute(spk)]
+    columns = {}
+    for row in connection.execute(sql_query):
+        name = row.column_name
+        dtype = row.data_type.lower()
+        primary_key = name in pk_columns
+        default = row.column_default
+        nullable = row.is_nullable
+        comment = row.comment
 
-    for (
-        name,
-        format_type,
-        default_,
-        notnull,
-        table_oid,
-        comment,
-        generated,
-        identity,
-    ) in rows:
-        column_info = self._get_column_info(
+        column_info = self._get_column_info(  # pylint: disable=protected-access
             name,
-            format_type,
-            default_,
-            notnull,
-            domains,
-            enums,
+            dtype,
+            default,
+            nullable,
             schema,
             comment,
-            generated,
-            identity,
         )
-        column_info["system_data_type"] = format_type
-        columns.append(column_info)
-    return columns
-
-
-def _get_numeric_args(charlen):
-    if charlen:
-        prec, scale = charlen.split(",")
-        return (int(prec), int(scale))
-    return ()
+        column_info.update({"primary_key": primary_key})
+        if columns.get(name) is None or comment:
+            columns[name] = column_info
+    return columns.values()
 
 
-def _get_interval_args(charlen, attype, kwargs: Dict):
-    field_match = re.match(r"interval (.+)", attype, re.I)
-    if charlen:
-        kwargs["precision"] = int(charlen)
-    if field_match:
-        kwargs["fields"] = field_match.group(1)
-    attype = "interval"
-    return (), attype, kwargs
-
+def _get_column_info(  # pylint: disable=too-many-locals,too-many-branches,too-many-statements
+    self,
+    name,
+    format_type,
+    default,
+    nullable,
+    schema,
+    comment,
+):
+    # strip (*) from character varying(5), timestamp(5)
+    # with time zone, geometry(POLYGON), etc.
+    attype = re.sub(r"\(.*\)", "", format_type)
 
-def _get_bit_var_args(charlen, kwargs):
-    kwargs["varying"] = True
+    charlen = re.search(r"\(([\d,]+)\)", format_type)
     if charlen:
-        return (int(charlen),), kwargs
-
-    return (), kwargs
-
+        charlen = charlen.group(1)
+    args = re.search(r"\((.*)\)", format_type)
+    if args and args.group(1):
+        args = tuple(re.split(r"\s*,\s*", args.group(1)))
+    else:
+        args = ()
+    kwargs = {}
 
-def get_column_args(
-    charlen: str, args: Tuple, kwargs: Dict, attype: str
-) -> Tuple[Tuple, Dict]:
-    """
-    Method to determine the args and kwargs
-    """
     if attype == "numeric":
-        args = _get_numeric_args(charlen)
-    elif attype == "double precision":
-        args = (53,)
-    elif attype == "integer":
+        if charlen:
+            prec, scale = charlen.split(",")
+            args = (int(prec), int(scale))
+        else:
+            args = ()
+    elif attype == "integer" or attype.startswith("geography"):
         args = ()
-    elif attype in ("timestamp with time zone", "time with time zone"):
+    elif attype in ("timestamptz", "timetz"):
         kwargs["timezone"] = True
-        if charlen:
-            kwargs["precision"] = int(charlen)
         args = ()
     elif attype in (
-        "timestamp without time zone",
-        "time without time zone",
+        "timestamp",
         "time",
     ):
         kwargs["timezone"] = False
+        args = ()
+    elif attype.startswith("interval"):
+        field_match = re.match(r"interval (.+)", attype, re.I)
         if charlen:
             kwargs["precision"] = int(charlen)
+        if field_match:
+            kwargs["fields"] = field_match.group(1)
+        attype = "interval"
         args = ()
-    elif attype == "bit varying":
-        args, kwargs = _get_bit_var_args(charlen, kwargs)
-    elif attype == "geometry":
-        args = ()
-    elif attype.startswith("interval"):
-        args, attype, kwargs = _get_interval_args(charlen, attype, kwargs)
     elif charlen:
         args = (int(charlen),)
+    if attype.upper() in self.ischema_names:
+        coltype = self.ischema_names[attype.upper()]
+    else:
+        coltype = None
 
-    return args, kwargs, attype
-
-
-def get_column_default(coltype, schema, default, generated):
-    """
-    Method to determine the default of column
-    """
-    autoincrement = False
-    # If a zero byte or blank string depending on driver (is also absent
-    # for older PG versions), then not a generated column. Otherwise, s =
-    # stored. (Other values might be added in the future.)
-    if generated not in (None, "", b"\x00"):
-        computed = {"sqltext": default, "persisted": generated in ("s", b"s")}
-        default = None
+    if coltype:
+        coltype = coltype(*args, **kwargs) if callable(coltype) else coltype
     else:
-        computed = None
+        util.warn(f"Did not recognize type '{attype}' of column '{name}'")
+        coltype = sqltypes.NULLTYPE
+    # adjust the default value
+    autoincrement = False
     if default is not None:
         match = re.search(r"""(nextval\(')([^']+)('.*$)""", default)
         if match is not None:
-            if issubclass(coltype._type_affinity, sqltypes.Integer):
+            if issubclass(
+                coltype._type_affinity,  # pylint: disable=protected-access
+                sqltypes.Integer,
+            ):
                 autoincrement = True
             # the default is related to a Sequence
             sch = schema
             if "." not in match.group(2) and sch is not None:
                 # unconditionally quote the schema name.  this could
                 # later be enhanced to obey quoting rules /
                 # "quote schema"
                 default = (
                     match.group(1)
                     + (f'"{sch}"')
                     + "."
                     + match.group(2)
                     + match.group(3)
                 )
-    return default, autoincrement, computed
 
-
-def _handle_array_type(attype):
-    return (
-        # strip '[]' from integer[], etc.
-        re.sub(r"\[\]$", "", attype),
-        attype.endswith("[]"),
-    )
-
-
-# pylint: disable=too-many-statements,too-many-branches,too-many-locals,too-many-arguments
-def get_column_info(
-    self,
-    name,
-    format_type,
-    default,
-    notnull,
-    domains,
-    enums,
-    schema,
-    comment,
-    generated,
-    identity,
-):
-    """
-    Method to return column info
-    """
-
-    if format_type is None:
-        no_format_type = True
-        attype = format_type = "no format_type()"
-        is_array = False
-    else:
-        no_format_type = False
-
-        # strip (*) from character varying(5), timestamp(5)
-        # with time zone, geometry(POLYGON), etc.
-        attype = re.sub(r"\(.*\)", "", format_type)
-
-        # strip '[]' from integer[], etc. and check if an array
-        attype, is_array = _handle_array_type(attype)
-
-    # strip quotes from case sensitive enum or domain names
-    enum_or_domain_key = tuple(util.quoted_token_parser(attype))
-
-    nullable = not notnull
-
-    charlen = re.search(r"\(([\d,]+)\)", format_type)
-    if charlen:
-        charlen = charlen.group(1)
-    args = re.search(r"\((.*)\)", format_type)
-    if args and args.group(1):
-        args = tuple(re.split(r"\s*,\s*", args.group(1)))
-    else:
-        args = ()
-    kwargs = {}
-
-    args, kwargs, attype = get_column_args(charlen, args, kwargs, attype)
-
-    while True:
-        # looping here to suit nested domains
-        if attype in self.ischema_names:
-            coltype = self.ischema_names[attype]
-            break
-        if enum_or_domain_key in enums:
-            enum = enums[enum_or_domain_key]
-            coltype = ENUM
-            kwargs["name"] = enum["name"]
-            if not enum["visible"]:
-                kwargs["schema"] = enum["schema"]
-            args = tuple(enum["labels"])
-            break
-        if enum_or_domain_key in domains:
-            domain = domains[enum_or_domain_key]
-            attype = domain["attype"]
-            attype, is_array = _handle_array_type(attype)
-            # strip quotes from case sensitive enum or domain names
-            enum_or_domain_key = tuple(util.quoted_token_parser(attype))
-            # A table can't override a not null on the domain,
-            # but can override nullable
-            nullable = nullable and domain["nullable"]
-            if domain["default"] and not default:
-                # It can, however, override the default
-                # value, but can't set it to null.
-                default = domain["default"]
-            continue
-        coltype = None
-        break
-
-    if coltype:
-        coltype = coltype(*args, **kwargs)
-        if is_array:
-            coltype = self.ischema_names["_array"](coltype)
-    elif no_format_type:
-        util.warn(f"PostgreSQL format_type() returned NULL for column '{name}'")
-        coltype = sqltypes.NULLTYPE
-    else:
-        util.warn(f"Did not recognize type '{attype}' of column '{name}'")
-        coltype = sqltypes.NULLTYPE
-
-    default, autoincrement, computed = get_column_default(
-        coltype=coltype, schema=schema, default=default, generated=generated
-    )
     column_info = {
         "name": name,
         "type": coltype,
         "nullable": nullable,
+        "system_data_type": format_type,
         "default": default,
-        "autoincrement": autoincrement or identity is not None,
+        "autoincrement": autoincrement,
         "comment": comment,
     }
-    if computed is not None:
-        column_info["computed"] = computed
-    if identity is not None:
-        column_info["identity"] = identity
     return column_info
 
 
 @reflection.cache
 def get_view_definition(
-    self, connection, table_name, schema=None, **kw
-):  # pylint: disable=unused-argument
-    return get_view_definition_wrapper(
+    self, connection, view_name, schema=None, **kw
+):  # pylint: disable=unused-argument,unused-argument
+    """
+    If we create a view as:
+        CREATE VIEW vendor_dimension_v AS
+        SELECT vendor_key, vendor_name
+        FROM public.vendor_dimension_new;
+    Then the VIEW_DEFINITION statement from V_CATALOG.VIEWS
+    will only contain the SELECT query:
+        SELECT vendor_key, vendor_name
+        FROM public.vendor_dimension_new;
+    We will add the `CREATE VIEW XYZ AS` piece
+    to ensure that the column lineage and target table
+    can be properly inferred.
+    """
+    if schema is not None:
+        schema_condition = f"lower(table_schema) = '{schema.lower()}'"
+    else:
+        schema_condition = "1"
+
+    sql_query = sql.text(
+        dedent(
+            VERTICA_VIEW_DEFINITION.format(
+                view_name=view_name.lower(), schema_condition=schema_condition
+            )
+        )
+    )
+    rows = list(connection.execute(sql_query))
+    if len(rows) >= 1:
+        return f"CREATE VIEW {view_name} AS {rows[0][0]}"
+    return None
+
+
+@reflection.cache
+def get_table_comment(
+    self, connection, table_name, schema=None, **kw  # pylint: disable=unused-argument
+):
+    return get_table_comment_wrapper(
         self,
         connection,
         table_name=table_name,
         schema=schema,
-        query=POSTGRES_VIEW_DEFINITIONS,
+        query=VERTICA_TABLE_COMMENTS,
     )
 
 
-def get_postgres_version(engine) -> Optional[str]:
-    """
-    return the postgres version in major.minor.patch format
-    """
-    try:
-        results = engine.execute(POSTGRES_GET_SERVER_VERSION)
-        for res in results:
-            version_string = str(res[0])
-            opening_parenthesis_index = version_string.find("(")
-            if opening_parenthesis_index != -1:
-                return version_string[:opening_parenthesis_index].strip()
-            return version_string
-    except Exception as err:
-        logger.warning(f"Unable to fetch the Postgres Version - {err}")
-        logger.debug(traceback.format_exc())
-    return None
+VerticaDialect.get_columns = get_columns
+VerticaDialect._get_column_info = _get_column_info  # pylint: disable=protected-access
+VerticaDialect.get_view_definition = get_view_definition
+VerticaDialect.get_all_table_comments = get_all_table_comments
+VerticaDialect.get_table_comment = get_table_comment
 
 
-def get_postgres_time_column_name(engine) -> str:
+class VerticaSource(CommonDbSourceService, MultiDBSource):
     """
-    Return the correct column name for the time column based on postgres version
+    Implements the necessary methods to extract
+    Database metadata from Vertica Source
     """
-    time_column_name = "total_exec_time"
-    postgres_version = get_postgres_version(engine)
-    if postgres_version and version.parse(postgres_version) < version.parse(
-        OLD_POSTGRES_VERSION
+
+    def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
+        super().__init__(config, metadata)
+        self.schema_desc_map = {}
+
+    @classmethod
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
     ):
-        time_column_name = "total_time"
-    return time_column_name
+        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection: VerticaConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, VerticaConnection):
+            raise InvalidSourceException(
+                f"Expected VerticaConnection, but got {connection}"
+            )
+        return cls(config, metadata)
+
+    def get_schema_description(self, schema_name: str) -> Optional[str]:
+        """
+        Method to fetch the schema description
+        """
+        return self.schema_desc_map.get(schema_name)
+
+    def set_schema_description_map(self) -> None:
+        self.schema_desc_map = get_schema_descriptions(
+            self.engine, VERTICA_SCHEMA_COMMENTS
+        )
+
+    def get_configured_database(self) -> Optional[str]:
+        return self.service_connection.database
+
+    def get_database_names_raw(self) -> Iterable[str]:
+        yield from self._execute_database_query(VERTICA_LIST_DATABASES)
+
+    def get_database_names(self) -> Iterable[str]:
+        configured_db = self.config.serviceConnection.__root__.config.database
+        if configured_db:
+            self.set_inspector(database_name=configured_db)
+            self.set_schema_description_map()
+            yield configured_db
+        else:
+            for new_database in self.get_database_names_raw():
+                database_fqn = fqn.build(
+                    self.metadata,
+                    entity_type=Database,
+                    service_name=self.context.get().database_service,
+                    database_name=new_database,
+                )
+
+                if filter_by_database(
+                    self.source_config.databaseFilterPattern,
+                    database_fqn
+                    if self.source_config.useFqnForFiltering
+                    else new_database,
+                ):
+                    self.status.filter(database_fqn, "Database Filtered Out")
+                    continue
+
+                try:
+                    self.set_inspector(database_name=new_database)
+                    self.set_schema_description_map()
+                    yield new_database
+                except Exception as exc:
+                    logger.debug(traceback.format_exc())
+                    logger.error(
+                        f"Error trying to connect to database {new_database}: {exc}"
+                    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/presto/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/connection.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,95 +8,112 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-from functools import partial
+from functools import partial, singledispatch
 from typing import Optional
-from urllib.parse import quote_plus
 
+from airflow import settings
 from sqlalchemy.engine import Engine
-from sqlalchemy.inspection import inspect
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.prestoConnection import (
-    PrestoConnection,
+from metadata.generated.schema.entity.services.connections.database.mysqlConnection import (
+    MysqlConnection,
 )
-from metadata.ingestion.connections.builders import (
-    create_generic_db_connection,
-    get_connection_args_common,
+from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
+    PostgresConnection,
+)
+from metadata.generated.schema.entity.services.connections.database.sqliteConnection import (
+    SQLiteConnection,
+)
+from metadata.generated.schema.entity.services.connections.pipeline.airflowConnection import (
+    AirflowConnection,
+)
+from metadata.generated.schema.entity.services.connections.pipeline.backendConnection import (
+    BackendConnection,
 )
 from metadata.ingestion.connections.test_connections import (
-    execute_inspector_func,
+    SourceConnectionException,
     test_connection_engine_step,
     test_connection_steps,
-    test_query,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.presto.queries import PRESTO_SHOW_CATALOGS
 
 
-def get_connection_url(connection: PrestoConnection) -> str:
-    url = f"{connection.scheme.value}://"
-    if connection.username:
-        url += f"{quote_plus(connection.username)}"
-        if connection.password:
-            url += f":{quote_plus(connection.password.get_secret_value())}"
-        url += "@"
-    url += f"{connection.hostPort}"
-    if connection.catalog:
-        url += f"/{connection.catalog}"
-    if connection.databaseSchema:
-        url += f"?schema={quote_plus(connection.databaseSchema)}"
-    return url
+# Only import when needed
+# pylint: disable=import-outside-toplevel
+@singledispatch
+def _get_connection(airflow_connection) -> Engine:
+    """
+    Internal call for Airflow connection build
+    """
+    raise NotImplementedError(f"Not support connection type {airflow_connection}")
+
+
+@_get_connection.register
+def _(_: BackendConnection) -> Engine:
+    with settings.Session() as session:
+        return session.get_bind()
+
+
+@_get_connection.register
+def _(airflow_connection: MysqlConnection) -> Engine:
+    from metadata.ingestion.source.database.mysql.connection import (
+        get_connection as get_mysql_connection,
+    )
+
+    return get_mysql_connection(airflow_connection)
+
+
+@_get_connection.register
+def _(airflow_connection: PostgresConnection) -> Engine:
+    from metadata.ingestion.source.database.postgres.connection import (
+        get_connection as get_postgres_connection,
+    )
+
+    return get_postgres_connection(airflow_connection)
 
 
-def get_connection(connection: PrestoConnection) -> Engine:
+@_get_connection.register
+def _(airflow_connection: SQLiteConnection) -> Engine:
+    from metadata.ingestion.source.database.sqlite.connection import (
+        get_connection as get_sqlite_connection,
+    )
+
+    return get_sqlite_connection(airflow_connection)
+
+
+def get_connection(connection: AirflowConnection) -> Engine:
     """
     Create connection
     """
-    return create_generic_db_connection(
-        connection=connection,
-        get_connection_url_fn=get_connection_url,
-        get_connection_args_fn=get_connection_args_common,
-    )
+    try:
+        return _get_connection(connection.connection)
+    except Exception as exc:
+        msg = f"Unknown error connecting with {connection}: {exc}."
+        raise SourceConnectionException(msg) from exc
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: PrestoConnection,
+    service_connection: AirflowConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    def custom_executor_for_table():
-        inspector = inspect(engine)
-        schema_name = inspector.get_schema_names()
-        if schema_name:
-            for schema in schema_name:
-                table_name = inspector.get_table_names(schema)
-                return table_name
-        return None
-
-    test_fn = {
-        "CheckAccess": partial(test_connection_engine_step, engine),
-        "GetDatabases": partial(
-            test_query, engine=engine, statement=PRESTO_SHOW_CATALOGS
-        ),
-        "GetSchemas": partial(execute_inspector_func, engine, "get_schema_names"),
-        "GetTables": custom_executor_for_table,
-    }
+    test_fn = {"CheckAccess": partial(test_connection_engine_step, engine)}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/presto/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/presto/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,18 +11,18 @@
 """
 Presto source module
 """
 
 import re
 import traceback
 from copy import deepcopy
-from typing import Iterable
+from typing import Iterable, Optional
 
 from pyhive.sqlalchemy_presto import PrestoDialect, _type_map
-from sqlalchemy import inspect, types, util
+from sqlalchemy import types, util
 from sqlalchemy.engine import reflection
 
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.services.connections.database.prestoConnection import (
     PrestoConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
@@ -114,15 +114,17 @@
 
 class PrestoSource(CommonDbSourceService):
     """
     Presto does not support querying by table type: Getting views is not supported.
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: PrestoConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, PrestoConnection):
             raise InvalidSourceException(
                 f"Expected PrestoConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -134,30 +136,31 @@
         :param database_name: new database to set
         """
         logger.info(f"Ingesting from catalog: {database_name}")
 
         new_service_connection = deepcopy(self.service_connection)
         new_service_connection.catalog = database_name
         self.engine = get_connection(new_service_connection)
-        self.inspector = inspect(self.engine)
+        self._connection_map = {}  # Lazy init as well
+        self._inspector_map = {}
 
     def get_database_names(self) -> Iterable[str]:
         configured_catalog = self.service_connection.catalog
         if configured_catalog:
             self.set_inspector(database_name=configured_catalog)
             yield configured_catalog
         else:
             results = self.connection.execute("SHOW CATALOGS")
             for res in results:
                 if res:
                     new_catalog = res[0]
                     database_fqn = fqn.build(
                         self.metadata,
                         entity_type=Database,
-                        service_name=self.context.database_service,
+                        service_name=self.context.get().database_service,
                         database_name=new_catalog,
                     )
                     if filter_by_database(
                         self.source_config.databaseFilterPattern,
                         database_fqn
                         if self.source_config.useFqnForFiltering
                         else new_catalog,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/presto/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/presto/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/query/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sqlite/metadata.py`

 * *Files 23% similar despite different names*

```diff
@@ -4,27 +4,43 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Common Query Log Connector
+Sqlite source implementation.
+Useful for testing!
 """
+
+from typing import Optional
+
+from metadata.generated.schema.entity.services.connections.database.sqliteConnection import (
+    SQLiteConnection,
+)
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
+from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.lineage_source import LineageSource
+from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
+
 
+class SqliteSource(CommonDbSourceService):
+    """
+    Implements the necessary methods to extract
+    Database metadata from Sqlite Source
+    """
 
-class QueryLogLineageSource(LineageSource):
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection = config.serviceConnection.__root__.config
+        if not isinstance(connection, SQLiteConnection):
+            raise InvalidSourceException(
+                f"Expected SQLiteConnection, but got {connection}"
+            )
         return cls(config, metadata)
-
-    def prepare(self):
-        """
-        Nothing to prepare for Query Log Lineage
-        """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/query/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/query/usage.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,29 +8,32 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Common Query Log Connector
 """
 from datetime import datetime
+from typing import Optional
 
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.database.usage_source import UsageSource
 
 
 class QueryLogUsageSource(UsageSource):
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.analysis_date = datetime.today().strftime("%Y-%m-%d %H:%M:%S")
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         return cls(config, metadata)
 
     def prepare(self):
         """
         Nothing to prepare for Query Log Usage
         """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/query_parser_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/query_parser_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/connection.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,74 +8,67 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
+from functools import partial
 from typing import Optional
 
-from sqlalchemy.engine import Engine
-
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.redshiftConnection import (
-    RedshiftConnection,
-    SslMode,
+from metadata.generated.schema.entity.services.connections.metadata.amundsenConnection import (
+    AmundsenConnection,
 )
-from metadata.ingestion.connections.builders import (
-    create_generic_db_connection,
-    get_connection_args_common,
-    get_connection_url_common,
-    init_empty_connection_arguments,
+from metadata.ingestion.connections.test_connections import (
+    SourceConnectionException,
+    test_connection_steps,
 )
-from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.redshift.queries import (
-    REDSHIFT_GET_DATABASE_NAMES,
-    REDSHIFT_TEST_GET_QUERIES,
-    REDSHIFT_TEST_PARTITION_DETAILS,
+from metadata.ingestion.source.metadata.amundsen.client import Neo4JConfig, Neo4jHelper
+from metadata.ingestion.source.metadata.amundsen.queries import (
+    NEO4J_AMUNDSEN_USER_QUERY,
 )
 
 
-def get_connection(connection: RedshiftConnection) -> Engine:
+def get_connection(connection: AmundsenConnection) -> Neo4jHelper:
     """
     Create connection
     """
-    if connection.sslMode:
-        if not connection.connectionArguments:
-            connection.connectionArguments = init_empty_connection_arguments()
-        connection.connectionArguments.__root__["sslmode"] = connection.sslMode.value
-        if connection.sslMode in (SslMode.verify_ca, SslMode.verify_full):
-            connection.connectionArguments.__root__[
-                "sslrootcert"
-            ] = connection.sslConfig.__root__.certificatePath
-    return create_generic_db_connection(
-        connection=connection,
-        get_connection_url_fn=get_connection_url_common,
-        get_connection_args_fn=get_connection_args_common,
-    )
+    try:
+        neo4j_config = Neo4JConfig(
+            username=connection.username,
+            password=connection.password.get_secret_value(),
+            neo4j_url=connection.hostPort,
+            max_connection_life_time=connection.maxConnectionLifeTime,
+            neo4j_encrypted=connection.encrypted,
+            neo4j_validate_ssl=connection.validateSSL,
+        )
+        return Neo4jHelper(neo4j_config)
+    except Exception as exc:
+        msg = f"Unknown error connecting with {connection}: {exc}."
+        raise SourceConnectionException(msg)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    engine: Engine,
-    service_connection: RedshiftConnection,
+    client: Neo4jHelper,
+    service_connection: AmundsenConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-    queries = {
-        "GetQueries": REDSHIFT_TEST_GET_QUERIES,
-        "GetDatabases": REDSHIFT_GET_DATABASE_NAMES,
-        "GetPartitionTableDetails": REDSHIFT_TEST_PARTITION_DETAILS,
+
+    test_fn = {
+        "CheckAccess": partial(client.execute_query, query=NEO4J_AMUNDSEN_USER_QUERY)
     }
-    test_connection_db_common(
+
+    test_connection_steps(
         metadata=metadata,
-        engine=engine,
-        service_connection=service_connection,
+        test_fn=test_fn,
+        service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
-        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/metadata.py`

 * *Files 24% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 Redshift source ingestion
 """
 
 import re
 import traceback
 from typing import Dict, Iterable, List, Optional, Tuple
 
-from sqlalchemy import inspect, sql
+from sqlalchemy import sql
 from sqlalchemy.dialects.postgresql.base import PGDialect
 from sqlalchemy.engine.reflection import Inspector
 from sqlalchemy_redshift.dialect import RedshiftDialect, RedshiftDialectMixin
 
 from metadata.generated.schema.api.data.createStoredProcedure import (
     CreateStoredProcedureRequest,
 )
@@ -28,40 +28,49 @@
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.storedProcedure import (
     Language,
     StoredProcedureCode,
 )
 from metadata.generated.schema.entity.data.table import (
     ConstraintType,
-    IntervalType,
+    PartitionColumnDetails,
+    PartitionIntervalTypes,
+    Table,
     TableConstraint,
     TablePartition,
     TableType,
 )
 from metadata.generated.schema.entity.services.connections.database.redshiftConnection import (
     RedshiftConnection,
 )
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.generated.schema.type.basic import EntityName
+from metadata.ingestion.api.delete import delete_entity_by_name
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.database.common_db_source import (
     CommonDbSourceService,
     TableNameAndType,
 )
+from metadata.ingestion.source.database.incremental_metadata_extraction import (
+    IncrementalConfig,
+)
 from metadata.ingestion.source.database.life_cycle_query_mixin import (
     LifeCycleQueryMixin,
 )
 from metadata.ingestion.source.database.multi_db_source import MultiDBSource
+from metadata.ingestion.source.database.redshift.incremental_table_processor import (
+    RedshiftIncrementalTableProcessor,
+)
 from metadata.ingestion.source.database.redshift.models import RedshiftStoredProcedure
 from metadata.ingestion.source.database.redshift.queries import (
     REDSHIFT_GET_ALL_RELATION_INFO,
     REDSHIFT_GET_DATABASE_NAMES,
     REDSHIFT_GET_STORED_PROCEDURE_QUERIES,
     REDSHIFT_GET_STORED_PROCEDURES,
     REDSHIFT_LIFE_CYCLE_QUERY,
@@ -76,14 +85,18 @@
     get_table_comment,
 )
 from metadata.ingestion.source.database.stored_procedures_mixin import (
     QueryByProcedure,
     StoredProcedureMixin,
 )
 from metadata.utils import fqn
+from metadata.utils.execution_time_tracker import (
+    calculate_execution_time,
+    calculate_execution_time_generator,
+)
 from metadata.utils.filters import filter_by_database
 from metadata.utils.helpers import get_start_and_end
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.sqlalchemy_utils import get_all_table_comments
 
 logger = ingestion_logger()
 
@@ -114,36 +127,57 @@
     LifeCycleQueryMixin, StoredProcedureMixin, CommonDbSourceService, MultiDBSource
 ):
     """
     Implements the necessary methods to extract
     Database metadata from Redshift Source
     """
 
-    def __init__(self, config, metadata):
+    def __init__(
+        self,
+        config: WorkflowSource,
+        metadata,
+        incremental_configuration: IncrementalConfig,
+    ):
         super().__init__(config, metadata)
         self.partition_details = {}
         self.life_cycle_query = REDSHIFT_LIFE_CYCLE_QUERY
+        self.context.get_global().deleted_tables = []
+        self.incremental = incremental_configuration
+        self.incremental_table_processor: Optional[
+            RedshiftIncrementalTableProcessor
+        ] = None
+
+        if self.incremental.enabled:
+            logger.info(
+                "Starting Incremental Metadata Extraction.\n\t Considering Table changes from %s",
+                self.incremental.start_datetime_utc,
+            )
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: RedshiftConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, RedshiftConnection):
             raise InvalidSourceException(
                 f"Expected RedshiftConnection, but got {connection}"
             )
-        return cls(config, metadata)
+        incremental_config = IncrementalConfig.create(
+            config.sourceConfig.config.incremental, pipeline_name, metadata
+        )
+        return cls(config, metadata, incremental_config)
 
     def get_partition_details(self) -> None:
         """
         Populate partition details
         """
         try:
             self.partition_details.clear()
-            results = self.engine.execute(REDSHIFT_PARTITION_DETAILS).fetchall()
+            results = self.connection.execute(REDSHIFT_PARTITION_DETAILS).fetchall()
             for row in results:
                 self.partition_details[f"{row.schema}.{row.table}"] = row.diststyle
         except Exception as exe:
             logger.debug(traceback.format_exc())
             logger.debug(f"Failed to fetch partition details due: {exe}")
 
     def query_table_names_and_types(
@@ -154,82 +188,166 @@
         """
 
         result = self.connection.execute(
             sql.text(REDSHIFT_GET_ALL_RELATION_INFO),
             {"schema": schema_name},
         )
 
+        if self.incremental.enabled:
+            result = [
+                (name, relkind)
+                for name, relkind in result
+                if name
+                in self.incremental_table_processor.get_not_deleted(
+                    schema_name=schema_name
+                )
+            ]
+
         return [
             TableNameAndType(
                 name=name, type_=STANDARD_TABLE_TYPES.get(relkind, TableType.Regular)
             )
             for name, relkind in result
         ]
 
+    def query_view_names_and_types(
+        self, schema_name: str
+    ) -> Iterable[TableNameAndType]:
+        """
+        Connect to the source database to get the view
+        name and type. By default, use the inspector method
+        to get the names and pass the View type.
+
+        This is useful for sources where we need fine-grained
+        logic on how to handle table types, e.g., material views,...
+        """
+
+        result = self.inspector.get_view_names(schema_name) or []
+
+        if self.incremental.enabled:
+            result = [
+                name
+                for name in result
+                if name
+                in self.incremental_table_processor.get_not_deleted(
+                    schema_name=schema_name
+                )
+            ]
+
+        return [
+            TableNameAndType(name=table_name, type_=TableType.View)
+            for table_name in result
+        ]
+
     def get_configured_database(self) -> Optional[str]:
         if not self.service_connection.ingestAllDatabases:
             return self.service_connection.database
         return None
 
     def get_database_names_raw(self) -> Iterable[str]:
         yield from self._execute_database_query(REDSHIFT_GET_DATABASE_NAMES)
 
+    def _set_incremental_table_processor(self, database: str):
+        """Prepares the needed data for doing incremental metadata extration for a given database.
+
+        1. Queries Redshift to get the changes done after the `self.incremental.start_datetime_utc`
+        2. Sets the table map with the changes within the RedshiftIncrementalTableProcessor
+        3. Sets the deleted tables in the context
+        """
+        if self.incremental.enabled:
+            self.incremental_table_processor = RedshiftIncrementalTableProcessor.create(
+                self.connection, self.inspector.default_schema_name
+            )
+
+            self.incremental_table_processor.set_table_map(
+                database=database, start_date=self.incremental.start_datetime_utc
+            )
+
+            self.context.get_global().deleted_tables.extend(
+                fqn.build(
+                    metadata=self.metadata,
+                    entity_type=Table,
+                    service_name=self.context.get().database_service,
+                    database_name=database,
+                    schema_name=schema_name,
+                    table_name=table_name,
+                )
+                for schema_name, table_name in self.incremental_table_processor.get_deleted()
+            )
+
     def get_database_names(self) -> Iterable[str]:
         if not self.config.serviceConnection.__root__.config.ingestAllDatabases:
-            self.inspector = inspect(self.engine)
             self.get_partition_details()
+
+            self._set_incremental_table_processor(
+                self.config.serviceConnection.__root__.config.database
+            )
+
             yield self.config.serviceConnection.__root__.config.database
         else:
             for new_database in self.get_database_names_raw():
                 database_fqn = fqn.build(
                     self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
+                    service_name=self.context.get().database_service,
                     database_name=new_database,
                 )
 
                 if filter_by_database(
                     self.source_config.databaseFilterPattern,
-                    database_fqn
-                    if self.source_config.useFqnForFiltering
-                    else new_database,
+                    (
+                        database_fqn
+                        if self.source_config.useFqnForFiltering
+                        else new_database
+                    ),
                 ):
                     self.status.filter(database_fqn, "Database Filtered Out")
                     continue
 
                 try:
                     self.set_inspector(database_name=new_database)
                     self.get_partition_details()
+
+                    self._set_incremental_table_processor(new_database)
+
                     yield new_database
                 except Exception as exc:
                     logger.debug(traceback.format_exc())
                     logger.error(
                         f"Error trying to connect to database {new_database}: {exc}"
                     )
 
-    def _get_partition_key(self, diststyle: str) -> Optional[List[str]]:
+    def _get_partition_key(self, diststyle: str) -> Optional[str]:
         try:
             regex = re.match(r"KEY\((\w+)\)", diststyle)
             if regex:
-                return [regex.group(1)]
+                return regex.group(1)
         except Exception as err:
             logger.debug(traceback.format_exc())
             logger.warning(err)
         return None
 
+    @calculate_execution_time()
     def get_table_partition_details(
         self, table_name: str, schema_name: str, inspector: Inspector
-    ) -> Tuple[bool, TablePartition]:
+    ) -> Tuple[bool, Optional[TablePartition]]:
         diststyle = self.partition_details.get(f"{schema_name}.{table_name}")
         if diststyle:
-            partition_details = TablePartition(
-                columns=self._get_partition_key(diststyle),
-                intervalType=IntervalType.COLUMN_VALUE,
-            )
-            return True, partition_details
+            distkey = self._get_partition_key(diststyle)
+            if distkey is not None:
+                partition_details = TablePartition(
+                    columns=[
+                        PartitionColumnDetails(
+                            columnName=distkey,
+                            intervalType=PartitionIntervalTypes.COLUMN_VALUE,
+                            interval=None,
+                        )
+                    ]
+                )
+                return True, partition_details
         return False, None
 
     def process_additional_table_constraints(
         self, column: dict, table_constraints: List[TableConstraint]
     ) -> None:
         """
         Process DIST_KEY & SORT_KEY column properties
@@ -250,23 +368,24 @@
                     columns=[column.get("name")],
                 )
             )
 
     def get_stored_procedures(self) -> Iterable[RedshiftStoredProcedure]:
         """List Snowflake stored procedures"""
         if self.source_config.includeStoredProcedures:
-            results = self.engine.execute(
+            results = self.connection.execute(
                 REDSHIFT_GET_STORED_PROCEDURES.format(
-                    schema_name=self.context.database_schema,
+                    schema_name=self.context.get().database_schema,
                 )
             ).all()
             for row in results:
                 stored_procedure = RedshiftStoredProcedure.parse_obj(dict(row))
                 yield stored_procedure
 
+    @calculate_execution_time_generator()
     def yield_stored_procedure(
         self, stored_procedure: RedshiftStoredProcedure
     ) -> Iterable[Either[CreateStoredProcedureRequest]]:
         """Prepare the stored procedure payload"""
 
         try:
             stored_procedure_request = CreateStoredProcedureRequest(
@@ -274,17 +393,17 @@
                 storedProcedureCode=StoredProcedureCode(
                     language=Language.SQL,
                     code=stored_procedure.definition,
                 ),
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 ),
             )
             yield Either(right=stored_procedure_request)
 
             self.register_record_stored_proc_request(stored_procedure_request)
 
         except Exception as exc:
@@ -300,15 +419,38 @@
         """
         Return the dictionary associating stored procedures to the
         queries they triggered
         """
         start, _ = get_start_and_end(self.source_config.queryLogDuration)
         query = REDSHIFT_GET_STORED_PROCEDURE_QUERIES.format(
             start_date=start,
-            database_name=self.context.database,
+            database_name=self.context.get().database,
         )
 
         queries_dict = self.procedure_queries_dict(
             query=query,
         )
 
         return queries_dict
+
+    def mark_tables_as_deleted(self):
+        """
+        Use the current inspector to mark tables as deleted
+        """
+        if self.incremental.enabled:
+            if not self.context.get().__dict__.get("database"):
+                raise ValueError(
+                    "No Database found in the context. We cannot run the table deletion."
+                )
+
+            if self.source_config.markDeletedTables:
+                logger.info(
+                    f"Mark Deleted Tables set to True. Processing database [{self.context.get().database}]"
+                )
+                yield from delete_entity_by_name(
+                    self.metadata,
+                    entity_type=Table,
+                    entity_names=self.context.get_global().deleted_tables,
+                    mark_deleted_entity=self.source_config.markDeletedTables,
+                )
+        else:
+            yield from super().mark_tables_as_deleted()
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_ip.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,21 +4,27 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
+# pylint: disable=abstract-method
+
 """
-Redshift models
+Expand sqlalchemy types to map them to OpenMetadata DataType
 """
-from typing import Optional
+from sqlalchemy.sql.sqltypes import String, TypeDecorator
+
+from metadata.utils.logger import profiler_logger
 
-from pydantic import BaseModel
+logger = profiler_logger()
 
 
-class RedshiftStoredProcedure(BaseModel):
-    """Redshift stored procedure list query results"""
+class CustomIP(TypeDecorator):
+    """
+    Convert RowVersion
+    """
 
-    name: str
-    owner: Optional[str]
-    definition: str
+    impl = String
+    cache_ok = True
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/queries.py`

 * *Files 3% similar despite different names*

```diff
@@ -244,15 +244,15 @@
         c.oid as "rel_oid",
         c.relname,
         CASE c.reldiststyle
         WHEN 0 THEN 'EVEN' WHEN 1 THEN 'KEY' WHEN 8 THEN 'ALL' END
         AS "diststyle",
         c.relowner AS "owner_id",
         u.usename AS "owner_name",
-        TRIM(TRAILING ';' FROM 
+        TRIM(TRAILING ';' FROM
         'create view ' || n.nspname || '.' || c.relname || ' as ' ||pg_catalog.pg_get_viewdef(c.oid, true))
         AS "view_definition",
         pg_catalog.array_to_string(c.relacl, '\n') AS "privileges"
     FROM pg_catalog.pg_class c
             LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
             JOIN pg_catalog.pg_user u ON u.usesysid = c.relowner
     WHERE c.relkind IN ('r', 'v', 'm', 'S', 'f')
@@ -341,22 +341,37 @@
     null as query_schema_name,
     q.query_start_time,
     q.query_end_time,
     q.query_user_name
 from SP_HISTORY sp
   join Q_HISTORY q
     on sp.procedure_session_id = q.query_session_id
-   and q.query_start_time between sp.procedure_start_time and sp.procedure_end_time 
+   and q.query_start_time between sp.procedure_start_time and sp.procedure_end_time
    and q.query_end_time between sp.procedure_start_time and sp.procedure_end_time
 order by procedure_start_time DESC
     """
 )
 
 REDSHIFT_LIFE_CYCLE_QUERY = textwrap.dedent(
     """
 select "table" as table_name,
 create_time as created_at
 from pg_catalog.svv_table_info o
 where o.schema = '{schema_name}'
 and o.database = '{database_name}'
 """
 )
+
+REDSHIFT_TABLE_CHANGES_QUERY = """
+SELECT
+    query_text
+FROM SYS_QUERY_HISTORY
+WHERE status = 'success'
+  AND (
+    query_type = 'DDL' OR
+    (query_type = 'UTILITY' AND query_text ilike '%%COMMENT ON%%') OR
+    (query_type = 'CTAS' AND query_text ilike '%%CREATE TABLE%%')
+  )
+  and database_name = '{database}'
+  and end_time >= '{start_date}'
+ORDER BY end_time DESC
+"""
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/query_parser.py`

 * *Files 6% similar despite different names*

```diff
@@ -34,15 +34,17 @@
     """
     Redshift base for usage and lineage
     """
 
     filters: str
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: RedshiftConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, RedshiftConnection):
             raise InvalidSourceException(
                 f"Expected RedshiftConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/redshift/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -29,26 +29,28 @@
 
 from metadata.ingestion.source.database.column_type_parser import create_sqlalchemy_type
 from metadata.ingestion.source.database.redshift.queries import (
     REDSHIFT_GET_ALL_RELATIONS,
     REDSHIFT_GET_SCHEMA_COLUMN_INFO,
     REDSHIFT_TABLE_COMMENTS,
 )
+from metadata.utils.execution_time_tracker import calculate_execution_time
 from metadata.utils.sqlalchemy_utils import get_table_comment_wrapper
 
 sa_version = Version(sa.__version__)
 
 ischema_names = pg_ischema_names
 GEOGRAPHY = create_sqlalchemy_type("GEOGRAPHY")
 ischema_names["geography"] = GEOGRAPHY
 ischema_names.update({"binary varying": sqltypes.VARBINARY})
 ischema_names.update(REDSHIFT_ISCHEMA_NAMES)
 
 
 # pylint: disable=protected-access
+@calculate_execution_time()
 @reflection.cache
 def get_columns(self, connection, table_name, schema=None, **kw):
     """
     Return information about columns in `table_name`.
 
     Overrides interface
     :meth:`~sqlalchemy.engine.interfaces.Dialect.get_columns`.
@@ -76,14 +78,15 @@
         column_info["distkey"] = col.distkey
         column_info["sortkey"] = col.sortkey
         column_info["system_data_type"] = col.format_type
         columns.append(column_info)
     return columns
 
 
+@calculate_execution_time()
 def _get_column_info(self, *args, **kwargs):
     """
     Get column info
 
     Args:
         *args:
         **kwargs:
@@ -108,14 +111,15 @@
     if "info" not in column_info:
         column_info["info"] = {}
     if encode and encode != "none":
         column_info["info"]["encode"] = encode
     return column_info
 
 
+@calculate_execution_time()
 @reflection.cache
 def _get_schema_column_info(
     self, connection, schema=None, **kw
 ):  # pylint: disable=unused-argument
     """
     Get schema column info
 
@@ -126,21 +130,20 @@
     Returns:
 
     This method is responsible for fetching all the column details like
     name, type, constraints, distkey and sortkey etc.
     """
     schema_clause = f"AND schema = '{schema if schema else ''}'"
     all_columns = defaultdict(list)
-    with connection.connect() as cnct:
-        result = cnct.execute(
-            REDSHIFT_GET_SCHEMA_COLUMN_INFO.format(schema_clause=schema_clause)
-        )
-        for col in result:
-            key = RelationKey(col.table_name, col.schema, connection)
-            all_columns[key].append(col)
+    result = connection.execute(
+        REDSHIFT_GET_SCHEMA_COLUMN_INFO.format(schema_clause=schema_clause)
+    )
+    for col in result:
+        key = RelationKey(col.table_name, col.schema, connection)
+        all_columns[key].append(col)
     return dict(all_columns)
 
 
 def _handle_array_type(attype):
     return (
         # strip '[]' from integer[], etc.
         re.sub(r"\[\]$", "", attype),
@@ -266,14 +269,15 @@
 def _get_charlen(format_type):
     charlen = re.search(r"\(([\d,]+)\)", format_type)
     if charlen:
         charlen = charlen.group(1)
     return charlen
 
 
+@calculate_execution_time()
 @reflection.cache
 def _get_pg_column_info(  # pylint: disable=too-many-locals,too-many-arguments, unused-argument
     self,
     name,
     format_type,
     default,
     notnull,
@@ -349,27 +353,29 @@
         comment,
         computed,
     )
 
     return column_info
 
 
+@calculate_execution_time()
 @reflection.cache
 def get_table_comment(
     self, connection, table_name, schema=None, **kw  # pylint: disable=unused-argument
 ):
     return get_table_comment_wrapper(
         self,
         connection,
         table_name=table_name,
         schema=schema,
         query=REDSHIFT_TABLE_COMMENTS,
     )
 
 
+@calculate_execution_time()
 @reflection.cache
 def _get_all_relation_info(self, connection, **kw):  # pylint: disable=unused-argument
     # pylint: disable=consider-using-f-string
     schema = kw.get("schema", None)
     schema_clause = "AND schema = '{schema}'".format(schema=schema) if schema else ""
 
     table_name = kw.get("table_name", None)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/salesforce/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/salesforce/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/salesforce/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/salesforce/metadata.py`

 * *Files 3% similar despite different names*

```diff
@@ -77,15 +77,17 @@
         self.metadata = metadata
         self.service_connection = self.config.serviceConnection.__root__.config
         self.client = get_connection(self.service_connection)
         self.table_constraints = None
         self.database_source_state = set()
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: SalesforceConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SalesforceConnection):
             raise InvalidSourceException(
                 f"Expected SalesforceConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -108,15 +110,15 @@
         """
         From topology.
         Prepare a database request and pass it to the sink
         """
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
             )
         )
 
     def get_database_schema_names(self) -> Iterable[str]:
         """
         return schema names
         """
@@ -131,47 +133,47 @@
         """
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name,
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                 ),
             )
         )
 
     def get_tables_name_and_type(self) -> Optional[Iterable[Tuple[str, str]]]:
         """
         Handle table and views.
 
         Fetches them up using the context information and
         the inspector set when preparing the db.
 
         :return: tables or views, depending on config
         """
-        schema_name = self.context.database_schema
+        schema_name = self.context.get().database_schema
 
         try:
             if self.service_connection.sobjectName:
                 table_name = self.standardize_table_name(
                     schema_name, self.service_connection.sobjectName
                 )
                 yield table_name, TableType.Regular
             else:
                 for salesforce_object in self.client.describe()["sobjects"]:
                     table_name = salesforce_object["name"]
                     table_name = self.standardize_table_name(schema_name, table_name)
                     table_fqn = fqn.build(
                         self.metadata,
                         entity_type=Table,
-                        service_name=self.context.database_service,
-                        database_name=self.context.database,
-                        schema_name=self.context.database_schema,
+                        service_name=self.context.get().database_service,
+                        database_name=self.context.get().database,
+                        schema_name=self.context.get().database_schema,
                         table_name=table_name,
                     )
                     if filter_by_table(
                         self.config.sourceConfig.config.tableFilterPattern,
                         table_fqn
                         if self.config.sourceConfig.config.useFqnForFiltering
                         else table_name,
@@ -236,17 +238,17 @@
                 tableType=table_type,
                 description=self.get_table_description(table_name),
                 columns=columns,
                 tableConstraints=table_constraints,
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                 ),
                 sourceUrl=self.get_source_url(
                     table_name=table_name,
                 ),
             )
             yield Either(right=table_request)
             self.register_record(table_request=table_request)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sample_data.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sample_data.py`

 * *Files 3% similar despite different names*

```diff
@@ -277,14 +277,19 @@
                 encoding=UTF_8,
             )
         )
         self.database_service = self.metadata.get_service_or_create(
             entity=DatabaseService, config=WorkflowSource(**self.database_service_json)
         )
 
+        self.glue_database_service = self.metadata.get_service_or_create(
+            entity=DatabaseService,
+            config=WorkflowSource(**self.glue_database_service_json),
+        )
+
         self.kafka_service_json = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/topics/service.json",
                 "r",
                 encoding=UTF_8,
             )
         )
@@ -529,15 +534,17 @@
                 sample_data_folder + "/data_insights/data_insights.json",
                 "r",
                 encoding=UTF_8,
             )
         )
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: CustomDatabaseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, CustomDatabaseConnection):
             raise InvalidSourceException(
                 f"Expected CustomDatabaseConnection, but got {connection}"
             )
@@ -545,16 +552,16 @@
 
     def prepare(self):
         """Nothing to prepare"""
 
     def _iter(self, *_, **__) -> Iterable[Entity]:
         yield from self.ingest_teams()
         yield from self.ingest_users()
-        yield from self.ingest_glue()
         yield from self.ingest_tables()
+        yield from self.ingest_glue()
         yield from self.ingest_stored_procedures()
         yield from self.ingest_topics()
         yield from self.ingest_charts()
         yield from self.ingest_data_models()
         yield from self.ingest_dashboards()
         yield from self.ingest_looker()
         yield from self.ingest_pipelines()
@@ -600,42 +607,42 @@
 
             yield Either(right=team_to_ingest)
 
     def ingest_glue(self) -> Iterable[Either[Entity]]:
         """Ingest Sample Data for glue database source"""
 
         db = CreateDatabaseRequest(
-            name=self.database["name"],
-            description=self.database["description"],
-            service=self.database_service.fullyQualifiedName,
+            name=self.glue_database["name"],
+            description=self.glue_database["description"],
+            service=self.glue_database_service.fullyQualifiedName,
         )
 
         yield Either(right=db)
 
         database_entity = fqn.build(
             self.metadata,
             entity_type=Database,
-            service_name=self.database_service.name.__root__,
+            service_name=self.glue_database_service.fullyQualifiedName.__root__,
             database_name=db.name.__root__,
         )
 
         database_object = self.metadata.get_by_name(
             entity=Database, fqn=database_entity
         )
         schema = CreateDatabaseSchemaRequest(
-            name=self.database_schema["name"],
-            description=self.database_schema["description"],
+            name=self.glue_database_schema["name"],
+            description=self.glue_database_schema["description"],
             database=database_object.fullyQualifiedName,
         )
         yield Either(right=schema)
 
         database_schema_entity = fqn.build(
             self.metadata,
             entity_type=DatabaseSchema,
-            service_name=self.database_service.name.__root__,
+            service_name=self.glue_database_service.fullyQualifiedName.__root__,
             database_name=db.name.__root__,
             schema_name=schema.name.__root__,
         )
 
         database_schema_object = self.metadata.get_by_name(
             entity=DatabaseSchema, fqn=database_schema_entity
         )
@@ -647,14 +654,37 @@
                 columns=table["columns"],
                 databaseSchema=database_schema_object.fullyQualifiedName,
                 tableConstraints=table.get("tableConstraints"),
                 tableType=table["tableType"],
             )
             yield Either(right=table_request)
 
+        database_schema_entity = fqn.build(
+            self.metadata,
+            entity_type=DatabaseSchema,
+            service_name=self.database_service.fullyQualifiedName.__root__,
+            database_name=self.database["name"],
+            schema_name=self.database_schema["name"],
+        )
+
+        database_schema_object = self.metadata.get_by_name(
+            entity=DatabaseSchema, fqn=database_schema_entity
+        )
+
+        for table in self.glue_tables["tables"]:
+            table_request = CreateTableRequest(
+                name=table["name"],
+                description=table["description"],
+                columns=table["columns"],
+                databaseSchema=database_schema_object.fullyQualifiedName,
+                tableConstraints=table.get("tableConstraints"),
+                tableType=table["tableType"],
+            )
+            yield Either(right=table_request)
+
     def ingest_tables(self) -> Iterable[Either[Entity]]:
         """Ingest Sample Tables"""
 
         db = CreateDatabaseRequest(
             name=self.database["name"],
             description=self.database["description"],
             service=self.database_service.fullyQualifiedName.__root__,
@@ -1487,14 +1517,27 @@
                                 TestResultValue.parse_obj(res_value)
                                 for res_value in result["testResultValues"]
                             ],
                         ),
                         test_case_name=case.fullyQualifiedName.__root__,
                     )
                     yield Either(right=test_case_result_req)
+            if test_case_results.get("failedRowsSample"):
+                self.metadata.ingest_failed_rows_sample(
+                    case,
+                    TableData(
+                        rows=test_case_results["failedRowsSample"]["rows"],
+                        columns=test_case_results["failedRowsSample"]["columns"],
+                    ),
+                )
+            if test_case_results.get("inspectionQuery"):
+                self.metadata.ingest_inspection_query(
+                    case,
+                    test_case_results["inspectionQuery"],
+                )
 
     def ingest_data_insights(self) -> Iterable[Either[OMetaDataInsightSample]]:
         """Iterate over all the data insights and ingest them"""
         data: Dict[str, List] = self.data_insight_data["reports"]
 
         for report_type, report_data in data.items():
             i = 0
@@ -1577,15 +1620,17 @@
 
             if life_cycle["accessed"].get("accessedBy"):
                 life_cycle_data.accessed.accessedBy = self.get_accessed_by(
                     life_cycle["accessed"]["accessedBy"]["name"]
                 )
 
             life_cycle_request = OMetaLifeCycleData(
-                entity=table, life_cycle=life_cycle_data
+                entity=Table,
+                entity_fqn=table_life_cycle["fqn"],
+                life_cycle=life_cycle_data,
             )
             yield Either(right=life_cycle_request)
 
     def get_accessed_by(self, accessed_by) -> EntityReference:
         return self.metadata.get_entity_reference(entity=User, fqn=accessed_by)
 
     def close(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sample_usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sample_usage.py`

 * *Files 3% similar despite different names*

```diff
@@ -65,15 +65,17 @@
         with open(self.query_log_csv, "r", encoding="utf-8") as fin:
             self.query_logs = [dict(i) for i in csv.DictReader(fin)]
         self.service = self.metadata.get_service_or_create(
             entity=DatabaseService, config=config
         )
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: CustomDatabaseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, CustomDatabaseConnection):
             raise InvalidSourceException(
                 f"Expected CustomDatabaseConnection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/saphana/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/saphana/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/saphana/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/saphana/metadata.py`

 * *Files 5% similar despite different names*

```diff
@@ -7,17 +7,15 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 SAP Hana source module
 """
-from typing import Iterable
-
-from sqlalchemy import inspect
+from typing import Iterable, Optional
 
 from metadata.generated.schema.entity.services.connections.database.sapHanaConnection import (
     SapHanaConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -32,28 +30,31 @@
 class SaphanaSource(CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Mysql Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: SapHanaConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SapHanaConnection):
             raise InvalidSourceException(
                 f"Expected SapHanaConnection, but got {connection}"
             )
         return cls(config, metadata)
 
     def get_database_names(self) -> Iterable[str]:
         """
         Check if the db is configured, or query the name
         """
-        self.inspector = inspect(self.engine)
+        self._connection_map = {}  # Lazy init as well
+        self._inspector_map = {}
 
         if getattr(self.service_connection.connection, "database"):
             yield self.service_connection.connection.database
 
         else:
             try:
                 yield self.connection.execute(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/extension_attr.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/extension_attr.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sas/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sas/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -118,15 +118,20 @@
 
         self.add_table_custom_attributes()
 
         self.databases = None
         self.database_schemas = None
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         logger.info(f"running create {config_dict}")
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: SASConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SASConnection):
             raise InvalidSourceException(
                 f"Expected SASConnection, but got {connection}"
             )
@@ -849,15 +854,15 @@
 
     def yield_database(
         self, database_name: str
     ) -> Iterable[Either[CreateDatabaseRequest]]:
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
             )
         )
 
     def get_database_schema_names(self) -> Iterable[Tuple[str, str]]:
         for database, database_schemas in self.database_schemas.items():
             for database_schema in database_schemas:
                 yield database, database_schema
@@ -867,15 +872,15 @@
     ) -> Iterable[Either[CreateDatabaseSchemaRequest]]:
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name[1],
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
+                    service_name=self.context.get().database_service,
                     database_name=schema_name[0],
                 ),
             )
         )
 
     def yield_tag(
         self, schema_name: str
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/singlestore/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/singlestore/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/singlestore/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/singlestore/metadata.py`

 * *Files 10% similar despite different names*

```diff
@@ -7,14 +7,16 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Singlestore source ingestion
 """
+from typing import Optional
+
 from sqlalchemy.dialects.mysql.base import ischema_names
 from sqlalchemy.dialects.mysql.reflection import MySQLTableDefinitionParser
 
 from metadata.generated.schema.entity.services.connections.database.singleStoreConnection import (
     SingleStoreConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
@@ -36,15 +38,17 @@
 class SinglestoreSource(CommonDbSourceService):
     """
     Implements the necessary methods to extract
     Database metadata from Singlestore Source
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: SingleStoreConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SingleStoreConnection):
             raise InvalidSourceException(
                 f"Expected SingleStoreConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/openlineage/metadata.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,598 +4,517 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Snowflake source module
+OpenLineage source to extract metadata from Kafka events
 """
 import json
 import traceback
-from typing import Dict, Iterable, List, Optional, Tuple
-
-import sqlparse
-from snowflake.sqlalchemy.custom_types import VARIANT
-from snowflake.sqlalchemy.snowdialect import SnowflakeDialect, ischema_names
-from sqlalchemy.engine.reflection import Inspector
-from sqlparse.sql import Function, Identifier
-
-from metadata.generated.schema.api.data.createStoredProcedure import (
-    CreateStoredProcedureRequest,
-)
-from metadata.generated.schema.entity.data.database import Database
+from collections import defaultdict
+from itertools import groupby, product
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from metadata.generated.schema.api.data.createPipeline import CreatePipelineRequest
+from metadata.generated.schema.api.data.createTable import CreateTableRequest
+from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
-from metadata.generated.schema.entity.data.storedProcedure import StoredProcedureCode
-from metadata.generated.schema.entity.data.table import (
-    IntervalType,
-    TablePartition,
-    TableType,
-)
-from metadata.generated.schema.entity.services.connections.database.snowflakeConnection import (
-    SnowflakeConnection,
+from metadata.generated.schema.entity.data.pipeline import Pipeline
+from metadata.generated.schema.entity.data.table import Column, Table
+from metadata.generated.schema.entity.services.connections.pipeline.openLineageConnection import (
+    OpenLineageConnection,
 )
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.type.basic import EntityName, SourceUrl
+from metadata.generated.schema.type.entityLineage import (
+    ColumnLineage,
+    EntitiesEdge,
+    LineageDetails,
+    Source,
+)
+from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import InvalidSourceException
-from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
+from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.column_type_parser import create_sqlalchemy_type
-from metadata.ingestion.source.database.common_db_source import (
-    CommonDbSourceService,
-    TableNameAndType,
-)
-from metadata.ingestion.source.database.life_cycle_query_mixin import (
-    LifeCycleQueryMixin,
-)
-from metadata.ingestion.source.database.multi_db_source import MultiDBSource
-from metadata.ingestion.source.database.snowflake.models import (
-    STORED_PROC_LANGUAGE_MAP,
-    SnowflakeStoredProcedure,
-)
-from metadata.ingestion.source.database.snowflake.queries import (
-    SNOWFLAKE_DESC_STORED_PROCEDURE,
-    SNOWFLAKE_FETCH_ALL_TAGS,
-    SNOWFLAKE_GET_CLUSTER_KEY,
-    SNOWFLAKE_GET_CURRENT_ACCOUNT,
-    SNOWFLAKE_GET_DATABASE_COMMENTS,
-    SNOWFLAKE_GET_DATABASES,
-    SNOWFLAKE_GET_ORGANIZATION_NAME,
-    SNOWFLAKE_GET_SCHEMA_COMMENTS,
-    SNOWFLAKE_GET_STORED_PROCEDURE_QUERIES,
-    SNOWFLAKE_GET_STORED_PROCEDURES,
-    SNOWFLAKE_LIFE_CYCLE_QUERY,
-    SNOWFLAKE_SESSION_TAG_QUERY,
-)
-from metadata.ingestion.source.database.snowflake.utils import (
-    _current_database_schema,
-    get_columns,
-    get_foreign_keys,
-    get_pk_constraint,
-    get_schema_columns,
-    get_table_comment,
-    get_table_names,
-    get_table_names_reflection,
-    get_unique_constraints,
-    get_view_definition,
-    get_view_names,
-    get_view_names_reflection,
-    normalize_names,
-)
-from metadata.ingestion.source.database.stored_procedures_mixin import (
-    QueryByProcedure,
-    StoredProcedureMixin,
+from metadata.ingestion.source.pipeline.openlineage.models import (
+    EventType,
+    LineageEdge,
+    LineageNode,
+    OpenLineageEvent,
+    TableDetails,
+    TableFQN,
+)
+from metadata.ingestion.source.pipeline.openlineage.utils import (
+    FQNNotFoundException,
+    message_to_open_lineage_event,
 )
+from metadata.ingestion.source.pipeline.pipeline_service import PipelineServiceSource
 from metadata.utils import fqn
-from metadata.utils.filters import filter_by_database
-from metadata.utils.helpers import get_start_and_end
 from metadata.utils.logger import ingestion_logger
-from metadata.utils.sqlalchemy_utils import get_all_table_comments
-from metadata.utils.tag_utils import get_ometa_tag_and_classification
-
-ischema_names["VARIANT"] = VARIANT
-ischema_names["GEOGRAPHY"] = create_sqlalchemy_type("GEOGRAPHY")
-ischema_names["GEOMETRY"] = create_sqlalchemy_type("GEOMETRY")
 
 logger = ingestion_logger()
 
 
-SnowflakeDialect._json_deserializer = json.loads  # pylint: disable=protected-access
-SnowflakeDialect.get_table_names = get_table_names
-SnowflakeDialect.get_view_names = get_view_names
-SnowflakeDialect.get_all_table_comments = get_all_table_comments
-SnowflakeDialect.normalize_name = normalize_names
-SnowflakeDialect.get_table_comment = get_table_comment
-SnowflakeDialect.get_view_definition = get_view_definition
-SnowflakeDialect.get_unique_constraints = get_unique_constraints
-SnowflakeDialect._get_schema_columns = (  # pylint: disable=protected-access
-    get_schema_columns
-)
-Inspector.get_table_names = get_table_names_reflection
-Inspector.get_view_names = get_view_names_reflection
-SnowflakeDialect._current_database_schema = (  # pylint: disable=protected-access
-    _current_database_schema
-)
-SnowflakeDialect.get_pk_constraint = get_pk_constraint
-SnowflakeDialect.get_foreign_keys = get_foreign_keys
-SnowflakeDialect.get_columns = get_columns
+class OpenlineageSource(PipelineServiceSource):
+    """
+    Implements the necessary methods of PipelineServiceSource to facilitate registering OpenLineage pipelines with
+    metadata into Open Metadata.
 
+    Works under the assumption that OpenLineage integrations produce events to Kafka topic, which is a source of events
+    for this connector.
 
-class SnowflakeSource(
-    LifeCycleQueryMixin, StoredProcedureMixin, CommonDbSourceService, MultiDBSource
-):
-    """
-    Implements the necessary methods to extract
-    Database metadata from Snowflake Source
-    """
+    Only 'SUCCESS' OpenLineage events are taken into account in this connector.
 
-    def __init__(self, config, metadata):
-        super().__init__(config, metadata)
-        self.partition_details = {}
-        self.schema_desc_map = {}
-        self.database_desc_map = {}
-
-        self._account: Optional[str] = None
-        self._org_name: Optional[str] = None
-        self.life_cycle_query = SNOWFLAKE_LIFE_CYCLE_QUERY
+    Configuring OpenLineage integrations: https://openlineage.io/docs/integrations/about
+    """
 
     @classmethod
     def create(cls, config_dict, metadata: OpenMetadata):
+        """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: SnowflakeConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, SnowflakeConnection):
+        connection: OpenLineageConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, OpenLineageConnection):
             raise InvalidSourceException(
-                f"Expected SnowflakeConnection, but got {connection}"
+                f"Expected OpenLineageConnection, but got {connection}"
             )
         return cls(config, metadata)
 
-    @property
-    def account(self) -> Optional[str]:
-        """
-        Query the account information
-            ref https://docs.snowflake.com/en/sql-reference/functions/current_account_name
-        """
-        if self._account is None:
-            self._account = self._get_current_account()
+    def prepare(self):
+        """Nothing to prepare"""
 
-        return self._account
+    def close(self) -> None:
+        self.metadata.close()
 
-    @property
-    def org_name(self) -> Optional[str]:
-        """
-        Query the Organization information.
-            ref https://docs.snowflake.com/en/sql-reference/functions/current_organization_name
+    @classmethod
+    def _get_table_details(cls, data: Dict) -> TableDetails:
         """
-        if self._org_name is None:
-            self._org_name = self._get_org_name()
-
-        return self._org_name
+        extracts table entity schema and name from input/output entry collected from Open Lineage.
 
-    def set_session_query_tag(self) -> None:
+        :param data: single entry from inputs/outputs objects
+        :return: TableDetails object with schema and name
         """
-        Method to set query tag for current session
-        """
-        if self.service_connection.queryTag:
-            self.engine.execute(
-                SNOWFLAKE_SESSION_TAG_QUERY.format(
-                    query_tag=self.service_connection.queryTag
-                )
-            )
+        symlinks = data.get("facets", {}).get("symlinks", {}).get("identifiers", [])
 
-    def set_partition_details(self) -> None:
-        self.partition_details.clear()
-        results = self.engine.execute(SNOWFLAKE_GET_CLUSTER_KEY).all()
-        for row in results:
-            if row.CLUSTERING_KEY:
-                self.partition_details[
-                    f"{row.TABLE_SCHEMA}.{row.TABLE_NAME}"
-                ] = row.CLUSTERING_KEY
-
-    def set_schema_description_map(self) -> None:
-        results = self.engine.execute(SNOWFLAKE_GET_SCHEMA_COMMENTS).all()
-        for row in results:
-            self.schema_desc_map[(row.DATABASE_NAME, row.SCHEMA_NAME)] = row.COMMENT
-
-    def set_database_description_map(self) -> None:
-        if not self.database_desc_map:
-            results = self.engine.execute(SNOWFLAKE_GET_DATABASE_COMMENTS).all()
-            for row in results:
-                self.database_desc_map[row.DATABASE_NAME] = row.COMMENT
-
-    def get_schema_description(self, schema_name: str) -> Optional[str]:
-        """
-        Method to fetch the schema description
-        """
-        return self.schema_desc_map.get((self.context.database, schema_name))
-
-    def get_database_description(self, database_name: str) -> Optional[str]:
-        """
-        Method to fetch the database description
-        """
-        return self.database_desc_map.get(database_name)
-
-    def get_configured_database(self) -> Optional[str]:
-        return self.service_connection.database
-
-    def get_database_names_raw(self) -> Iterable[str]:
-        results = self.connection.execute(SNOWFLAKE_GET_DATABASES)
-        for res in results:
-            row = list(res)
-            yield row[1]
-
-    def get_database_names(self) -> Iterable[str]:
-        configured_db = self.config.serviceConnection.__root__.config.database
-        if configured_db:
-            self.set_inspector(configured_db)
-            self.set_session_query_tag()
-            self.set_partition_details()
-            self.set_schema_description_map()
-            self.set_database_description_map()
-            yield configured_db
+        # for some OL events name can be extracted from dataset facet but symlinks is preferred so - if present - we
+        # use it instead
+        if len(symlinks) > 0:
+            try:
+                # @todo verify if table can have multiple identifiers pointing at it
+                name = symlinks[0]["name"]
+            except (KeyError, IndexError):
+                raise ValueError(
+                    "input table name cannot be retrieved from symlinks.identifiers facet."
+                )
         else:
-            for new_database in self.get_database_names_raw():
-                database_fqn = fqn.build(
-                    self.metadata,
-                    entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=new_database,
+            try:
+                name = data["name"]
+            except KeyError:
+                raise ValueError(
+                    "input table name cannot be retrieved from name attribute."
                 )
 
-                if filter_by_database(
-                    self.source_config.databaseFilterPattern,
-                    database_fqn
-                    if self.source_config.useFqnForFiltering
-                    else new_database,
-                ):
-                    self.status.filter(database_fqn, "Database Filtered Out")
-                    continue
-
-                try:
-                    self.set_inspector(database_name=new_database)
-                    self.set_session_query_tag()
-                    self.set_partition_details()
-                    self.set_schema_description_map()
-                    self.set_database_description_map()
-                    yield new_database
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.warning(
-                        f"Error trying to connect to database {new_database}: {exc}"
-                    )
-
-    def __get_identifier_from_function(self, function_token: Function) -> List:
-        identifiers = []
-        for token in function_token.get_parameters():
-            if isinstance(token, Function):
-                # get column names from nested functions
-                identifiers.extend(self.__get_identifier_from_function(token))
-            elif isinstance(token, Identifier):
-                identifiers.append(token.get_real_name())
-        return identifiers
+        name_parts = name.split(".")
 
-    def parse_column_name_from_expr(self, cluster_key_expr: str) -> Optional[List[str]]:
-        try:
-            parser = sqlparse.parse(cluster_key_expr)
-            if not parser:
-                return []
-            result = []
-            tokens_list = parser[0].tokens
-            for token in tokens_list:
-                if isinstance(token, Function):
-                    result.extend(self.__get_identifier_from_function(token))
-                elif isinstance(token, Identifier):
-                    result.append(token.get_real_name())
-            return result
-        except Exception as err:
-            logger.debug(traceback.format_exc())
-            logger.warning(f"Failed to parse cluster key - {err}")
-        return None
-
-    def __fix_partition_column_case(
-        self,
-        table_name: str,
-        schema_name: str,
-        inspector: Inspector,
-        partition_columns: Optional[List[str]],
-    ) -> List[str]:
-        if partition_columns:
-            columns = []
-            table_columns = inspector.get_columns(
-                table_name=table_name, schema=schema_name
-            )
-            for pcolumn in partition_columns:
-                for tcolumn in table_columns:
-                    if tcolumn["name"].lower() == pcolumn.lower():
-                        columns.append(tcolumn["name"])
-                        break
-            return columns
-        return []
-
-    def get_table_partition_details(
-        self, table_name: str, schema_name: str, inspector: Inspector
-    ) -> Tuple[bool, TablePartition]:
-        cluster_key = self.partition_details.get(f"{schema_name}.{table_name}")
-        if cluster_key:
-            partition_columns = self.parse_column_name_from_expr(cluster_key)
-            partition_details = TablePartition(
-                columns=self.__fix_partition_column_case(
-                    table_name, schema_name, inspector, partition_columns
-                ),
-                intervalType=IntervalType.COLUMN_VALUE,
+        if len(name_parts) < 2:
+            raise ValueError(
+                f"input table name should be of 'schema.table' format! Received: {name}"
             )
-            return True, partition_details
-        return False, None
 
-    def yield_tag(
-        self, schema_name: str
-    ) -> Iterable[Either[OMetaTagAndClassification]]:
-        if self.source_config.includeTags:
-            result = []
+        # we take last two elements to explicitly collect schema and table names
+        # in BigQuery Open Lineage events name_parts would be list of 3 elements as first one is GCP Project ID
+        # however, concept of GCP Project ID is not represented in Open Metadata and hence - we need to skip this part
+        return TableDetails(name=name_parts[-1], schema=name_parts[-2])
+
+    def _get_table_fqn(self, table_details: TableDetails) -> Optional[str]:
+        try:
+            return self._get_table_fqn_from_om(table_details)
+        except FQNNotFoundException:
             try:
-                result = self.connection.execute(
-                    SNOWFLAKE_FETCH_ALL_TAGS.format(
-                        database_name=self.context.database,
-                        schema_name=schema_name,
-                    )
-                )
+                schema_fqn = self._get_schema_fqn_from_om(table_details.schema)
 
-            except Exception as exc:
-                try:
-                    logger.debug(traceback.format_exc())
-                    logger.warning(
-                        f"Error fetching tags {exc}. Trying with quoted names"
-                    )
-                    result = self.connection.execute(
-                        SNOWFLAKE_FETCH_ALL_TAGS.format(
-                            database_name=f'"{self.context.database}"',
-                            schema_name=f'"{self.context.database_schema}"',
-                        )
-                    )
-                except Exception as inner_exc:
-                    yield Either(
-                        left=StackTraceError(
-                            name="Tags and Classifications",
-                            error=f"Failed to fetch tags due to [{inner_exc}]",
-                            stackTrace=traceback.format_exc(),
-                        )
-                    )
+                return f"{schema_fqn}.{table_details.name}"
+            except FQNNotFoundException:
+                return None
+
+    def _get_table_fqn_from_om(self, table_details: TableDetails) -> Optional[str]:
+        """
+        Based on partial schema and table names look for matching table object in open metadata.
+        :param schema: schema name
+        :param table: table name
+        :return: fully qualified name of a Table in Open Metadata
+        """
+        result = None
+        services = self.get_db_service_names()
+        for db_service in services:
+            result = fqn.build(
+                metadata=self.metadata,
+                entity_type=Table,
+                service_name=db_service,
+                database_name=None,
+                schema_name=table_details.schema,
+                table_name=table_details.name,
+            )
+        if not result:
+            raise FQNNotFoundException(
+                f"Table FQN not found for table: {table_details} within services: {services}"
+            )
+        return result
 
-            for res in result:
-                row = list(res)
-                fqn_elements = [name for name in row[2:] if name]
-                yield from get_ometa_tag_and_classification(
-                    tag_fqn=fqn._build(  # pylint: disable=protected-access
-                        self.context.database_service, *fqn_elements
-                    ),
-                    tags=[row[1]],
-                    classification_name=row[0],
-                    tag_description="SNOWFLAKE TAG VALUE",
-                    classification_description="SNOWFLAKE TAG NAME",
-                )
+    def _get_schema_fqn_from_om(self, schema: str) -> Optional[str]:
+        """
+        Based on partial schema name look for any matching DatabaseSchema object in open metadata.
 
-    def query_table_names_and_types(
-        self, schema_name: str
-    ) -> Iterable[TableNameAndType]:
-        """
-        Connect to the source database to get the table
-        name and type. By default, use the inspector method
-        to get the names and pass the Regular type.
-
-        This is useful for sources where we need fine-grained
-        logic on how to handle table types, e.g., external, foreign,...
-        """
-        table_list = [
-            TableNameAndType(name=table_name)
-            for table_name in self.inspector.get_table_names(
-                schema=schema_name,
+        :param schema: schema name
+        :return: fully qualified name of a DatabaseSchema in Open Metadata
+        """
+        result = None
+        services = self.get_db_service_names()
+
+        for db_service in services:
+            result = fqn.build(
+                metadata=self.metadata,
+                entity_type=DatabaseSchema,
+                service_name=db_service,
+                database_name=None,
+                schema_name=schema,
+                skip_es_search=False,
             )
-        ]
 
-        table_list.extend(
-            [
-                TableNameAndType(name=table_name, type_=TableType.External)
-                for table_name in self.inspector.get_table_names(
-                    schema=schema_name, external_tables=True
-                )
-            ]
-        )
+            if result:
+                return result
 
-        if self.service_connection.includeTransientTables:
-            table_list.extend(
-                [
-                    TableNameAndType(name=table_name, type_=TableType.Transient)
-                    for table_name in self.inspector.get_table_names(
-                        schema=schema_name,
-                        include_transient_tables=True,
-                    )
-                ]
+        if not result:
+            raise FQNNotFoundException(
+                f"Schema FQN not found within services: {services}"
             )
 
-        return table_list
+        return result
 
-    def _get_org_name(self) -> Optional[str]:
-        try:
-            res = self.engine.execute(SNOWFLAKE_GET_ORGANIZATION_NAME).one()
-            if res:
-                return res.NAME
-        except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.debug(f"Failed to fetch Organization name due to: {exc}")
-        return None
+    @classmethod
+    def _render_pipeline_name(cls, pipeline_details: OpenLineageEvent) -> str:
+        """
+        Renders pipeline name from parent facet of run facet. It is our expectation that every OL event contains parent
+        run facet so we can always create pipeline entities and link them to lineage events.
 
-    def _get_current_account(self) -> Optional[str]:
-        try:
-            res = self.engine.execute(SNOWFLAKE_GET_CURRENT_ACCOUNT).one()
-            if res:
-                return res.ACCOUNT
-        except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.debug(f"Failed to fetch current account due to: {exc}")
-        return None
+        :param run_facet: Open Lineage run facet
+        :return: pipeline name (not fully qualified name)
+        """
+        run_facet = pipeline_details.run_facet
 
-    def _get_source_url_root(
-        self, database_name: Optional[str] = None, schema_name: Optional[str] = None
-    ) -> str:
-        url = (
-            f"https://app.snowflake.com/{self.org_name.lower()}"
-            f"/{self.account.lower()}/#/data/databases/{database_name}"
-        )
-        if schema_name:
-            url = f"{url}/schemas/{schema_name}"
+        namespace = run_facet["facets"]["parent"]["job"]["namespace"]
+        name = run_facet["facets"]["parent"]["job"]["name"]
 
-        return url
+        return f"{namespace}-{name}"
 
-    def get_source_url(
-        self,
-        database_name: Optional[str] = None,
-        schema_name: Optional[str] = None,
-        table_name: Optional[str] = None,
-        table_type: Optional[TableType] = None,
-    ) -> Optional[str]:
+    @classmethod
+    def _filter_event_by_type(
+        cls, event: OpenLineageEvent, event_type: EventType
+    ) -> Optional[Dict]:
+        """
+        returns event if it's of particular event_type.
+        for example - for lineage events we will be only looking for EventType.COMPLETE event type.
+
+        :param event: Open Lineage raw event.
+        :param event_type: type of event we are looking for.
+        :return: Open Lineage event if matches event_type, otherwise None
         """
-        Method to get the source url for snowflake
+        return event if event.event_type == event_type else {}
+
+    @classmethod
+    def _get_om_table_columns(cls, table_input: Dict) -> Optional[List]:
         """
-        try:
-            if self.account and self.org_name:
-                tab_type = "view" if table_type == TableType.View else "table"
-                url = self._get_source_url_root(
-                    database_name=database_name, schema_name=schema_name
-                )
-                if table_name:
-                    url = f"{url}/{tab_type}/{table_name}"
-                return url
-        except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.error(f"Unable to get source url: {exc}")
-        return None
 
-    def query_view_names_and_types(
-        self, schema_name: str
-    ) -> Iterable[TableNameAndType]:
+        :param table_input:
+        :return:
         """
-        Connect to the source database to get the view
-        name and type. By default, use the inspector method
-        to get the names and pass the View type.
+        try:
+            fields = table_input["facets"]["schema"]["fields"]
 
-        This is useful for sources where we need fine-grained
-        logic on how to handle table types, e.g., material views,...
+            # @todo check if this way of passing type is ok
+            columns = [
+                Column(name=f.get("name"), dataType=f.get("type").upper())
+                for f in fields
+            ]
+            return columns
+        except KeyError:
+            return None
+
+    def get_create_table_request(self, table: Dict) -> Optional[Either]:
         """
+        If certain table from Open Lineage events doesn't already exist in Open Metadata, register appropriate entity.
+        This makes sense especially for output facet of OpenLineage event - as database service ingestion is a scheduled
+        process we might fall into situation where we received Open Lineage event about creation of a table that is yet
+        to be ingested by database service ingestion process. To avoid missing on such lineage scenarios, we will create
+        table entity beforehand.
 
-        regular_views = [
-            TableNameAndType(name=view_name, type_=TableType.View)
-            for view_name in self.inspector.get_view_names(schema_name) or []
-        ]
+        :param table: single object from inputs/outputs facet
+        :return: request to create the entity (if needed)
+        """
+        om_table_fqn = None
 
-        materialized_views = [
-            TableNameAndType(name=view_name, type_=TableType.MaterializedView)
-            for view_name in self.inspector.get_view_names(
-                schema_name, materialized_views=True
+        try:
+            table_details = OpenlineageSource._get_table_details(table)
+        except ValueError as e:
+            return Either(
+                left=StackTraceError(
+                    name="",
+                    error=f"Failed to get partial table name: {e}",
+                    stackTrace=traceback.format_exc(),
+                )
             )
-            or []
-        ]
+        try:
+            om_table_fqn = self._get_table_fqn_from_om(table_details)
 
-        return regular_views + materialized_views
+            # if fqn found then it means table is already registered and we don't need to render create table request
+            return None
+        except FQNNotFoundException:
+            pass
 
-    def get_stored_procedures(self) -> Iterable[SnowflakeStoredProcedure]:
-        """List Snowflake stored procedures"""
-        if self.source_config.includeStoredProcedures:
-            results = self.engine.execute(
-                SNOWFLAKE_GET_STORED_PROCEDURES.format(
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
-                )
-            ).all()
-            for row in results:
-                stored_procedure = SnowflakeStoredProcedure.parse_obj(dict(row))
-                if stored_procedure.definition is None:
-                    logger.debug(
-                        f"Missing ownership permissions on procedure {stored_procedure.name}."
-                        " Trying to fetch description via DESCRIBE."
-                    )
-                    stored_procedure.definition = self.describe_procedure_definition(
-                        stored_procedure
+        # If OM Table FQN was not found based on OL Partial Name - we need to register it.
+        if not om_table_fqn:
+            try:
+                om_schema_fqn = self._get_schema_fqn_from_om(table_details.schema)
+            except FQNNotFoundException as e:
+                return Either(
+                    left=StackTraceError(
+                        name="",
+                        error=f"Failed to get fully qualified schema name: {e}",
+                        stackTrace=traceback.format_exc(),
                     )
-                yield stored_procedure
+                )
 
-    def describe_procedure_definition(
-        self, stored_procedure: SnowflakeStoredProcedure
-    ) -> str:
-        """
-        We can only get the SP definition via the INFORMATION_SCHEMA.PROCEDURES if the
-        user has OWNERSHIP grants, which will not always be the case.
-
-        Then, if the procedure is created with `EXECUTE AS CALLER`, we can still try to
-        get the definition with a DESCRIBE.
-        """
-        res = self.engine.execute(
-            SNOWFLAKE_DESC_STORED_PROCEDURE.format(
-                database_name=self.context.database,
-                schema_name=self.context.database_schema,
-                procedure_name=stored_procedure.name,
-                procedure_signature=stored_procedure.unquote_signature(),
+            # After finding schema fqn (based on partial schema name) we know where we can create table
+            # and we move forward with creating request.
+            if om_schema_fqn:
+                columns = OpenlineageSource._get_om_table_columns(table) or []
+
+                request = CreateTableRequest(
+                    name=table_details.name,
+                    columns=columns,
+                    databaseSchema=om_schema_fqn,
+                )
+
+                return Either(right=request)
+
+        return None
+
+    @classmethod
+    def _get_ol_table_name(cls, table: Dict) -> str:
+        return "/".join(table.get(f) for f in ["namespace", "name"]).replace("//", "/")
+
+    def _build_ol_name_to_fqn_map(self, tables: List):
+        result = {}
+
+        for table in tables:
+            table_fqn = self._get_table_fqn(OpenlineageSource._get_table_details(table))
+
+            if table_fqn:
+                result[OpenlineageSource._get_ol_table_name(table)] = table_fqn
+
+        return result
+
+    @classmethod
+    def _create_output_lineage_dict(
+        cls, lineage_info: List[Tuple[str, str, str, str]]
+    ) -> Dict[str, Dict[str, List[ColumnLineage]]]:
+        result = defaultdict(lambda: defaultdict(list))
+        for (output_table, input_table, output_column), group in groupby(
+            lineage_info, lambda x: x[:3]
+        ):
+            input_columns = [input_col for _, _, _, input_col in group]
+
+            result[output_table][input_table] += [
+                ColumnLineage(toColumn=output_column, fromColumns=input_columns)
+            ]
+
+        return result
+
+    def _get_column_lineage(
+        self, inputs: List, outputs: List
+    ) -> Dict[str, Dict[str, List[ColumnLineage]]]:
+        _result: List = []
+
+        ol_name_to_fqn_map = self._build_ol_name_to_fqn_map(inputs + outputs)
+
+        for table in outputs:
+            output_table_fqn = self._get_table_fqn(
+                OpenlineageSource._get_table_details(table)
             )
-        )
-        return dict(res.all()).get("body", "")
+            for field_name, field_spec in (
+                table.get("facets", {})
+                .get("columnLineage", {})
+                .get("fields", {})
+                .items()
+            ):
+                for input_field in field_spec.get("inputFields", []):
+                    input_table_ol_name = OpenlineageSource._get_ol_table_name(
+                        input_field
+                    )
 
-    def yield_stored_procedure(
-        self, stored_procedure: SnowflakeStoredProcedure
-    ) -> Iterable[Either[CreateStoredProcedureRequest]]:
-        """Prepare the stored procedure payload"""
+                    _result.append(  # output table, input table, output column, input column
+                        (
+                            output_table_fqn,
+                            ol_name_to_fqn_map.get(input_table_ol_name),
+                            f"{output_table_fqn}.{field_name}",
+                            f'{ol_name_to_fqn_map.get(input_table_ol_name)}.{input_field.get("field")}',
+                        )
+                    )
 
+        return OpenlineageSource._create_output_lineage_dict(_result)
+
+    def yield_pipeline(
+        self, pipeline_details: OpenLineageEvent
+    ) -> Iterable[Either[CreatePipelineRequest]]:
+        pipeline_name = self.get_pipeline_name(pipeline_details)
         try:
-            stored_procedure_request = CreateStoredProcedureRequest(
-                name=EntityName(__root__=stored_procedure.name),
-                description=stored_procedure.comment,
-                storedProcedureCode=StoredProcedureCode(
-                    language=STORED_PROC_LANGUAGE_MAP.get(stored_procedure.language),
-                    code=stored_procedure.definition,
-                ),
-                databaseSchema=fqn.build(
-                    metadata=self.metadata,
-                    entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
-                ),
-                sourceUrl=SourceUrl(
-                    __root__=self._get_source_url_root(
-                        database_name=self.context.database,
-                        schema_name=self.context.database_schema,
-                    )
-                    + f"/procedure/{stored_procedure.name}"
-                    + f"{stored_procedure.signature if stored_procedure.signature else ''}"
-                ),
+            description = f"""```json
+            {json.dumps(pipeline_details.run_facet, indent=4).strip()}```"""
+            request = CreatePipelineRequest(
+                name=pipeline_name,
+                service=self.context.pipeline_service,
+                description=description,
             )
-            yield Either(right=stored_procedure_request)
-            self.register_record_stored_proc_request(stored_procedure_request)
 
-        except Exception as exc:
+            yield Either(right=request)
+            self.register_record(pipeline_request=request)
+        except ValueError:
             yield Either(
                 left=StackTraceError(
-                    name=stored_procedure.name,
-                    error=f"Error yielding Stored Procedure [{stored_procedure.name}] due to [{exc}]",
-                    stackTrace=traceback.format_exc(),
-                )
+                    name=pipeline_name,
+                    message="Failed to collect metadata required for pipeline creation.",
+                ),
+                stackTrace=traceback.format_exc(),
             )
 
-    def get_stored_procedure_queries_dict(self) -> Dict[str, List[QueryByProcedure]]:
-        """
-        Return the dictionary associating stored procedures to the
-        queries they triggered
-        """
-        start, _ = get_start_and_end(self.source_config.queryLogDuration)
-        query = SNOWFLAKE_GET_STORED_PROCEDURE_QUERIES.format(
-            start_date=start,
-        )
+    def yield_pipeline_lineage_details(
+        self, pipeline_details: OpenLineageEvent
+    ) -> Iterable[Either[AddLineageRequest]]:
+        inputs, outputs = pipeline_details.inputs, pipeline_details.outputs
+
+        input_edges: List[LineageNode] = []
+        output_edges: List[LineageNode] = []
+
+        for spec in [(inputs, input_edges), (outputs, output_edges)]:
+            tables, tables_list = spec
+
+            for table in tables:
+                create_table_request = self.get_create_table_request(table)
 
-        queries_dict = self.procedure_queries_dict(
-            query=query,
+                if create_table_request:
+                    yield create_table_request
+
+                table_fqn = self._get_table_fqn(
+                    OpenlineageSource._get_table_details(table)
+                )
+
+                if table_fqn:
+                    tables_list.append(
+                        LineageNode(
+                            fqn=TableFQN(value=table_fqn),
+                            uuid=self.metadata.get_by_name(Table, table_fqn).id,
+                        )
+                    )
+
+        edges = [
+            LineageEdge(from_node=n[0], to_node=n[1])
+            for n in product(input_edges, output_edges)
+        ]
+
+        column_lineage = self._get_column_lineage(inputs, outputs)
+
+        pipeline_fqn = fqn.build(
+            metadata=self.metadata,
+            entity_type=Pipeline,
+            service_name=self.context.pipeline_service,
+            pipeline_name=self.context.pipeline,
         )
 
-        return queries_dict
+        pipeline_entity = self.metadata.get_by_name(entity=Pipeline, fqn=pipeline_fqn)
+        for edge in edges:
+            yield Either(
+                right=AddLineageRequest(
+                    edge=EntitiesEdge(
+                        fromEntity=EntityReference(
+                            id=edge.from_node.uuid, type=edge.from_node.node_type
+                        ),
+                        toEntity=EntityReference(
+                            id=edge.to_node.uuid, type=edge.to_node.node_type
+                        ),
+                        lineageDetails=LineageDetails(
+                            pipeline=EntityReference(
+                                id=pipeline_entity.id.__root__,
+                                type="pipeline",
+                            ),
+                            description=f"Lineage extracted from OpenLineage job: {pipeline_details.job['name']}",
+                            source=Source.OpenLineage,
+                            columnsLineage=column_lineage.get(
+                                edge.to_node.fqn.value, {}
+                            ).get(edge.from_node.fqn.value, []),
+                        ),
+                    ),
+                )
+            )
+
+    def get_pipelines_list(self) -> Optional[List[Any]]:
+        """Get List of all pipelines"""
+        try:
+            consumer = self.client
+            session_active = True
+            empty_msg_cnt = 0
+            pool_timeout = self.service_connection.poolTimeout
+            while session_active:
+                message = consumer.poll(timeout=pool_timeout)
+                if message is None:
+                    logger.debug("no new messages")
+                    empty_msg_cnt += 1
+                    if (
+                        empty_msg_cnt * pool_timeout
+                        > self.service_connection.sessionTimeout
+                    ):
+                        # There is no new messages, timeout is passed
+                        session_active = False
+                else:
+                    logger.debug(f"new message {message.value()}")
+                    empty_msg_cnt = 0
+                    try:
+                        _result = message_to_open_lineage_event(
+                            json.loads(message.value())
+                        )
+                        result = self._filter_event_by_type(_result, EventType.COMPLETE)
+                        if result:
+                            yield result
+                    except Exception as e:
+                        logger.debug(e)
+
+        except Exception as e:
+            traceback.print_exc()
+
+            raise InvalidSourceException(f"Failed to read from Kafka: {str(e)}")
+
+        finally:
+            # Close down consumer to commit final offsets.
+            # @todo address this
+            consumer.close()
+
+    def get_pipeline_name(self, pipeline_details: OpenLineageEvent) -> str:
+        return OpenlineageSource._render_pipeline_name(pipeline_details)
+
+    def yield_pipeline_status(
+        self, pipeline_details: OpenLineageEvent
+    ) -> Iterable[Either[OMetaPipelineStatus]]:
+        pass
+
+    def mark_pipelines_as_deleted(self):
+        """
+        OpenLineage pipelines are coming from streaming data and hence subsequent executions of ingestion processes
+        can cause deletion of a pipeline. Because of this we turn off pipeline deletion by overwriting this method
+        and leaving it blank. Setting 'Mark Deleted Pipelines' in ingestion process will have no effect!
+        """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/models.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,15 +8,16 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Snowflake models
 """
 import urllib
-from typing import Optional
+from datetime import datetime
+from typing import List, Optional
 
 from pydantic import BaseModel, Field, validator
 from requests.utils import quote
 
 from metadata.generated.schema.entity.data.storedProcedure import Language
 from metadata.utils.logger import ingestion_logger
 
@@ -68,7 +69,29 @@
             return f"({quote(', '.join(clean_signature_list))})"
         except Exception as exc:
             logger.warning(f"Error cleaning up Stored Procedure signature - [{exc}]")
             return signature
 
     def unquote_signature(self) -> Optional[str]:
         return urllib.parse.unquote(self.signature) if self.signature else "()"
+
+
+class SnowflakeTable(BaseModel):
+    """Models the items returned from the Table and View Queries used to get the entities to process.
+    :name: Holds the table/view name.
+    :deleted: Holds either a datetime if the table was deleted or None.
+    """
+
+    name: str
+    deleted: Optional[datetime]
+
+
+class SnowflakeTableList(BaseModel):
+    """Understands how to return the deleted and not deleted tables/views from a given list."""
+
+    tables: List[SnowflakeTable]
+
+    def get_deleted(self) -> List[SnowflakeTable]:
+        return [table for table in self.tables if table.deleted]
+
+    def get_not_deleted(self) -> List[SnowflakeTable]:
+        return [table for table in self.tables if not table.deleted]
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/queries.py`

 * *Files 25% similar despite different names*

```diff
@@ -41,49 +41,163 @@
     select TAG_NAME, TAG_VALUE, OBJECT_DATABASE, OBJECT_SCHEMA, OBJECT_NAME, COLUMN_NAME
     from snowflake.account_usage.tag_references
     where OBJECT_DATABASE = '{database_name}'
       and OBJECT_SCHEMA = '{schema_name}'
 """
 )
 
-SNOWFLAKE_GET_TABLE_NAMES = """
-select TABLE_NAME from information_schema.tables 
-where TABLE_SCHEMA = '{}' and TABLE_TYPE = 'BASE TABLE'
-"""
-
 SNOWFLAKE_GET_EXTERNAL_TABLE_NAMES = """
-select TABLE_NAME from information_schema.tables 
-where TABLE_SCHEMA = '{}' AND TABLE_TYPE = 'EXTERNAL TABLE'
+select TABLE_NAME, NULL from information_schema.tables
+where TABLE_SCHEMA = '{schema}' AND TABLE_TYPE = 'EXTERNAL TABLE'
 """
 
+SNOWFLAKE_INCREMENTAL_GET_EXTERNAL_TABLE_NAMES = """
+select TABLE_NAME, DELETED
+from (
+    select
+        TABLE_NAME,
+        DELETED,
+        ROW_NUMBER() over (
+            partition by TABLE_NAME order by LAST_DDL desc
+        ) as ROW_NUMBER
+    from snowflake.account_usage.tables
+    where TABLE_CATALOG = '{database}'
+      and TABLE_SCHEMA = '{schema}'
+      and TABLE_TYPE = 'EXTERNAL TABLE'
+      and DATE_PART(epoch_millisecond, LAST_DDL) >= '{date}'
+)
+where ROW_NUMBER = 1
+"""
 
 SNOWFLAKE_GET_WITHOUT_TRANSIENT_TABLE_NAMES = """
-select TABLE_NAME from information_schema.tables 
-where TABLE_SCHEMA = '{}' 
-AND TABLE_TYPE = 'BASE TABLE' 
+select TABLE_NAME, NULL from information_schema.tables
+where TABLE_SCHEMA = '{schema}'
+AND TABLE_TYPE = 'BASE TABLE'
 AND IS_TRANSIENT != 'YES'
+AND IS_DYNAMIC != 'YES'
+"""
+
+SNOWFLAKE_INCREMENTAL_GET_WITHOUT_TRANSIENT_TABLE_NAMES = """
+select TABLE_NAME, DELETED
+from (
+    select
+        TABLE_NAME,
+        DELETED,
+        ROW_NUMBER() over (
+            partition by TABLE_NAME order by LAST_DDL desc
+        ) as ROW_NUMBER
+    from snowflake.account_usage.tables
+    where TABLE_CATALOG = '{database}'
+    and TABLE_SCHEMA = '{schema}'
+    and TABLE_TYPE = 'BASE TABLE'
+    and IS_TRANSIENT != 'YES'
+    AND IS_DYNAMIC != 'YES'
+    and DATE_PART(epoch_millisecond, LAST_DDL) >= '{date}'
+)
+where ROW_NUMBER = 1
 """
 
 SNOWFLAKE_GET_VIEW_NAMES = """
-select TABLE_NAME from information_schema.tables 
-where TABLE_SCHEMA = '{}' and TABLE_TYPE = 'VIEW'
+select TABLE_NAME, NULL from information_schema.tables
+where TABLE_SCHEMA = '{schema}' and TABLE_TYPE = 'VIEW'
+"""
+
+SNOWFLAKE_INCREMENTAL_GET_VIEW_NAMES = """
+select TABLE_NAME, DELETED
+from (
+    select
+        TABLE_NAME,
+        DELETED,
+        ROW_NUMBER() over (
+            partition by TABLE_NAME order by LAST_DDL desc
+        ) as ROW_NUMBER
+    from snowflake.account_usage.tables
+    where  TABLE_CATALOG = '{database}'
+    and TABLE_SCHEMA = '{schema}'
+    and TABLE_TYPE = 'VIEW'
+    and DATE_PART(epoch_millisecond, LAST_DDL) >= '{date}'
+)
+where ROW_NUMBER = 1
 """
 
 SNOWFLAKE_GET_MVIEW_NAMES = """
-select TABLE_NAME from information_schema.tables 
-where TABLE_SCHEMA = '{}' and TABLE_TYPE = 'MATERIALIZED VIEW'
+select TABLE_NAME, NULL from information_schema.tables
+where TABLE_SCHEMA = '{schema}' and TABLE_TYPE = 'MATERIALIZED VIEW'
+"""
+
+SNOWFLAKE_INCREMENTAL_GET_MVIEW_NAMES = """
+select TABLE_NAME, DELETED
+from (
+    select
+        TABLE_NAME,
+        DELETED,
+        ROW_NUMBER() over (
+            partition by TABLE_NAME order by LAST_DDL desc
+        ) as ROW_NUMBER
+    from snowflake.account_usage.tables
+    where  TABLE_CATALOG = '{database}'
+    and TABLE_SCHEMA = '{schema}'
+    and TABLE_TYPE = 'MATERIALIZED VIEW'
+    and DATE_PART(epoch_millisecond, LAST_DDL) >= '{date}'
+)
+where ROW_NUMBER = 1
 """
 
 SNOWFLAKE_GET_TRANSIENT_NAMES = """
-select TABLE_NAME from information_schema.tables 
-where TABLE_SCHEMA = '{}' 
-AND TABLE_TYPE = 'BASE TABLE' 
+select TABLE_NAME, NULL from information_schema.tables
+where TABLE_SCHEMA = '{schema}'
+AND TABLE_TYPE = 'BASE TABLE'
 AND IS_TRANSIENT = 'YES'
 """
 
+SNOWFLAKE_INCREMENTAL_GET_TRANSIENT_NAMES = """
+select TABLE_NAME, DELETED
+from (
+    select
+        TABLE_NAME,
+        DELETED,
+        ROW_NUMBER() over (
+            partition by TABLE_NAME order by LAST_DDL desc
+        ) as ROW_NUMBER
+    from snowflake.account_usage.tables
+    where TABLE_CATALOG = '{database}'
+    and TABLE_SCHEMA = '{schema}'
+    and TABLE_TYPE = 'BASE TABLE'
+    and IS_TRANSIENT = 'YES'
+    and DATE_PART(epoch_millisecond, LAST_DDL) >= '{date}'
+)
+where ROW_NUMBER = 1
+"""
+
+SNOWFLAKE_GET_DYNAMIC_TABLE_NAMES = """
+select TABLE_NAME, NULL from information_schema.tables
+where TABLE_SCHEMA = '{schema}'
+AND TABLE_TYPE = 'BASE TABLE'
+AND IS_DYNAMIC = 'YES'
+"""
+
+SNOWFLAKE_INCREMENTAL_GET_DYNAMIC_TABLE_NAMES = """
+select TABLE_NAME, DELETED
+from (
+    select
+        TABLE_NAME,
+        DELETED,
+        ROW_NUMBER() over (
+            partition by TABLE_NAME order by LAST_DDL desc
+        ) as ROW_NUMBER
+    from snowflake.account_usage.tables
+    where TABLE_CATALOG = '{database}'
+    and TABLE_SCHEMA = '{schema}'
+    and TABLE_TYPE = 'BASE TABLE'
+    and IS_DYNAMIC = 'YES'
+    and DATE_PART(epoch_millisecond, LAST_DDL) >= '{date}'
+)
+where ROW_NUMBER = 1
+"""
+
 SNOWFLAKE_GET_COMMENTS = textwrap.dedent(
     """
   select
     TABLE_SCHEMA "schema",
     TABLE_NAME "table_name",
     COMMENT "table_comment"
 from information_schema.TABLES
@@ -92,43 +206,47 @@
 """
 )
 
 SNOWFLAKE_GET_CLUSTER_KEY = """
   select CLUSTERING_KEY,
           TABLE_SCHEMA,
           TABLE_NAME
-  from   information_schema.tables 
+  from   information_schema.tables
   where  TABLE_TYPE = 'BASE TABLE'
   and CLUSTERING_KEY is not null
 """
 
 
 SNOWFLAKE_GET_SCHEMA_COMMENTS = """
-SELECT 
+SELECT
       catalog_name DATABASE_NAME,
       SCHEMA_NAME,
-      COMMENT 
+      COMMENT
 FROM information_schema.schemata
 """
 
 
 SNOWFLAKE_GET_DATABASE_COMMENTS = """
 select DATABASE_NAME,COMMENT from information_schema.databases
 """
 
+SNOWFLAKE_GET_EXTERNAL_LOCATIONS = """
+SHOW EXTERNAL TABLES IN DATABASE "{database_name}"
+"""
+
 SNOWFLAKE_TEST_FETCH_TAG = """
 select TAG_NAME from snowflake.account_usage.tag_references limit 1
 """
 
 SNOWFLAKE_TEST_GET_QUERIES = """
 SELECT query_text from snowflake.account_usage.query_history limit 1
 """
 
 SNOWFLAKE_TEST_GET_TABLES = """
-SELECT TABLE_NAME FROM "{database_name}".information_schema.tables LIMIT 1 
+SELECT TABLE_NAME FROM "{database_name}".information_schema.tables LIMIT 1
 """
 
 SNOWFLAKE_GET_DATABASES = "SHOW DATABASES"
 
 
 SNOWFLAKE_GET_SCHEMA_COLUMNS = """
 SELECT /* sqlalchemy:_get_schema_columns */
@@ -151,26 +269,26 @@
 
 SNOWFLAKE_GET_ORGANIZATION_NAME = "SELECT CURRENT_ORGANIZATION_NAME() AS NAME"
 
 SNOWFLAKE_GET_CURRENT_ACCOUNT = "SELECT CURRENT_ACCOUNT_NAME() AS ACCOUNT"
 
 SNOWFLAKE_LIFE_CYCLE_QUERY = textwrap.dedent(
     """
-select 
+select
 table_name as table_name,
 created as created_at
 from snowflake.account_usage.tables
 where table_schema = '{schema_name}'
 and table_catalog = '{database_name}'
 """
 )
 
 SNOWFLAKE_GET_STORED_PROCEDURES = textwrap.dedent(
     """
-SELECT 
+SELECT
   PROCEDURE_NAME AS name,
   PROCEDURE_OWNER AS owner,
   PROCEDURE_LANGUAGE AS language,
   PROCEDURE_DEFINITION AS definition,
   ARGUMENT_SIGNATURE AS signature,
   COMMENT as comment
 FROM INFORMATION_SCHEMA.PROCEDURES
@@ -189,15 +307,15 @@
     SELECT
       QUERY_TEXT,
       SESSION_ID,
       START_TIME,
       END_TIME
     FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY SP
     WHERE QUERY_TYPE = 'CALL'
-      AND START_TIME >= '{start_date}' 
+      AND START_TIME >= '{start_date}'
 ),
 Q_HISTORY AS (
     SELECT
       QUERY_TYPE,
       QUERY_TEXT,
       SESSION_ID,
       START_TIME,
@@ -206,15 +324,15 @@
       USER_NAME,
       SCHEMA_NAME,
       DATABASE_NAME
     FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY SP
     WHERE QUERY_TYPE <> 'CALL'
       AND QUERY_TEXT NOT LIKE '/* {{"app": "OpenMetadata", %%}} */%%'
       AND QUERY_TEXT NOT LIKE '/* {{"app": "dbt", %%}} */%%'
-      AND START_TIME >= '{start_date}' 
+      AND START_TIME >= '{start_date}'
 )
 SELECT
   Q.QUERY_TYPE AS QUERY_TYPE,
   Q.DATABASE_NAME AS QUERY_DATABASE_NAME,
   Q.SCHEMA_NAME AS QUERY_SCHEMA_NAME,
   SP.QUERY_TEXT AS PROCEDURE_TEXT,
   SP.START_TIME AS PROCEDURE_START_TIME,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/query_parser.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,15 +36,17 @@
 
 class SnowflakeQueryParserSource(QueryParserSource, ABC):
     """
     Snowflake base for Usage and Lineage
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: SnowflakeConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SnowflakeConnection):
             raise InvalidSourceException(
                 f"Expected SnowflakeConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/snowflake/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/snowflake/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sql_column_handler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sql_column_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,14 +22,15 @@
     ColumnName,
     Constraint,
     ConstraintType,
     DataType,
     TableConstraint,
 )
 from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
+from metadata.utils.execution_time_tracker import calculate_execution_time
 from metadata.utils.helpers import clean_up_starting_ending_double_quotes_in_string
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class SqlColumnHandlerMixin:
@@ -191,14 +192,15 @@
             ] = ColumnTypeParser._parse_primitive_datatype_string(  # pylint: disable=protected-access
                 array_data_type_display[6:-1]
             )[
                 "dataType"
             ]
         return Column(**parsed_string)
 
+    @calculate_execution_time()
     def get_columns_and_constraints(  # pylint: disable=too-many-locals
         self, schema_name: str, table_name: str, db_name: str, inspector: Inspector
     ) -> Tuple[
         Optional[List[Column]], Optional[List[TableConstraint]], Optional[List[Dict]]
     ]:
         """
         Get columns types and constraints information
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sqlalchemy_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/sqlalchemy_source.py`

 * *Files 1% similar despite different names*

```diff
@@ -20,29 +20,29 @@
 from metadata.generated.schema.entity.data.table import Column
 from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
     DatabaseServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.ingestion.models.topology import TopologyContext
+from metadata.ingestion.models.topology import TopologyContextManager
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class SqlAlchemySource(ABC):
     """
     Sqlalchemy Source Abstract Class
     """
 
     engine: Engine
     metadata: OpenMetadata
-    context: TopologyContext
+    context: TopologyContextManager
     database_source_state: Set
     source_config: DatabaseServiceMetadataPipeline
     config: WorkflowSource
 
     @abstractmethod
     def standardize_table_name(self, schema_name: str, table: str) -> Tuple[str, str]:
         """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sqlite/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/connection.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,51 +15,55 @@
 from typing import Optional
 
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.sqliteConnection import (
-    SQLiteConnection,
+from metadata.generated.schema.entity.services.connections.database.verticaConnection import (
+    VerticaConnection,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
+    get_connection_url_common,
 )
 from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.database.vertica.queries import (
+    VERTICA_LIST_DATABASES,
+    VERTICA_TEST_GET_QUERIES,
+)
 
 
-def get_connection_url(connection: SQLiteConnection) -> str:
-    database_mode = connection.databaseMode if connection.databaseMode else ":memory:"
-
-    return f"{connection.scheme.value}:///{database_mode}"
-
-
-def get_connection(connection: SQLiteConnection) -> Engine:
+def get_connection(connection: VerticaConnection) -> Engine:
     """
     Create connection
     """
     return create_generic_db_connection(
         connection=connection,
-        get_connection_url_fn=get_connection_url,
+        get_connection_url_fn=get_connection_url_common,
         get_connection_args_fn=get_connection_args_common,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: SQLiteConnection,
+    service_connection: VerticaConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
+    queries = {
+        "GetQueries": VERTICA_TEST_GET_QUERIES,
+        "GetDatabases": VERTICA_LIST_DATABASES,
+    }
     test_connection_db_common(
         metadata=metadata,
         engine=engine,
         service_connection=service_connection,
         automation_workflow=automation_workflow,
+        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/sqlite/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/redpanda/metadata.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,39 +4,35 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Sqlite source implementation.
-Useful for testing!
+RedPanda source ingestion
 """
+from typing import Optional
 
-from metadata.generated.schema.entity.services.connections.database.sqliteConnection import (
-    SQLiteConnection,
+from metadata.generated.schema.entity.services.connections.messaging.redpandaConnection import (
+    RedpandaConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
-
+from metadata.ingestion.source.messaging.common_broker_source import CommonBrokerSource
 
-class SqliteSource(CommonDbSourceService):
-    """
-    Implements the necessary methods to extract
-    Database metadata from Sqlite Source
-    """
 
+class RedpandaSource(CommonBrokerSource):
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection = config.serviceConnection.__root__.config
-        if not isinstance(connection, SQLiteConnection):
+        connection: RedpandaConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, RedpandaConnection):
             raise InvalidSourceException(
-                f"Expected SQLiteConnection, but got {connection}"
+                f"Expected RedpandaConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/stored_procedures_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/stored_procedures_mixin.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,15 +33,15 @@
 from metadata.generated.schema.type.basic import SqlQuery, Timestamp
 from metadata.generated.schema.type.entityLineage import Source as LineageSource
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.status import Status
 from metadata.ingestion.lineage.models import ConnectionTypeDialectMapper
 from metadata.ingestion.lineage.sql_lineage import get_lineage_by_query
-from metadata.ingestion.models.topology import TopologyContext
+from metadata.ingestion.models.topology import TopologyContextManager
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.stored_procedures import get_procedure_name_from_call
 from metadata.utils.time_utils import convert_timestamp_to_milliseconds
 
 logger = ingestion_logger()
 
@@ -77,15 +77,15 @@
     5. Ingest the Query
 
     This Mixin is in charge from 3 - 5 in order to handle this process efficiently.
 
     It should be inherited in those Sources that implement Stored Procedure ingestion.
     """
 
-    context: TopologyContext
+    context: TopologyContextManager
     status: Status
     source_config: DatabaseServiceMetadataPipeline
     engine: Engine
     metadata: OpenMetadata
 
     @abstractmethod
     def get_stored_procedure_queries_dict(self) -> Dict[str, List[QueryByProcedure]]:
@@ -144,24 +144,24 @@
 
         return False
 
     def yield_procedure_lineage(
         self, query_by_procedure: QueryByProcedure, procedure: StoredProcedure
     ) -> Iterable[Either[AddLineageRequest]]:
         """Add procedure lineage from its query"""
-        self.context.stored_procedure_query_lineage = False
+        self.context.get().stored_procedure_query_lineage = False
         if self.is_lineage_query(
             query_type=query_by_procedure.query_type,
             query_text=query_by_procedure.query_text,
         ):
-            self.context.stored_procedure_query_lineage = True
+            self.context.get().stored_procedure_query_lineage = True
             for either_lineage in get_lineage_by_query(
                 self.metadata,
                 query=query_by_procedure.query_text,
-                service_name=self.context.database_service,
+                service_name=self.context.get().database_service,
                 database_name=query_by_procedure.query_database_name,
                 schema_name=query_by_procedure.query_schema_name,
                 dialect=ConnectionTypeDialectMapper.dialect_of(
                     self.service_connection.type.value
                 ),
                 timeout_seconds=self.source_config.queryParsingTimeoutLimit,
                 lineage_source=LineageSource.QueryLineage,
@@ -189,29 +189,31 @@
                         int(query_by_procedure.query_start_time.timestamp())
                     )
                 ),
                 triggeredBy=EntityReference(
                     id=procedure.id,
                     type="storedProcedure",
                 ),
-                processedLineage=bool(self.context.stored_procedure_query_lineage),
-                service=self.context.database_service,
+                processedLineage=bool(
+                    self.context.get().stored_procedure_query_lineage
+                ),
+                service=self.context.get().database_service,
             )
         )
 
     def yield_procedure_lineage_and_queries(
         self,
     ) -> Iterable[Either[Union[AddLineageRequest, CreateQueryRequest]]]:
         """Get all the queries and procedures list and yield them"""
-        if self.context.stored_procedures:
+        if self.context.get().stored_procedures:
             logger.info("Processing Lineage for Stored Procedures")
             # First, get all the query history
             queries_dict = self.get_stored_procedure_queries_dict()
             # Then for each procedure, iterate over all its queries
-            for procedure_fqn in self.context.stored_procedures:
+            for procedure_fqn in self.context.get().stored_procedures:
                 procedure = self.metadata.get_by_name(
                     entity=StoredProcedure, fqn=procedure_fqn
                 )
                 if procedure:
                     logger.debug(f"Processing Lineage for [{procedure.name}]")
                     for query_by_procedure in (
                         queries_dict.get(procedure.name.__root__.lower()) or []
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/connection.py`

 * *Files 8% similar despite different names*

```diff
@@ -87,14 +87,21 @@
     return get_connection_args_common(connection)
 
 
 def get_connection(connection: TrinoConnection) -> Engine:
     """
     Create connection
     """
+    if connection.verify:
+        connection.connectionArguments = (
+            connection.connectionArguments or init_empty_connection_arguments()
+        )
+        connection.connectionArguments.__root__["verify"] = {
+            "verify": connection.verify
+        }
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url,
         get_connection_args_fn=get_connection_args,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/metadata.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 Trino source implementation.
 """
 import re
 import traceback
 from copy import deepcopy
 from typing import Any, Dict, Iterable, List, Optional, Tuple
 
-from sqlalchemy import exc, inspect, sql, util
+from sqlalchemy import exc, sql, util
 from sqlalchemy.engine.base import Connection
 from sqlalchemy.sql import sqltypes
 from trino.sqlalchemy import datatype, error
 from trino.sqlalchemy.datatype import JSON
 from trino.sqlalchemy.dialect import TrinoDialect
 
 from metadata.generated.schema.entity.data.database import Database
@@ -179,15 +179,17 @@
     """
 
     ColumnTypeParser._COLUMN_TYPE_MAPPING[  # pylint: disable=protected-access
         JSON
     ] = "JSON"
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: TrinoConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, TrinoConnection):
             raise InvalidSourceException(
                 f"Expected TrinoConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -199,30 +201,31 @@
         :param database_name: new database to set
         """
         logger.info(f"Ingesting from catalog: {database_name}")
 
         new_service_connection = deepcopy(self.service_connection)
         new_service_connection.catalog = database_name
         self.engine = get_connection(new_service_connection)
-        self.inspector = inspect(self.engine)
+        self._connection_map = {}  # Lazy init as well
+        self._inspector_map = {}
 
     def get_database_names(self) -> Iterable[str]:
         configured_catalog = self.service_connection.catalog
         if configured_catalog:
             self.set_inspector(database_name=configured_catalog)
             yield configured_catalog
         else:
             results = self.connection.execute("SHOW CATALOGS")
             for res in results:
                 if res:
                     new_catalog = res[0]
                     database_fqn = fqn.build(
                         self.metadata,
                         entity_type=Database,
-                        service_name=self.context.database_service,
+                        service_name=self.context.get().database_service,
                         database_name=new_catalog,
                     )
                     if filter_by_database(
                         self.source_config.databaseFilterPattern,
                         database_fqn
                         if self.source_config.useFqnForFiltering
                         else new_catalog,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/mssql/query_parser.py`

 * *Files 20% similar despite different names*

```diff
@@ -5,39 +5,42 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Trino usage module
+MSSQL usage module
 """
 from abc import ABC
+from typing import Optional
 
-from metadata.generated.schema.entity.services.connections.database.trinoConnection import (
-    TrinoConnection,
+from metadata.generated.schema.entity.services.connections.database.mssqlConnection import (
+    MssqlConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.database.query_parser_source import QueryParserSource
 
 
-class TrinoQueryParserSource(QueryParserSource, ABC):
+class MssqlQueryParserSource(QueryParserSource, ABC):
     """
-    Trino base for Usage and Lineage
+    MSSQL base for Usage and Lineage
     """
 
     filters: str
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: TrinoConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, TrinoConnection):
+        connection: MssqlConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, MssqlConnection):
             raise InvalidSourceException(
-                f"Expected TrinoConnection, but got {connection}"
+                f"Expected MssqlConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/trino/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/trino/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/lineage.py`

 * *Files 2% similar despite different names*

```diff
@@ -68,15 +68,17 @@
 
     def prepare(self):
         """
         By default, there's nothing to prepare
         """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: UnityCatalogConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, UnityCatalogConnection):
             raise InvalidSourceException(
                 f"Expected UnityCatalogConnection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -47,19 +47,21 @@
     DatabaseServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import InvalidSourceException
-from metadata.ingestion.lineage.sql_lineage import get_column_fqn
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
 from metadata.ingestion.source.database.database_service import DatabaseServiceSource
+from metadata.ingestion.source.database.external_table_lineage_mixin import (
+    ExternalTableLineageMixin,
+)
 from metadata.ingestion.source.database.multi_db_source import MultiDBSource
 from metadata.ingestion.source.database.stored_procedures_mixin import QueryByProcedure
 from metadata.ingestion.source.database.unitycatalog.connection import get_connection
 from metadata.ingestion.source.database.unitycatalog.models import (
     ColumnJson,
     ElementType,
     ForeignConstrains,
@@ -70,46 +72,52 @@
 from metadata.utils.db_utils import get_view_lineage
 from metadata.utils.filters import filter_by_database, filter_by_schema, filter_by_table
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-class UnitycatalogSource(DatabaseServiceSource, MultiDBSource):
+class UnitycatalogSource(
+    ExternalTableLineageMixin, DatabaseServiceSource, MultiDBSource
+):
     """
     Implements the necessary methods to extract
     Database metadata from Databricks Source using
     the unity catalog source
     """
 
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__()
         self.config = config
         self.source_config: DatabaseServiceMetadataPipeline = (
             self.config.sourceConfig.config
         )
-        self.context.table_views = []
+        self.context.get_global().table_views = []
         self.metadata = metadata
         self.service_connection: UnityCatalogConnection = (
             self.config.serviceConnection.__root__.config
         )
+        self.external_location_map = {}
         self.client = get_connection(self.service_connection)
         self.connection_obj = self.client
         self.table_constraints = []
+        self.context.storage_location = None
         self.test_connection()
 
     def get_configured_database(self) -> Optional[str]:
         return self.service_connection.catalog
 
     def get_database_names_raw(self) -> Iterable[str]:
         for catalog in self.client.catalogs.list():
             yield catalog.name
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: UnityCatalogConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, UnityCatalogConnection):
             raise InvalidSourceException(
                 f"Expected UnityCatalogConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -129,15 +137,15 @@
             yield self.service_connection.catalog
         else:
             for catalog_name in self.get_database_names_raw():
                 try:
                     database_fqn = fqn.build(
                         self.metadata,
                         entity_type=Database,
-                        service_name=self.context.database_service,
+                        service_name=self.context.get().database_service,
                         database_name=catalog_name,
                     )
                     if filter_by_database(
                         self.config.sourceConfig.config.databaseFilterPattern,
                         database_fqn
                         if self.config.sourceConfig.config.useFqnForFiltering
                         else catalog_name,
@@ -163,30 +171,30 @@
         """
         From topology.
         Prepare a database request and pass it to the sink
         """
         yield Either(
             right=CreateDatabaseRequest(
                 name=database_name,
-                service=self.context.database_service,
+                service=self.context.get().database_service,
             )
         )
 
     def get_database_schema_names(self) -> Iterable[str]:
         """
         return schema names
         """
-        catalog_name = self.context.database
+        catalog_name = self.context.get().database
         for schema in self.client.schemas.list(catalog_name=catalog_name):
             try:
                 schema_fqn = fqn.build(
                     self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                     schema_name=schema.name,
                 )
                 if filter_by_schema(
                     self.config.sourceConfig.config.schemaFilterPattern,
                     schema_fqn
                     if self.config.sourceConfig.config.useFqnForFiltering
                     else schema.name,
@@ -212,43 +220,43 @@
         """
         yield Either(
             right=CreateDatabaseSchemaRequest(
                 name=schema_name,
                 database=fqn.build(
                     metadata=self.metadata,
                     entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                 ),
             )
         )
 
     def get_tables_name_and_type(self) -> Iterable[Tuple[str, str]]:
         """
         Handle table and views.
 
         Fetches them up using the context information and
         the inspector set when preparing the db.
 
         :return: tables or views, depending on config
         """
-        schema_name = self.context.database_schema
-        catalog_name = self.context.database
+        schema_name = self.context.get().database_schema
+        catalog_name = self.context.get().database
         for table in self.client.tables.list(
             catalog_name=catalog_name,
             schema_name=schema_name,
         ):
             try:
                 table_name = table.name
                 table_fqn = fqn.build(
                     self.metadata,
                     entity_type=Table,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
-                    schema_name=self.context.database_schema,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
+                    schema_name=self.context.get().database_schema,
                     table_name=table_name,
                 )
                 if filter_by_table(
                     self.config.sourceConfig.config.tableFilterPattern,
                     table_fqn
                     if self.config.sourceConfig.config.useFqnForFiltering
                     else table_name,
@@ -259,15 +267,15 @@
                     )
                     continue
                 table_type: TableType = TableType.Regular
                 if table.table_type.value.lower() == TableType.View.value.lower():
                     table_type: TableType = TableType.View
                 if table.table_type.value.lower() == TableType.External.value.lower():
                     table_type: TableType = TableType.External
-                self.context.table_data = table
+                self.context.get().table_data = table
                 yield table_name, table_type
             except Exception as exc:
                 self.status.failed(
                     StackTraceError(
                         name=table.Name,
                         error=f"Unexpected exception to get table [{table.Name}]: {exc}",
                         stackTrace=traceback.format_exc(),
@@ -278,17 +286,21 @@
         self, table_name_and_type: Tuple[str, str]
     ) -> Iterable[Either[CreateTableRequest]]:
         """
         From topology.
         Prepare a table request and pass it to the sink
         """
         table_name, table_type = table_name_and_type
-        table = self.client.tables.get(self.context.table_data.full_name)
-        schema_name = self.context.database_schema
-        db_name = self.context.database
+        table = self.client.tables.get(self.context.get().table_data.full_name)
+        schema_name = self.context.get().database_schema
+        db_name = self.context.get().database
+        if table.storage_location and not table.storage_location.startswith("dbfs"):
+            self.external_location_map[
+                (db_name, schema_name, table_name)
+            ] = table.storage_location
         table_constraints = None
         try:
             columns = self.get_columns(table.columns)
             (
                 primary_constraints,
                 foreign_constraints,
             ) = self.get_table_constraints(table.table_constraints)
@@ -302,23 +314,23 @@
                 tableType=table_type,
                 description=table.comment,
                 columns=columns,
                 tableConstraints=table_constraints,
                 databaseSchema=fqn.build(
                     metadata=self.metadata,
                     entity_type=DatabaseSchema,
-                    service_name=self.context.database_service,
-                    database_name=self.context.database,
+                    service_name=self.context.get().database_service,
+                    database_name=self.context.get().database,
                     schema_name=schema_name,
                 ),
             )
             yield Either(right=table_request)
 
             if table_type == TableType.View or table.view_definition:
-                self.context.table_views.append(
+                self.context.get_global().table_views.append(
                     TableView(
                         table_name=table_name,
                         schema_name=schema_name,
                         db_name=db_name,
                         view_definition=(
                             f'CREATE VIEW "{db_name}"."{schema_name}"'
                             f'."{table_name}" AS {table.view_definition}'
@@ -371,26 +383,25 @@
 
         table_constraints = []
         for column in foreign_columns:
             referred_column_fqns = []
             ref_table_fqn = column.parent_table
             table_fqn_list = fqn.split(ref_table_fqn)
 
-            referred_table = fqn.search_table_from_es(
-                metadata=self.metadata,
+            referred_table_fqn = fqn.build(
+                self.metadata,
+                entity_type=Table,
                 table_name=table_fqn_list[2],
                 schema_name=table_fqn_list[1],
                 database_name=table_fqn_list[0],
-                service_name=self.context.database_service,
+                service_name=self.context.get().database_service,
             )
-            if referred_table:
+            if referred_table_fqn:
                 for parent_column in column.parent_columns:
-                    col_fqn = get_column_fqn(
-                        table_entity=referred_table, column=parent_column
-                    )
+                    col_fqn = fqn._build(referred_table_fqn, parent_column, quote=False)
                     if col_fqn:
                         referred_column_fqns.append(col_fqn)
             else:
                 continue
 
             table_constraints.append(
                 TableConstraint(
@@ -481,20 +492,20 @@
                 column_json=ColumnJson.parse_obj(json.loads(column.type_json)),
             )
             yield parsed_column
 
     def yield_view_lineage(self) -> Iterable[Either[AddLineageRequest]]:
         logger.info("Processing Lineage for Views")
         for view in [
-            v for v in self.context.table_views if v.view_definition is not None
+            v for v in self.context.get().table_views if v.view_definition is not None
         ]:
             yield from get_view_lineage(
                 view=view,
                 metadata=self.metadata,
-                service_name=self.context.database_service,
+                service_name=self.context.get().database_service,
                 connection_type=self.service_connection.type.value,
             )
 
     def yield_tag(
         self, schema_name: str
     ) -> Iterable[Either[OMetaTagAndClassification]]:
         """No tags being processed"""
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/query_parser.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 UnityCatalog Query parser module
 """
 from abc import ABC
+from typing import Optional
 
 from metadata.generated.schema.entity.services.connections.database.unityCatalogConnection import (
     UnityCatalogConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -45,15 +46,17 @@
 
     # pylint: disable=super-init-not-called
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         self._init_super(config=config, metadata=metadata)
         self.client = UnityCatalogClient(self.service_connection)
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: UnityCatalogConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, UnityCatalogConnection):
             raise InvalidSourceException(
                 f"Expected UnityCatalogConnection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/unitycatalog/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/unitycatalog/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/usage_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/usage_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/doris/connection.py`

 * *Files 5% similar despite different names*

```diff
@@ -15,55 +15,48 @@
 from typing import Optional
 
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.verticaConnection import (
-    VerticaConnection,
+from metadata.generated.schema.entity.services.connections.database.dorisConnection import (
+    DorisConnection,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
     get_connection_url_common,
 )
-from metadata.ingestion.connections.test_connections import test_connection_db_common
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.vertica.queries import (
-    VERTICA_LIST_DATABASES,
-    VERTICA_TEST_GET_QUERIES,
+from metadata.ingestion.connections.test_connections import (
+    test_connection_db_schema_sources,
 )
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
 
 
-def get_connection(connection: VerticaConnection) -> Engine:
+def get_connection(connection: DorisConnection) -> Engine:
     """
     Create connection
     """
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url_common,
         get_connection_args_fn=get_connection_args_common,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: VerticaConnection,
+    service_connection: DorisConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-    queries = {
-        "GetQueries": VERTICA_TEST_GET_QUERIES,
-        "GetDatabases": VERTICA_LIST_DATABASES,
-    }
-    test_connection_db_common(
+    test_connection_db_schema_sources(
         metadata=metadata,
         engine=engine,
         service_connection=service_connection,
         automation_workflow=automation_workflow,
-        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/lineage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/helpers.py`

 * *Files 21% similar despite different names*

```diff
@@ -4,332 +4,471 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Vertica source implementation.
+Helpers module for ingestion related methods
 """
+
+from __future__ import annotations
+
+import itertools
 import re
-import traceback
-from textwrap import dedent
-from typing import Iterable, Optional
-
-from sqlalchemy import sql, util
-from sqlalchemy.engine import reflection
-from sqlalchemy.sql import sqltypes
-from sqlalchemy_vertica.base import VerticaDialect, ischema_names
-
-from metadata.generated.schema.entity.data.database import Database
-from metadata.generated.schema.entity.services.connections.database.verticaConnection import (
-    VerticaConnection,
-)
-from metadata.generated.schema.metadataIngestion.workflow import (
-    Source as WorkflowSource,
-)
-from metadata.ingestion.api.steps import InvalidSourceException
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.column_type_parser import create_sqlalchemy_type
-from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
-from metadata.ingestion.source.database.multi_db_source import MultiDBSource
-from metadata.ingestion.source.database.vertica.queries import (
-    VERTICA_GET_COLUMNS,
-    VERTICA_GET_PRIMARY_KEYS,
-    VERTICA_LIST_DATABASES,
-    VERTICA_SCHEMA_COMMENTS,
-    VERTICA_TABLE_COMMENTS,
-    VERTICA_VIEW_DEFINITION,
-)
-from metadata.utils import fqn
-from metadata.utils.filters import filter_by_database
-from metadata.utils.logger import ingestion_logger
-from metadata.utils.sqlalchemy_utils import (
-    get_all_table_comments,
-    get_schema_descriptions,
-    get_table_comment_wrapper,
-)
-
-logger = ingestion_logger()
-
-ischema_names.update(
-    {
-        "UUID": create_sqlalchemy_type("UUID"),
-        "GEOGRAPHY": create_sqlalchemy_type("GEOGRAPHY"),
-    }
-)
+import shutil
+import sys
+from datetime import datetime, timedelta
+from math import floor, log
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple, Union
+
+import sqlparse
+from sqlparse.sql import Statement
+
+from metadata.generated.schema.entity.data.chart import ChartType
+from metadata.generated.schema.entity.data.table import Column, Table
+from metadata.generated.schema.entity.feed.suggestion import Suggestion, SuggestionType
+from metadata.generated.schema.entity.services.databaseService import DatabaseService
+from metadata.generated.schema.type.basic import EntityLink
+from metadata.generated.schema.type.tagLabel import TagLabel
+from metadata.utils.constants import DEFAULT_DATABASE
+from metadata.utils.logger import utils_logger
 
+logger = utils_logger()
+
+
+class BackupRestoreArgs:
+    def __init__(  # pylint: disable=too-many-arguments
+        self,
+        host: str,
+        user: str,
+        password: str,
+        database: str,
+        port: str,
+        options: List[str],
+        arguments: List[str],
+        schema: Optional[str] = None,
+    ):
+        self.host = host
+        self.user = user
+        self.password = password
+        self.database = database
+        self.port = port
+        self.options = options
+        self.arguments = arguments
+        self.schema = schema
 
-@reflection.cache
-def get_columns(
-    self, connection, table_name, schema=None, **kw
-):  # pylint: disable=too-many-locals,unused-argument
-    """
-    Method to handle column details
-    """
-    if schema is not None:
-        schema_condition = f"lower(table_schema) = '{schema.lower()}'"
-    else:
-        schema_condition = "1"
-
-    sql_query = sql.text(
-        dedent(
-            VERTICA_GET_COLUMNS.format(
-                table=table_name.lower(), schema_condition=schema_condition
-            )
-        )
+
+class DockerActions:
+    def __init__(
+        self,
+        start: bool,
+        stop: bool,
+        pause: bool,
+        resume: bool,
+        clean: bool,
+        reset_db: bool,
+    ):
+        self.start = start
+        self.stop = stop
+        self.pause = pause
+        self.resume = resume
+        self.clean = clean
+        self.reset_db = reset_db
+
+
+om_chart_type_dict = {
+    "line": ChartType.Line,
+    "big_number": ChartType.Line,
+    "big_number_total": ChartType.Line,
+    "dual_line": ChartType.Line,
+    "line_multi": ChartType.Line,
+    "table": ChartType.Table,
+    "dist_bar": ChartType.Bar,
+    "bar": ChartType.Bar,
+    "box_plot": ChartType.BoxPlot,
+    "boxplot": ChartType.BoxPlot,
+    "histogram": ChartType.Histogram,
+    "treemap": ChartType.Area,
+    "area": ChartType.Area,
+    "pie": ChartType.Pie,
+    "text": ChartType.Text,
+    "scatter": ChartType.Scatter,
+}
+
+
+def pretty_print_time_duration(duration: Union[int, float]) -> str:
+    """
+    Method to format and display the time
+    """
+
+    days = divmod(duration, 86400)[0]
+    hours = divmod(duration, 3600)[0]
+    minutes = divmod(duration, 60)[0]
+    seconds = round(divmod(duration, 60)[1], 2)
+    if days:
+        return f"{days}day(s) {hours}h {minutes}m {seconds}s"
+    if hours:
+        return f"{hours}h {minutes}m {seconds}s"
+    if minutes:
+        return f"{minutes}m {seconds}s"
+    return f"{seconds}s"
+
+
+def get_start_and_end(duration: int = 0):
+    """
+    Method to return start and end time based on duration
+    """
+
+    today = datetime.utcnow()
+    start = (today + timedelta(0 - duration)).replace(
+        hour=0, minute=0, second=0, microsecond=0
     )
+    # Add one day to make sure we are handling today's queries
+    end = (today + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
+    return start, end
+
+
+def snake_to_camel(snake_str):
+    """
+    Method to convert snake case text to camel case
+    """
+    split_str = snake_str.split("_")
+    split_str[0] = split_str[0].capitalize()
+    if len(split_str) > 1:
+        split_str[1:] = [u.title() for u in split_str[1:]]
+    return "".join(split_str)
+
+
+def datetime_to_ts(date: Optional[datetime]) -> Optional[int]:
+    """
+    Convert a given date to a timestamp as an Int in milliseconds
+    """
+    return int(date.timestamp() * 1_000) if date else None
 
-    spk = sql.text(
-        dedent(
-            VERTICA_GET_PRIMARY_KEYS.format(
-                table=table_name.lower(), schema_condition=schema_condition
-            )
-        )
+
+def get_formatted_entity_name(name: str) -> Optional[str]:
+    """
+    Method to get formatted entity name
+    """
+
+    return (
+        name.replace("[", "").replace("]", "").replace("<default>.", "")
+        if name
+        else None
     )
 
-    pk_columns = [x[0] for x in connection.execute(spk)]
-    columns = {}
-    for row in connection.execute(sql_query):
-        name = row.column_name
-        dtype = row.data_type.lower()
-        primary_key = name in pk_columns
-        default = row.column_default
-        nullable = row.is_nullable
-        comment = row.comment
-
-        column_info = self._get_column_info(  # pylint: disable=protected-access
-            name,
-            dtype,
-            default,
-            nullable,
-            schema,
-            comment,
-        )
-        column_info.update({"primary_key": primary_key})
-        if columns.get(name) is None or comment:
-            columns[name] = column_info
-    return columns.values()
-
-
-def _get_column_info(  # pylint: disable=too-many-locals,too-many-branches,too-many-statements
-    self,
-    name,
-    format_type,
-    default,
-    nullable,
-    schema,
-    comment,
-):
-    # strip (*) from character varying(5), timestamp(5)
-    # with time zone, geometry(POLYGON), etc.
-    attype = re.sub(r"\(.*\)", "", format_type)
-
-    charlen = re.search(r"\(([\d,]+)\)", format_type)
-    if charlen:
-        charlen = charlen.group(1)
-    args = re.search(r"\((.*)\)", format_type)
-    if args and args.group(1):
-        args = tuple(re.split(r"\s*,\s*", args.group(1)))
-    else:
-        args = ()
-    kwargs = {}
-
-    if attype == "numeric":
-        if charlen:
-            prec, scale = charlen.split(",")
-            args = (int(prec), int(scale))
-        else:
-            args = ()
-    elif attype == "integer" or attype.startswith("geography"):
-        args = ()
-    elif attype in ("timestamptz", "timetz"):
-        kwargs["timezone"] = True
-        args = ()
-    elif attype in (
-        "timestamp",
-        "time",
-    ):
-        kwargs["timezone"] = False
-        args = ()
-    elif attype.startswith("interval"):
-        field_match = re.match(r"interval (.+)", attype, re.I)
-        if charlen:
-            kwargs["precision"] = int(charlen)
-        if field_match:
-            kwargs["fields"] = field_match.group(1)
-        attype = "interval"
-        args = ()
-    elif charlen:
-        args = (int(charlen),)
-    if attype.upper() in self.ischema_names:
-        coltype = self.ischema_names[attype.upper()]
-    else:
-        coltype = None
-
-    if coltype:
-        coltype = coltype(*args, **kwargs) if callable(coltype) else coltype
-    else:
-        util.warn(f"Did not recognize type '{attype}' of column '{name}'")
-        coltype = sqltypes.NULLTYPE
-    # adjust the default value
-    autoincrement = False
-    if default is not None:
-        match = re.search(r"""(nextval\(')([^']+)('.*$)""", default)
-        if match is not None:
-            if issubclass(
-                coltype._type_affinity,  # pylint: disable=protected-access
-                sqltypes.Integer,
-            ):
-                autoincrement = True
-            # the default is related to a Sequence
-            sch = schema
-            if "." not in match.group(2) and sch is not None:
-                # unconditionally quote the schema name.  this could
-                # later be enhanced to obey quoting rules /
-                # "quote schema"
-                default = (
-                    match.group(1)
-                    + (f'"{sch}"')
-                    + "."
-                    + match.group(2)
-                    + match.group(3)
-                )
-
-    column_info = {
-        "name": name,
-        "type": coltype,
-        "nullable": nullable,
-        "system_data_type": format_type,
-        "default": default,
-        "autoincrement": autoincrement,
-        "comment": comment,
-    }
-    return column_info
 
+def replace_special_with(raw: str, replacement: str) -> str:
+    """
+    Replace special characters in a string by a hyphen
+    :param raw: raw string to clean
+    :param replacement: string used to replace
+    :return: clean string
+    """
+    return re.sub(r"[^a-zA-Z0-9]", replacement, raw)
 
-@reflection.cache
-def get_view_definition(
-    self, connection, view_name, schema=None, **kw
-):  # pylint: disable=unused-argument,unused-argument
-    """
-    If we create a view as:
-        CREATE VIEW vendor_dimension_v AS
-        SELECT vendor_key, vendor_name
-        FROM public.vendor_dimension_new;
-    Then the VIEW_DEFINITION statement from V_CATALOG.VIEWS
-    will only contain the SELECT query:
-        SELECT vendor_key, vendor_name
-        FROM public.vendor_dimension_new;
-    We will add the `CREATE VIEW XYZ AS` piece
-    to ensure that the column lineage and target table
-    can be properly inferred.
-    """
-    if schema is not None:
-        schema_condition = f"lower(table_schema) = '{schema.lower()}'"
-    else:
-        schema_condition = "1"
-
-    sql_query = sql.text(
-        dedent(
-            VERTICA_VIEW_DEFINITION.format(
-                view_name=view_name.lower(), schema_condition=schema_condition
-            )
-        )
+
+def get_standard_chart_type(raw_chart_type: str) -> ChartType.Other:
+    """
+    Get standard chart type supported by OpenMetadata based on raw chart type input
+    :param raw_chart_type: raw chart type to be standardize
+    :return: standard chart type
+    """
+    if raw_chart_type is not None:
+        return om_chart_type_dict.get(raw_chart_type.lower(), ChartType.Other)
+    return ChartType.Other
+
+
+def find_in_iter(element: Any, container: Iterable[Any]) -> Optional[Any]:
+    """
+    If the element is in the container, return it.
+    Otherwise, return None
+    :param element: to find
+    :param container: container with element
+    :return: element or None
+    """
+    return next((elem for elem in container if elem == element), None)
+
+
+def find_column_in_table(
+    column_name: str, table: Table, case_sensitive: bool = True
+) -> Optional[Column]:
+    """
+    If the column exists in the table, return it
+    """
+
+    def equals(first: str, second: str) -> bool:
+        if case_sensitive:
+            return first == second
+        return first.lower() == second.lower()
+
+    return next(
+        (col for col in table.columns if equals(col.name.__root__, column_name)), None
     )
-    rows = list(connection.execute(sql_query))
-    if len(rows) >= 1:
-        return f"CREATE VIEW {view_name} AS {rows[0][0]}"
-    return None
 
 
-@reflection.cache
-def get_table_comment(
-    self, connection, table_name, schema=None, **kw  # pylint: disable=unused-argument
-):
-    return get_table_comment_wrapper(
-        self,
-        connection,
-        table_name=table_name,
-        schema=schema,
-        query=VERTICA_TABLE_COMMENTS,
+def find_suggestion(
+    suggestions: List[Suggestion],
+    suggestion_type: SuggestionType,
+    entity_link: EntityLink,
+) -> Optional[Suggestion]:
+    """Given a list of suggestions, a suggestion type and an entity link, find
+    one suggestion in the list that matches the criteria
+    """
+    return next(
+        (
+            sugg
+            for sugg in suggestions
+            if sugg.type == suggestion_type and sugg.entityLink == entity_link
+        ),
+        None,
     )
 
 
-VerticaDialect.get_columns = get_columns
-VerticaDialect._get_column_info = _get_column_info  # pylint: disable=protected-access
-VerticaDialect.get_view_definition = get_view_definition
-VerticaDialect.get_all_table_comments = get_all_table_comments
-VerticaDialect.get_table_comment = get_table_comment
-
-
-class VerticaSource(CommonDbSourceService, MultiDBSource):
-    """
-    Implements the necessary methods to extract
-    Database metadata from Vertica Source
-    """
-
-    def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
-        super().__init__(config, metadata)
-        self.schema_desc_map = {}
-
-    @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
-        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: VerticaConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, VerticaConnection):
-            raise InvalidSourceException(
-                f"Expected VerticaConnection, but got {connection}"
-            )
-        return cls(config, metadata)
-
-    def get_schema_description(self, schema_name: str) -> Optional[str]:
-        """
-        Method to fetch the schema description
-        """
-        return self.schema_desc_map.get(schema_name)
-
-    def set_schema_description_map(self) -> None:
-        self.schema_desc_map = get_schema_descriptions(
-            self.engine, VERTICA_SCHEMA_COMMENTS
-        )
-
-    def get_configured_database(self) -> Optional[str]:
-        return self.service_connection.database
-
-    def get_database_names_raw(self) -> Iterable[str]:
-        yield from self._execute_database_query(VERTICA_LIST_DATABASES)
-
-    def get_database_names(self) -> Iterable[str]:
-        configured_db = self.config.serviceConnection.__root__.config.database
-        if configured_db:
-            self.set_inspector(database_name=configured_db)
-            self.set_schema_description_map()
-            yield configured_db
-        else:
-            for new_database in self.get_database_names_raw():
-                database_fqn = fqn.build(
-                    self.metadata,
-                    entity_type=Database,
-                    service_name=self.context.database_service,
-                    database_name=new_database,
-                )
-
-                if filter_by_database(
-                    self.source_config.databaseFilterPattern,
-                    database_fqn
-                    if self.source_config.useFqnForFiltering
-                    else new_database,
-                ):
-                    self.status.filter(database_fqn, "Database Filtered Out")
-                    continue
-
-                try:
-                    self.set_inspector(database_name=new_database)
-                    self.set_schema_description_map()
-                    yield new_database
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.error(
-                        f"Error trying to connect to database {new_database}: {exc}"
-                    )
+def find_column_in_table_with_index(
+    column_name: str, table: Table
+) -> Optional[Tuple[int, Column]]:
+    """Return a column and its index in a Table Entity
+
+    Args:
+         column_name (str): column to find
+         table (Table): Table Entity
+
+    Return:
+          A tuple of Index, Column if the column is found
+    """
+    col_index, col = next(
+        (
+            (col_index, col)
+            for col_index, col in enumerate(table.columns)
+            if str(col.name.__root__).lower() == column_name.lower()
+        ),
+        (None, None),
+    )
+
+    return col_index, col
+
+
+def list_to_dict(original: Optional[List[str]], sep: str = "=") -> Dict[str, str]:
+    """
+    Given a list with strings that have a separator,
+    convert that to a dictionary of key-value pairs
+    """
+    if not original:
+        return {}
+
+    split_original = [
+        (elem.split(sep)[0], elem.split(sep)[1]) for elem in original if sep in elem
+    ]
+    return dict(split_original)
+
+
+def clean_up_starting_ending_double_quotes_in_string(string: str) -> str:
+    """Remove start and ending double quotes in a string
+
+    Args:
+        string (str): a string
+
+    Raises:
+        TypeError: An error occure checking the type of `string`
+
+    Returns:
+        str: a string with no double quotes
+    """
+    if not isinstance(string, str):
+        raise TypeError(f"{string}, must be of type str, instead got `{type(string)}`")
+
+    return string.strip('"')
+
+
+def insensitive_replace(raw_str: str, to_replace: str, replace_by: str) -> str:
+    """Replace `to_replace` by `replace_by` in `raw_str` ignoring the raw_str case.
+
+    Args:
+        raw_str:str: Define the string that will be searched
+        to_replace:str: Specify the string to be replaced
+        replace_by:str: Replace the to_replace:str parameter in the raw_str:str string
+
+    Returns:
+        A string where the given to_replace is replaced by replace_by in raw_str, ignoring case
+    """
+
+    return re.sub(to_replace, replace_by, raw_str, flags=re.IGNORECASE | re.DOTALL)
+
+
+def insensitive_match(raw_str: str, to_match: str) -> bool:
+    """Match `to_match` in `raw_str` ignoring the raw_str case.
+
+    Args:
+        raw_str:str: Define the string that will be searched
+        to_match:str: Specify the string to be matched
+
+    Returns:
+        True if `to_match` matches in `raw_str`, ignoring case. Otherwise, false.
+    """
+
+    return re.match(to_match, raw_str, flags=re.IGNORECASE | re.DOTALL) is not None
+
+
+def get_entity_tier_from_tags(tags: list[TagLabel]) -> Optional[str]:
+    """_summary_
+
+    Args:
+        tags (list[TagLabel]): list of tags
+
+    Returns:
+        Optional[str]
+    """
+    if not tags:
+        return None
+    return next(
+        (
+            tag.tagFQN.__root__
+            for tag in tags
+            if tag.tagFQN.__root__.lower().startswith("tier")
+        ),
+        None,
+    )
+
+
+def format_large_string_numbers(number: Union[float, int]) -> str:
+    """Format large string number to a human readable format.
+    (e.g. 1,000,000 -> 1M, 1,000,000,000 -> 1B, etc)
+
+    Args:
+        number: number
+    """
+    if number == 0:
+        return "0"
+    units = ["", "K", "M", "B", "T"]
+    constant_k = 1000.0
+    magnitude = int(floor(log(abs(number), constant_k)))
+    if magnitude >= len(units):
+        return f"{int(number / constant_k**magnitude)}e{magnitude*3}"
+    return f"{number / constant_k**magnitude:.3f}{units[magnitude]}"
+
+
+def clean_uri(uri: str) -> str:
+    """
+    if uri is like http://localhost:9000/
+    then remove the end / and
+    make it http://localhost:9000
+    """
+    return uri[:-1] if uri.endswith("/") else uri
+
+
+def deep_size_of_dict(obj: dict) -> int:
+    """Get deepsize of dict data structure
+
+    Args:
+        obj (dict): dict data structure
+    Returns:
+        int: size of dict data structure
+    """
+    # pylint: disable=unnecessary-lambda-assignment
+    dict_handler = lambda elmt: itertools.chain.from_iterable(elmt.items())
+    handlers = {
+        dict: dict_handler,
+        list: iter,
+    }
+
+    seen = set()
+
+    def sizeof(obj) -> int:
+        if id(obj) in seen:
+            return 0
+
+        seen.add(id(obj))
+        size = sys.getsizeof(obj, 0)
+        for type_, handler in handlers.items():
+            if isinstance(obj, type_):
+                size += sum(map(sizeof, handler(obj)))
+                break
+
+        return size
+
+    return sizeof(obj)
+
+
+def is_safe_sql_query(sql_query: str) -> bool:
+    """Validate SQL query
+    Args:
+        sql_query (str): SQL query
+    Returns:
+        bool
+    """
+
+    forbiden_token = {
+        "CREATE",
+        "ALTER",
+        "DROP",
+        "TRUNCATE",
+        "COMMENT",
+        "RENAME",
+        "INSERT",
+        "UPDATE",
+        "DELETE",
+        "MERGE",
+        "CALL",
+        "EXPLAIN PLAN",
+        "LOCK TABLE",
+        "UNLOCK TABLE",
+        "GRANT",
+        "REVOKE",
+        "COMMIT",
+        "ROLLBACK",
+        "SAVEPOINT",
+        "SET TRANSACTION",
+    }
+
+    if sql_query is None:
+        return True
+
+    parsed_queries: Tuple[Statement] = sqlparse.parse(sql_query)
+    for parsed_query in parsed_queries:
+        validation = [
+            token.normalized in forbiden_token for token in parsed_query.tokens
+        ]
+        if any(validation):
+            return False
+    return True
+
+
+def get_database_name_for_lineage(
+    db_service_entity: DatabaseService, default_db_name: Optional[str]
+) -> Optional[str]:
+    # If the database service supports multiple db or
+    # database service connection details are not available
+    # then pick the database name available from api response
+    if db_service_entity.connection is None or hasattr(
+        db_service_entity.connection.config, "supportsDatabase"
+    ):
+        return default_db_name
+
+    # otherwise if it is an single db source then use "databaseName"
+    # and if databaseName field is not available or is empty then use
+    # "default" as database name
+    return (
+        db_service_entity.connection.config.__dict__.get("databaseName")
+        or DEFAULT_DATABASE
+    )
+
+
+def delete_dir_content(directory: str) -> None:
+    location = Path(directory)
+    if location.is_dir():
+        logger.info("Location exists, cleaning it up")
+        shutil.rmtree(directory)
+
+
+def init_staging_dir(directory: str) -> None:
+    """
+    Prepare the the staging directory
+    """
+    delete_dir_content(directory=directory)
+    location = Path(directory)
+    logger.info(f"Creating the directory to store staging data in {location}")
+    location.mkdir(parents=True, exist_ok=True)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/query_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/query_parser.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Vertica usage module
 """
 from abc import ABC
-from typing import Iterable
+from typing import Iterable, Optional
 
 from metadata.generated.schema.entity.services.connections.database.verticaConnection import (
     VerticaConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -39,15 +39,17 @@
     To allow the lineage to happen for all the ingested databases
     we'll need to iterate over them.
     """
 
     filters: str
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: VerticaConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, VerticaConnection):
             raise InvalidSourceException(
                 f"Expected VerticaConnection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/database/vertica/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/vertica/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/common_broker_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/common_broker_source.py`

 * *Files 1% similar despite different names*

```diff
@@ -100,15 +100,15 @@
                 for key, value in SchemaType.__members__.items()
             }
             logger.info(f"Fetching topic schema {topic_details.topic_name}")
             topic_schema = self._parse_topic_metadata(topic_details.topic_name)
             logger.info(f"Fetching topic config {topic_details.topic_name}")
             topic = CreateTopicRequest(
                 name=topic_details.topic_name,
-                service=self.context.messaging_service,
+                service=self.context.get().messaging_service,
                 partitions=len(topic_details.topic_metadata.partitions),
                 replicationFactor=len(
                     topic_details.topic_metadata.partitions.get(0).replicas
                 ),
             )
             topic_config_resource = self.admin_client.describe_configs(
                 [
@@ -215,16 +215,16 @@
     ) -> Iterable[Either[TopicSampleData]]:
         """
         Method to Get Sample Data of Messaging Entity
         """
         topic_fqn = fqn.build(
             metadata=self.metadata,
             entity_type=Topic,
-            service_name=self.context.messaging_service,
-            topic_name=self.context.topic,
+            service_name=self.context.get().messaging_service,
+            topic_name=self.context.get().topic,
         )
         topic_entity = self.metadata.get_by_name(entity=TopicEntity, fqn=topic_fqn)
         if topic_entity and self.generate_sample_data:
             topic_name = topic_details.topic_name
             sample_data = []
             messages = None
             try:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kafka/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kafka/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kinesis/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/atlas/connection.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,50 +8,46 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-
 from typing import Optional
 
-from metadata.clients.aws_client import AWSClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.messaging.kinesisConnection import (
-    KinesisConnection,
+from metadata.generated.schema.entity.services.connections.metadata.atlasConnection import (
+    AtlasConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.utils.logger import ingestion_logger
-
-logger = ingestion_logger()
+from metadata.ingestion.source.metadata.atlas.client import AtlasClient
 
 
-def get_connection(connection: KinesisConnection):
+def get_connection(connection: AtlasConnection) -> AtlasClient:
     """
     Create connection
     """
-    return AWSClient(connection.awsConfig).get_kinesis_client()
+    return AtlasClient(connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client,
-    service_connection: KinesisConnection,
+    client: AtlasClient,
+    service_connection: AtlasConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"GetTopics": client.list_streams}
+    test_fn = {"CheckAccess": client.list_entities}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kinesis/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kinesis/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -58,21 +58,27 @@
 
 class KinesisSource(MessagingServiceSource):
     """
     Implements the necessary methods to extract
     topics metadata from Kinesis Source
     """
 
-    def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
+    def __init__(
+        self,
+        config: WorkflowSource,
+        metadata: OpenMetadata,
+    ):
         super().__init__(config, metadata)
         self.generate_sample_data = self.config.sourceConfig.config.generateSampleData
         self.kinesis = self.connection
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: KinesisConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, KinesisConnection):
             raise InvalidSourceException(
                 f"Expected KinesisConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -120,15 +126,15 @@
                 f"https://{self.service_connection.awsConfig.awsRegion}.console.aws.amazon.com/kinesis/home"
                 f"?region={self.service_connection.awsConfig.awsRegion}#/streams/details/"
                 f"{topic_details.topic_name}/monitoring"
             )
 
             topic = CreateTopicRequest(
                 name=topic_details.topic_name,
-                service=self.context.messaging_service,
+                service=self.context.get().messaging_service,
                 partitions=len(topic_details.topic_metadata.partitions),
                 retentionTime=self._compute_retention_time(
                     topic_details.topic_metadata.summary
                 ),
                 maximumMessageSize=MAX_MESSAGE_SIZE,
                 sourceUrl=source_url,
             )
@@ -195,16 +201,16 @@
         self, topic_details: BrokerTopicDetails
     ) -> Iterable[OMetaTopicSampleData]:
         """Method to Get Sample Data of Messaging Entity"""
         try:
             topic_fqn = fqn.build(
                 metadata=self.metadata,
                 entity_type=Topic,
-                service_name=self.context.messaging_service,
-                topic_name=self.context.topic,
+                service_name=self.context.get().messaging_service,
+                topic_name=self.context.get().topic,
             )
             topic_entity = self.metadata.get_by_name(entity=Topic, fqn=topic_fqn)
             if topic_entity and self.generate_sample_data:
                 yield Either(
                     right=OMetaTopicSampleData(
                         topic=topic_entity,
                         sample_data=self._get_sample_data(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/kinesis/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kinesis/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/messaging_service.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/messaging_service.py`

 * *Files 1% similar despite different names*

```diff
@@ -33,15 +33,15 @@
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import Source
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.delete_entity import DeleteEntity
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
-    TopologyContext,
+    TopologyContextManager,
     TopologyNode,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_topic
 from metadata.utils.logger import ingestion_logger
@@ -110,15 +110,15 @@
 
     source_config: MessagingServiceMetadataPipeline
     config: WorkflowSource
     # Big union of types we want to fetch dynamically
     service_connection: MessagingConnection.__fields__["config"].type_
 
     topology = MessagingServiceTopology()
-    context = TopologyContext.create(topology)
+    context = TopologyContextManager(topology)
     topic_source_state: Set = set()
 
     def __init__(
         self,
         config: WorkflowSource,
         metadata: OpenMetadata,
     ):
@@ -199,15 +199,15 @@
         """Method to mark the topics as deleted"""
         if self.source_config.markDeletedTopics:
             yield from delete_entity_from_source(
                 metadata=self.metadata,
                 entity_type=Topic,
                 entity_source_state=self.topic_source_state,
                 mark_deleted_entity=self.source_config.markDeletedTopics,
-                params={"service": self.context.messaging_service},
+                params={"service": self.context.get().messaging_service},
             )
 
     def register_record(self, topic_request: CreateTopicRequest) -> None:
         """
         Mark the topic record as scanned and update the topic_source_state
         """
         topic_fqn = fqn.build(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/messaging/redpanda/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/redpanda/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qlikcloud/connection.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,67 +8,49 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-from functools import partial
 from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.metadata.amundsenConnection import (
-    AmundsenConnection,
-)
-from metadata.ingestion.connections.test_connections import (
-    SourceConnectionException,
-    test_connection_steps,
+from metadata.generated.schema.entity.services.connections.dashboard.qlikCloudConnection import (
+    QlikCloudConnection,
 )
+from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.metadata.amundsen.client import Neo4JConfig, Neo4jHelper
-from metadata.ingestion.source.metadata.amundsen.queries import (
-    NEO4J_AMUNDSEN_USER_QUERY,
-)
+from metadata.ingestion.source.dashboard.qlikcloud.client import QlikCloudClient
 
 
-def get_connection(connection: AmundsenConnection) -> Neo4jHelper:
+def get_connection(connection: QlikCloudConnection) -> QlikCloudClient:
     """
     Create connection
     """
-    try:
-        neo4j_config = Neo4JConfig(
-            username=connection.username,
-            password=connection.password.get_secret_value(),
-            neo4j_url=connection.hostPort,
-            max_connection_life_time=connection.maxConnectionLifeTime,
-            neo4j_encrypted=connection.encrypted,
-            neo4j_validate_ssl=connection.validateSSL,
-        )
-        return Neo4jHelper(neo4j_config)
-    except Exception as exc:
-        msg = f"Unknown error connecting with {connection}: {exc}."
-        raise SourceConnectionException(msg)
+    return QlikCloudClient(connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: Neo4jHelper,
-    service_connection: AmundsenConnection,
+    client: QlikCloudClient,
+    service_connection: QlikCloudConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {
-        "CheckAccess": partial(client.execute_query, query=NEO4J_AMUNDSEN_USER_QUERY)
-    }
+    def custom_executor():
+        return client.get_dashboards_list()
+
+    test_fn = {"GetDashboards": custom_executor}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -122,15 +122,17 @@
         self.connection_obj = self.client
         self.database_service_map = {
             service.value.lower(): service.value for service in DatabaseServiceType
         }
         self.test_connection()
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: AmundsenConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, AmundsenConnection):
             raise InvalidSourceException(
                 f"Expected AmundsenConnection, but got {connection}"
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/amundsen/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/amundsen/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/atlas/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/atlas/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/atlas/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/fivetran/connection.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,41 +13,41 @@
 Source connection handler
 """
 from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.metadata.atlasConnection import (
-    AtlasConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.fivetranConnection import (
+    FivetranConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.metadata.atlas.client import AtlasClient
+from metadata.ingestion.source.pipeline.fivetran.client import FivetranClient
 
 
-def get_connection(connection: AtlasConnection) -> AtlasClient:
+def get_connection(connection: FivetranConnection) -> FivetranClient:
     """
     Create connection
     """
-    return AtlasClient(connection)
+    return FivetranClient(connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: AtlasClient,
-    service_connection: AtlasConnection,
+    client: FivetranClient,
+    service_connection: FivetranConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"CheckAccess": client.list_entities}
+    test_fn = {"GetPipelines": client.list_groups}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/metadata/atlas/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/metadata/atlas/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -89,15 +89,17 @@
                 self.service_connection.entity_type: {"db": "db", "column": "columns"}
             },
             "Topic": {"Topic": {"schema": "schema"}},
         }
         self.test_connection()
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: AtlasConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, AtlasConnection):
             raise InvalidSourceException(
                 f"Expected AtlasConnection, but got {connection}"
             )
         return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/mlflow/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airbyte/connection.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,48 +10,44 @@
 #  limitations under the License.
 
 """
 Source connection handler
 """
 from typing import Optional
 
-from mlflow.tracking import MlflowClient
-
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.mlmodel.mlflowConnection import (
-    MlflowConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.airbyteConnection import (
+    AirbyteConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.pipeline.airbyte.client import AirbyteClient
 
 
-def get_connection(connection: MlflowConnection) -> MlflowClient:
+def get_connection(connection: AirbyteConnection) -> AirbyteClient:
     """
     Create connection
     """
-    return MlflowClient(
-        tracking_uri=connection.trackingUri,
-        registry_uri=connection.registryUri,
-    )
+    return AirbyteClient(connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: MlflowClient,
-    service_connection: MlflowConnection,
+    client: AirbyteClient,
+    service_connection: AirbyteConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"GetModels": client.search_registered_models}
+    test_fn = {"GetPipelines": client.list_workspaces}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -51,15 +51,17 @@
     Source implementation to ingest MLFlow data.
 
     We will iterate on the registered ML Models
     and prepare an iterator of CreateMlModelRequest
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: MlflowConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, MlflowConnection):
             raise InvalidSourceException(
                 f"Expected MlFlowConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -122,15 +124,15 @@
             description=model.description,
             algorithm=self._get_algorithm(),  # Setting this to a constant
             mlHyperParameters=self._get_hyper_params(run.data),
             mlFeatures=self._get_ml_features(
                 run.data, latest_version.run_id, model.name
             ),
             mlStore=self._get_ml_store(latest_version),
-            service=self.context.mlmodel_service,
+            service=self.context.get().mlmodel_service,
             sourceUrl=source_url,
         )
         yield Either(right=mlmodel_request)
         self.register_record(mlmodel_request=mlmodel_request)
 
     def _get_hyper_params(  # pylint: disable=arguments-differ
         self,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/mlmodel_service.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/mlmodel_service.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,15 +35,15 @@
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import Source
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.delete_entity import DeleteEntity
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
-    TopologyContext,
+    TopologyContextManager,
     TopologyNode,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.utils import fqn
 from metadata.utils.logger import ingestion_logger
 
@@ -96,15 +96,15 @@
 
     source_config: MlModelServiceMetadataPipeline
     config: WorkflowSource
     # Big union of types we want to fetch dynamically
     service_connection: MlModelConnection.__fields__["config"].type_
 
     topology = MlModelServiceTopology()
-    context = TopologyContext.create(topology)
+    context = TopologyContextManager(topology)
     mlmodel_source_state: Set = set()
 
     def __init__(
         self,
         config: WorkflowSource,
         metadata: OpenMetadata,
     ):
@@ -175,15 +175,15 @@
         """Method to mark the mlmodels as deleted"""
         if self.source_config.markDeletedMlModels:
             yield from delete_entity_from_source(
                 metadata=self.metadata,
                 entity_type=MlModel,
                 entity_source_state=self.mlmodel_source_state,
                 mark_deleted_entity=self.source_config.markDeletedMlModels,
-                params={"service": self.context.mlmodel_service},
+                params={"service": self.context.get().mlmodel_service},
             )
 
     def register_record(self, mlmodel_request: CreateMlModelRequest) -> None:
         """
         Mark the mlmodel record as scanned and update
         the mlmodel_source_state
         """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -63,15 +63,17 @@
     """
 
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.sagemaker = self.client
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: SageMakerConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SageMakerConnection):
             raise InvalidSourceException(
                 f"Expected SageMakerConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -132,15 +134,15 @@
         Prepare the Request model
         """
         try:
             mlmodel_request = CreateMlModelRequest(
                 name=model.name,
                 algorithm=self._get_algorithm(),  # Setting this to a constant
                 mlStore=self._get_ml_store(model.name),
-                service=self.context.mlmodel_service,
+                service=self.context.get().mlmodel_service,
             )
             yield Either(right=mlmodel_request)
             self.register_record(mlmodel_request=mlmodel_request)
         except Exception as exc:  # pylint: disable=broad-except
             yield Either(
                 left=StackTraceError(
                     name=model.name,
@@ -154,15 +156,18 @@
         model_name: str,
     ) -> Optional[MlStore]:
         """
         Get the Ml Store for the model
         """
         try:
             model_info = self.sagemaker.describe_model(ModelName=model_name)
-            return MlStore(imageRepository=model_info["PrimaryContainer"]["Image"])
+            storage = model_info.get("PrimaryContainer", {}).get("ModelDataUrl")
+            image_repository = model_info.get("PrimaryContainer", {}).get("Image")
+            if image_repository or storage:
+                return MlStore(storage=storage, imageRepository=image_repository)
         except ValidationError as err:
             logger.debug(traceback.format_exc())
             logger.warning(
                 f"Validation error adding the MlModel store from model description: {model_name} - {err}"
             )
         except Exception as err:
             logger.debug(traceback.format_exc())
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airbyte/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airbyte/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airbyte/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/domopipeline/connection.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,46 +8,60 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
+
 from typing import Optional
 
+from pydomo import Domo
+
+from metadata.clients.domo_client import DomoClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.airbyteConnection import (
-    AirbyteConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.domoPipelineConnection import (
+    DomoPipelineConnection,
+)
+from metadata.ingestion.connections.test_connections import (
+    SourceConnectionException,
+    test_connection_steps,
 )
-from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.pipeline.airbyte.client import AirbyteClient
 
 
-def get_connection(connection: AirbyteConnection) -> AirbyteClient:
+def get_connection(connection: DomoPipelineConnection) -> Domo:
     """
     Create connection
     """
-    return AirbyteClient(connection)
+    try:
+        return DomoClient(connection)
+    except Exception as exc:
+        msg = f"Unknown error connecting with {connection}: {exc}."
+        raise SourceConnectionException(msg)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: AirbyteClient,
-    service_connection: AirbyteConnection,
+    connection: Domo,
+    service_connection: DomoPipelineConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"GetPipelines": client.list_workspaces}
+    def custom_executor():
+        result = connection.get_pipelines()
+        return list(result)
+
+    test_fn = {"GetPipelines": custom_executor}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airbyte/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airbyte/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Airbyte source to extract metadata
 """
 
-from typing import Iterable
+from typing import Iterable, Optional
 
 from pydantic import BaseModel
 
 from metadata.generated.schema.api.data.createPipeline import CreatePipelineRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.pipeline import (
     Pipeline,
@@ -71,15 +71,17 @@
 class AirbyteSource(PipelineServiceSource):
     """
     Implements the necessary methods ot extract
     Pipeline metadata from Airflow's metadata db
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: AirbyteConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, AirbyteConnection):
             raise InvalidSourceException(
                 f"Expected AirbyteConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -112,15 +114,15 @@
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details.connection.get("connectionId"),
             displayName=pipeline_details.connection.get("name"),
             sourceUrl=connection_url,
             tasks=self.get_connections_jobs(
                 pipeline_details.connection, connection_url
             ),
-            service=self.context.pipeline_service,
+            service=self.context.get().pipeline_service,
         )
         yield Either(right=pipeline_request)
         self.register_record(pipeline_request=pipeline_request)
 
     def yield_pipeline_status(
         self, pipeline_details: AirbytePipelineDetails
     ) -> Iterable[Either[OMetaPipelineStatus]]:
@@ -167,16 +169,16 @@
                     ).value,
                     taskStatus=task_status,
                     timestamp=created_at,
                 )
                 pipeline_fqn = fqn.build(
                     metadata=self.metadata,
                     entity_type=Pipeline,
-                    service_name=self.context.pipeline_service,
-                    pipeline_name=self.context.pipeline,
+                    service_name=self.context.get().pipeline_service,
+                    pipeline_name=self.context.get().pipeline,
                 )
                 yield Either(
                     right=OMetaPipelineStatus(
                         pipeline_fqn=pipeline_fqn,
                         pipeline_status=pipeline_status,
                     )
                 )
@@ -231,16 +233,16 @@
 
             if not from_entity and not to_entity:
                 continue
 
             pipeline_fqn = fqn.build(
                 metadata=self.metadata,
                 entity_type=Pipeline,
-                service_name=self.context.pipeline_service,
-                pipeline_name=self.context.pipeline,
+                service_name=self.context.get().pipeline_service,
+                pipeline_name=self.context.get().pipeline,
             )
             pipeline_entity = self.metadata.get_by_name(
                 entity=Pipeline, fqn=pipeline_fqn
             )
 
             lineage_details = LineageDetails(
                 pipeline=EntityReference(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/presto/connection.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,112 +8,109 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-from functools import partial, singledispatch
+from functools import partial
 from typing import Optional
+from urllib.parse import quote_plus
 
-from airflow import settings
 from sqlalchemy.engine import Engine
+from sqlalchemy.inspection import inspect
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.mysqlConnection import (
-    MysqlConnection,
+from metadata.generated.schema.entity.services.connections.database.prestoConnection import (
+    PrestoConnection,
 )
-from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
-    PostgresConnection,
-)
-from metadata.generated.schema.entity.services.connections.database.sqliteConnection import (
-    SQLiteConnection,
-)
-from metadata.generated.schema.entity.services.connections.pipeline.airflowConnection import (
-    AirflowConnection,
-)
-from metadata.generated.schema.entity.services.connections.pipeline.backendConnection import (
-    BackendConnection,
+from metadata.ingestion.connections.builders import (
+    create_generic_db_connection,
+    get_connection_args_common,
+    init_empty_connection_arguments,
 )
 from metadata.ingestion.connections.test_connections import (
-    SourceConnectionException,
+    execute_inspector_func,
     test_connection_engine_step,
     test_connection_steps,
+    test_query,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.database.presto.queries import PRESTO_SHOW_CATALOGS
 
 
-# Only import when needed
-# pylint: disable=import-outside-toplevel
-@singledispatch
-def _get_connection(airflow_connection) -> Engine:
-    """
-    Internal call for Airflow connection build
-    """
-    raise NotImplementedError(f"Not support connection type {airflow_connection}")
-
-
-@_get_connection.register
-def _(_: BackendConnection) -> Engine:
-    with settings.Session() as session:
-        return session.get_bind()
-
-
-@_get_connection.register
-def _(airflow_connection: MysqlConnection) -> Engine:
-    from metadata.ingestion.source.database.mysql.connection import (
-        get_connection as get_mysql_connection,
-    )
-
-    return get_mysql_connection(airflow_connection)
+def get_connection_url(connection: PrestoConnection) -> str:
+    url = f"{connection.scheme.value}://"
+    if connection.username:
+        url += f"{quote_plus(connection.username)}"
+        if connection.password:
+            url += f":{quote_plus(connection.password.get_secret_value())}"
+        url += "@"
+    url += f"{connection.hostPort}"
+    if connection.catalog:
+        url += f"/{connection.catalog}"
+    if connection.databaseSchema:
+        url += f"?schema={quote_plus(connection.databaseSchema)}"
+    return url
 
 
-@_get_connection.register
-def _(airflow_connection: PostgresConnection) -> Engine:
-    from metadata.ingestion.source.database.postgres.connection import (
-        get_connection as get_postgres_connection,
-    )
-
-    return get_postgres_connection(airflow_connection)
-
-
-@_get_connection.register
-def _(airflow_connection: SQLiteConnection) -> Engine:
-    from metadata.ingestion.source.database.sqlite.connection import (
-        get_connection as get_sqlite_connection,
-    )
-
-    return get_sqlite_connection(airflow_connection)
-
-
-def get_connection(connection: AirflowConnection) -> Engine:
+def get_connection(connection: PrestoConnection) -> Engine:
     """
     Create connection
     """
-    try:
-        return _get_connection(connection.connection)
-    except Exception as exc:
-        msg = f"Unknown error connecting with {connection}: {exc}."
-        raise SourceConnectionException(msg) from exc
+    connection.connectionArguments = (
+        connection.connectionArguments or init_empty_connection_arguments()
+    )
+    if connection.protocol:
+        connection.connectionArguments.__root__["protocol"] = connection.protocol
+    if connection.verify:
+        connection.connectionArguments = (
+            connection.connectionArguments or init_empty_connection_arguments()
+        )
+        connection.connectionArguments.__root__["requests_kwargs"] = {
+            "verify": connection.verify
+        }
+
+    return create_generic_db_connection(
+        connection=connection,
+        get_connection_url_fn=get_connection_url,
+        get_connection_args_fn=get_connection_args_common,
+    )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: AirflowConnection,
+    service_connection: PrestoConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"CheckAccess": partial(test_connection_engine_step, engine)}
+    def custom_executor_for_table():
+        inspector = inspect(engine)
+        schema_name = inspector.get_schema_names()
+        if schema_name:
+            for schema in schema_name:
+                table_name = inspector.get_table_names(schema)
+                return table_name
+        return None
+
+    test_fn = {
+        "CheckAccess": partial(test_connection_engine_step, engine),
+        "GetDatabases": partial(
+            test_query, engine=engine, statement=PRESTO_SHOW_CATALOGS
+        ),
+        "GetSchemas": partial(execute_inspector_func, engine, "get_schema_names"),
+        "GetTables": custom_executor_for_table,
+    }
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/metadata.py`

 * *Files 5% similar despite different names*

```diff
@@ -14,17 +14,19 @@
 import traceback
 from collections import Counter
 from datetime import datetime
 from enum import Enum
 from typing import Iterable, List, Optional, cast
 
 from airflow.models import BaseOperator, DagRun, TaskInstance
+from airflow.models.dag import DagModel
 from airflow.models.serialized_dag import SerializedDagModel
 from airflow.serialization.serialized_objects import SerializedDAG
 from pydantic import BaseModel, ValidationError
+from sqlalchemy import join
 from sqlalchemy.orm import Session
 
 from metadata.generated.schema.api.data.createPipeline import CreatePipelineRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.pipeline import (
     Pipeline,
     PipelineStatus,
@@ -107,15 +109,17 @@
         config: WorkflowSource,
         metadata: OpenMetadata,
     ):
         super().__init__(config, metadata)
         self._session = None
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata) -> "AirflowSource":
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ) -> "AirflowSource":
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: AirflowConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, AirflowConnection):
             raise InvalidSourceException(
                 f"Expected AirflowConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -221,15 +225,15 @@
         self, pipeline_details: AirflowDagDetails
     ) -> Iterable[Either[OMetaPipelineStatus]]:
         try:
             dag_run_list = self.get_pipeline_status(pipeline_details.dag_id)
 
             for dag_run in dag_run_list:
                 if (
-                    dag_run.run_id and self.context.task_names
+                    dag_run.run_id and self.context.get().task_names
                 ):  # Airflow dags can have old task which are turned off/commented out in code
                     tasks = self.get_task_instances(
                         dag_id=dag_run.dag_id,
                         run_id=dag_run.run_id,
                         serialized_tasks=pipeline_details.tasks,
                     )
 
@@ -241,29 +245,29 @@
                             ),
                             startTime=datetime_to_ts(task.start_date),
                             endTime=datetime_to_ts(
                                 task.end_date
                             ),  # Might be None for running tasks
                         )  # Log link might not be present in all Airflow versions
                         for task in tasks
-                        if task.task_id in self.context.task_names
+                        if task.task_id in self.context.get().task_names
                     ]
 
                     pipeline_status = PipelineStatus(
                         taskStatus=task_statuses,
                         executionStatus=STATUS_MAP.get(
                             dag_run.state, StatusType.Pending.value
                         ),
                         timestamp=datetime_to_ts(dag_run.execution_date),
                     )
                     pipeline_fqn = fqn.build(
                         metadata=self.metadata,
                         entity_type=Pipeline,
-                        service_name=self.context.pipeline_service,
-                        pipeline_name=self.context.pipeline,
+                        service_name=self.context.get().pipeline_service,
+                        pipeline_name=self.context.get().pipeline,
                     )
                     yield Either(
                         right=OMetaPipelineStatus(
                             pipeline_fqn=pipeline_fqn,
                             pipeline_status=pipeline_status,
                         )
                     )
@@ -285,19 +289,31 @@
         """
 
         json_data_column = (
             SerializedDagModel._data  # For 2.3.0 onwards # pylint: disable=protected-access
             if hasattr(SerializedDagModel, "_data")
             else SerializedDagModel.data  # For 2.2.5 and 2.1.4
         )
-        for serialized_dag in self.session.query(
+
+        session_query = self.session.query(
             SerializedDagModel.dag_id,
             json_data_column,
             SerializedDagModel.fileloc,
-        ).yield_per(100):
+        )
+        if not self.source_config.includeUnDeployedPipelines:
+            session_query = session_query.select_from(
+                join(
+                    SerializedDagModel,
+                    DagModel,
+                    SerializedDagModel.dag_id == DagModel.dag_id,
+                )
+            ).filter(
+                DagModel.is_paused == False  # pylint: disable=singleton-comparison
+            )
+        for serialized_dag in session_query.yield_per(100):
             try:
                 data = serialized_dag[1]["dag"]
                 dag = AirflowDagDetails(
                     dag_id=serialized_dag[0],
                     fileloc=serialized_dag[2],
                     data=AirflowDag.parse_obj(serialized_dag[1]),
                     max_active_runs=data.get("max_active_runs", None),
@@ -413,47 +429,47 @@
                 pipelineLocation=pipeline_details.fileloc,
                 startDate=pipeline_details.start_date.isoformat()
                 if pipeline_details.start_date
                 else None,
                 tasks=self.get_tasks_from_dag(
                     pipeline_details, self.service_connection.hostPort
                 ),
-                service=self.context.pipeline_service,
+                service=self.context.get().pipeline_service,
                 owner=self.get_owner(pipeline_details.owner),
                 scheduleInterval=pipeline_details.schedule_interval,
             )
             yield Either(right=pipeline_request)
             self.register_record(pipeline_request=pipeline_request)
-            self.context.task_names = {
+            self.context.get().task_names = {
                 task.name for task in pipeline_request.tasks or []
             }
         except TypeError as err:
-            self.context.task_names = set()
+            self.context.get().task_names = set()
             yield Either(
                 left=StackTraceError(
                     name=pipeline_details.dag_id,
                     error=(
                         f"Error building DAG information from {pipeline_details}. There might be Airflow version"
                         f" incompatibilities - {err}"
                     ),
                     stackTrace=traceback.format_exc(),
                 )
             )
         except ValidationError as err:
-            self.context.task_names = set()
+            self.context.get().task_names = set()
             yield Either(
                 left=StackTraceError(
                     name=pipeline_details.dag_id,
                     error=f"Error building pydantic model for {pipeline_details} - {err}",
                     stackTrace=traceback.format_exc(),
                 )
             )
 
         except Exception as err:
-            self.context.task_names = set()
+            self.context.get().task_names = set()
             yield Either(
                 left=StackTraceError(
                     name=pipeline_details.dag_id,
                     error=f"Wild error ingesting pipeline {pipeline_details} - {err}",
                     stackTrace=traceback.format_exc(),
                 )
             )
@@ -468,16 +484,16 @@
         """
 
         # If the context is not set because of an error upstream,
         # we don't want to continue the processing
         pipeline_fqn = fqn.build(
             metadata=self.metadata,
             entity_type=Pipeline,
-            service_name=self.context.pipeline_service,
-            pipeline_name=self.context.pipeline,
+            service_name=self.context.get().pipeline_service,
+            pipeline_name=self.context.get().pipeline,
         )
         pipeline_entity = self.metadata.get_by_name(entity=Pipeline, fqn=pipeline_fqn)
         if not pipeline_entity:
             return
 
         lineage_details = LineageDetails(
             pipeline=EntityReference(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/airflow/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/airflow/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -64,15 +64,17 @@
 class DagsterSource(PipelineServiceSource):
     """
     Implements the necessary methods ot extract
     Pipeline metadata from Dagster's metadata db
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: DagsterConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DagsterConnection):
             raise InvalidSourceException(
                 f"Expected DagsterConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -87,16 +89,16 @@
                         down_stream_tasks.append(task.solid.name)
         return down_stream_tasks or None
 
     def _get_task_list(self, pipeline_name: str) -> Optional[List[Task]]:
         """Method to collect all the tasks from dagster and return it in a task list"""
         jobs = self.client.get_jobs(
             pipeline_name=pipeline_name,
-            repository_name=self.context.repository_name,
-            repository_location=self.context.repository_location,
+            repository_name=self.context.get().repository_name,
+            repository_location=self.context.get().repository_location,
         )
         task_list: List[Task] = []
         if jobs:
             for job in jobs.solidHandles or []:
                 try:
                     task = Task(
                         name=job.handleID,
@@ -122,18 +124,18 @@
 
         try:
             pipeline_request = CreatePipelineRequest(
                 name=pipeline_details.id.replace(":", ""),
                 displayName=pipeline_details.name,
                 description=pipeline_details.description,
                 tasks=self._get_task_list(pipeline_name=pipeline_details.name),
-                service=self.context.pipeline_service,
+                service=self.context.get().pipeline_service,
                 tags=get_tag_labels(
                     metadata=self.metadata,
-                    tags=[self.context.repository_name],
+                    tags=[self.context.get().repository_name],
                     classification_name=DAGSTER_TAG_CATEGORY,
                     include_tags=self.source_config.includeTags,
                 ),
                 sourceUrl=self.get_source_url(
                     pipeline_name=pipeline_details.name, task_name=None
                 ),
             )
@@ -146,15 +148,15 @@
                     error=f"Error to yield pipeline for {pipeline_details}: {exc}",
                     stackTrace=traceback.format_exc(),
                 )
             )
 
     def yield_tag(self, *_, **__) -> Iterable[Either[OMetaTagAndClassification]]:
         yield from get_ometa_tag_and_classification(
-            tags=[self.context.repository_name],
+            tags=[self.context.get().repository_name],
             classification_name=DAGSTER_TAG_CATEGORY,
             tag_description="Dagster Tag",
             classification_description="Tags associated with dagster entities",
             include_tags=self.source_config.includeTags,
         )
 
     def _get_task_status(
@@ -183,16 +185,16 @@
                 timestamp=round(convert_timestamp_to_milliseconds(run.endTime))
                 if run.endTime
                 else None,
             )
             pipeline_fqn = fqn.build(
                 metadata=self.metadata,
                 entity_type=Pipeline,
-                service_name=self.context.pipeline_service,
-                pipeline_name=self.context.pipeline,
+                service_name=self.context.get().pipeline_service,
+                pipeline_name=self.context.get().pipeline,
             )
             pipeline_status_yield = OMetaPipelineStatus(
                 pipeline_fqn=pipeline_fqn,
                 pipeline_status=pipeline_status,
             )
             yield Either(right=pipeline_status_yield)
         except Exception as exc:
@@ -207,27 +209,27 @@
     def yield_pipeline_status(
         self, pipeline_details: DagsterPipeline
     ) -> Iterable[Either[OMetaPipelineStatus]]:
         """Yield the pipeline and task status"""
         pipeline_fqn = fqn.build(
             metadata=self.metadata,
             entity_type=Pipeline,
-            service_name=self.context.pipeline_service,
-            pipeline_name=self.context.pipeline,
+            service_name=self.context.get().pipeline_service,
+            pipeline_name=self.context.get().pipeline,
         )
         pipeline_entity = self.metadata.get_by_name(
             entity=Pipeline, fqn=pipeline_fqn, fields=["tasks"]
         )
         for task in pipeline_entity.tasks or []:
             try:
                 runs = self.client.get_task_runs(
                     task.name,
                     pipeline_name=pipeline_details.name,
-                    repository_name=self.context.repository_name,
-                    repository_location=self.context.repository_location,
+                    repository_name=self.context.get().repository_name,
+                    repository_location=self.context.get().repository_location,
                 )
                 for run in runs.solidHandle.stepStats.nodes or []:
                     yield from self._get_task_status(run=run, task_name=task.name)
             except Exception as exc:
                 yield Either(
                     left=StackTraceError(
                         name=f"{pipeline_details.name} Pipeline Status",
@@ -244,16 +246,16 @@
         """
 
     def get_pipelines_list(self) -> Iterable[DagsterPipeline]:
         """Get List of all pipelines"""
         try:
             results = self.client.get_run_list()
             for result in results:
-                self.context.repository_location = result.location.name
-                self.context.repository_name = result.name
+                self.context.get().repository_location = result.location.name
+                self.context.get().repository_name = result.name
                 for job in result.pipelines or []:
                     yield job
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Unable to get pipelines list\n"
                 f"Please check if dagster is running correctly and is in good state: {exc}"
@@ -268,15 +270,15 @@
     ) -> Optional[str]:
         """
         Method to get source url for pipelines and tasks for dagster
         """
         try:
             url = (
                 f"{clean_uri(self.service_connection.host)}/locations/"
-                f"{self.context.repository_location}/jobs/{pipeline_name}/"
+                f"{self.context.get().repository_location}/jobs/{pipeline_name}/"
             )
             if task_name:
                 url = f"{url}{task_name}"
             return url
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Error to get pipeline url: {exc}")
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/dagster/queries.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/dagster/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #  limitations under the License.
 
 """
 Databricks pipeline source to extract metadata
 """
 
 import traceback
-from typing import Any, Iterable, List
+from typing import Any, Iterable, List, Optional
 
 from pydantic import ValidationError
 
 from metadata.generated.schema.api.data.createPipeline import CreatePipelineRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.pipeline import (
     Pipeline,
@@ -64,15 +64,17 @@
 class DatabrickspipelineSource(PipelineServiceSource):
     """
     Implements the necessary methods ot extract
     Pipeline metadata from Databricks Jobs API
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: DatabricksPipelineConnection = (
             config.serviceConnection.__root__.config
         )
         if not isinstance(connection, DatabricksPipelineConnection):
             raise InvalidSourceException(
@@ -87,22 +89,22 @@
     def get_pipeline_name(self, pipeline_details: dict) -> str:
         return pipeline_details["settings"].get("name")
 
     def yield_pipeline(
         self, pipeline_details: Any
     ) -> Iterable[Either[CreatePipelineRequest]]:
         """Method to Get Pipeline Entity"""
-        self.context.job_id_list = []
+        self.context.get().job_id_list = []
         try:
             pipeline_request = CreatePipelineRequest(
                 name=pipeline_details["job_id"],
                 displayName=pipeline_details["settings"].get("name"),
                 description=pipeline_details["settings"].get("name"),
                 tasks=self.get_tasks(pipeline_details),
-                service=self.context.pipeline_service,
+                service=self.context.get().pipeline_service,
             )
             yield Either(right=pipeline_request)
             self.register_record(pipeline_request=pipeline_request)
 
         except TypeError as err:
             yield Either(
                 left=StackTraceError(
@@ -129,15 +131,15 @@
                     error=f"Wild error ingesting pipeline {pipeline_details} - {err}",
                     stackTrace=traceback.format_exc(),
                 )
             )
 
     def get_tasks(self, pipeline_details: dict) -> List[Task]:
         task_list = []
-        self.context.append(key="job_id_list", value=pipeline_details["job_id"])
+        self.context.get().append(key="job_id_list", value=pipeline_details["job_id"])
 
         downstream_tasks = self.get_downstream_tasks(
             pipeline_details["settings"].get("tasks")
         )
         for task in pipeline_details["settings"].get("tasks", []):
             task_list.append(
                 Task(
@@ -188,15 +190,15 @@
                 dependent_tasks[task["task_key"]] = [v["task_key"] for v in depends_on]
             else:
                 dependent_tasks[task["task_key"]] = None
 
         return dependent_tasks
 
     def yield_pipeline_status(self, pipeline_details) -> Iterable[OMetaPipelineStatus]:
-        for job_id in self.context.job_id_list:
+        for job_id in self.context.get().job_id_list:
             try:
                 runs = self.client.get_job_runs(job_id=job_id)
                 for attempt in runs or []:
                     for task_run in attempt["tasks"]:
                         task_status = []
                         task_status.append(
                             TaskStatus(
@@ -223,16 +225,16 @@
                                 attempt["state"].get("result_state"),
                                 StatusType.Failed,
                             ),
                         )
                         pipeline_fqn = fqn.build(
                             metadata=self.metadata,
                             entity_type=Pipeline,
-                            service_name=self.context.pipeline_service,
-                            pipeline_name=self.context.pipeline,
+                            service_name=self.context.get().pipeline_service,
+                            pipeline_name=self.context.get().pipeline,
                         )
                         yield Either(
                             right=OMetaPipelineStatus(
                                 pipeline_fqn=pipeline_fqn,
                                 pipeline_status=pipeline_status,
                             )
                         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/domopipeline/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,57 +11,44 @@
 
 """
 Source connection handler
 """
 
 from typing import Optional
 
-from pydomo import Domo
-
-from metadata.clients.domo_client import DomoClient
+from metadata.clients.aws_client import AWSClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.domoPipelineConnection import (
-    DomoPipelineConnection,
-)
-from metadata.ingestion.connections.test_connections import (
-    SourceConnectionException,
-    test_connection_steps,
+from metadata.generated.schema.entity.services.connections.pipeline.gluePipelineConnection import (
+    GluePipelineConnection,
 )
+from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 
 
-def get_connection(connection: DomoPipelineConnection) -> Domo:
+def get_connection(connection: GluePipelineConnection):
     """
     Create connection
     """
-    try:
-        return DomoClient(connection)
-    except Exception as exc:
-        msg = f"Unknown error connecting with {connection}: {exc}."
-        raise SourceConnectionException(msg)
+    return AWSClient(connection.awsConfig).get_glue_client()
 
 
 def test_connection(
     metadata: OpenMetadata,
-    connection: Domo,
-    service_connection: DomoPipelineConnection,
+    client,
+    service_connection: GluePipelineConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    def custom_executor():
-        result = connection.get_pipelines()
-        return list(result)
-
-    test_fn = {"GetPipelines": custom_executor}
+    test_fn = {"GetPipelines": client.list_workflows}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -55,15 +55,17 @@
 class DomopipelineSource(PipelineServiceSource):
     """
     Implements the necessary methods to extract
     Pipeline metadata from Domo's metadata db
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config = WorkflowSource.parse_obj(config_dict)
         connection: DomoPipelineConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DomoPipelineConnection):
             raise InvalidSourceException(
                 f"Expected DomoPipelineConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -90,15 +92,15 @@
             )
 
             pipeline_request = CreatePipelineRequest(
                 name=pipeline_name,
                 displayName=pipeline_details.get("name"),
                 description=pipeline_details.get("description", ""),
                 tasks=[task],
-                service=self.context.pipeline_service,
+                service=self.context.get().pipeline_service,
                 startDate=pipeline_details.get("created"),
                 sourceUrl=source_url,
             )
             yield Either(right=pipeline_request)
             self.register_record(pipeline_request=pipeline_request)
 
         except KeyError as err:
@@ -160,16 +162,16 @@
                         run_state.lower(), StatusType.Pending.value
                     ),
                     timestamp=end_time,
                 )
                 pipeline_fqn = fqn.build(
                     metadata=self.metadata,
                     entity_type=Pipeline,
-                    service_name=self.context.pipeline_service,
-                    pipeline_name=self.context.pipeline,
+                    service_name=self.context.get().pipeline_service,
+                    pipeline_name=self.context.get().pipeline,
                 )
                 yield Either(
                     right=OMetaPipelineStatus(
                         pipeline_fqn=pipeline_fqn,
                         pipeline_status=pipeline_status,
                     )
                 )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/fivetran/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/fivetran/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/fivetran/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/connection.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,41 +13,42 @@
 Source connection handler
 """
 from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.fivetranConnection import (
-    FivetranConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.splineConnection import (
+    SplineConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.pipeline.fivetran.client import FivetranClient
+from metadata.ingestion.source.pipeline.spline.client import SplineClient
 
 
-def get_connection(connection: FivetranConnection) -> FivetranClient:
+def get_connection(connection: SplineConnection) -> SplineClient:
     """
     Create connection
     """
-    return FivetranClient(connection)
+
+    return SplineClient(config=connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: FivetranClient,
-    service_connection: FivetranConnection,
+    client: SplineClient,
+    service_connection: SplineConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"GetPipelines": client.list_groups}
+    test_fn = {"GetPipelines": client.get_pipelines_test_connection}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/fivetran/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/fivetran/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -63,15 +63,17 @@
 class FivetranSource(PipelineServiceSource):
     """
     Implements the necessary methods ot extract
     Pipeline metadata from Fivetran's REST API
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: FivetranConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, FivetranConnection):
             raise InvalidSourceException(
                 f"Expected FivetranConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -93,15 +95,15 @@
         :param pipeline_details: pipeline_details object from fivetran
         :return: Create Pipeline request with tasks
         """
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details.pipeline_name,
             displayName=pipeline_details.pipeline_display_name,
             tasks=self.get_connections_jobs(pipeline_details),
-            service=self.context.pipeline_service,
+            service=self.context.get().pipeline_service,
             sourceUrl=self.get_source_url(
                 connector_id=pipeline_details.source.get("id"),
                 group_id=pipeline_details.group.get("id"),
                 source_name=pipeline_details.source.get("service"),
             ),
         )
         yield Either(right=pipeline_request)
@@ -159,16 +161,16 @@
                 to_entity = self.metadata.get_by_name(entity=Table, fqn=to_fqn)
                 if not from_entity or not to_entity:
                     logger.info(f"Lineage Skipped for {from_fqn} - {to_fqn}")
                     continue
                 pipeline_fqn = fqn.build(
                     metadata=self.metadata,
                     entity_type=Pipeline,
-                    service_name=self.context.pipeline_service,
-                    pipeline_name=self.context.pipeline,
+                    service_name=self.context.get().pipeline_service,
+                    pipeline_name=self.context.get().pipeline,
                 )
                 pipeline_entity = self.metadata.get_by_name(
                     entity=Pipeline, fqn=pipeline_fqn
                 )
                 lineage_details = LineageDetails(
                     pipeline=EntityReference(
                         id=pipeline_entity.id.__root__, type="pipeline"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/database/redshift/connection.py`

 * *Files 24% similar despite different names*

```diff
@@ -8,47 +8,64 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-
 from typing import Optional
 
-from metadata.clients.aws_client import AWSClient
+from sqlalchemy.engine import Engine
+
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.gluePipelineConnection import (
-    GluePipelineConnection,
+from metadata.generated.schema.entity.services.connections.database.redshiftConnection import (
+    RedshiftConnection,
 )
-from metadata.ingestion.connections.test_connections import test_connection_steps
+from metadata.ingestion.connections.builders import (
+    create_generic_db_connection,
+    get_connection_args_common,
+    get_connection_url_common,
+)
+from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.database.redshift.queries import (
+    REDSHIFT_GET_DATABASE_NAMES,
+    REDSHIFT_TEST_GET_QUERIES,
+    REDSHIFT_TEST_PARTITION_DETAILS,
+)
 
 
-def get_connection(connection: GluePipelineConnection):
+def get_connection(connection: RedshiftConnection) -> Engine:
     """
     Create connection
     """
-    return AWSClient(connection.awsConfig).get_glue_client()
+    return create_generic_db_connection(
+        connection=connection,
+        get_connection_url_fn=get_connection_url_common,
+        get_connection_args_fn=get_connection_args_common,
+    )
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client,
-    service_connection: GluePipelineConnection,
+    engine: Engine,
+    service_connection: RedshiftConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-
-    test_fn = {"GetPipelines": client.list_workflows}
-
-    test_connection_steps(
+    queries = {
+        "GetQueries": REDSHIFT_TEST_GET_QUERIES,
+        "GetDatabases": REDSHIFT_GET_DATABASE_NAMES,
+        "GetPartitionTableDetails": REDSHIFT_TEST_PARTITION_DETAILS,
+    }
+    test_connection_db_common(
         metadata=metadata,
-        test_fn=test_fn,
-        service_type=service_connection.type.value,
+        engine=engine,
+        service_connection=service_connection,
         automation_workflow=automation_workflow,
+        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #  limitations under the License.
 
 """
 Glue pipeline source to extract metadata
 """
 
 import traceback
-from typing import Any, Iterable, List
+from typing import Any, Iterable, List, Optional
 
 from metadata.generated.schema.api.data.createPipeline import CreatePipelineRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.pipeline import (
     Pipeline,
     PipelineStatus,
     StatusType,
@@ -68,15 +68,17 @@
     def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.task_id_mapping = {}
         self.job_name_list = set()
         self.glue = self.connection
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: GluePipelineConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, GluePipelineConnection):
             raise InvalidSourceException(
                 f"Expected GlueConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -99,15 +101,15 @@
             f"workflows/view/{pipeline_details[NAME]}"
         )
         self.job_name_list = set()
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details[NAME],
             displayName=pipeline_details[NAME],
             tasks=self.get_tasks(pipeline_details),
-            service=self.context.pipeline_service,
+            service=self.context.get().pipeline_service,
             sourceUrl=source_url,
         )
         yield Either(right=pipeline_request)
         self.register_record(pipeline_request=pipeline_request)
 
     def get_tasks(self, pipeline_details: Any) -> List[Task]:
         task_list = []
@@ -168,16 +170,16 @@
                         executionStatus=STATUS_MAP.get(
                             attempt["JobRunState"].lower(), StatusType.Pending
                         ).value,
                     )
                     pipeline_fqn = fqn.build(
                         metadata=self.metadata,
                         entity_type=Pipeline,
-                        service_name=self.context.pipeline_service,
-                        pipeline_name=self.context.pipeline,
+                        service_name=self.context.get().pipeline_service,
+                        pipeline_name=self.context.get().pipeline,
                     )
                     yield Either(
                         right=OMetaPipelineStatus(
                             pipeline_fqn=pipeline_fqn,
                             pipeline_status=pipeline_status,
                         )
                     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/nifi/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/nifi/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/nifi/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/connection.py`

 * *Files 19% similar despite different names*

```diff
@@ -13,60 +13,49 @@
 Source connection handler
 """
 from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.nifiConnection import (
-    BasicAuthentication,
-    NifiConnection,
+from metadata.generated.schema.entity.services.connections.dashboard.powerBIConnection import (
+    PowerBIConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.pipeline.nifi.client import NifiClient
+from metadata.ingestion.source.dashboard.powerbi.client import (
+    PowerBiApiClient,
+    PowerBiClient,
+)
+from metadata.ingestion.source.dashboard.powerbi.file_client import PowerBiFileClient
 
 
-def get_connection(connection: NifiConnection) -> NifiClient:
+def get_connection(connection: PowerBIConnection) -> PowerBiApiClient:
     """
     Create connection
     """
-    if isinstance(connection.nifiConfig, BasicAuthentication):
-        return NifiClient(
-            host_port=connection.hostPort,
-            username=connection.nifiConfig.username,
-            password=connection.nifiConfig.password.get_secret_value()
-            if connection.nifiConfig.password
-            else None,
-            verify=connection.nifiConfig.verifySSL,
-        )
-
-    return NifiClient(
-        host_port=connection.hostPort,
-        ca_file_path=connection.nifiConfig.certificateAuthorityPath,
-        client_cert_path=connection.nifiConfig.clientCertificatePath,
-        client_key_path=connection.nifiConfig.clientkeyPath,
+    file_client = None
+    if connection.pbitFilesSource:
+        file_client = PowerBiFileClient(connection)
+    return PowerBiClient(
+        api_client=PowerBiApiClient(connection), file_client=file_client
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: NifiClient,
-    service_connection: NifiConnection,
+    client: PowerBiClient,
+    service_connection: PowerBIConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-
-    def custom_executor():
-        list(client.list_process_groups())
-
-    test_fn = {"GetPipelines": custom_executor}
+    test_fn = {"GetDashboards": client.api_client.fetch_dashboards}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/nifi/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/nifi/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -77,15 +77,17 @@
 class NifiSource(PipelineServiceSource):
     """
     Implements the necessary methods ot extract
     Pipeline metadata from Airflow's metadata db
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: NifiConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, NifiConnection):
             raise InvalidSourceException(
                 f"Expected NifiConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -138,15 +140,15 @@
         :return: Create Pipeline request with tasks
         """
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details.id_,
             displayName=pipeline_details.name,
             sourceUrl=f"{clean_uri(self.service_connection.hostPort)}{pipeline_details.uri}",
             tasks=self._get_tasks_from_details(pipeline_details),
-            service=self.context.pipeline_service,
+            service=self.context.get().pipeline_service,
         )
         yield Either(right=pipeline_request)
         self.register_record(pipeline_request=pipeline_request)
 
     def yield_pipeline_status(
         self, pipeline_details: NifiPipelineDetails
     ) -> Iterable[Either[OMetaPipelineStatus]]:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/pipeline_service.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/pipeline_service.py`

 * *Files 1% similar despite different names*

```diff
@@ -33,15 +33,15 @@
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.delete_entity import DeleteEntity
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
-    TopologyContext,
+    TopologyContextManager,
     TopologyNode,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_pipeline
 from metadata.utils.logger import ingestion_logger
@@ -112,15 +112,15 @@
 
     source_config: PipelineServiceMetadataPipeline
     config: WorkflowSource
     # Big union of types we want to fetch dynamically
     service_connection: PipelineConnection.__fields__["config"].type_
 
     topology = PipelineServiceTopology()
-    context = TopologyContext.create(topology)
+    context = TopologyContextManager(topology)
     pipeline_source_state: Set = set()
 
     def __init__(
         self,
         config: WorkflowSource,
         metadata: OpenMetadata,
     ):
@@ -224,14 +224,24 @@
         """Method to mark the pipelines as deleted"""
         if self.source_config.markDeletedPipelines:
             yield from delete_entity_from_source(
                 metadata=self.metadata,
                 entity_type=Pipeline,
                 entity_source_state=self.pipeline_source_state,
                 mark_deleted_entity=self.source_config.markDeletedPipelines,
-                params={"service": self.context.pipeline_service},
+                params={"service": self.context.get().pipeline_service},
             )
 
+    def get_db_service_names(self) -> List[str]:
+        """
+        Get the list of db service names
+        """
+        return (
+            self.source_config.lineageInformation.dbServiceNames or []
+            if self.source_config.lineageInformation
+            else []
+        )
+
     def prepare(self):
         """
         Method to implement any required logic before starting the ingestion process
         """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/client.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/messaging/kafka/metadata.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,51 +4,51 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Source connection handler
+Kafka source ingestion
 """
 from typing import Optional
 
-from metadata.generated.schema.entity.automations.workflow import (
-    Workflow as AutomationWorkflow,
+from metadata.generated.schema.entity.services.connections.messaging.kafkaConnection import (
+    KafkaConnection,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.splineConnection import (
-    SplineConnection,
+from metadata.generated.schema.metadataIngestion.workflow import (
+    Source as WorkflowSource,
 )
-from metadata.ingestion.connections.test_connections import test_connection_steps
+from metadata.ingestion.api.steps import InvalidSourceException
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.pipeline.spline.client import SplineClient
-
-
-def get_connection(connection: SplineConnection) -> SplineClient:
-    """
-    Create connection
-    """
-
-    return SplineClient(config=connection)
-
-
-def test_connection(
-    metadata: OpenMetadata,
-    client: SplineClient,
-    service_connection: SplineConnection,
-    automation_workflow: Optional[AutomationWorkflow] = None,
-) -> None:
-    """
-    Test connection. This can be executed either as part
-    of a metadata workflow or during an Automation Workflow
-    """
+from metadata.ingestion.source.messaging.common_broker_source import CommonBrokerSource
+from metadata.utils.ssl_manager import SSLManager
 
-    test_fn = {"GetPipelines": client.get_pipelines_test_connection}
 
-    test_connection_steps(
-        metadata=metadata,
-        test_fn=test_fn,
-        service_type=service_connection.type.value,
-        automation_workflow=automation_workflow,
-    )
+class KafkaSource(CommonBrokerSource):
+    def __init__(self, config: WorkflowSource, metadata: OpenMetadata):
+        self.ssl_manager = None
+        service_connection = config.serviceConnection.__root__.config
+        if service_connection.schemaRegistrySSL:
+
+            self.ssl_manager = SSLManager(
+                ca=service_connection.schemaRegistrySSL.__root__.caCertificate,
+                key=service_connection.schemaRegistrySSL.__root__.sslKey,
+                cert=service_connection.schemaRegistrySSL.__root__.sslCertificate,
+            )
+            service_connection = self.ssl_manager.setup_ssl(
+                config.serviceConnection.__root__.config.sslConfig
+            )
+        super().__init__(config, metadata)
+
+    @classmethod
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
+        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection: KafkaConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, KafkaConnection):
+            raise InvalidSourceException(
+                f"Expected KafkaConnection, but got {connection}"
+            )
+        return cls(config, metadata)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -52,15 +52,17 @@
 class SplineSource(PipelineServiceSource):
     """
     Implements the necessary methods ot extract
     Pipeline metadata from Airflow's metadata db
     """
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: SplineConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SplineConnection):
             raise InvalidSourceException(
                 f"Expected SplineConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -94,30 +96,30 @@
                 f"overview/{pipeline_details.executionEventId}"
             )
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details.executionEventId,
             displayName=pipeline_details.applicationName,
             sourceUrl=connection_url,
             tasks=self.get_connections_jobs(pipeline_details, connection_url),
-            service=self.context.pipeline_service,
+            service=self.context.get().pipeline_service,
         )
         yield Either(right=pipeline_request)
         self.register_record(pipeline_request=pipeline_request)
 
     def yield_pipeline_status(
         self, pipeline_details: ExecutionEvent
     ) -> Iterable[Either[OMetaPipelineStatus]]:
         """pipeline status not supported for spline connector"""
 
     def _get_table_entity(
         self, database_name: str, schema_name: str, table_name: str
     ) -> Optional[Table]:
         if not table_name:
             return None
-        for service_name in self.source_config.dbServiceNames:
+        for service_name in self.get_db_service_names():
             table_fqn = fqn.build(
                 metadata=self.metadata,
                 entity_type=Table,
                 table_name=table_name,
                 service_name=service_name,
                 schema_name=schema_name,
                 database_name=database_name,
@@ -159,15 +161,15 @@
 
     def yield_pipeline_lineage_details(  # pylint: disable=too-many-locals
         self, pipeline_details: ExecutionEvent
     ) -> Iterable[Either[AddLineageRequest]]:
         """
         Parse all the executions available and create lineage
         """
-        if not self.source_config.dbServiceNames:
+        if not self.get_db_service_names():
             return
         lineage_details = self.client.get_lineage_details(
             pipeline_details.executionPlanId
         )
         if (
             lineage_details
             and lineage_details.executionPlan
@@ -218,16 +220,16 @@
                     if to_entity
                     else None
                 )
                 if from_table and to_table:
                     pipeline_fqn = fqn.build(
                         metadata=self.metadata,
                         entity_type=Pipeline,
-                        service_name=self.context.pipeline_service,
-                        pipeline_name=self.context.pipeline,
+                        service_name=self.context.get().pipeline_service,
+                        pipeline_name=self.context.get().pipeline,
                     )
                     pipeline_entity = self.metadata.get_by_name(
                         entity=Pipeline, fqn=pipeline_fqn
                     )
                     yield Either(
                         right=AddLineageRequest(
                             edge=EntitiesEdge(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/pipeline/spline/utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/pipeline/spline/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/elasticsearch/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/elasticsearch/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/elasticsearch/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/elasticsearch/metadata.py`

 * *Files 6% similar despite different names*

```diff
@@ -52,15 +52,17 @@
     """
 
     def __init__(self, config: Source, metadata: OpenMetadata):
         super().__init__(config, metadata)
         self.client: Elasticsearch = self.connection
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: ElasticsearchConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, ElasticsearchConnection):
             raise InvalidSourceException(
                 f"Expected ElasticsearchConnection, but got {connection}"
             )
         return cls(config, metadata)
@@ -92,41 +94,41 @@
         if index_name:
             search_index_request = CreateSearchIndexRequest(
                 name=index_name,
                 displayName=index_name,
                 searchIndexSettings=search_index_details.get(index_name, {}).get(
                     "settings", {}
                 ),
-                service=self.context.search_service,
+                service=self.context.get().search_service,
                 fields=parse_es_index_mapping(
                     search_index_details.get(index_name, {}).get("mappings")
                 ),
             )
             yield Either(right=search_index_request)
             self.register_record(search_index_request=search_index_request)
 
     def yield_search_index_sample_data(
         self, search_index_details: Any
     ) -> Iterable[Either[OMetaIndexSampleData]]:
         """
         Method to Get Sample Data of Search Index Entity
         """
-        if self.source_config.includeSampleData and self.context.search_index:
+        if self.source_config.includeSampleData and self.context.get().search_index:
             sample_data = self.client.search(
-                index=self.context.search_index,
+                index=self.context.get().search_index,
                 q=WILDCARD_SEARCH,
                 size=self.source_config.sampleSize,
                 request_timeout=self.service_connection.connectionTimeoutSecs,
             )
 
             search_index_fqn = fqn.build(
                 metadata=self.metadata,
                 entity_type=SearchIndex,
-                service_name=self.context.search_service,
-                search_index_name=self.context.search_index,
+                service_name=self.context.get().search_service,
+                search_index_name=self.context.get().search_index,
             )
             search_index_entity = self.metadata.get_by_name(
                 entity=SearchIndex, fqn=search_index_fqn
             )
 
             yield Either(
                 right=OMetaIndexSampleData(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/elasticsearch/parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/elasticsearch/parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/search/search_service.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/search/search_service.py`

 * *Files 5% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 from metadata.ingestion.api.steps import Source
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.delete_entity import DeleteEntity
 from metadata.ingestion.models.search_index_data import OMetaIndexSampleData
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
-    TopologyContext,
+    TopologyContextManager,
     TopologyNode,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_search_index
 from metadata.utils.logger import ingestion_logger
@@ -106,15 +106,15 @@
 
     source_config: SearchServiceMetadataPipeline
     config: WorkflowSource
     # Big union of types we want to fetch dynamically
     service_connection: SearchConnection.__fields__["config"].type_
 
     topology = SearchServiceTopology()
-    context = TopologyContext.create(topology)
+    context = TopologyContextManager(topology)
     index_source_state: Set = set()
 
     def __init__(
         self,
         config: WorkflowSource,
         metadata: OpenMetadata,
     ):
@@ -191,15 +191,15 @@
         """Method to mark the search index as deleted"""
         if self.source_config.markDeletedSearchIndexes:
             yield from delete_entity_from_source(
                 metadata=self.metadata,
                 entity_type=SearchIndex,
                 entity_source_state=self.index_source_state,
                 mark_deleted_entity=self.source_config.markDeletedSearchIndexes,
-                params={"service": self.context.search_service},
+                params={"service": self.context.get().search_service},
             )
 
     def register_record(self, search_index_request: CreateSearchIndexRequest) -> None:
         """
         Mark the search index record as scanned and update the index_source_state
         """
         index_fqn = fqn.build(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/sqa_types.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/sqa_types.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,28 +9,47 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Define custom types as wrappers on top of
 existing SQA types to have a bridge between
 SQA dialects and OM rich type system
 """
+
 from sqlalchemy import types
+from sqlalchemy.sql.sqltypes import TypeDecorator
+
+from metadata.utils.sqlalchemy_utils import convert_numpy_to_list
 
 
 class SQAMap(types.String):
     """
     Custom Map type definition
     """
 
 
-class SQAStruct(types.String):
+class SQAStruct(TypeDecorator):
     """
     Custom Struct type definition
     """
 
+    impl = types.String
+    cache_ok = True
+
+    def process_result_value(self, value, dialect):
+        """This is executed during result retrieval
+
+        Args:
+            value: database record
+            dialect: database dialect
+        Returns:
+            python list conversion of ndarray
+        """
+
+        return convert_numpy_to_list(value)
+
 
 class SQADateTimeRange(types.String):
     """
     Custom DateTimeRange type definition
     """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/s3/connection.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/s3/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/s3/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/s3/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -79,35 +79,38 @@
         self.s3_client = self.connection.s3_client
         self.cloudwatch_client = self.connection.cloudwatch_client
 
         self._bucket_cache: Dict[str, Container] = {}
         self.s3_reader = get_reader(config_source=S3Config(), client=self.s3_client)
 
     @classmethod
-    def create(cls, config_dict, metadata: OpenMetadata):
+    def create(
+        cls, config_dict, metadata: OpenMetadata, pipeline_name: Optional[str] = None
+    ):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: S3Connection = config.serviceConnection.__root__.config
         if not isinstance(connection, S3Connection):
-            raise InvalidSourceException(
-                f"Expected S3StoreConnection, but got {connection}"
-            )
+            raise InvalidSourceException(f"Expected S3Connection, but got {connection}")
         return cls(config, metadata)
 
     def get_containers(self) -> Iterable[S3ContainerDetails]:
         bucket_results = self.fetch_buckets()
 
         for bucket_response in bucket_results:
             bucket_name = bucket_response.name
             try:
                 # We always generate the parent container (the bucket)
                 yield self._generate_unstructured_container(
                     bucket_response=bucket_response
                 )
                 container_fqn = fqn._build(  # pylint: disable=protected-access
-                    *(self.context.objectstore_service, self.context.container)
+                    *(
+                        self.context.get().objectstore_service,
+                        self.context.get().container,
+                    )
                 )
                 container_entity = self.metadata.get_by_name(
                     entity=Container, fqn=container_fqn
                 )
                 self._bucket_cache[bucket_name] = container_entity
                 parent_entity: EntityReference = EntityReference(
                     id=self._bucket_cache[bucket_name].id.__root__, type="container"
@@ -179,18 +182,19 @@
     ) -> Iterable[Either[CreateContainerRequest]]:
         container_request = CreateContainerRequest(
             name=container_details.name,
             prefix=container_details.prefix,
             numberOfObjects=container_details.number_of_objects,
             size=container_details.size,
             dataModel=container_details.data_model,
-            service=self.context.objectstore_service,
+            service=self.context.get().objectstore_service,
             parent=container_details.parent,
             sourceUrl=container_details.sourceUrl,
             fileFormats=container_details.file_formats,
+            fullPath=container_details.fullPath,
         )
         yield Either(right=container_request)
         self.register_record(container_request=container_request)
 
     def _generate_container_details(
         self,
         bucket_response: S3BucketResponse,
@@ -209,29 +213,33 @@
                 metadata_entry=metadata_entry,
                 config_source=S3Config(
                     securityConfig=self.service_connection.awsConfig
                 ),
                 client=self.s3_client,
             )
             if columns:
+                prefix = (
+                    f"{KEY_SEPARATOR}{metadata_entry.dataPath.strip(KEY_SEPARATOR)}"
+                )
                 return S3ContainerDetails(
                     name=metadata_entry.dataPath.strip(KEY_SEPARATOR),
-                    prefix=f"{KEY_SEPARATOR}{metadata_entry.dataPath.strip(KEY_SEPARATOR)}",
+                    prefix=prefix,
                     creation_date=bucket_response.creation_date.isoformat(),
                     number_of_objects=self._fetch_metric(
                         bucket_name=bucket_name, metric=S3Metric.NUMBER_OF_OBJECTS
                     ),
                     size=self._fetch_metric(
                         bucket_name=bucket_name, metric=S3Metric.BUCKET_SIZE_BYTES
                     ),
                     file_formats=[container.FileFormat(metadata_entry.structureFormat)],
                     data_model=ContainerDataModel(
                         isPartitioned=metadata_entry.isPartitioned, columns=columns
                     ),
                     parent=parent,
+                    fullPath=self._get_full_path(bucket_name, prefix),
                     sourceUrl=self._get_object_source_url(
                         bucket_name=bucket_name,
                         prefix=metadata_entry.dataPath.strip(KEY_SEPARATOR),
                     ),
                 )
         return None
 
@@ -332,17 +340,35 @@
                 bucket_name=bucket_response.name, metric=S3Metric.NUMBER_OF_OBJECTS
             ),
             size=self._fetch_metric(
                 bucket_name=bucket_response.name, metric=S3Metric.BUCKET_SIZE_BYTES
             ),
             file_formats=[],
             data_model=None,
+            fullPath=self._get_full_path(bucket_name=bucket_response.name),
             sourceUrl=self._get_bucket_source_url(bucket_name=bucket_response.name),
         )
 
+    def _clean_path(self, path: str) -> str:
+        return path.strip(KEY_SEPARATOR)
+
+    def _get_full_path(self, bucket_name: str, prefix: str = None) -> Optional[str]:
+        """
+        Method to get the full path of the file
+        """
+        if bucket_name is None:
+            return None
+
+        full_path = f"s3://{self._clean_path(bucket_name)}"
+
+        if prefix:
+            full_path += f"/{self._clean_path(prefix)}"
+
+        return full_path
+
     def _get_sample_file_path(
         self, bucket_name: str, metadata_entry: MetadataEntry
     ) -> Optional[str]:
         """
         Given a bucket and a metadata entry, returns the full path key to a file which can then be used to infer schema
         or None in the case of a non-structured metadata entry, or if no such keys can be found
         """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/s3/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/s3/models.py`

 * *Files 3% similar despite different names*

```diff
@@ -74,7 +74,11 @@
     parent: Optional[EntityReference] = Field(
         None,
         description="Reference to the parent container",
     )
     sourceUrl: Optional[basic.SourceUrl] = Field(
         None, description="Source URL of the container."
     )
+
+    fullPath: Optional[str] = Field(
+        None, description="Full path of the container/file."
+    )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/source/storage/storage_service.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/storage/storage_service.py`

 * *Files 1% similar despite different names*

```diff
@@ -37,15 +37,15 @@
 from metadata.ingestion.api.models import Either
 from metadata.ingestion.api.steps import Source
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.delete_entity import DeleteEntity
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
-    TopologyContext,
+    TopologyContextManager,
     TopologyNode,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.ingestion.source.database.glue.models import Column
 from metadata.readers.dataframe.models import DatalakeTableSchemaWrapper
 from metadata.readers.dataframe.reader_factory import SupportedTypes
@@ -108,15 +108,15 @@
     source_config: StorageServiceMetadataPipeline
     config: WorkflowSource
     metadata: OpenMetadata
     # Big union of types we want to fetch dynamically
     service_connection: StorageConnection.__fields__["config"].type_
 
     topology = StorageServiceTopology()
-    context = TopologyContext.create(topology)
+    context = TopologyContextManager(topology)
     container_source_state: Set = set()
 
     global_manifest: Optional[ManifestMetadataConfig]
 
     def __init__(
         self,
         config: WorkflowSource,
@@ -187,15 +187,15 @@
             ).fullyQualifiedName.__root__
             if container_request.parent
             else None
         )
         container_fqn = fqn.build(
             self.metadata,
             entity_type=Container,
-            service_name=self.context.objectstore_service,
+            service_name=self.context.get().objectstore_service,
             parent_container=parent_container,
             container_name=container_request.name.__root__,
         )
 
         self.container_source_state.add(container_fqn)
 
     def test_connection(self) -> None:
@@ -206,15 +206,15 @@
         """Method to mark the containers as deleted"""
         if self.source_config.markDeletedContainers:
             yield from delete_entity_from_source(
                 metadata=self.metadata,
                 entity_type=Container,
                 entity_source_state=self.container_source_state,
                 mark_deleted_entity=self.source_config.markDeletedContainers,
-                params={"service": self.context.objectstore_service},
+                params={"service": self.context.get().objectstore_service},
             )
 
     def yield_create_request_objectstore_service(self, config: WorkflowSource):
         yield Either(
             right=self.metadata.get_create_service_from_source(
                 entity=StorageService, config=config
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/ingestion/stage/table_usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/stage/table_usage.py`

 * *Files 1% similar despite different names*

```diff
@@ -66,15 +66,20 @@
         self.wrote_something = False
 
     @property
     def name(self) -> str:
         return "Table Usage"
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata):
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ):
         config = TableStageConfig.parse_obj(config_dict)
         return cls(config, metadata)
 
     def init_location(self) -> None:
         """
         Prepare the usage location
         """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/mixins/pandas/pandas_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/mixins/pandas/pandas_mixin.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 import random
 from typing import cast
 
 from metadata.data_quality.validations.table.pandas.tableRowInsertedCountToBeBetween import (
     TableRowInsertedCountToBeBetweenValidator,
 )
 from metadata.generated.schema.entity.data.table import (
-    PartitionIntervalType,
+    PartitionIntervalTypes,
     PartitionProfilerConfig,
     ProfileSampleType,
 )
 from metadata.readers.dataframe.models import DatalakeTableSchemaWrapper
 from metadata.utils.datalake.datalake_utils import fetch_dataframe
 from metadata.utils.logger import test_suite_logger
 
@@ -43,27 +43,27 @@
         """
         self.table_partition_config = cast(
             PartitionProfilerConfig, self.table_partition_config
         )
         partition_field = self.table_partition_config.partitionColumnName
         if (
             self.table_partition_config.partitionIntervalType
-            == PartitionIntervalType.COLUMN_VALUE
+            == PartitionIntervalTypes.COLUMN_VALUE
         ):
             return [
                 df[
                     df[partition_field].isin(
                         self.table_partition_config.partitionValues
                     )
                 ]
                 for df in dfs
             ]
         if (
             self.table_partition_config.partitionIntervalType
-            == PartitionIntervalType.INTEGER_RANGE
+            == PartitionIntervalTypes.INTEGER_RANGE
         ):
             return [
                 df[
                     df[partition_field].between(
                         self.table_partition_config.partitionIntegerRangeStart,
                         self.table_partition_config.partitionIntegerRangeEnd,
                     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/mixins/sqalchemy/sqa_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/mixins/sqalchemy/sqa_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/parsers/avro_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/parsers/avro_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/parsers/json_schema_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/parsers/json_schema_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/parsers/protobuf_parser.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/parsers/protobuf_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/parsers/schema_parsers.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/parsers/schema_parsers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/pii/constants.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/constants.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/pii/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/pii/ner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/ner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/pii/processor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/processor.py`

 * *Files 5% similar despite different names*

```diff
@@ -77,15 +77,20 @@
         """Load the NER Scanner only if called"""
         if self._ner_scanner is None:
             self._ner_scanner = NERScanner()
 
         return self._ner_scanner
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata) -> "Step":
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ) -> "Step":
         config = parse_workflow_config_gracefully(config_dict)
         return cls(config=config, metadata=metadata)
 
     def close(self) -> None:
         """Nothing to close"""
 
     @staticmethod
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/pii/scanners/column_name_scanner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/scanners/column_name_scanner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/pii/scanners/ner_scanner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/pii/scanners/ner_scanner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/api/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/api/models.py`

 * *Files 2% similar despite different names*

```diff
@@ -118,15 +118,15 @@
 
     def __str__(self):
         """Return the table name being processed"""
         return f"Table [{self.table.name.__root__}]"
 
 
 class ThreadPoolMetrics(ConfigModel):
-    """thread pool metric"""
+    """A container for all metrics to be computed on the same thread."""
 
     metrics: Union[List[Union[Type[Metric], CustomMetric]], Type[Metric]]
     metric_type: MetricTypes
     column: Optional[Union[Column, SQALikeColumn]]
     table: Union[Table, DeclarativeMeta]
 
     class Config:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/pandas/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/pandas/profiler_interface.py`

 * *Files 8% similar despite different names*

```diff
@@ -18,30 +18,31 @@
 from collections import defaultdict
 from copy import deepcopy
 from datetime import datetime, timezone
 from typing import Dict, List, Optional
 
 from sqlalchemy import Column
 
-from metadata.generated.schema.entity.data.table import CustomMetricProfile, TableData
+from metadata.generated.schema.entity.data.table import (
+    CustomMetricProfile,
+    DataType,
+    TableData,
+)
 from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
     DatalakeConnection,
 )
 from metadata.generated.schema.tests.customMetric import CustomMetric
 from metadata.mixins.pandas.pandas_mixin import PandasInterfaceMixin
 from metadata.profiler.api.models import ThreadPoolMetrics
 from metadata.profiler.interface.profiler_interface import ProfilerInterface
 from metadata.profiler.metrics.core import MetricTypes
 from metadata.profiler.metrics.registry import Metrics
-from metadata.readers.dataframe.models import DatalakeTableSchemaWrapper
+from metadata.profiler.processor.metric_filter import MetricFilter
 from metadata.utils.constants import COMPLEX_COLUMN_SEPARATOR, SAMPLE_DATA_DEFAULT_COUNT
-from metadata.utils.datalake.datalake_utils import (
-    GenericDataFrameColumnParser,
-    fetch_dataframe,
-)
+from metadata.utils.datalake.datalake_utils import GenericDataFrameColumnParser
 from metadata.utils.logger import profiler_interface_registry_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = profiler_interface_registry_logger()
 
 
 class PandasProfilerInterface(ProfilerInterface, PandasInterfaceMixin):
@@ -81,37 +82,56 @@
             thread_count,
             timeout_seconds,
             sample_data_count,
             **kwargs,
         )
 
         self.client = self.connection.client
-        self.dfs = self._convert_table_to_list_of_dataframe_objects()
+        self.dfs = self.return_ometa_dataframes_sampled(
+            service_connection_config=self.service_connection_config,
+            client=self.client,
+            table=self.table_entity,
+            profile_sample_config=profile_sample_config,
+        )
         self.sampler = self._get_sampler()
-        self.complex_dataframe_sample = deepcopy(self.sampler.random_sample())
-
-    def _convert_table_to_list_of_dataframe_objects(self):
-        """From a table entity, return the corresponding dataframe object
+        self.complex_dataframe_sample = deepcopy(
+            self.sampler.random_sample(is_sampled=True)
+        )
+        self.complex_df()
 
-        Returns:
-            List[DataFrame]
-        """
-        data = fetch_dataframe(
-            config_source=self.service_connection_config.configSource,
-            client=self.client,
-            file_fqn=DatalakeTableSchemaWrapper(
-                key=self.table_entity.name.__root__,
-                bucket_name=self.table_entity.databaseSchema.name,
-                file_extension=self.table_entity.fileFormat,
-            ),
+    def complex_df(self):
+        """Assign DataTypes to dataframe columns as per the parsed column type"""
+        coltype_mapping_df = []
+        data_formats = (
+            GenericDataFrameColumnParser._data_formats  # pylint: disable=protected-access
         )
+        for index, df in enumerate(self.complex_dataframe_sample):
+            if index == 0:
+                for col in self.table.columns:
+                    coltype = next(
+                        (
+                            key
+                            for key, value in data_formats.items()
+                            if col.dataType == value
+                        ),
+                        None,
+                    )
+                    if coltype and col.dataType not in {DataType.JSON, DataType.ARRAY}:
+                        coltype_mapping_df.append(coltype)
+                    else:
+                        coltype_mapping_df.append("object")
 
-        if not data:
-            raise TypeError(f"Couldn't fetch {self.table_entity.name.__root__}")
-        return data
+            try:
+                self.complex_dataframe_sample[index] = df.astype(
+                    dict(zip(df.keys(), coltype_mapping_df))
+                )
+            except (TypeError, ValueError) as err:
+                self.complex_dataframe_sample[index] = df
+                logger.warning(f"NaN/NoneType found in the Dataframe: {err}")
+                break
 
     def _get_sampler(self):
         """Get dataframe sampler from config"""
         from metadata.profiler.processor.sampler.sampler_factory import (  # pylint: disable=import-outside-toplevel
             sampler_factory_,
         )
 
@@ -167,27 +187,27 @@
             column: the column to compute the metrics against
             metrics: list of metrics to compute
         Returns:
             dictionnary of results
         """
         import pandas as pd  # pylint: disable=import-outside-toplevel
 
+        row_dict = {}
         try:
-            row_dict = {}
             for metric in metrics:
                 metric_resp = metric(column).df_fn(runner)
                 row_dict[metric.name()] = (
                     None if pd.isnull(metric_resp) else metric_resp
                 )
-            return row_dict
         except Exception as exc:
             logger.debug(
                 f"{traceback.format_exc()}\nError trying to compute profile for {exc}"
             )
             raise RuntimeError(exc)
+        return row_dict
 
     def _compute_query_metrics(
         self,
         metric: Metrics,
         runner: List,
         column,
         *args,
@@ -279,15 +299,15 @@
         return None
 
     def compute_metrics(
         self,
         metric_func: ThreadPoolMetrics,
     ):
         """Run metrics in processor worker"""
-        logger.debug(f"Running profiler for {metric_func.table}")
+        logger.debug(f"Running profiler for {metric_func.table.name.__root__}")
         try:
             row = None
             if self.complex_dataframe_sample:
                 row = self._get_metric_fn[metric_func.metric_type.value](
                     metric_func.metrics,
                     self.complex_dataframe_sample,
                     column=metric_func.column,
@@ -356,21 +376,22 @@
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Unexpected exception computing metrics: {exc}")
             return None
 
     def get_all_metrics(
         self,
-        metric_funcs: list,
+        metric_funcs: List[ThreadPoolMetrics],
     ):
         """get all profiler metrics"""
 
         profile_results = {"table": {}, "columns": defaultdict(dict)}
         metric_list = [
-            self.compute_metrics(metric_func) for metric_func in metric_funcs
+            self.compute_metrics(metric_func)
+            for metric_func in MetricFilter.filter_empty_metrics(metric_funcs)
         ]
         for metric_result in metric_list:
             profile, column, metric_type = metric_result
             if profile:
                 if metric_type == MetricTypes.Table.value:
                     profile_results["table"].update(profile)
                 if metric_type == MetricTypes.System.value:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/profiler_interface.py`

 * *Files 2% similar despite different names*

```diff
@@ -421,16 +421,20 @@
     @abstractmethod
     def _compute_static_metrics(
         self,
         metrics: List[Metrics],
         runner,
         *args,
         **kwargs,
-    ):
-        """Get metrics"""
+    ) -> Dict[str, Any]:
+        """Get metrics
+        Return:
+            Dict[str, Any]: dict of metrics tio be merged into the final column profile. Keys need to be compatible with
+            the `metadata.generated.schema.entity.data.table.ColumnProfile` schema.
+        """
         raise NotImplementedError
 
     @abstractmethod
     def _compute_query_metrics(
         self,
         metric: Metrics,
         runner,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/profiler_interface_factory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/profiler_interface_factory.py`

 * *Files 9% similar despite different names*

```diff
@@ -23,30 +23,38 @@
 )
 from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
     DatalakeConnection,
 )
 from metadata.generated.schema.entity.services.connections.database.db2Connection import (
     Db2Connection,
 )
+from metadata.generated.schema.entity.services.connections.database.dynamoDBConnection import (
+    DynamoDBConnection,
+)
 from metadata.generated.schema.entity.services.connections.database.mariaDBConnection import (
     MariaDBConnection,
 )
+from metadata.generated.schema.entity.services.connections.database.mongoDBConnection import (
+    MongoDBConnection,
+)
 from metadata.generated.schema.entity.services.connections.database.singleStoreConnection import (
     SingleStoreConnection,
 )
 from metadata.generated.schema.entity.services.connections.database.snowflakeConnection import (
     SnowflakeConnection,
 )
 from metadata.generated.schema.entity.services.connections.database.trinoConnection import (
     TrinoConnection,
 )
 from metadata.generated.schema.entity.services.connections.database.unityCatalogConnection import (
     UnityCatalogConnection,
 )
 from metadata.generated.schema.entity.services.databaseService import DatabaseConnection
+from metadata.profiler.factory import Factory
+from metadata.profiler.interface.nosql.profiler_interface import NoSQLProfilerInterface
 from metadata.profiler.interface.pandas.profiler_interface import (
     PandasProfilerInterface,
 )
 from metadata.profiler.interface.profiler_interface import ProfilerInterface
 from metadata.profiler.interface.sqlalchemy.bigquery.profiler_interface import (
     BigQueryProfilerInterface,
 )
@@ -72,35 +80,15 @@
     TrinoProfilerInterface,
 )
 from metadata.profiler.interface.sqlalchemy.unity_catalog.profiler_interface import (
     UnityCatalogProfilerInterface,
 )
 
 
-class ProfilerInterfaceFactory:
-    """Creational factory for profiler interface objects"""
-
-    def __init__(self):
-        self._interface_type = {}
-
-    def register(self, interface_type: str, interface_class):
-        """Register a new interface"""
-        self._interface_type[interface_type] = interface_class
-
-    def register_many(self, interface_dict):
-        """
-        Registers multiple profiler interfaces at once.
-
-        Args:
-            interface_dict: A dictionary mapping connection class names (strings) to their
-            corresponding profiler interface classes.
-        """
-        for interface_type, interface_class in interface_dict.items():
-            self.register(interface_type, interface_class)
-
+class ProfilerInterfaceFactory(Factory):
     def create(self, interface_type: str, *args, **kwargs):
         """Create interface object based on interface type"""
         interface_class = self._interface_type.get(interface_type)
         if not interface_class:
             interface_class = self._interface_type.get(DatabaseConnection.__name__)
         interface_class = cast(ProfilerInterface, interface_class)
         return interface_class.create(*args, **kwargs)
@@ -114,10 +102,11 @@
     DatalakeConnection.__name__: PandasProfilerInterface,
     MariaDBConnection.__name__: MariaDBProfilerInterface,
     SnowflakeConnection.__name__: SnowflakeProfilerInterface,
     TrinoConnection.__name__: TrinoProfilerInterface,
     UnityCatalogConnection.__name__: UnityCatalogProfilerInterface,
     DatabricksConnection.__name__: DatabricksProfilerInterface,
     Db2Connection.__name__: DB2ProfilerInterface,
+    MongoDBConnection.__name__: NoSQLProfilerInterface,
+    DynamoDBConnection.__name__: NoSQLProfilerInterface,
 }
-
 profiler_interface_factory.register_many(profilers)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/bigquery/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/bigquery/profiler_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/databricks/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/unity_catalog/profiler_interface.py`

 * *Files 26% similar despite different names*

```diff
@@ -11,16 +11,20 @@
 
 """
 Interfaces with database for all database engine
 supporting sqlalchemy abstraction layer
 """
 
 
-from metadata.profiler.interface.sqlalchemy.profiler_interface import (
-    SQAProfilerInterface,
+from metadata.ingestion.source.database.databricks.connection import (
+    get_connection as databricks_get_connection,
+)
+from metadata.profiler.interface.sqlalchemy.databricks.profiler_interface import (
+    DatabricksProfilerInterface,
 )
 
 
-class DatabricksProfilerInterface(SQAProfilerInterface):
-    def __init__(self, service_connection_config, **kwargs):
-        super().__init__(service_connection_config=service_connection_config, **kwargs)
+class UnityCatalogProfilerInterface(DatabricksProfilerInterface):
+    def create_session(self):
+        self.connection = databricks_get_connection(self.service_connection_config)
+        super().create_session()
         self.set_catalog(self.session)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/db2/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/db2/profiler_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/mariadb/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/mariadb/profiler_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/profiler_interface.py`

 * *Files 1% similar despite different names*

```diff
@@ -35,14 +35,15 @@
 from metadata.profiler.metrics.core import MetricTypes
 from metadata.profiler.metrics.registry import Metrics
 from metadata.profiler.metrics.static.mean import Mean
 from metadata.profiler.metrics.static.stddev import StdDev
 from metadata.profiler.metrics.static.sum import Sum
 from metadata.profiler.orm.functions.table_metric_computer import TableMetricComputer
 from metadata.profiler.orm.registry import Dialects
+from metadata.profiler.processor.metric_filter import MetricFilter
 from metadata.profiler.processor.runner import QueryRunner
 from metadata.utils.constants import SAMPLE_DATA_DEFAULT_COUNT
 from metadata.utils.custom_thread_pool import CustomThreadPoolExecutor
 from metadata.utils.helpers import is_safe_sql_query
 from metadata.utils.logger import profiler_interface_registry_logger
 
 logger = profiler_interface_registry_logger()
@@ -476,15 +477,15 @@
         profile_results = {"table": dict(), "columns": defaultdict(dict)}
         with CustomThreadPoolExecutor(max_workers=self._thread_count) as pool:
             futures = [
                 pool.submit(
                     self.compute_metrics_in_thread,
                     metric_func,
                 )
-                for metric_func in metric_funcs
+                for metric_func in MetricFilter.filter_empty_metrics(metric_funcs)
             ]
 
             for future in futures:
                 if future.cancelled():
                     continue
 
                 try:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/single_store/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/single_store/profiler_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/snowflake/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/snowflake/profiler_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/interface/sqlalchemy/trino/profiler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/interface/sqlalchemy/trino/profiler_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/distinct_ratio.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/distinct_ratio.py`

 * *Files 14% similar despite different names*

```diff
@@ -12,28 +12,29 @@
 """
 Distinct Ratio Composed Metric definition
 """
 # pylint: disable=duplicate-code
 
 from typing import Any, Dict, Optional, Tuple
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import ComposedMetric
 from metadata.profiler.metrics.static.count import Count
 from metadata.profiler.metrics.static.distinct_count import DistinctCount
 
 
 class DistinctRatio(ComposedMetric):
     """
     Given the total count and distinct count,
     compute the distinct ratio
     """
 
     @classmethod
     def name(cls):
-        return "distinctProportion"
+        return MetricType.distinctProportion.value
 
     @classmethod
     def required_metrics(cls) -> Tuple[str, ...]:
         return Count.name(), DistinctCount.name()
 
     @property
     def metric_type(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/duplicate_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/duplicate_count.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,28 +12,29 @@
 """
 Count Duplicates Composed Metric definition
 """
 # pylint: disable=duplicate-code
 
 from typing import Any, Dict, Optional, Tuple
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import ComposedMetric
 from metadata.profiler.metrics.static.count import Count
 from metadata.profiler.metrics.static.distinct_count import DistinctCount
 
 
 class DuplicateCount(ComposedMetric):
     """
     Given the total count and the distinct count,
     compute the number of rows that are duplicates
     """
 
     @classmethod
     def name(cls):
-        return "duplicateCount"
+        return MetricType.duplicateCount.value
 
     @classmethod
     def required_metrics(cls) -> Tuple[str, ...]:
         return Count.name(), DistinctCount.name()
 
     @property
     def metric_type(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/ilike_ratio.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/ilike_ratio.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,28 +12,29 @@
 """
 ILIKE Ratio Composed Metric definition
 """
 # pylint: disable=duplicate-code
 
 from typing import Any, Dict, Optional, Tuple
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import ComposedMetric
 from metadata.profiler.metrics.static.count import Count
 from metadata.profiler.metrics.static.ilike_count import ILikeCount
 
 
 class ILikeRatio(ComposedMetric):
     """
     Given the total count and ILIKE count,
     compute the ILIKE ratio
     """
 
     @classmethod
     def name(cls):
-        return "iLikeRatio"
+        return MetricType.iLikeRatio.value
 
     @classmethod
     def required_metrics(cls) -> Tuple[str, ...]:
         return Count.name(), ILikeCount.name()
 
     @property
     def metric_type(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/iqr.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/iqr.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,27 +12,28 @@
 """
 Inter Quartile Range Composed Metric definition
 """
 # pylint: disable=duplicate-code
 
 from typing import Any, Dict, Optional, Tuple
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import ComposedMetric
 from metadata.profiler.metrics.window.first_quartile import FirstQuartile
 from metadata.profiler.metrics.window.third_quartile import ThirdQuartile
 
 
 class InterQuartileRange(ComposedMetric):
     """
     Given the first and third quartile compute the IQR,
     """
 
     @classmethod
     def name(cls):
-        return "interQuartileRange"
+        return MetricType.interQuartileRange.value
 
     @classmethod
     def required_metrics(cls) -> Tuple[str, ...]:
         return FirstQuartile.name(), ThirdQuartile.name()
 
     @property
     def metric_type(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/like_ratio.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/like_ratio.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,28 +12,29 @@
 """
 LIKE Ratio Composed Metric definition
 """
 # pylint: disable=duplicate-code
 
 from typing import Any, Dict, Optional, Tuple
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import ComposedMetric
 from metadata.profiler.metrics.static.count import Count
 from metadata.profiler.metrics.static.like_count import LikeCount
 
 
 class LikeRatio(ComposedMetric):
     """
     Given the total count and LIKE count,
     compute the LIKE ratio
     """
 
     @classmethod
     def name(cls):
-        return "likeRatio"
+        return MetricType.likeRatio.value
 
     @classmethod
     def required_metrics(cls) -> Tuple[str, ...]:
         return Count.name(), LikeCount.name()
 
     @property
     def metric_type(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/non_parametric_skew.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/non_parametric_skew.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,28 +12,29 @@
 """
 Non Parametric Skew definition
 """
 # pylint: disable=duplicate-code
 
 from typing import Any, Dict, Optional, Tuple
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import ComposedMetric
 from metadata.profiler.metrics.static.mean import Mean
 from metadata.profiler.metrics.static.stddev import StdDev
 from metadata.profiler.metrics.window.median import Median
 
 
 class NonParametricSkew(ComposedMetric):
     """
     Return the non parametric skew of a column
     """
 
     @classmethod
     def name(cls):
-        return "nonParametricSkew"
+        return MetricType.nonParametricSkew.value
 
     @classmethod
     def required_metrics(cls) -> Tuple[str, ...]:
         return Mean.name(), StdDev.name(), Median.name()
 
     @property
     def metric_type(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/null_ratio.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/null_missing_count.py`

 * *Files 20% similar despite different names*

```diff
@@ -6,52 +6,68 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Null Ratio Composed Metric definition
+Null Count Metric definition
 """
 # pylint: disable=duplicate-code
 
-from typing import Any, Dict, Optional, Tuple
 
-from metadata.profiler.metrics.core import ComposedMetric
-from metadata.profiler.metrics.static.count import Count
-from metadata.profiler.metrics.static.null_count import NullCount
+from sqlalchemy import case, column
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
+from metadata.profiler.metrics.core import StaticMetric, _label
+from metadata.profiler.orm.functions.sum import SumFn
 
-class NullRatio(ComposedMetric):
+
+class NullMissingCount(StaticMetric):
     """
-    Given the total count and null count,
-    compute the null ratio
+    NULL + Empty COUNT Metric
+
+    Given a column, return the null count.
+
+    We are building a CASE WHEN structure:
+    ```
+    SUM(
+        CASE is not null THEN 1
+        ELSE 0
+    )
+    ```
     """
 
     @classmethod
     def name(cls):
-        return "nullProportion"
-
-    @classmethod
-    def required_metrics(cls) -> Tuple[str, ...]:
-        return Count.name(), NullCount.name()
+        """
+        Returns the name of the metric.
+        """
+        return MetricType.nullCount.value
 
     @property
     def metric_type(self):
         """
-        Override default metric_type definition as
-        we now don't care about the column
+        Returns the type of the metric.
         """
-        return float
+        return int
 
-    def fn(self, res: Dict[str, Any]) -> Optional[float]:
+    @_label
+    def fn(self):
         """
-        Safely compute null ratio based on the profiler
-        results of other Metrics
+        Returns the SQLAlchemy function for calculating the metric.
         """
-        res_count = res.get(Count.name())
-        res_null = res.get(NullCount.name())
-
-        if res_count is not None and res_null is not None:
-            return res_null / (res_null + res_count)
+        return SumFn(
+            case(
+                [
+                    (column(self.col.name, self.col.type).is_(None), 1),
+                    (column(self.col.name, self.col.type).__eq__(""), 1),
+                ],
+                else_=0,
+            )
+        )
 
-        return None
+    def df_fn(self, dfs=None):
+        """
+        Returns the pandas function for calculating the metric.
+        """
+        return sum(df[self.col.name].isnull().sum() for df in dfs)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/composed/unique_ratio.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/composed/unique_ratio.py`

 * *Files 10% similar despite different names*

```diff
@@ -12,28 +12,29 @@
 """
 Unique Ratio Composed Metric definition
 """
 # pylint: disable=duplicate-code
 
 from typing import Any, Dict, Optional, Tuple
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import ComposedMetric
 from metadata.profiler.metrics.static.count import Count
 from metadata.profiler.metrics.static.unique_count import UniqueCount
 
 
 class UniqueRatio(ComposedMetric):
     """
     Given the total count and unique count,
     compute the unique ratio
     """
 
     @classmethod
     def name(cls):
-        return "uniqueProportion"
+        return MetricType.uniqueProportion.value
 
     @classmethod
     def required_metrics(cls) -> Tuple[str, ...]:
         return Count.name(), UniqueCount.name()
 
     @property
     def metric_type(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/core.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/core.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,19 +14,22 @@
 """
 
 # pylint: disable=invalid-name
 
 from abc import ABC, abstractmethod
 from enum import Enum
 from functools import wraps
-from typing import Any, Dict, Optional, Tuple, TypeVar
+from typing import Any, Callable, Dict, Optional, Tuple, TypeVar
 
 from sqlalchemy import Column
 from sqlalchemy.orm import DeclarativeMeta, Session
 
+from metadata.generated.schema.entity.data.table import Table
+from metadata.profiler.adaptors.nosql_adaptor import NoSQLAdaptor
+
 # When creating complex metrics, use inherit_cache = CACHE
 CACHE = True
 
 
 def _label(_fn):
     """
     Decorator factory (based on self) to
@@ -83,14 +86,17 @@
         _new_cls.__init__ = _new_init
 
         return _new_cls
 
     return inner
 
 
+T = TypeVar("T")
+
+
 class Metric(ABC):
     """
     Parent class metric
 
     We have 3 types of Metrics:
     - StaticMetric
     - TimeMetric
@@ -149,14 +155,21 @@
         String(length=256) -> str
 
         We can override this for things like
         variance, where it will be a float
         """
         return self.col.type.python_type if self.col else None
 
+    def nosql_fn(self, client: NoSQLAdaptor) -> Callable[[Table], Optional[T]]:
+        """
+        Return the function to be used for NoSQL clients to calculate the metric.
+        By default, returns a "do nothing" function that returns None.
+        """
+        return lambda table: None
+
 
 TMetric = TypeVar("TMetric", bound=Metric)
 
 
 class StaticMetric(Metric, ABC):
     """
     Static metric definition
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/hybrid/histogram.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/hybrid/histogram.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 """
 import math
 from typing import Any, Dict, List, Optional, Union, cast
 
 from sqlalchemy import and_, case, column, func
 from sqlalchemy.orm import DeclarativeMeta, Session
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.composed.iqr import InterQuartileRange
 from metadata.profiler.metrics.core import HybridMetric
 from metadata.profiler.metrics.static.count import Count
 from metadata.profiler.metrics.static.max import Max
 from metadata.profiler.metrics.static.min import Min
 from metadata.profiler.orm.functions.length import LenFn
 from metadata.profiler.orm.registry import is_concatenable, is_quantifiable
@@ -40,15 +41,15 @@
 
     - For a quantifiable value, return the usual AVG
     - For a concatenable (str, text...) return the AVG length
     """
 
     @classmethod
     def name(cls):
-        return "histogram"
+        return MetricType.histogram.value
 
     @property
     def metric_type(self):
         return dict
 
     @staticmethod
     def _get_bin_width(iqr: float, row_count: float) -> Union[float, int]:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/registry.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/registry.py`

 * *Files 2% similar despite different names*

```diff
@@ -38,14 +38,15 @@
 from metadata.profiler.metrics.static.max_length import MaxLength
 from metadata.profiler.metrics.static.mean import Mean
 from metadata.profiler.metrics.static.min import Min
 from metadata.profiler.metrics.static.min_length import MinLength
 from metadata.profiler.metrics.static.not_like_count import NotLikeCount
 from metadata.profiler.metrics.static.not_regexp_match_count import NotRegexCount
 from metadata.profiler.metrics.static.null_count import NullCount
+from metadata.profiler.metrics.static.null_missing_count import NullMissingCount
 from metadata.profiler.metrics.static.regexp_match_count import RegexCount
 from metadata.profiler.metrics.static.row_count import RowCount
 from metadata.profiler.metrics.static.stddev import StdDev
 from metadata.profiler.metrics.static.sum import Sum
 from metadata.profiler.metrics.static.unique_count import UniqueCount
 from metadata.profiler.metrics.system.system import System
 from metadata.profiler.metrics.window.first_quartile import FirstQuartile
@@ -99,7 +100,10 @@
     THIRD_QUARTILE = ThirdQuartile
 
     # System Metrics
     SYSTEM = System
 
     # Hybrid Metrics
     HISTOGRAM = Histogram
+
+    # Missing Count
+    NULL_MISSING_COUNT = NullMissingCount
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/column_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/column_count.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 from typing import cast
 
 from sqlalchemy import inspect, literal
 from sqlalchemy.ext.compiler import compiles
 from sqlalchemy.orm import DeclarativeMeta
 from sqlalchemy.sql.functions import FunctionElement
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import CACHE, StaticMetric, _label
 from metadata.profiler.orm.registry import Dialects
 
 
 class ColumnCountFn(FunctionElement):
     name = __qualname__
     inherit_cache = CACHE
@@ -56,15 +57,15 @@
     add_props(table=table)(Metrics.COLUMN_COUNT.value)
     """
 
     table: DeclarativeMeta
 
     @classmethod
     def name(cls):
-        return "columnCount"
+        return MetricType.columnCount.value
 
     @classmethod
     def is_col_metric(cls) -> bool:
         """
         Mark the class as a Table Metric
         """
         return False
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/column_names.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/column_names.py`

 * *Files 12% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 
 import sqlalchemy
 from sqlalchemy import inspect, literal
 from sqlalchemy.ext.compiler import compiles
 from sqlalchemy.orm import DeclarativeMeta
 from sqlalchemy.sql.functions import FunctionElement
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import CACHE, StaticMetric, _label
 from metadata.profiler.orm.registry import Dialects
 
 
 class ColunNameFn(FunctionElement):
     name = __qualname__
     inherit_cache = CACHE
@@ -56,15 +57,15 @@
     add_props(table=table)(Metrics.COLUMN_NAMES.value)
     """
 
     table: DeclarativeMeta
 
     @classmethod
     def name(cls):
-        return "columnNames"
+        return MetricType.columnNames.value
 
     @classmethod
     def is_col_metric(cls) -> bool:
         """
         Mark the class as a Table Metric
         """
         return False
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/count.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 Count Metric definition
 """
 # pylint: disable=duplicate-code
 
 
 from sqlalchemy import column, func
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.count import CountFn
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
 
@@ -29,15 +30,15 @@
     COUNT Metric
 
     Given a column, return the count. Ignores NULL values
     """
 
     @classmethod
     def name(cls):
-        return "valuesCount"
+        return MetricType.valuesCount.value
 
     @property
     def metric_type(self):
         return int
 
     @_label
     def fn(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/count_in_set.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/count_in_set.py`

 * *Files 5% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 """
 # pylint: disable=duplicate-code
 import traceback
 from typing import List
 
 from sqlalchemy import case, column
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.sum import SumFn
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
 
@@ -36,15 +37,15 @@
     add_props(values=["John"])(Metrics.COUNT_IN_SET.value)
     """
 
     values: List[str]
 
     @classmethod
     def name(cls):
-        return "countInSet"
+        return MetricType.countInSet.value
 
     @property
     def metric_type(self):
         return int
 
     @_label
     def fn(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/distinct_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/distinct_count.py`

 * *Files 7% similar despite different names*

```diff
@@ -10,17 +10,19 @@
 #  limitations under the License.
 
 """
 Distinct Count Metric definition
 """
 # pylint: disable=duplicate-code
 
+import json
 
 from sqlalchemy import column, distinct, func
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.count import CountFn
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
 
@@ -29,15 +31,15 @@
     DISTINCT_COUNT Metric
 
     Given a column, count the number of distinct values
     """
 
     @classmethod
     def name(cls):
-        return "distinctCount"
+        return MetricType.distinctCount.value
 
     @property
     def metric_type(self):
         return int
 
     @_label
     def fn(self):
@@ -53,15 +55,22 @@
         # pylint: disable=import-outside-toplevel
         from collections import Counter
 
         try:
             counter = Counter()
             for df in dfs:
                 df_col_value = df[self.col.name].dropna().to_list()
-                counter.update(df_col_value)
+                try:
+                    counter.update(df_col_value)
+                except TypeError as err:
+                    if isinstance(df_col_value, list):
+                        for value in df_col_value:
+                            counter.update([json.dumps(value)])
+                    else:
+                        raise err
             return len(counter.keys())
         except Exception as err:
             logger.debug(
                 f"Don't know how to process type {self.col.type}"
                 f" when computing Distinct Count.\n Error: {err}"
             )
             return 0
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/ilike_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/not_regexp_match_count.py`

 * *Files 18% similar despite different names*

```diff
@@ -6,50 +6,82 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-ILIKE Count Metric definition
+Regex Count Metric definition
 """
 # pylint: disable=duplicate-code
 
-from sqlalchemy import case, column
+from sqlalchemy import case, column, not_
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.sum import SumFn
+from metadata.profiler.orm.registry import is_concatenable
 
 
-class ILikeCount(StaticMetric):
+class NotRegexCount(StaticMetric):
     """
-    ILIKE_COUNT Metric: case-insensitive LIKE
+    NOT_REGEX_COUNT Metric
 
     Given a column, and an expression, return the number of
-    rows that match it
+    rows that match the forbidden regex pattern
 
     This Metric needs to be initialised passing the expression to check
-    add_props(expression="j%")(Metrics.ILIKE_COUNT.value)
+    add_props(expression="j%")(Metrics.NOT_REGEX_COUNT.value)
     """
 
     expression: str
 
     @classmethod
     def name(cls):
-        return "iLikeCount"
+        return MetricType.notRegexCount.value
 
     @property
     def metric_type(self):
         return int
 
+    def _is_concatenable(self):
+        return is_concatenable(self.col.type)
+
     @_label
     def fn(self):
+        """sqlalchemy function"""
         if not hasattr(self, "expression"):
             raise AttributeError(
-                "ILike Count requires an expression to be set: add_props(expression=...)(Metrics.ILIKE_COUNT)"
+                "Not Regex Count requires an expression to be set: add_props(expression=...)(Metrics.NOT_REGEX_COUNT)"
             )
         return SumFn(
             case(
-                [(column(self.col.name, self.col.type).ilike(self.expression), 1)],
-                else_=0,
+                [
+                    (
+                        not_(
+                            column(self.col.name, self.col.type).regexp_match(
+                                self.expression
+                            )
+                        ),
+                        0,
+                    )
+                ],
+                else_=1,
+            )
+        )
+
+    def df_fn(self, dfs):
+        """pandas function"""
+        if not hasattr(self, "expression"):
+            raise AttributeError(
+                "Regex Count requires an expression to be set: add_props(expression=...)(Metrics.REGEX_COUNT)"
+            )
+        if self._is_concatenable():
+            return sum(
+                df[self.col.name][
+                    df[self.col.name].astype(str).str.contains(self.expression)
+                ].count()
+                for df in dfs
             )
+        raise TypeError(
+            f"Don't know how to process type {self.col.type} when computing Not RegExp Match Count"
         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/like_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/like_count.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 """
 Like Count Metric definition
 """
 # pylint: disable=duplicate-code
 
 from sqlalchemy import case, column
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.sum import SumFn
 
 
 class LikeCount(StaticMetric):
     """
     LIKE_COUNT Metric
@@ -31,15 +32,15 @@
     add_props(expression="j%")(Metrics.LIKE_COUNT.value)
     """
 
     expression: str
 
     @classmethod
     def name(cls):
-        return "likeCount"
+        return MetricType.likeCount.value
 
     @property
     def metric_type(self):
         return int
 
     @_label
     def fn(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/max.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/max_length.py`

 * *Files 19% similar despite different names*

```diff
@@ -6,93 +6,76 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Max Metric definition
+MAX_LENGTH Metric definition
 """
 # pylint: disable=duplicate-code
 
 
-from sqlalchemy import TIME, column
-from sqlalchemy.ext.compiler import compiles
-from sqlalchemy.sql.functions import GenericFunction
+from sqlalchemy import column, func
 
-from metadata.profiler.metrics.core import CACHE, StaticMetric, _label
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
+from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.length import LenFn
-from metadata.profiler.orm.registry import (
-    FLOAT_SET,
-    Dialects,
-    is_concatenable,
-    is_date_time,
-    is_quantifiable,
-)
-
-
-class MaxFn(GenericFunction):
-    name = __qualname__
-    inherit_cache = CACHE
-
-
-@compiles(MaxFn)
-def _(element, compiler, **kw):
-    col = compiler.process(element.clauses, **kw)
-    return f"MAX({col})"
-
-
-@compiles(MaxFn, Dialects.Trino)
-def _(element, compiler, **kw):
-    col = compiler.process(element.clauses, **kw)
-    first_clause = element.clauses.clauses[0]
-    # Check if the first clause is an instance of LenFn and its type is not in FLOAT_SET
-    # or if the type of the first clause is date time
-    if (
-        isinstance(first_clause, LenFn)
-        and type(first_clause.clauses.clauses[0].type) not in FLOAT_SET
-    ) or is_date_time(first_clause.type):
-        # If the condition is true, return the maximum value of the column
-        return f"MAX({col})"
-    return f"IF(is_nan(MAX({col})), NULL, MAX({col}))"
-
-
-@compiles(MaxFn, Dialects.MySQL)
-@compiles(MaxFn, Dialects.MariaDB)
-def _(element, compiler, **kw):
-    col = compiler.process(element.clauses, **kw)
-    col_type = element.clauses.clauses[0].type
-    if isinstance(col_type, TIME):
-        # Mysql Sqlalchemy returns timedelta which is not supported pydantic type
-        # hence we profile the time by modifying it in seconds
-        return f"MAX(TIME_TO_SEC({col}))"
-    return f"MAX({col})"
+from metadata.profiler.orm.registry import is_concatenable
+from metadata.utils.logger import profiler_logger
 
+logger = profiler_logger()
 
-class Max(StaticMetric):
+
+class MaxLength(StaticMetric):
     """
-    MAX Metric
+    MAX_LENGTH Metric
+
+    Given a column, return the MIN LENGTH value.
 
-    Given a column, return the max value.
+    Only works for concatenable types
     """
 
     @classmethod
     def name(cls):
-        return "max"
+        return MetricType.maxLength.value
+
+    @property
+    def metric_type(self):
+        return int
+
+    def _is_concatenable(self):
+        return is_concatenable(self.col.type)
 
     @_label
     def fn(self):
         """sqlalchemy function"""
-        if is_concatenable(self.col.type):
-            return MaxFn(LenFn(column(self.col.name, self.col.type)))
-        if (not is_quantifiable(self.col.type)) and (not is_date_time(self.col.type)):
-            return None
-        return MaxFn(column(self.col.name, self.col.type))
+        if self._is_concatenable():
+            return func.max(LenFn(column(self.col.name, self.col.type)))
+
+        logger.debug(
+            f"Don't know how to process type {self.col.type} when computing MAX_LENGTH"
+        )
+        return None
 
+    # pylint: disable=import-outside-toplevel
     def df_fn(self, dfs=None):
-        """pandas function"""
-        if is_quantifiable(self.col.type):
-            return max((df[self.col.name].max() for df in dfs))
-        if is_date_time(self.col.type):
-            max_ = max((df[self.col.name].max() for df in dfs))
-            return int(max_.timestamp() * 1000)
-        return 0
+        """dataframe function"""
+        from numpy import vectorize
+
+        length_vectorize_func = vectorize(len)
+        if self._is_concatenable():
+            max_length_list = []
+
+            for df in dfs:
+                if any(df[self.col.name].dropna()):
+                    max_length_list.append(
+                        length_vectorize_func(
+                            df[self.col.name].dropna().astype(str)
+                        ).max()
+                    )
+            if max_length_list:
+                return max(max_length_list)
+        logger.debug(
+            f"Don't know how to process type {self.col.type} when computing MAX_LENGTH"
+        )
+        return None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/max_length.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/min_length.py`

 * *Files 9% similar despite different names*

```diff
@@ -6,75 +6,77 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-MAX_LENGTH Metric definition
+MIN_LENGTH Metric definition
 """
 # pylint: disable=duplicate-code
 
 
 from sqlalchemy import column, func
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.length import LenFn
 from metadata.profiler.orm.registry import is_concatenable
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
 
-class MaxLength(StaticMetric):
+class MinLength(StaticMetric):
     """
-    MAX_LENGTH Metric
+    MIN_LENGTH Metric
 
     Given a column, return the MIN LENGTH value.
 
     Only works for concatenable types
     """
 
     @classmethod
     def name(cls):
-        return "maxLength"
+        return MetricType.minLength.value
 
     @property
     def metric_type(self):
         return int
 
     def _is_concatenable(self):
         return is_concatenable(self.col.type)
 
     @_label
     def fn(self):
         """sqlalchemy function"""
         if self._is_concatenable():
-            return func.max(LenFn(column(self.col.name, self.col.type)))
+            return func.min(LenFn(column(self.col.name, self.col.type)))
 
         logger.debug(
-            f"Don't know how to process type {self.col.type} when computing MAX_LENGTH"
+            f"Don't know how to process type {self.col.type} when computing MIN_LENGTH"
         )
         return None
 
     # pylint: disable=import-outside-toplevel
     def df_fn(self, dfs=None):
         """dataframe function"""
         from numpy import vectorize
 
         length_vectorize_func = vectorize(len)
+
         if self._is_concatenable():
-            max_length_list = []
+            min_length_list = []
 
             for df in dfs:
-                if any(df[self.col.name]):
-                    max_length_list.append(
+                if any(df[self.col.name].dropna()):
+                    min_length_list.append(
                         length_vectorize_func(
                             df[self.col.name].dropna().astype(str)
-                        ).max()
+                        ).min()
                     )
-            if max_length_list:
-                return max(max_length_list)
+            if min_length_list:
+                return min(min_length_list)
         logger.debug(
-            f"Don't know how to process type {self.col.type} when computing MAX_LENGTH"
+            f"Don't know how to process type {self.col.type} when computing MIN_LENGTH"
         )
         return None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/mean.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/mean.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,34 +8,38 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 AVG Metric definition
 """
-# pylint: disable=duplicate-code
-
-
-from typing import List, cast
+from functools import partial
+from typing import Callable, List, Optional, cast
 
 from sqlalchemy import column, func
 from sqlalchemy.ext.compiler import compiles
 from sqlalchemy.sql.functions import GenericFunction
 
-from metadata.profiler.metrics.core import CACHE, StaticMetric, _label
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
+from metadata.generated.schema.entity.data.table import Table
+from metadata.profiler.adaptors.nosql_adaptor import NoSQLAdaptor
+from metadata.profiler.metrics.core import CACHE, StaticMetric, T, _label
 from metadata.profiler.orm.functions.length import LenFn
 from metadata.profiler.orm.registry import (
     FLOAT_SET,
     Dialects,
     is_concatenable,
     is_date_time,
     is_quantifiable,
 )
 from metadata.utils.logger import profiler_logger
 
+# pylint: disable=duplicate-code
+
+
 logger = profiler_logger()
 
 
 # pylint: disable=invalid-name
 class avg(GenericFunction):
     name = "avg"
     inherit_cache = CACHE
@@ -81,15 +85,15 @@
 
     - For a quantifiable value, return the usual AVG
     - For a concatenable (str, text...) return the AVG length
     """
 
     @classmethod
     def name(cls):
-        return "mean"
+        return MetricType.mean.value
 
     @property
     def metric_type(self):
         return float
 
     @_label
     def fn(self):
@@ -111,34 +115,38 @@
         import pandas as pd
         from numpy import average, vectorize
 
         dfs = cast(List[pd.DataFrame], dfs)
 
         means = []
         weights = []
-
-        if is_quantifiable(self.col.type):
-            for df in dfs:
-                mean = df[self.col.name].mean()
-                if not pd.isnull(mean):
-                    means.append(mean)
-                    weights.append(df[self.col.name].count())
-
-        if is_concatenable(self.col.type):
-            length_vectorize_func = vectorize(len)
-            for df in dfs:
+        length_vectorize_func = vectorize(len)
+        for df in dfs:
+            processed_df = df[self.col.name].dropna()
+            try:
                 mean = None
-                if any(df[self.col.name]):
-                    mean = length_vectorize_func(
-                        df[self.col.name].dropna().astype(str)
-                    ).mean()
+                if is_quantifiable(self.col.type):
+                    mean = processed_df.mean()
+                if is_concatenable(self.col.type):
+                    mean = length_vectorize_func(processed_df.astype(str)).mean()
                 if not pd.isnull(mean):
                     means.append(mean)
-                    weights.append(df[self.col.name].dropna().count())
+                    weights.append(processed_df.count())
+            except Exception as err:
+                logger.debug(
+                    f"Error while computing mean for column {self.col.name}: {err}"
+                )
+                return None
 
         if means:
             return average(means, weights=weights)
 
         logger.warning(
             f"Don't know how to process type {self.col.type} when computing MEAN"
         )
         return None
+
+    def nosql_fn(self, adaptor: NoSQLAdaptor) -> Callable[[Table], Optional[T]]:
+        """nosql function"""
+        if is_quantifiable(self.col.type):
+            return partial(adaptor.mean, column=self.col)
+        return lambda table: None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/min.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/window/first_quartile.py`

 * *Files 25% similar despite different names*

```diff
@@ -6,93 +6,95 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Min Metric definition
+First Quartile definition
 """
 # pylint: disable=duplicate-code
 
-from sqlalchemy import TIME, column
-from sqlalchemy.ext.compiler import compiles
-from sqlalchemy.sql.functions import GenericFunction
+from typing import List, cast
 
-from metadata.profiler.metrics.core import CACHE, StaticMetric, _label
+from sqlalchemy import column
+
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
+from metadata.profiler.metrics.core import StaticMetric, _label
+from metadata.profiler.metrics.window.percentille_mixin import PercentilMixin
 from metadata.profiler.orm.functions.length import LenFn
-from metadata.profiler.orm.registry import (
-    FLOAT_SET,
-    Dialects,
-    is_concatenable,
-    is_date_time,
-    is_quantifiable,
-)
-
-
-class MinFn(GenericFunction):
-    name = __qualname__
-    inherit_cache = CACHE
-
-
-@compiles(MinFn)
-def _(element, compiler, **kw):
-    col = compiler.process(element.clauses, **kw)
-    return f"MIN({col})"
-
-
-@compiles(MinFn, Dialects.Trino)
-def _(element, compiler, **kw):
-    col = compiler.process(element.clauses, **kw)
-    first_clause = element.clauses.clauses[0]
-    # Check if the first clause is an instance of LenFn and its type is not in FLOAT_SET
-    # or if the type of the first clause is date time
-    if (
-        isinstance(first_clause, LenFn)
-        and type(first_clause.clauses.clauses[0].type) not in FLOAT_SET
-    ) or is_date_time(first_clause.type):
-        # If the condition is true, return the minimum value of the column
-        return f"MIN({col})"
-    return f"IF(is_nan(MIN({col})), NULL, MIN({col}))"
-
-
-@compiles(MinFn, Dialects.MySQL)
-@compiles(MinFn, Dialects.MariaDB)
-def _(element, compiler, **kw):
-    col = compiler.process(element.clauses, **kw)
-    col_type = element.clauses.clauses[0].type
-    if isinstance(col_type, TIME):
-        # Mysql Sqlalchemy returns timedelta which is not supported pydantic type
-        # hence we profile the time by modifying it in seconds
-        return f"MIN(TIME_TO_SEC({col}))"
-    return f"MIN({col})"
+from metadata.profiler.orm.registry import is_concatenable, is_quantifiable
+from metadata.utils.logger import profiler_logger
+
+logger = profiler_logger()
 
 
-class Min(StaticMetric):
+class FirstQuartile(StaticMetric, PercentilMixin):
     """
-    MIN Metric
+    First Quartile Metric
+
+    Given a column, return the first quartile value.
 
-    Given a column, return the min value.
+    - For a quantifiable value, return first quartile value
     """
 
     @classmethod
     def name(cls):
-        return "min"
+        return MetricType.firstQuartile.value
+
+    @classmethod
+    def is_window_metric(cls):
+        return True
+
+    @property
+    def metric_type(self):
+        return float
 
     @_label
     def fn(self):
         """sqlalchemy function"""
-        if is_concatenable(self.col.type):
-            return MinFn(LenFn(column(self.col.name, self.col.type)))
+        if is_quantifiable(self.col.type):
+            # col fullname is only needed for MySQL and SQLite
+            return self._compute_sqa_fn(
+                column(self.col.name, self.col.type),
+                self.col.table.fullname if self.col.table is not None else None,
+                0.25,
+            )
 
-        if (not is_quantifiable(self.col.type)) and (not is_date_time(self.col.type)):
-            return None
-        return MinFn(column(self.col.name, self.col.type))
+        if is_concatenable(self.col.type):
+            return self._compute_sqa_fn(
+                LenFn(column(self.col.name, self.col.type)),
+                self.col.table.fullname if self.col.table is not None else None,
+                0.25,
+            )
+
+        logger.debug(
+            f"Don't know how to process type {self.col.type} when computing First Quartile"
+        )
+        return None
 
     def df_fn(self, dfs=None):
-        """pandas function"""
+        """Dataframe function"""
+        # pylint: disable=import-outside-toplevel
+        import pandas as pd
+
+        df = cast(List[pd.DataFrame], dfs)
+
         if is_quantifiable(self.col.type):
-            return min((df[self.col.name].min() for df in dfs))
-        if is_date_time(self.col.type):
-            min_ = min((df[self.col.name].min() for df in dfs))
-            return int(min_.timestamp() * 1000)
-        return 0
+            # we can't compute the first quartile unless we have
+            # the entire set. Median of Medians could be used
+            # though it would required set to be sorted before hand
+            try:
+                df = pd.concat([df[self.col.name] for df in dfs])
+            except MemoryError:
+                logger.error(
+                    f"Unable to compute Median for {self.col.name} due to memory constraints."
+                    f"We recommend using a smaller sample size or partitionning."
+                )
+                return None
+            # check if nan
+            first_quartile = df.quantile(0.25, interpolation="midpoint")
+            return None if pd.isnull(first_quartile) else first_quartile
+        logger.debug(
+            f"Don't know how to process type {self.col.type} when computing First Quartile"
+        )
+        return None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/min_length.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/null_count.py`

 * *Files 26% similar despite different names*

```diff
@@ -6,76 +6,52 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-MIN_LENGTH Metric definition
+Null Count Metric definition
 """
 # pylint: disable=duplicate-code
 
 
-from sqlalchemy import column, func
+from sqlalchemy import case, column
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
-from metadata.profiler.orm.functions.length import LenFn
-from metadata.profiler.orm.registry import is_concatenable
-from metadata.utils.logger import profiler_logger
+from metadata.profiler.orm.functions.sum import SumFn
 
-logger = profiler_logger()
 
-
-class MinLength(StaticMetric):
+class NullCount(StaticMetric):
     """
-    MIN_LENGTH Metric
+    NULL COUNT Metric
 
-    Given a column, return the MIN LENGTH value.
+    Given a column, return the null count.
 
-    Only works for concatenable types
+    We are building a CASE WHEN structure:
+    ```
+    SUM(
+        CASE is not null THEN 1
+        ELSE 0
+    )
+    ```
     """
 
     @classmethod
     def name(cls):
-        return "minLength"
+        return MetricType.nullCount.value
 
     @property
     def metric_type(self):
         return int
 
-    def _is_concatenable(self):
-        return is_concatenable(self.col.type)
-
     @_label
     def fn(self):
         """sqlalchemy function"""
-        if self._is_concatenable():
-            return func.min(LenFn(column(self.col.name, self.col.type)))
-
-        logger.debug(
-            f"Don't know how to process type {self.col.type} when computing MIN_LENGTH"
+        return SumFn(
+            case([(column(self.col.name, self.col.type).is_(None), 1)], else_=0)
         )
-        return None
 
-    # pylint: disable=import-outside-toplevel
     def df_fn(self, dfs=None):
-        """dataframe function"""
-        from numpy import vectorize
-
-        length_vectorize_func = vectorize(len)
-
-        if self._is_concatenable():
-            min_length_list = []
-
-            for df in dfs:
-                if any(df[self.col.name]):
-                    min_length_list.append(
-                        length_vectorize_func(
-                            df[self.col.name].dropna().astype(str)
-                        ).min()
-                    )
-            if min_length_list:
-                return min(min_length_list)
-        logger.debug(
-            f"Don't know how to process type {self.col.type} when computing MIN_LENGTH"
-        )
-        return None
+        """pandas function"""
+        return sum(df[self.col.name].isnull().sum() for df in dfs)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/not_like_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/not_like_count.py`

 * *Files 14% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 """
 Like Count Metric definition
 """
 # pylint: disable=duplicate-code
 
 from sqlalchemy import case, column
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.sum import SumFn
 
 
 class NotLikeCount(StaticMetric):
     """
     NOT_LIKE_COUNT Metric
@@ -31,15 +32,15 @@
     add_props(expression="j%")(Metrics.NOT_LIKE_COUNT.value)
     """
 
     expression: str
 
     @classmethod
     def name(cls):
-        return "notLikeCount"
+        return MetricType.notLikeCount.value
 
     @property
     def metric_type(self):
         return int
 
     @_label
     def fn(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/not_regexp_match_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/regexp_match_count.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,77 +10,77 @@
 #  limitations under the License.
 
 """
 Regex Count Metric definition
 """
 # pylint: disable=duplicate-code
 
-from sqlalchemy import case, column, not_
+from sqlalchemy import case, column
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.sum import SumFn
 from metadata.profiler.orm.registry import is_concatenable
 
 
-class NotRegexCount(StaticMetric):
+class RegexCount(StaticMetric):
     """
-    NOT_REGEX_COUNT Metric
+    REGEX_COUNT Metric
 
     Given a column, and an expression, return the number of
-    rows that match the forbidden regex pattern
+    rows that match it
 
     This Metric needs to be initialised passing the expression to check
-    add_props(expression="j%")(Metrics.NOT_REGEX_COUNT.value)
+    add_props(expression="j.*")(Metrics.REGEX_COUNT.value)
     """
 
     expression: str
 
     @classmethod
     def name(cls):
-        return "notRegexCount"
+        return MetricType.regexCount.value
 
     @property
     def metric_type(self):
         return int
 
     def _is_concatenable(self):
         return is_concatenable(self.col.type)
 
     @_label
     def fn(self):
         """sqlalchemy function"""
         if not hasattr(self, "expression"):
             raise AttributeError(
-                "Not Regex Count requires an expression to be set: add_props(expression=...)(Metrics.NOT_REGEX_COUNT)"
+                "Regex Count requires an expression to be set: add_props(expression=...)(Metrics.REGEX_COUNT)"
             )
         return SumFn(
             case(
                 [
                     (
-                        not_(
-                            column(self.col.name, self.col.type).regexp_match(
-                                self.expression
-                            )
+                        column(self.col.name, self.col.type).regexp_match(
+                            self.expression
                         ),
-                        0,
+                        1,
                     )
                 ],
-                else_=1,
+                else_=0,
             )
         )
 
     def df_fn(self, dfs):
         """pandas function"""
+
         if not hasattr(self, "expression"):
             raise AttributeError(
                 "Regex Count requires an expression to be set: add_props(expression=...)(Metrics.REGEX_COUNT)"
             )
         if self._is_concatenable():
             return sum(
                 df[self.col.name][
                     df[self.col.name].astype(str).str.contains(self.expression)
                 ].count()
                 for df in dfs
             )
         raise TypeError(
-            f"Don't know how to process type {self.col.type} when computing Not RegExp Match Count"
+            f"Don't know how to process type {self.col.type} when computing RegExp Match Count"
         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/null_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/base.py`

 * *Files 22% similar despite different names*

```diff
@@ -4,53 +4,60 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Null Count Metric definition
+Base local reader
 """
-# pylint: disable=duplicate-code
-
+import traceback
+from abc import ABC, abstractmethod
+from typing import List, Optional, Union
 
-from sqlalchemy import case, column
+from metadata.utils.logger import ingestion_logger
 
-from metadata.profiler.metrics.core import StaticMetric, _label
-from metadata.profiler.orm.functions.sum import SumFn
+logger = ingestion_logger()
 
 
-class NullCount(StaticMetric):
+class ReadException(Exception):
+    """
+    To be raised by any errors with the read calls
     """
-    NULL COUNT Metric
 
-    Given a column, return the null count.
 
-    We are building a CASE WHEN structure:
-    ```
-    SUM(
-        CASE is not null THEN 1
-        ELSE 0
-    )
-    ```
+class Reader(ABC):
+    """
+    Abstract class for all readers
     """
 
-    @classmethod
-    def name(cls):
-        return "nullCount"
-
-    @property
-    def metric_type(self):
-        return int
-
-    @_label
-    def fn(self):
-        """sqlalchemy function"""
-        return SumFn(
-            case([(column(self.col.name, self.col.type).is_(None), 1)], else_=0)
-        )
-
-    def df_fn(self, dfs=None):
-        """pandas function"""
-        return sum(df[self.col.name].isnull().sum() for df in dfs)
+    @abstractmethod
+    def read(self, path: str, **kwargs) -> Union[str, bytes]:
+        """
+        Given a string, return a string
+        """
+        raise NotImplementedError("Missing read implementation")
+
+    @abstractmethod
+    def _get_tree(self) -> List[str]:
+        """
+        Return the filenames of the root
+        """
+        raise NotImplementedError("Missing get_tree implementation")
+
+    def get_tree(self) -> Optional[List[str]]:
+        """
+        If something happens, return None
+        """
+        try:
+            return self._get_tree()
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            logger.error(f"Error getting file tree [{err}]")
+        return None
+
+    def download(self, path: str, local_file_path: str, **kwargs):
+        """
+        Given a path, download the file
+        """
+        # To be implemented by required readers
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/regexp_match_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/sum.py`

 * *Files 21% similar despite different names*

```diff
@@ -6,80 +6,67 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Regex Count Metric definition
+SUM Metric definition
 """
-# pylint: disable=duplicate-code
+from functools import partial
+from typing import Callable, Optional
 
-from sqlalchemy import case, column
+from sqlalchemy import column
 
-from metadata.profiler.metrics.core import StaticMetric, _label
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
+from metadata.generated.schema.entity.data.table import Table
+from metadata.profiler.adaptors.nosql_adaptor import NoSQLAdaptor
+from metadata.profiler.metrics.core import StaticMetric, T, _label
+from metadata.profiler.orm.functions.length import LenFn
 from metadata.profiler.orm.functions.sum import SumFn
-from metadata.profiler.orm.registry import is_concatenable
+from metadata.profiler.orm.registry import is_concatenable, is_quantifiable
+
+# pylint: disable=duplicate-code
 
 
-class RegexCount(StaticMetric):
+class Sum(StaticMetric):
     """
-    REGEX_COUNT Metric
+    SUM Metric
 
-    Given a column, and an expression, return the number of
-    rows that match it
+    Given a column, return the sum of its values.
 
-    This Metric needs to be initialised passing the expression to check
-    add_props(expression="j.*")(Metrics.REGEX_COUNT.value)
+    Only works for quantifiable types
     """
 
-    expression: str
-
     @classmethod
     def name(cls):
-        return "regexCount"
-
-    @property
-    def metric_type(self):
-        return int
-
-    def _is_concatenable(self):
-        return is_concatenable(self.col.type)
+        return MetricType.sum.value
 
     @_label
     def fn(self):
         """sqlalchemy function"""
-        if not hasattr(self, "expression"):
-            raise AttributeError(
-                "Regex Count requires an expression to be set: add_props(expression=...)(Metrics.REGEX_COUNT)"
-            )
-        return SumFn(
-            case(
-                [
-                    (
-                        column(self.col.name, self.col.type).regexp_match(
-                            self.expression
-                        ),
-                        1,
-                    )
-                ],
-                else_=0,
-            )
-        )
+        if is_quantifiable(self.col.type):
+            return SumFn(column(self.col.name, self.col.type))
+
+        if is_concatenable(self.col.type):
+            return SumFn(LenFn(column(self.col.name, self.col.type)))
+
+        return None
 
-    def df_fn(self, dfs):
+    def df_fn(self, dfs=None):
         """pandas function"""
 
-        if not hasattr(self, "expression"):
-            raise AttributeError(
-                "Regex Count requires an expression to be set: add_props(expression=...)(Metrics.REGEX_COUNT)"
-            )
-        if self._is_concatenable():
-            return sum(
-                df[self.col.name][
-                    df[self.col.name].astype(str).str.contains(self.expression)
-                ].count()
-                for df in dfs
-            )
-        raise TypeError(
-            f"Don't know how to process type {self.col.type} when computing RegExp Match Count"
-        )
+        if is_quantifiable(self.col.type):
+            try:
+                return sum(df[self.col.name].sum() for df in dfs)
+            except (TypeError, ValueError):
+                try:
+                    return sum(df[self.col.name].astype(float).sum() for df in dfs)
+                except Exception:
+                    return None
+        return None
+
+    def nosql_fn(self, adaptor: NoSQLAdaptor) -> Callable[[Table], Optional[T]]:
+        """nosql function"""
+        if is_quantifiable(self.col.type):
+            return partial(adaptor.sum, column=self.col)
+        return lambda table: None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/row_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/secrets_manager.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,52 +1,36 @@
-#  Copyright 2021 Collate
+#  Copyright 2022 Collate
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Table Count Metric definition
+Secrets manager interface
 """
-# pylint: disable=duplicate-code
+from abc import abstractmethod
 
+from metadata.utils.logger import ingestion_logger
+from metadata.utils.singleton import Singleton
 
-from sqlalchemy import func
+logger = ingestion_logger()
 
-from metadata.profiler.metrics.core import StaticMetric, _label
 
-
-class RowCount(StaticMetric):
+class SecretsManager(metaclass=Singleton):
     """
-    ROW_NUMBER Metric
+    Abstract class implemented by different secrets' manager providers.
 
-    Count all rows on a table
+    It contains a set of auxiliary methods for adding missing fields which have been encrypted in the secrets' manager
+    providers.
     """
 
-    @classmethod
-    def name(cls):
-        return "rowCount"
-
-    @classmethod
-    def is_col_metric(cls) -> bool:
+    @abstractmethod
+    def get_string_value(self, secret_id: str) -> str:
         """
-        Mark the class as a Table Metric
+        :param secret_id: The secret id to retrieve
+        :return: The value of the secret
         """
-        return False
-
-    @property
-    def metric_type(self):
-        return int
-
-    @_label
-    def fn(self):
-        """sqlalchemy function"""
-        return func.count()
-
-    def df_fn(self, dfs=None):
-        """pandas function"""
-        return sum(len(df.index) for df in dfs)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/stddev.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/stddev.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 # pylint: disable=consider-using-f-string,duplicate-code
 
 
 from sqlalchemy import column
 from sqlalchemy.ext.compiler import compiles
 from sqlalchemy.sql.functions import FunctionElement
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import CACHE, StaticMetric, _label
 from metadata.profiler.orm.functions.length import LenFn
 from metadata.profiler.orm.registry import (
     FLOAT_SET,
     Dialects,
     is_concatenable,
     is_date_time,
@@ -99,15 +100,15 @@
     STD Metric
 
     Given a column, return the Standard Deviation value.
     """
 
     @classmethod
     def name(cls):
-        return "stddev"
+        return MetricType.stddev.value
 
     @property
     def metric_type(self):
         return float
 
     @_label
     def fn(self):
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/static/unique_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/window/third_quartile.py`

 * *Files 24% similar despite different names*

```diff
@@ -6,77 +6,95 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Unique Count Metric definition
+Median Metric definition
 """
-from typing import Optional
+# pylint: disable=duplicate-code
 
-from sqlalchemy import column, func
-from sqlalchemy.orm import DeclarativeMeta, Session
+from typing import List, cast
 
-from metadata.profiler.metrics.core import QueryMetric
-from metadata.profiler.orm.functions.unique_count import _unique_count_query_mapper
-from metadata.profiler.orm.registry import NOT_COMPUTE
+from sqlalchemy import column
+
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
+from metadata.profiler.metrics.core import StaticMetric, _label
+from metadata.profiler.metrics.window.percentille_mixin import PercentilMixin
+from metadata.profiler.orm.functions.length import LenFn
+from metadata.profiler.orm.registry import is_concatenable, is_quantifiable
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
 
-class UniqueCount(QueryMetric):
+class ThirdQuartile(StaticMetric, PercentilMixin):
     """
-    UNIQUE_COUNT Metric
+    Third Quartile Metric
+
+    Given a column, return the third quartile value.
 
-    Given a column, count the number of values appearing only once
+    - For a quantifiable value, return third quartile value
     """
 
     @classmethod
     def name(cls):
-        return "uniqueCount"
+        return MetricType.thirdQuartile.value
+
+    @classmethod
+    def is_window_metric(cls):
+        return True
 
     @property
     def metric_type(self):
-        return int
+        return float
 
-    def query(
-        self, sample: Optional[DeclarativeMeta], session: Optional[Session] = None
-    ):
-        """
-        Build the Unique Count metric
-        """
-        if not session:
-            raise AttributeError(
-                "We are missing the session attribute to compute the UniqueCount."
+    @_label
+    def fn(self):
+        """sqlalchemy function"""
+        if is_quantifiable(self.col.type):
+            # col fullname is only needed for MySQL and SQLite
+            return self._compute_sqa_fn(
+                column(self.col.name, self.col.type),
+                self.col.table.fullname if self.col.table is not None else None,
+                0.75,
             )
 
-        if self.col.type.__class__.__name__ in NOT_COMPUTE:
-            return None
+        if is_concatenable(self.col.type):
+            return self._compute_sqa_fn(
+                LenFn(column(self.col.name, self.col.type)),
+                self.col.table.fullname if self.col.table is not None else None,
+                0.75,
+            )
 
-        # Run all queries on top of the sampled data
-        col = column(self.col.name, self.col.type)
-        unique_count_query = _unique_count_query_mapper[session.bind.dialect.name](
-            col, session, sample
+        logger.debug(
+            f"Don't know how to process type {self.col.type} when computing Third Quartile"
         )
-        only_once_cte = unique_count_query.cte("only_once")
-        return session.query(func.count().label(self.name())).select_from(only_once_cte)
+        return None
 
     def df_fn(self, dfs=None):
-        """
-        Build the Unique Count metric
-        """
-        from collections import Counter  # pylint: disable=import-outside-toplevel
-
-        try:
-            counter = Counter()
-            for df in dfs:
-                df_col_value = df[self.col.name].dropna().to_list()
-                counter.update(df_col_value)
-            return len([key for key, value in counter.items() if value == 1])
-        except Exception as err:
-            logger.debug(
-                f"Don't know how to process type {self.col.type}"
-                f" when computing Unique Count.\n Error: {err}"
-            )
-            return 0
+        """Dataframe function"""
+        # pylint: disable=import-outside-toplevel
+        import pandas as pd
+
+        df = cast(List[pd.DataFrame], dfs)
+
+        if is_quantifiable(self.col.type):
+            # we can't compute the median unless we have
+            # the entire set. Median of Medians could be used
+            # though it would required set to be sorted before hand
+            try:
+                df = pd.concat([df[self.col.name] for df in dfs])
+            except MemoryError:
+                logger.error(
+                    f"Unable to compute Median for {self.col.name} due to memory constraints."
+                    f"We recommend using a smaller sample size or partitionning."
+                )
+                return None
+            # check if nan
+            third_quartile = df.quantile(0.75, interpolation="midpoint")
+            return None if pd.isnull(third_quartile) else third_quartile
+        logger.debug(
+            f"Don't know how to process type {self.col.type} when computing Third Quartile"
+        )
+        return None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/dml_operation.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/dml_operation.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/queries/bigquery.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/queries/bigquery.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/queries/redshift.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/queries/redshift.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/queries/snowflake.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/queries/snowflake.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/system/system.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/system/system.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,14 +16,15 @@
 import traceback
 from collections import defaultdict
 from typing import Dict, List, Optional
 
 from sqlalchemy import text
 from sqlalchemy.orm import DeclarativeMeta, Session
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.generated.schema.entity.services.connections.database.bigQueryConnection import (
     BigQueryConnection,
 )
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.profiler.metrics.core import SystemMetric
 from metadata.profiler.metrics.system.dml_operation import (
@@ -426,15 +427,15 @@
     @classmethod
     def is_system_metrics(cls) -> bool:
         """True if returns system metrics"""
         return True
 
     @classmethod
     def name(cls):
-        return "system"
+        return MetricType.system.value
 
     def _manage_cache(self, max_size_in_bytes: int = MAX_SIZE_IN_BYTES) -> None:
         """manage cache and clears it if it exceeds the max size
 
         Args:
             max_size_in_bytes (int, optional): max size of cache in bytes. Defaults to 2147483648.
         Returns:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/window/first_quartile.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/window/median.py`

 * *Files 13% similar despite different names*

```diff
@@ -6,43 +6,44 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-First Quartile definition
+Median Metric definition
 """
 # pylint: disable=duplicate-code
 
 from typing import List, cast
 
 from sqlalchemy import column
 
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.metrics.window.percentille_mixin import PercentilMixin
 from metadata.profiler.orm.functions.length import LenFn
 from metadata.profiler.orm.registry import is_concatenable, is_quantifiable
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
 
-class FirstQuartile(StaticMetric, PercentilMixin):
+class Median(StaticMetric, PercentilMixin):
     """
-    First Quartile Metric
+    Median Metric
 
-    Given a column, return the first quartile value.
+    Given a column, return the Median value.
 
-    - For a quantifiable value, return first quartile value
+    - For a quantifiable value, return the usual Median
     """
 
     @classmethod
     def name(cls):
-        return "firstQuartile"
+        return MetricType.median.value
 
     @classmethod
     def is_window_metric(cls):
         return True
 
     @property
     def metric_type(self):
@@ -52,48 +53,51 @@
     def fn(self):
         """sqlalchemy function"""
         if is_quantifiable(self.col.type):
             # col fullname is only needed for MySQL and SQLite
             return self._compute_sqa_fn(
                 column(self.col.name, self.col.type),
                 self.col.table.fullname if self.col.table is not None else None,
-                0.25,
+                0.5,
             )
 
         if is_concatenable(self.col.type):
             return self._compute_sqa_fn(
                 LenFn(column(self.col.name, self.col.type)),
                 self.col.table.fullname if self.col.table is not None else None,
-                0.25,
+                0.5,
             )
 
         logger.debug(
-            f"Don't know how to process type {self.col.type} when computing First Quartile"
+            f"Don't know how to process type {self.col.type} when computing Median"
         )
         return None
 
     def df_fn(self, dfs=None):
         """Dataframe function"""
-        # pylint: disable=import-outside-toplevel
-        import pandas as pd
+        import pandas as pd  # pylint: disable=import-outside-toplevel
 
-        df = cast(List[pd.DataFrame], dfs)
+        dfs = cast(List[pd.DataFrame], dfs)
 
         if is_quantifiable(self.col.type):
-            # we can't compute the first quartile unless we have
+            # we can't compute the median unless we have
             # the entire set. Median of Medians could be used
             # though it would required set to be sorted before hand
             try:
                 df = pd.concat([df[self.col.name] for df in dfs])
             except MemoryError:
                 logger.error(
                     f"Unable to compute Median for {self.col.name} due to memory constraints."
                     f"We recommend using a smaller sample size or partitionning."
                 )
                 return None
-            # check if nan
-            first_quartile = df.quantile(0.25, interpolation="midpoint")
-            return None if pd.isnull(first_quartile) else first_quartile
+            try:
+                median = df.median()
+            except Exception as err:
+                logger.error(
+                    f"Unable to compute Median for {self.col.name} due to error: {err}"
+                )
+            return None if pd.isnull(median) else median
         logger.debug(
-            f"Don't know how to process type {self.col.type} when computing First Quartile"
+            f"Don't know how to process type {self.col.type} when computing Median"
         )
         return None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/window/median.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/local.py`

 * *Files 27% similar despite different names*

```diff
@@ -4,94 +4,89 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Median Metric definition
+Local Reader
 """
-# pylint: disable=duplicate-code
-
-from typing import List, cast
-
-from sqlalchemy import column
+import os
+import traceback
+from pathlib import Path
+from typing import List, Optional, Union
 
-from metadata.profiler.metrics.core import StaticMetric, _label
-from metadata.profiler.metrics.window.percentille_mixin import PercentilMixin
-from metadata.profiler.orm.functions.length import LenFn
-from metadata.profiler.orm.registry import is_concatenable, is_quantifiable
-from metadata.utils.logger import profiler_logger
+from metadata.readers.file.base import Reader, ReadException
+from metadata.utils.constants import UTF_8
+from metadata.utils.logger import ingestion_logger
 
-logger = profiler_logger()
+logger = ingestion_logger()
 
 
-class Median(StaticMetric, PercentilMixin):
+class LocalReader(Reader):
     """
-    Median Metric
-
-    Given a column, return the Median value.
-
-    - For a quantifiable value, return the usual Median
+    Read files locally
     """
 
-    @classmethod
-    def name(cls):
-        return "median"
-
-    @classmethod
-    def is_window_metric(cls):
-        return True
-
-    @property
-    def metric_type(self):
-        return float
-
-    @_label
-    def fn(self):
-        """sqlalchemy function"""
-        if is_quantifiable(self.col.type):
-            # col fullname is only needed for MySQL and SQLite
-            return self._compute_sqa_fn(
-                column(self.col.name, self.col.type),
-                self.col.table.fullname if self.col.table is not None else None,
-                0.5,
-            )
+    def __init__(self, base_path: Optional[Path] = None):
+        self.base_path = base_path or Path(__file__)
 
-        if is_concatenable(self.col.type):
-            return self._compute_sqa_fn(
-                LenFn(column(self.col.name, self.col.type)),
-                self.col.table.fullname if self.col.table is not None else None,
-                0.5,
+    def read(self, path: str, **kwargs) -> Union[str, bytes]:
+        """
+        simple local reader
+
+        If we cannot encode the file contents, we fallback and returns the bytes
+        to let the client use this data as needed.
+        """
+        try:
+            with open(self.base_path / path, encoding=UTF_8) as file:
+                return file.read()
+
+        except UnicodeDecodeError:
+            logger.debug(
+                "Cannot read the file with UTF-8 encoding. Trying to read bytes..."
             )
+            with open(self.base_path / path, "rb") as file:
+                return file.read()
 
-        logger.debug(
-            f"Don't know how to process type {self.col.type} when computing Median"
-        )
-        return None
-
-    def df_fn(self, dfs=None):
-        """Dataframe function"""
-        import pandas as pd  # pylint: disable=import-outside-toplevel
-
-        dfs = cast(List[pd.DataFrame], dfs)
-
-        if is_quantifiable(self.col.type):
-            # we can't compute the median unless we have
-            # the entire set. Median of Medians could be used
-            # though it would required set to be sorted before hand
-            try:
-                df = pd.concat([df[self.col.name] for df in dfs])
-            except MemoryError:
-                logger.error(
-                    f"Unable to compute Median for {self.col.name} due to memory constraints."
-                    f"We recommend using a smaller sample size or partitionning."
-                )
-                return None
-            median = df.median()
-            return None if pd.isnull(median) else median
-        logger.debug(
-            f"Don't know how to process type {self.col.type} when computing Median"
-        )
-        return None
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            raise ReadException(f"Error reading file [{path}] locally: {err}")
+
+    def _get_tree(self) -> Optional[List[str]]:
+        """
+        Return the tree with the files relative to the base path
+        """
+        return [
+            str(path).replace(str(self.base_path) + "/", "")
+            for path in Path(self.base_path).rglob("*")
+        ]
+
+    def get_local_files(
+        self, search_key: str, excluded_files: Optional[List[str]] = None
+    ) -> List[str]:
+        """Scan through local path recursively
+        and retuns file path based on `search_key`"""
+
+        if excluded_files is None:
+            excluded_files = []
+
+        file_paths = []
+        for root, _, file in os.walk(self.base_path):
+            for fle in file:
+                if search_key in fle and fle not in excluded_files:
+                    file_paths.append(f"{root}/{fle}")
+
+        return file_paths
+
+    def download(
+        self,
+        path: str,
+        local_file_path: str,
+        *,
+        bucket_name: str = None,
+        verbose: bool = True,
+        **__,
+    ):
+        # Nothing to download for local files
+        pass
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/metrics/window/third_quartile.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/min.py`

 * *Files 26% similar despite different names*

```diff
@@ -6,94 +6,112 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Median Metric definition
+Min Metric definition
 """
-# pylint: disable=duplicate-code
+from functools import partial
+from typing import Callable, Optional
 
-from typing import List, cast
+from sqlalchemy import TIME, column
+from sqlalchemy.ext.compiler import compiles
+from sqlalchemy.sql.functions import GenericFunction
+
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
+from metadata.generated.schema.entity.data.table import DataType, Table
+from metadata.profiler.adaptors.nosql_adaptor import NoSQLAdaptor
+from metadata.profiler.metrics.core import CACHE, StaticMetric, T, _label
+from metadata.profiler.orm.functions.length import LenFn
+from metadata.profiler.orm.registry import (
+    FLOAT_SET,
+    Dialects,
+    is_concatenable,
+    is_date_time,
+    is_quantifiable,
+)
 
-from sqlalchemy import column
+# pylint: disable=duplicate-code
 
-from metadata.profiler.metrics.core import StaticMetric, _label
-from metadata.profiler.metrics.window.percentille_mixin import PercentilMixin
-from metadata.profiler.orm.functions.length import LenFn
-from metadata.profiler.orm.registry import is_concatenable, is_quantifiable
-from metadata.utils.logger import profiler_logger
 
-logger = profiler_logger()
+class MinFn(GenericFunction):
+    name = __qualname__
+    inherit_cache = CACHE
+
+
+@compiles(MinFn)
+def _(element, compiler, **kw):
+    col = compiler.process(element.clauses, **kw)
+    return f"MIN({col})"
+
+
+@compiles(MinFn, Dialects.Trino)
+def _(element, compiler, **kw):
+    col = compiler.process(element.clauses, **kw)
+    first_clause = element.clauses.clauses[0]
+    # Check if the first clause is an instance of LenFn and its type is not in FLOAT_SET
+    # or if the type of the first clause is date time
+    if (
+        isinstance(first_clause, LenFn)
+        and type(first_clause.clauses.clauses[0].type) not in FLOAT_SET
+    ) or is_date_time(first_clause.type):
+        # If the condition is true, return the minimum value of the column
+        return f"MIN({col})"
+    return f"IF(is_nan(MIN({col})), NULL, MIN({col}))"
+
+
+@compiles(MinFn, Dialects.MySQL)
+@compiles(MinFn, Dialects.MariaDB)
+def _(element, compiler, **kw):
+    col = compiler.process(element.clauses, **kw)
+    col_type = element.clauses.clauses[0].type
+    if isinstance(col_type, TIME):
+        # Mysql Sqlalchemy returns timedelta which is not supported pydantic type
+        # hence we profile the time by modifying it in seconds
+        return f"MIN(TIME_TO_SEC({col}))"
+    return f"MIN({col})"
 
 
-class ThirdQuartile(StaticMetric, PercentilMixin):
+class Min(StaticMetric):
     """
-    Third Quartile Metric
+    MIN Metric
 
-    Given a column, return the third quartile value.
-
-    - For a quantifiable value, return third quartile value
+    Given a column, return the min value.
     """
 
     @classmethod
     def name(cls):
-        return "thirdQuartile"
-
-    @classmethod
-    def is_window_metric(cls):
-        return True
-
-    @property
-    def metric_type(self):
-        return float
+        return MetricType.min.value
 
     @_label
     def fn(self):
         """sqlalchemy function"""
-        if is_quantifiable(self.col.type):
-            # col fullname is only needed for MySQL and SQLite
-            return self._compute_sqa_fn(
-                column(self.col.name, self.col.type),
-                self.col.table.fullname if self.col.table is not None else None,
-                0.75,
-            )
-
         if is_concatenable(self.col.type):
-            return self._compute_sqa_fn(
-                LenFn(column(self.col.name, self.col.type)),
-                self.col.table.fullname if self.col.table is not None else None,
-                0.75,
-            )
-
-        logger.debug(
-            f"Don't know how to process type {self.col.type} when computing Third Quartile"
-        )
-        return None
+            return MinFn(LenFn(column(self.col.name, self.col.type)))
+
+        if (not is_quantifiable(self.col.type)) and (not is_date_time(self.col.type)):
+            return None
+        return MinFn(column(self.col.name, self.col.type))
 
     def df_fn(self, dfs=None):
-        """Dataframe function"""
-        # pylint: disable=import-outside-toplevel
+        """pandas function"""
         import pandas as pd
 
-        df = cast(List[pd.DataFrame], dfs)
-
         if is_quantifiable(self.col.type):
-            # we can't compute the median unless we have
-            # the entire set. Median of Medians could be used
-            # though it would required set to be sorted before hand
-            try:
-                df = pd.concat([df[self.col.name] for df in dfs])
-            except MemoryError:
-                logger.error(
-                    f"Unable to compute Median for {self.col.name} due to memory constraints."
-                    f"We recommend using a smaller sample size or partitionning."
-                )
-                return None
-            # check if nan
-            third_quartile = df.quantile(0.75, interpolation="midpoint")
-            return None if pd.isnull(third_quartile) else third_quartile
-        logger.debug(
-            f"Don't know how to process type {self.col.type} when computing Third Quartile"
-        )
+            return min((df[self.col.name].min() for df in dfs))
+        if is_date_time(self.col.type):
+            min_ = None
+            if self.col.type in {DataType.DATETIME, DataType.DATE}:
+                min_ = min((pd.to_datetime(df[self.col.name]).min() for df in dfs))
+                return None if pd.isnull(min_) else int(min_.timestamp() * 1000)
+            elif self.col.type == DataType.TIME:
+                min_ = min((pd.to_timedelta(df[self.col.name]).min() for df in dfs))
+                return None if pd.isnull(min_) else min_.seconds
         return None
+
+    def nosql_fn(self, adaptor: NoSQLAdaptor) -> Callable[[Table], Optional[T]]:
+        """nosql function"""
+        if is_quantifiable(self.col.type):
+            return partial(adaptor.min, column=self.col)
+        return lambda table: None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/base.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -123,15 +123,18 @@
     # Type takes positional arguments in the form of (name, bases, dict)
     orm = type(
         orm_name,  # Output class name
         (Base,),  # SQLAlchemy declarative base
         {
             "__tablename__": str(table.name.__root__),
             "__table_args__": {
-                "schema": orm_schema_name,
+                # SQLite does not support schemas
+                "schema": orm_schema_name
+                if table.serviceType != databaseService.DatabaseServiceType.SQLite
+                else None,
                 "extend_existing": True,  # Recreates the table ORM object if it already exists. Useful for testing
                 "quote": check_snowflake_case_sensitive(
                     table.serviceType, table.name.__root__
                 ),
             },
             **cols,
             "metadata": sqa_metadata_obj or Base.metadata,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/bigquery/converter.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/sqa_like_column.py`

 * *Files 19% similar despite different names*

```diff
@@ -6,26 +6,21 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Converter logic to transform an OpenMetadata Table Entity for Bigquery
-to an SQLAlchemy ORM class.
+Helper module to reploicate the behavior of SQLAlchemy Column object
 """
 
+from dataclasses import dataclass
 
-from metadata.generated.schema.entity.data.database import databaseService
-from metadata.generated.schema.entity.data.table import Column, DataType
-from metadata.profiler.orm.converter.common import CommonMapTypes
-from metadata.profiler.source.bigquery.type_mapper import bigquery_type_mapper
+from metadata.generated.schema.entity.data.table import DataType
 
 
-class BigqueryMapTypes(CommonMapTypes):
-    def return_custom_type(self, col: Column, table_service_type):
-        if (
-            table_service_type == databaseService.DatabaseServiceType.BigQuery
-            and col.dataType == DataType.STRUCT
-        ):
-            return bigquery_type_mapper(self._TYPE_MAP, col)
-        return super().return_custom_type(col, table_service_type)
+@dataclass
+class SQALikeColumn:
+    """Replicate somehow the behavior of SQLAlchemy Column object to have constant API access"""
+
+    name: str
+    type: DataType
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/common.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/common.py`

 * *Files 27% similar despite different names*

```diff
@@ -75,7 +75,54 @@
 
         if col.arrayDataType:
             return self._TYPE_MAP.get(col.dataType)(item_type=col.arrayDataType)
         return self.return_custom_type(col, table_service_type)
 
     def return_custom_type(self, col: Column, _):
         return self._TYPE_MAP.get(col.dataType)
+
+    @staticmethod
+    def map_sqa_to_om_types() -> dict:
+        """returns an ORM type"""
+        return {
+            sqlalchemy.NUMERIC: {DataType.NUMBER, DataType.NUMERIC},
+            sqlalchemy.SMALLINT: {
+                DataType.TINYINT,
+                DataType.SMALLINT,
+                DataType.BYTEINT,
+            },
+            sqlalchemy.INT: {DataType.INT},
+            sqlalchemy.BIGINT: {DataType.BIGINT},
+            CustomTypes.BYTES.value: {DataType.BLOB, DataType.BYTES},
+            sqlalchemy.FLOAT: {DataType.FLOAT},
+            sqlalchemy.DECIMAL: {DataType.DOUBLE, DataType.DECIMAL},
+            CustomTypes.TIMESTAMP.value: {DataType.TIMESTAMP},
+            sqlalchemy.TIME: {DataType.TIME},
+            sqlalchemy.DATE: {DataType.DATE},
+            sqlalchemy.DATETIME: {DataType.DATETIME},
+            sqlalchemy.Interval: {DataType.INTERVAL},
+            sqlalchemy.String: {DataType.STRING},
+            sqlalchemy.TEXT: {DataType.TEXT, DataType.MEDIUMTEXT},
+            sqlalchemy.CHAR: {DataType.CHAR},
+            sqlalchemy.VARCHAR: {DataType.VARCHAR},
+            sqlalchemy.BOOLEAN: {DataType.BOOLEAN},
+            sqlalchemy.LargeBinary: {
+                DataType.MEDIUMBLOB,
+                DataType.BINARY,
+                DataType.LONGBLOB,
+            },
+            sqlalchemy.VARBINARY: {DataType.VARBINARY},
+            CustomTypes.ARRAY.value: {DataType.ARRAY},
+            sqa_types.SQAMap: {DataType.MAP},
+            sqa_types.SQAStruct: {DataType.STRUCT},
+            sqa_types.SQAUnion: {DataType.UNION},
+            sqa_types.SQASet: {DataType.SET},
+            sqa_types.SQASGeography: {DataType.GEOGRAPHY},
+            sqlalchemy.Enum: {DataType.ENUM},
+            sqlalchemy.JSON: {DataType.JSON},
+            CustomTypes.UUID.value: {DataType.UUID},
+            CustomTypes.BYTEA.value: {DataType.BYTEA},
+            sqlalchemy.NVARCHAR: {DataType.NTEXT},
+            CustomTypes.IMAGE.value: {DataType.IMAGE},
+            CustomTypes.IP.value: {DataType.IPV6, DataType.IPV4},
+            CustomTypes.SQADATETIMERANGE.value: {DataType.DATETIMERANGE},
+        }
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/mssql/converter.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/mssql/converter.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/converter/snowflake/converter.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/converter/snowflake/converter.py`

 * *Files 15% similar despite different names*

```diff
@@ -18,20 +18,33 @@
 from metadata.generated.schema.entity.data.database import databaseService
 from metadata.generated.schema.entity.data.table import Column, DataType
 from metadata.profiler.orm.converter.common import CommonMapTypes
 from metadata.profiler.orm.registry import CustomTypes
 
 
 class SnowflakeMapTypes(CommonMapTypes):
+    """Snowflake mapper, inherits from CommonMapTypes"""
+
     def __init__(self) -> None:
         self._TYPE_MAP.update({DataType.BINARY: CustomTypes.BYTES.value})
 
     def return_custom_type(self, col: Column, table_service_type):
         if (
             table_service_type == databaseService.DatabaseServiceType.Snowflake
             and col.dataType == DataType.JSON
         ):
             # pylint: disable=import-outside-toplevel
             from snowflake.sqlalchemy import VARIANT
 
             return VARIANT
         return super().return_custom_type(col, table_service_type)
+
+    @staticmethod
+    def map_sqa_to_om_types() -> dict:
+        """returns an ORM type"""
+        # pylint: disable=import-outside-toplevel
+        from snowflake.sqlalchemy import VARIANT
+
+        return {
+            **CommonMapTypes.map_sqa_to_om_types(),
+            VARIANT: DataType.JSON,
+        }
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/concat.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/concat.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/conn_test.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/conn_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,13 +46,13 @@
     return "SELECT SESSION_USER()"
 
 
 @compiles(ConnTestFn, Dialects.Db2)
 @compiles(ConnTestFn, Dialects.IbmDbSa)
 @compiles(ConnTestFn, Dialects.Ibmi)
 def _(*_, **__):
-    return "SELECT 42 FROM SYSIBM.SYSDUMMY1;"
+    return "SELECT 42 FROM SYSIBM.SYSDUMMY1"
 
 
 @compiles(ConnTestFn, Dialects.Hana)
 def _(*_, **__):
     return "SELECT 42 FROM DUMMY"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/datetime.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/datetime.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/length.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/length.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/median.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/median.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/modulo.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/modulo.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/random_num.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/random_num.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/sum.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/sum.py`

 * *Files 6% similar despite different names*

```diff
@@ -28,19 +28,21 @@
 @compiles(SumFn)
 def _(element, compiler, **kw):
     """Cast to BIGINT to address overflow error from summing 32-bit int in most database dialects, #8430"""
     proc = compiler.process(element.clauses, **kw)
     return f"SUM(CAST({proc} AS BIGINT))"
 
 
+@compiles(SumFn, Dialects.Athena)
 @compiles(SumFn, Dialects.Trino)
+@compiles(SumFn, Dialects.Presto)
 def _(element, compiler, **kw):
-    """Cast to BIGINT to address cannot cast nan to bigint"""
+    """Cast to DECIMAL to address cannot cast nan to bigint"""
     proc = compiler.process(element.clauses, **kw)
-    return f"SUM(TRY_CAST({proc} AS BIGINT))"
+    return f"SUM(TRY_CAST({proc} AS DECIMAL))"
 
 
 @compiles(SumFn, Dialects.BigQuery)
 @compiles(SumFn, Dialects.Postgres)
 def _(element, compiler, **kw):
     """Handle case where column type is INTEGER but SUM returns a NUMBER"""
     proc = compiler.process(element.clauses, **kw)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/table_metric_computer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/table_metric_computer.py`

 * *Files 1% similar despite different names*

```diff
@@ -190,15 +190,15 @@
         create_date = cte(
             self._build_query(
                 [
                     Column("owner"),
                     Column("object_name").label("table_name"),
                     Column("created"),
                 ],
-                self._build_table("dba_objects", None),
+                self._build_table("all_objects", None),
                 [
                     func.lower(Column("owner")) == self.schema_name.lower(),
                     func.lower(Column("object_name")) == self.table_name.lower(),
                 ],
             )
         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/functions/unique_count.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/functions/unique_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/registry.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/registry.py`

 * *Files 2% similar despite different names*

```diff
@@ -88,14 +88,15 @@
     sqlalchemy.ARRAY.__name__,
     sqlalchemy.JSON.__name__,
     sqa_types.SQAMap.__name__,
     sqa_types.SQAStruct.__name__,
     sqa_types.SQASet.__name__,
     sqa_types.SQAUnion.__name__,
     sqa_types.SQASGeography.__name__,
+    DataType.GEOMETRY.value,
     DataType.ARRAY.value,
     DataType.JSON.value,
     CustomTypes.ARRAY.value.__name__,
     CustomTypes.SQADATETIMERANGE.value.__name__,
 }
 FLOAT_SET = {sqlalchemy.types.DECIMAL, sqlalchemy.types.FLOAT}
 
@@ -110,16 +111,14 @@
     DataType.TINYINT.value,
     DataType.DOUBLE.value,
     DataType.LONG.value,
 }
 
 CONCATENABLE_SET = {DataType.STRING.value, DataType.TEXT.value}
 
-DATATIME_SET = {DataType.DATETIME.value}
-
 
 # Now, let's define some helper methods to identify
 # the nature of an SQLAlchemy type
 def is_integer(_type) -> bool:
     """
     Check if sqlalchemy _type is derived from Integer
     """
@@ -134,15 +133,19 @@
 
 
 def is_date_time(_type) -> bool:
     """
     Check if sqlalchemy _type is derived from Date, Time or DateTime Type
     """
     if isinstance(_type, DataType):
-        return _type.value in DATATIME_SET
+        return _type.value in {
+            DataType.DATETIME.value,
+            DataType.TIME.value,
+            DataType.DATE.value,
+        }
     return (
         issubclass(_type.__class__, Date)
         or issubclass(_type.__class__, Time)
         or issubclass(_type.__class__, DateTime)
     )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/bytea_to_string.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/bytea_to_string.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_array.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/uuid.py`

 * *Files 19% similar despite different names*

```diff
@@ -5,46 +5,59 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-# pylint: disable=abstract-method
-
 """
 Expand sqlalchemy types to map them to OpenMetadata DataType
 """
-from sqlalchemy.sql.sqltypes import ARRAY, TypeDecorator
+# pylint: disable=duplicate-code,abstract-method
+import traceback
+from uuid import UUID
+
+from sqlalchemy.sql.sqltypes import String, TypeDecorator
 
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
 
-class CustomArray(TypeDecorator):
+class UUIDString(TypeDecorator):
     """
-    Convert numpy ndarray to python list
+    Convert Python bytestring to string with hexadecimal digits and back for storage.
     """
 
-    impl = ARRAY
+    impl = String
     cache_ok = True
 
     @property
     def python_type(self):
-        return list
+        return str
 
-    def process_result_value(self, value, dialect):
+    @staticmethod
+    def validate(value: str) -> UUID:
+        """
+        Make sure the data is of correct type
+        """
+        try:
+            return UUID(value)
+        except ValueError as err:
+            logger.debug(traceback.format_exc())
+            logger.error(f"Error converting value [{value}] to UUID: {err}")
+            raise err
+
+    def process_result_value(self, value: str, dialect):
         """This is executed during result retrieval
 
         Args:
             value: database record
             dialect: database dialect
         Returns:
-            python list conversion of ndarray
+            hex string representation of the byte value
         """
-        import numpy as np  # pylint: disable=import-outside-toplevel
-
-        if isinstance(value, np.ndarray):
-            return value.tolist()
+        return value
 
+    def process_literal_param(self, value, dialect):
+        self.validate(value)
         return value
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_datetimerange.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_datetimerange.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_hex_byte_string.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_hex_byte_string.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_image.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_image.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_ip.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/uuid_encoder.py`

 * *Files 23% similar despite different names*

```diff
@@ -5,26 +5,25 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-# pylint: disable=abstract-method
-
 """
-Expand sqlalchemy types to map them to OpenMetadata DataType
+UUID Encoder Module
 """
-from sqlalchemy.sql.sqltypes import String, TypeDecorator
-
-from metadata.utils.logger import profiler_logger
 
-logger = profiler_logger()
+import json
+from uuid import UUID
 
 
-class CustomIP(TypeDecorator):
+class UUIDEncoder(json.JSONEncoder):
     """
-    Convert RowVersion
+    UUID Encoder class
     """
 
-    impl = String
-    cache_ok = True
+    def default(self, o):
+        if isinstance(o, UUID):
+            # if the obj is uuid, we simply return the value of uuid
+            return str(o)
+        return json.JSONEncoder.default(self, o)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/custom_timestamp.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/orm/types/custom_timestamp.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/orm/types/uuid.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/gcs.py`

 * *Files 22% similar despite different names*

```diff
@@ -6,58 +6,63 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Expand sqlalchemy types to map them to OpenMetadata DataType
+Read files as string from S3
 """
-# pylint: disable=duplicate-code,abstract-method
 import traceback
-from uuid import UUID
+from typing import List
 
-from sqlalchemy.sql.sqltypes import String, TypeDecorator
+from metadata.readers.file.base import Reader, ReadException
+from metadata.utils.logger import ingestion_logger
 
-from metadata.utils.logger import profiler_logger
+logger = ingestion_logger()
 
-logger = profiler_logger()
 
-
-class UUIDString(TypeDecorator):
-    """
-    Convert Python bytestring to string with hexadecimal digits and back for storage.
+class GCSReader(Reader):
+    """GCS Reader
+    Class to read from buckets with prefix as paths
     """
 
-    impl = String
-    cache_ok = True
+    def __init__(self, client):
+        self.client = client
 
-    @property
-    def python_type(self):
-        return str
+    def read(
+        self, path: str, *, bucket_name: str = None, verbose: bool = True, **__
+    ) -> bytes:
+        try:
+            return (
+                self.client.get_bucket(bucket_name).get_blob(path).download_as_string()
+            )
+        except Exception as err:
+            if verbose:
+                logger.debug(traceback.format_exc())
+            raise ReadException(f"Error fetching file [{path}] from GCS: {err}")
 
-    @staticmethod
-    def validate(value: str) -> UUID:
+    def _get_tree(self) -> List[str]:
         """
-        Make sure the data is of correct type
+        We are not implementing this yet. This should
+        only be needed for now for the Datalake where we don't need
+        to traverse any directories.
         """
-        try:
-            return UUID(value)
-        except ValueError as err:
-            logger.debug(traceback.format_exc())
-            logger.error(f"Error converting value [{value}] to UUID: {err}")
-            raise err
-
-    def process_result_value(self, value: str, dialect):
-        """This is executed during result retrieval
-
-        Args:
-            value: database record
-            dialect: database dialect
-        Returns:
-            hex string representation of the byte value
-        """
-        return value
+        raise NotImplementedError("Not implemented")
 
-    def process_literal_param(self, value, dialect):
-        self.validate(value)
-        return value
+    def download(
+        self,
+        path: str,
+        local_file_path: str,
+        *,
+        bucket_name: str = None,
+        verbose: bool = True,
+        **__,
+    ) -> bytes:
+        try:
+            self.client.get_bucket(bucket_name).get_blob(path).download_to_filename(
+                local_file_path
+            )
+        except Exception as err:
+            if verbose:
+                logger.debug(traceback.format_exc())
+            raise ReadException(f"Error downloading file [{path}] from GCS: {err}")
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/core.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/core.py`

 * *Files 10% similar despite different names*

```diff
@@ -12,49 +12,51 @@
 """
 Main Profile definition and queries to execute
 """
 from __future__ import annotations
 
 import traceback
 from datetime import datetime, timezone
-from typing import Any, Dict, Generic, List, Optional, Set, Tuple, Type, cast
+from typing import Any, Dict, Generic, List, Optional, Set, Tuple, Type
 
 from pydantic import ValidationError
 from sqlalchemy import Column
 from sqlalchemy.orm import DeclarativeMeta
 
 from metadata.generated.schema.api.data.createTableProfile import (
     CreateTableProfileRequest,
 )
+from metadata.generated.schema.configuration.profilerConfiguration import (
+    ProfilerConfiguration,
+)
 from metadata.generated.schema.entity.data.table import (
     ColumnName,
     ColumnProfile,
     ColumnProfilerConfig,
     SystemProfile,
     TableData,
     TableProfile,
 )
+from metadata.generated.schema.settings.settings import Settings
 from metadata.generated.schema.tests.customMetric import (
     CustomMetric as CustomMetricEntity,
 )
 from metadata.profiler.api.models import ProfilerResponse, ThreadPoolMetrics
 from metadata.profiler.interface.profiler_interface import ProfilerInterface
 from metadata.profiler.metrics.core import (
     ComposedMetric,
-    CustomMetric,
     HybridMetric,
     MetricTypes,
     QueryMetric,
     StaticMetric,
-    SystemMetric,
     TMetric,
 )
-from metadata.profiler.metrics.registry import Metrics
 from metadata.profiler.metrics.static.row_count import RowCount
 from metadata.profiler.orm.registry import NOT_COMPUTE
+from metadata.profiler.processor.metric_filter import MetricFilter
 from metadata.profiler.processor.sample_data_handler import upload_sample_data
 from metadata.utils.constants import SAMPLE_DATA_DEFAULT_COUNT
 from metadata.utils.execution_time_tracker import calculate_execution_time
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
@@ -72,37 +74,51 @@
 
     A profiler is composed of:
     - A profiler_interface, used to run the profiler against a source.
     - An ORM Table. One profiler attacks one table at a time.
     - A tuple of metrics, from which we will construct queries.
     """
 
+    # pylint: disable=too-many-instance-attributes
+
     def __init__(
         self,
         *metrics: Type[TMetric],
         profiler_interface: ProfilerInterface,
         include_columns: Optional[List[ColumnProfilerConfig]] = None,
         exclude_columns: Optional[List[str]] = None,
+        global_profiler_configuration: Optional[Settings] = None,
     ):
         """
         :param metrics: Metrics to run. We are receiving the uninitialized classes
         :param profiler_interface: Where to run the queries
         :param table: DeclarativeMeta containing table info
         :param ignore_cols: List of columns to ignore when computing the profile
         :param profile_sample: % of rows to use for sampling column metrics
         """
-
+        self.global_profiler_configuration: Optional[ProfilerConfiguration] = (
+            global_profiler_configuration.config_value
+            if global_profiler_configuration
+            else None
+        )
         self.profiler_interface = profiler_interface
         self.source_config = self.profiler_interface.source_config
         self.include_columns = include_columns
         self.exclude_columns = exclude_columns
         self._metrics = metrics
         self._profile_date = int(datetime.now(tz=timezone.utc).timestamp() * 1000)
         self.profile_sample_config = self.profiler_interface.profile_sample_config
 
+        self.metric_filter = MetricFilter(
+            metrics=self.metrics,
+            global_profiler_config=self.global_profiler_configuration,
+            table_profiler_config=self.profiler_interface.table_entity.tableProfilerConfig,
+            column_profiler_config=self.include_columns,
+        )
+
         self.validate_composed_metric()
 
         # Initialize profiler results
         self._table_results: Dict[str, Any] = {}
         self._column_results: Dict[str, Any] = {}
         self._system_results: Optional[List[Dict]] = []
 
@@ -171,20 +187,14 @@
 
     def _get_included_columns(self) -> Optional[Set[str]]:
         """Get include columns for table being profiled"""
         if self.include_columns:
             return {include_col.columnName for include_col in self.include_columns}
         return {}
 
-    def _filter_metrics(self, _type: Type[TMetric]) -> List[Type[TMetric]]:
-        """
-        Filter metrics by type
-        """
-        return [metric for metric in self.metrics if issubclass(metric, _type)]
-
     def _check_profile_and_handle(
         self, profile: CreateTableProfileRequest
     ) -> CreateTableProfileRequest:
         """Check if the profile data are empty. if empty then raise else return
 
         Args:
             profile (CreateTableProfileRequest): profile data
@@ -192,96 +202,29 @@
         Raises:
             Exception: that will be caught in the workflow and add the entity to failure source and processor status
 
         Returns:
             CreateTableProfileRequest:
         """
         for attrs, val in profile.tableProfile:
-            if attrs not in {"timestamp", "profileSample", "profileSampleType"} and val:
+            if (
+                attrs not in {"timestamp", "profileSample", "profileSampleType"}
+                and val is not None
+            ):
                 return
 
         for col_element in profile.columnProfile:
             for attrs, val in col_element:
                 if attrs not in {"timestamp", "name"} and val is not None:
                     return
 
         raise RuntimeError(
             f"No profile data computed for {self.profiler_interface.table_entity.fullyQualifiedName.__root__}"
         )
 
-    @property
-    def static_metrics(self) -> List[Type[StaticMetric]]:
-        return self._filter_metrics(StaticMetric)
-
-    @property
-    def composed_metrics(self) -> List[Type[ComposedMetric]]:
-        return self._filter_metrics(ComposedMetric)
-
-    @property
-    def custom_metrics(self) -> List[Type[CustomMetric]]:
-        return self._filter_metrics(CustomMetric)
-
-    @property
-    def query_metrics(self) -> List[Type[QueryMetric]]:
-        return self._filter_metrics(QueryMetric)
-
-    @property
-    def system_metrics(self) -> List[Type[SystemMetric]]:
-        return self._filter_metrics(SystemMetric)
-
-    @property
-    def hybrid_metric(self) -> List[Type[HybridMetric]]:
-        return self._filter_metrics(HybridMetric)
-
-    def get_col_metrics(
-        self, metrics: List[Type[TMetric]], column: Optional[Column] = None
-    ) -> List[Type[TMetric]]:
-        """
-        Filter list of metrics for column metrics with allowed types
-        """
-
-        if column is None:
-            return [metric for metric in metrics if metric.is_col_metric()]
-
-        if (
-            self.profiler_interface.table_entity.tableProfilerConfig
-            and self.profiler_interface.table_entity.tableProfilerConfig.includeColumns
-        ) or (self.include_columns):
-            # include_columns is set from the `tableConfig` of the `ProfilerProcessorConfig` in the CLI config
-            # while `self.profiler_interface.table_entity.tableProfilerConfig.includeColumns` is set from the entity
-            # definition in the metadata service. This gets set either from the UI or the profiler entity page. Config
-            # ran from the CLI takes precedence over the entity definition.
-            columns = (
-                self.include_columns
-                if self.include_columns
-                else self.profiler_interface.table_entity.tableProfilerConfig.includeColumns
-            )
-            columns = cast(List[ColumnProfilerConfig], columns)
-            metric_names = next(
-                (
-                    include_columns.metrics
-                    for include_columns in columns
-                    if include_columns.columnName == column.name
-                ),
-                None,
-            )
-
-            if metric_names:
-                metric_names = {
-                    mtrc.lower() for mtrc in metric_names
-                }  # case insensitice
-                metrics = [
-                    Metric.value
-                    for Metric in Metrics
-                    if Metric.value.name().lower() in metric_names
-                    and Metric.value in metrics
-                ]
-
-        return [metric for metric in metrics if metric.is_col_metric()]
-
     def get_custom_metrics(
         self, column_name: Optional[str] = None
     ) -> Optional[List[CustomMetricEntity]]:
         """Get custom metrics for a table or column
 
         Args:
             column (Optional[str]): optional column name. If None will fetch table level custom metrics
@@ -313,15 +256,15 @@
     def validate_composed_metric(self) -> None:
         """
         Make sure that all composed metrics have
         the necessary ingredients defined in
         `required_metrics` attr
         """
         names = {metric.name() for metric in self.metrics}
-        for metric in self.composed_metrics:
+        for metric in self.metric_filter.composed_metrics:
             if not set(metric.required_metrics()).issubset(names):
                 raise MissingMetricException(
                     f"We need {metric.required_metrics()} for {metric.name}, but only got {names} in the profiler"
                 )
 
     def run_composed_metrics(self, col: Column):
         """
@@ -335,15 +278,17 @@
         current_col_results: Dict[str, Any] = self._column_results.get(col.name)
         if not current_col_results:
             logger.debug(
                 "We do not have any results to base our Composed Metrics. Stopping!"
             )
             return
 
-        for metric in self.get_col_metrics(self.composed_metrics):
+        for metric in self.metric_filter.get_column_metrics(
+            ComposedMetric, col, self.profiler_interface.table_entity.serviceType
+        ):
             # Composed metrics require the results as an argument
             logger.debug(f"Running composed metric {metric.name()} for {col.name}")
 
             self._column_results[col.name][
                 metric.name()
             ] = self.profiler_interface.get_composed_metrics(
                 col,
@@ -360,15 +305,17 @@
         logger.debug("Running distribution metrics...")
         current_col_results: Dict[str, Any] = self._column_results.get(col.name)
         if not current_col_results:
             logger.debug(
                 "We do not have any results to base our Hybrid Metrics. Stopping!"
             )
             return
-        for metric in self.get_col_metrics(self.hybrid_metric, col):
+        for metric in self.metric_filter.get_column_metrics(
+            HybridMetric, col, self.profiler_interface.table_entity.serviceType
+        ):
             logger.debug(f"Running hybrid metric {metric.name()} for {col.name}")
             self._column_results[col.name][
                 metric.name()
             ] = self.profiler_interface.get_hybrid_metrics(
                 col,
                 metric,
                 current_col_results,
@@ -376,15 +323,15 @@
             )
 
     def _prepare_table_metrics(self) -> List:
         """prepare table metrics"""
         metrics = []
         table_metrics = [
             metric
-            for metric in self.static_metrics
+            for metric in self.metric_filter.static_metrics
             if (not metric.is_col_metric() and not metric.is_system_metrics())
         ]
 
         custom_table_metrics = self.get_custom_metrics()
 
         if table_metrics:
             metrics.extend(
@@ -410,15 +357,15 @@
                 ]
             )
 
         return metrics
 
     def _prepare_system_metrics(self) -> List:
         """prepare system metrics"""
-        system_metrics = self.system_metrics
+        system_metrics = self.metric_filter.system_metrics
 
         if system_metrics:
             return [
                 ThreadPoolMetrics(
                     metrics=system_metric,  # metric functions
                     metric_type=MetricTypes.System,  # metric type for function mapping
                     column=None,  # column name
@@ -427,25 +374,29 @@
                 for system_metric in system_metrics
             ]
 
         return []
 
     def _prepare_column_metrics(self) -> List:
         """prepare column metrics"""
+        column_metrics_for_thread_pool = []
         columns = [
             column
             for column in self.columns
             if column.type.__class__.__name__ not in NOT_COMPUTE
         ]
-        column_metrics_for_thread_pool = []
         static_metrics = [
             ThreadPoolMetrics(
                 metrics=[
                     metric
-                    for metric in self.get_col_metrics(self.static_metrics, column)
+                    for metric in self.metric_filter.get_column_metrics(
+                        StaticMetric,
+                        column,
+                        self.profiler_interface.table_entity.serviceType,
+                    )
                     if not metric.is_window_metric()
                 ],
                 metric_type=MetricTypes.Static,
                 column=column,
                 table=self.table,
             )
             for column in columns
@@ -454,21 +405,27 @@
             ThreadPoolMetrics(
                 metrics=metric,
                 metric_type=MetricTypes.Query,
                 column=column,
                 table=self.table,
             )
             for column in columns
-            for metric in self.get_col_metrics(self.query_metrics, column)
+            for metric in self.metric_filter.get_column_metrics(
+                QueryMetric, column, self.profiler_interface.table_entity.serviceType
+            )
         ]
         window_metrics = [
             ThreadPoolMetrics(
                 metrics=[
                     metric
-                    for metric in self.get_col_metrics(self.static_metrics, column)
+                    for metric in self.metric_filter.get_column_metrics(
+                        StaticMetric,
+                        column,
+                        self.profiler_interface.table_entity.serviceType,
+                    )
                     if metric.is_window_metric()
                 ],
                 metric_type=MetricTypes.Window,
                 column=column,
                 table=self.table,
             )
             for column in columns
@@ -624,20 +581,24 @@
 
             table_profile = TableProfile(
                 timestamp=self.profile_date,
                 columnCount=self._table_results.get("columnCount"),
                 rowCount=self._table_results.get(RowCount.name()),
                 createDateTime=self._table_results.get("createDateTime"),
                 sizeInByte=self._table_results.get("sizeInBytes"),
-                profileSample=self.profile_sample_config.profile_sample
-                if self.profile_sample_config
-                else None,
-                profileSampleType=self.profile_sample_config.profile_sample_type
-                if self.profile_sample_config
-                else None,
+                profileSample=(
+                    self.profile_sample_config.profile_sample
+                    if self.profile_sample_config
+                    else None
+                ),
+                profileSampleType=(
+                    self.profile_sample_config.profile_sample_type
+                    if self.profile_sample_config
+                    else None
+                ),
                 customMetrics=self._table_results.get("customMetrics"),
             )
 
             if self._system_results:
                 system_profile = [
                     SystemProfile(**system_result)
                     for system_result in self._system_results
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/default.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/default.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,14 +12,17 @@
 """
 Default simple profiler to use
 """
 from typing import List, Optional
 
 from sqlalchemy.orm import DeclarativeMeta
 
+from metadata.generated.schema.configuration.profilerConfiguration import (
+    ProfilerConfiguration,
+)
 from metadata.generated.schema.entity.data.table import ColumnProfilerConfig
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.profiler.interface.profiler_interface import ProfilerInterface
 from metadata.profiler.metrics.core import Metric, add_props
 from metadata.profiler.metrics.registry import Metrics
 from metadata.profiler.processor.core import Profiler
@@ -69,18 +72,20 @@
     """
 
     def __init__(
         self,
         profiler_interface: ProfilerInterface,
         include_columns: Optional[List[ColumnProfilerConfig]] = None,
         exclude_columns: Optional[List[str]] = None,
+        global_profiler_configuration: Optional[ProfilerConfiguration] = None,
     ):
         _metrics = get_default_metrics(
             table=profiler_interface.table, ometa_client=profiler_interface.ometa_client
         )
 
         super().__init__(
             *_metrics,
             profiler_interface=profiler_interface,
             include_columns=include_columns,
             exclude_columns=exclude_columns,
+            global_profiler_configuration=global_profiler_configuration,
         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/handle_partition.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/handle_partition.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 from __future__ import annotations
 
 from typing import List
 
 from sqlalchemy import Column, text
 
 from metadata.generated.schema.entity.data.table import (
-    PartitionIntervalType,
+    PartitionIntervalTypes,
     PartitionProfilerConfig,
 )
 from metadata.profiler.orm.functions.modulo import ModuloFn
 from metadata.profiler.orm.functions.random_num import RandomNumFn
 from metadata.utils.logger import profiler_logger
 from metadata.utils.sqa_utils import (
     build_query_filter,
@@ -48,21 +48,21 @@
         partition_details (dict): _description_
         col (Column): _description_
 
     Returns:
         _type_: _description_
     """
     partition_field = partition_details.partitionColumnName
-    if partition_details.partitionIntervalType == PartitionIntervalType.COLUMN_VALUE:
+    if partition_details.partitionIntervalType == PartitionIntervalTypes.COLUMN_VALUE:
         return get_value_filter(
             Column(partition_field),
             partition_details.partitionValues,
         )
 
-    if partition_details.partitionIntervalType == PartitionIntervalType.INTEGER_RANGE:
+    if partition_details.partitionIntervalType == PartitionIntervalTypes.INTEGER_RANGE:
         return get_integer_range_filter(
             Column(partition_field),
             partition_details.partitionIntegerRangeStart,
             partition_details.partitionIntegerRangeEnd,
         )
 
     type_ = get_partition_col_type(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/processor.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/processor.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Profiler Processor Step
 """
 import traceback
-from typing import cast
+from typing import Optional, cast
 
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
 )
 from metadata.generated.schema.metadataIngestion.databaseServiceProfilerPipeline import (
     DatabaseServiceProfilerPipeline,
 )
@@ -78,13 +78,15 @@
             return Either(right=profile)
         finally:
             profiler_runner.close()
 
         return Either()
 
     @classmethod
-    def create(cls, config_dict: dict, _: OpenMetadata) -> "Step":
+    def create(
+        cls, config_dict: dict, _: OpenMetadata, pipeline_name: Optional[str] = None
+    ) -> "Step":
         config = parse_workflow_config_gracefully(config_dict)
         return cls(config=config)
 
     def close(self) -> None:
         """We are already closing the connections after each execution"""
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/runner.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/runner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sample_data_handler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sample_data_handler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/pandas/sampler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/pandas/sampler.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 import random
 from typing import List, Optional, cast
 
 from metadata.data_quality.validations.table.pandas.tableRowInsertedCountToBeBetween import (
     TableRowInsertedCountToBeBetweenValidator,
 )
 from metadata.generated.schema.entity.data.table import (
-    PartitionIntervalType,
+    PartitionIntervalTypes,
     PartitionProfilerConfig,
     ProfileSampleType,
     TableData,
 )
 from metadata.profiler.processor.sampler.sampler_interface import SamplerInterface
 from metadata.utils.sqa_like_column import SQALikeColumn
 
@@ -37,23 +37,23 @@
 
     def _partitioned_table(self):
         """Get partitioned table"""
         self._partition_details = cast(PartitionProfilerConfig, self._partition_details)
         partition_field = self._partition_details.partitionColumnName
         if (
             self._partition_details.partitionIntervalType
-            == PartitionIntervalType.COLUMN_VALUE
+            == PartitionIntervalTypes.COLUMN_VALUE
         ):
             return [
                 df[df[partition_field].isin(self._partition_details.partitionValues)]
                 for df in self.table
             ]
         if (
             self._partition_details.partitionIntervalType
-            == PartitionIntervalType.INTEGER_RANGE
+            == PartitionIntervalTypes.INTEGER_RANGE
         ):
             return [
                 df[
                     df[partition_field].between(
                         self._partition_details.partitionIntegerRangeStart,
                         self._partition_details.partitionIntegerRangeEnd,
                     )
@@ -132,29 +132,28 @@
         # Sample Data should not exceed sample limit
         for chunk in data_frame:
             rows.extend(self._fetch_rows(chunk[cols])[: self.sample_limit])
             if len(rows) >= self.sample_limit:
                 break
         return cols, rows
 
-    def random_sample(self):
+    def random_sample(self, is_sampled: bool = False):
         """Generate random sample from the table
 
         Returns:
             List[DataFrame]
         """
         if self._profile_sample_query:
             return self._rdn_sample_from_user_query()
 
         if self._partition_details:
             self.table = self._partitioned_table()
 
-        if not self.profile_sample:
+        if not self.profile_sample or is_sampled:
             return self.table
-
         return self._get_sampled_dataframe()
 
     def _fetch_rows(self, data_frame):
         return data_frame.dropna().values.tolist()
 
     def fetch_sample_data(
         self, columns: Optional[List[SQALikeColumn]] = None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sampler_factory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sampler_factory.py`

 * *Files 14% similar despite different names*

```diff
@@ -17,18 +17,25 @@
 
 from metadata.generated.schema.entity.services.connections.database.bigQueryConnection import (
     BigQueryConnection,
 )
 from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
     DatalakeConnection,
 )
+from metadata.generated.schema.entity.services.connections.database.dynamoDBConnection import (
+    DynamoDBConnection,
+)
+from metadata.generated.schema.entity.services.connections.database.mongoDBConnection import (
+    MongoDBConnection,
+)
 from metadata.generated.schema.entity.services.connections.database.trinoConnection import (
     TrinoConnection,
 )
 from metadata.generated.schema.entity.services.databaseService import DatabaseConnection
+from metadata.profiler.processor.sampler.nosql.sampler import NoSQLSampler
 from metadata.profiler.processor.sampler.pandas.sampler import DatalakeSampler
 from metadata.profiler.processor.sampler.sqlalchemy.bigquery.sampler import (
     BigQuerySampler,
 )
 from metadata.profiler.processor.sampler.sqlalchemy.sampler import SQASampler
 from metadata.profiler.processor.sampler.sqlalchemy.trino.sampler import TrinoSampler
 
@@ -55,7 +62,9 @@
 
 
 sampler_factory_ = SamplerFactory()
 sampler_factory_.register(DatabaseConnection.__name__, SQASampler)
 sampler_factory_.register(BigQueryConnection.__name__, BigQuerySampler)
 sampler_factory_.register(DatalakeConnection.__name__, DatalakeSampler)
 sampler_factory_.register(TrinoConnection.__name__, TrinoSampler)
+sampler_factory_.register(MongoDBConnection.__name__, NoSQLSampler)
+sampler_factory_.register(DynamoDBConnection.__name__, NoSQLSampler)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sampler_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sampler_interface.py`

 * *Files 0% similar despite different names*

```diff
@@ -13,27 +13,27 @@
 """
 
 from abc import ABC, abstractmethod
 from typing import Dict, List, Optional, Union
 
 from sqlalchemy import Column
 
-from metadata.generated.schema.entity.data.table import TableData
+from metadata.generated.schema.entity.data.table import Table, TableData
 from metadata.profiler.api.models import ProfileSampleConfig
 from metadata.utils.constants import SAMPLE_DATA_DEFAULT_COUNT
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class SamplerInterface(ABC):
     """Sampler interface"""
 
     def __init__(
         self,
         client,
-        table,
+        table: Table,
         profile_sample_config: Optional[ProfileSampleConfig] = None,
         partition_details: Optional[Dict] = None,
         profile_sample_query: Optional[str] = None,
         sample_data_count: Optional[int] = SAMPLE_DATA_DEFAULT_COUNT,
     ):
         self.profile_sample = None
         self.profile_sample_type = None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sqlalchemy/bigquery/sampler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sqlalchemy/bigquery/sampler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sqlalchemy/sampler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sqlalchemy/sampler.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 
 from sqlalchemy import Column, inspect, text
 from sqlalchemy.orm import DeclarativeMeta, Query, aliased
 from sqlalchemy.orm.util import AliasedClass
 from sqlalchemy.sql.sqltypes import Enum
 
 from metadata.generated.schema.entity.data.table import (
-    PartitionIntervalType,
+    PartitionIntervalTypes,
     PartitionProfilerConfig,
     ProfileSampleType,
     TableData,
 )
 from metadata.profiler.orm.functions.modulo import ModuloFn
 from metadata.profiler.orm.functions.random_num import RandomNumFn
 from metadata.profiler.orm.registry import Dialects
@@ -220,15 +220,15 @@
         type_ = get_partition_col_type(
             partition_field,
             self.table.__table__.c,
         )
 
         if (
             self._partition_details.partitionIntervalType
-            == PartitionIntervalType.COLUMN_VALUE
+            == PartitionIntervalTypes.COLUMN_VALUE
         ):
             return aliased(
                 self.table,
                 (
                     self.client.query(self.table)
                     .filter(
                         get_value_filter(
@@ -238,15 +238,15 @@
                     )
                     .subquery()
                 ),
             )
 
         if (
             self._partition_details.partitionIntervalType
-            == PartitionIntervalType.INTEGER_RANGE
+            == PartitionIntervalTypes.INTEGER_RANGE
         ):
             return aliased(
                 self.table,
                 (
                     self.client.query(self.table)
                     .filter(
                         get_integer_range_filter(
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/processor/sampler/sqlalchemy/trino/sampler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/processor/sampler/sqlalchemy/trino/sampler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/registry.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/base/profiler_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/base/profiler_source.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,14 +14,17 @@
 its interface
 """
 from copy import deepcopy
 from typing import List, Optional, Tuple, cast
 
 from sqlalchemy import MetaData
 
+from metadata.generated.schema.configuration.profilerConfiguration import (
+    ProfilerConfiguration,
+)
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.table import ColumnProfilerConfig, Table
 from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
     DatalakeConnection,
 )
 from metadata.generated.schema.entity.services.databaseService import (
@@ -33,17 +36,14 @@
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     OpenMetadataWorkflowConfig,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.profiler.api.models import ProfilerProcessorConfig, TableConfig
 from metadata.profiler.interface.profiler_interface import ProfilerInterface
-from metadata.profiler.interface.profiler_interface_factory import (
-    profiler_interface_factory,
-)
 from metadata.profiler.metrics.registry import Metrics
 from metadata.profiler.processor.core import Profiler
 from metadata.profiler.processor.default import DefaultProfiler, get_default_metrics
 from metadata.profiler.source.profiler_source_interface import ProfilerSourceInterface
 
 NON_SQA_DATABASE_CONNECTIONS = (DatalakeConnection,)
 
@@ -54,27 +54,29 @@
     """
 
     def __init__(
         self,
         config: OpenMetadataWorkflowConfig,
         database: DatabaseService,
         ometa_client: OpenMetadata,
+        global_profiler_configuration: ProfilerConfiguration,
     ):
         self.service_conn_config = self._copy_service_config(config, database)
         self.source_config = config.source.sourceConfig.config
         self.source_config = cast(
             DatabaseServiceProfilerPipeline, self.source_config
         )  # satisfy type checker
         self.profiler_config = ProfilerProcessorConfig.parse_obj(
             config.processor.dict().get("config")
         )
         self.ometa_client = ometa_client
         self.profiler_interface_type: str = self._get_profiler_interface_type(config)
         self.sqa_metadata = self._set_sqa_metadata()
         self._interface = None
+        self.global_profiler_configuration = global_profiler_configuration
 
     @property
     def interface(
         self,
     ) -> Optional[ProfilerInterface]:
         """Get the interface"""
         return self._interface
@@ -193,14 +195,18 @@
         config: Optional[TableConfig],
         profiler_config: Optional[ProfilerProcessorConfig],
         schema_entity: Optional[DatabaseSchema],
         database_entity: Optional[Database],
         db_service: Optional[DatabaseService],
     ) -> ProfilerInterface:
         """Create sqlalchemy profiler interface"""
+        from metadata.profiler.interface.profiler_interface_factory import (  # pylint: disable=import-outside-toplevel
+            profiler_interface_factory,
+        )
+
         profiler_interface: ProfilerInterface = profiler_interface_factory.create(
             self.profiler_interface_type,
             entity,
             schema_entity,
             database_entity,
             db_service,
             config,
@@ -269,14 +275,15 @@
         )
 
         if not profiler_config.profiler:
             return DefaultProfiler(
                 profiler_interface=profiler_interface,
                 include_columns=self._get_include_columns(entity, table_config),
                 exclude_columns=self._get_exclude_columns(entity, table_config),
+                global_profiler_configuration=self.global_profiler_configuration,
             )
 
         metrics = (
             [Metrics.get(name) for name in profiler_config.profiler.metrics]
             if profiler_config.profiler.metrics
             else get_default_metrics(
                 table=profiler_interface.table,
@@ -286,8 +293,9 @@
         )
 
         return Profiler(
             *metrics,  # type: ignore
             profiler_interface=profiler_interface,
             include_columns=self._get_include_columns(entity, table_config),
             exclude_columns=self._get_exclude_columns(entity, table_config),
+            global_profiler_configuration=self.global_profiler_configuration,
         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/bigquery/profiler_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/bigquery/profiler_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/bigquery/type_mapper.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/bigquery/type_mapper.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/databricks/profiler_source.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/databricks/profiler_source.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,12 @@
 """Extend the ProfilerSource class to add support for Databricks is_disconnect SQA method"""
 
+from metadata.generated.schema.configuration.profilerConfiguration import (
+    ProfilerConfiguration,
+)
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.metadataIngestion.workflow import (
     OpenMetadataWorkflowConfig,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.profiler.source.base.profiler_source import ProfilerSource
 
@@ -19,16 +22,17 @@
     """Databricks Profiler source"""
 
     def __init__(
         self,
         config: OpenMetadataWorkflowConfig,
         database: DatabaseService,
         ometa_client: OpenMetadata,
+        global_profiler_config: ProfilerConfiguration,
     ):
-        super().__init__(config, database, ometa_client)
+        super().__init__(config, database, ometa_client, global_profiler_config)
         self.set_is_disconnect()
 
     def set_is_disconnect(self):
         """Set the is_disconnect method for the Databricks dialect"""
         from databricks.sqlalchemy import (
             DatabricksDialect,  # pylint: disable=import-outside-toplevel
         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/mariadb/functions/median.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/mariadb/functions/median.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -122,21 +122,23 @@
         """
         Our source is the ometa client. Validate the
         health check before moving forward
         """
         self.metadata.health_check()
 
     def _iter(self, *_, **__) -> Iterable[Either[ProfilerSourceAndEntity]]:
+        global_profiler_config = self.metadata.get_profiler_config_settings()
         for database in self.get_database_entities():
             try:
                 profiler_source = profiler_source_factory.create(
                     self.config.source.type.lower(),
                     self.config,
                     database,
                     self.metadata,
+                    global_profiler_config,
                 )
                 for entity in self.get_table_entities(database=database):
                     yield Either(
                         right=ProfilerSourceAndEntity(
                             profiler_source=profiler_source,
                             entity=entity,
                         )
@@ -147,15 +149,20 @@
                         name=database.fullyQualifiedName.__root__,
                         error=f"Error listing source and entities for database due to [{exc}]",
                         stackTrace=traceback.format_exc(),
                     )
                 )
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata) -> "Step":
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ) -> "Step":
         config = parse_workflow_config_gracefully(config_dict)
         return cls(config=config, metadata=metadata)
 
     def filter_databases(self, database: Database) -> Optional[Database]:
         """Returns filtered database entities"""
         database_fqn = fqn.build(
             self.metadata,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/metadata_ext.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/metadata_ext.py`

 * *Files 4% similar despite different names*

```diff
@@ -113,14 +113,15 @@
             logger.info(f"Ingesting from database: {database_name}")
             new_service_connection.database = database_name
         self.engine = get_connection(new_service_connection)
         self.inspector = inspect(self.engine)
         self._connection = None  # Lazy init as well
 
     def _iter(self, *_, **__) -> Iterable[Either[ProfilerSourceAndEntity]]:
+        global_profiler_config = self.metadata.get_profiler_config_settings()
         for database_name in self.get_database_names():
             try:
                 database_entity = fqn.search_database_from_es(
                     database_name=database_name,
                     metadata=self.metadata,
                     service_name=None,
                 )
@@ -146,14 +147,15 @@
                             continue
 
                         profiler_source = profiler_source_factory.create(
                             self.config.source.type.lower(),
                             self.config,
                             database_entity,
                             self.metadata,
+                            global_profiler_config,
                         )
                         yield Either(
                             right=ProfilerSourceAndEntity(
                                 profiler_source=profiler_source,
                                 entity=table_entity,
                             )
                         )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/profiler_source_factory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/profiler_source_factory.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/profiler_source_interface.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/profiler_source_interface.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/profiler/source/single_store/functions/median.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/source/single_store/functions/median.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/avro.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/avro.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,21 +10,16 @@
 #  limitations under the License.
 
 """
 Avro DataFrame reader
 """
 import io
 
-from avro.datafile import DataFileReader
-from avro.errors import InvalidAvroBinaryEncoding
-from avro.io import DatumReader
-
 from metadata.generated.schema.entity.data.table import Column
 from metadata.generated.schema.type.schema import DataTypeTopic
-from metadata.parsers.avro_parser import parse_avro_schema
 from metadata.readers.dataframe.base import DataFrameReader
 from metadata.readers.dataframe.common import dataframe_to_chunks
 from metadata.readers.dataframe.models import DatalakeColumnWrapper
 from metadata.utils.constants import UTF_8
 
 PD_AVRO_FIELD_MAP = {
     DataTypeTopic.BOOLEAN.value: "bool",
@@ -47,16 +42,21 @@
 
     @staticmethod
     def read_from_avro(avro_text: bytes) -> DatalakeColumnWrapper:
         """
         Method to parse the avro data from storage sources
         """
         # pylint: disable=import-outside-toplevel
+        from avro.datafile import DataFileReader
+        from avro.errors import InvalidAvroBinaryEncoding
+        from avro.io import DatumReader
         from pandas import DataFrame, Series
 
+        from metadata.parsers.avro_parser import parse_avro_schema
+
         try:
             elements = DataFileReader(io.BytesIO(avro_text), DatumReader())
             if elements.meta.get(AVRO_SCHEMA):
                 return DatalakeColumnWrapper(
                     columns=parse_avro_schema(
                         schema=elements.meta.get(AVRO_SCHEMA).decode(UTF_8), cls=Column
                     ),
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/base.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/base.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/common.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/common.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/dsv.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/dsv.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/json.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/json.py`

 * *Files 1% similar despite different names*

```diff
@@ -29,15 +29,15 @@
 
 def _get_json_text(key: str, text: bytes, decode: bool) -> Union[str, bytes]:
     processed_text: Union[str, bytes] = text
     if key.endswith(".gz"):
         processed_text = gzip.decompress(text)
     if key.endswith(".zip"):
         with zipfile.ZipFile(io.BytesIO(text)) as zip_file:
-            processed_text = zip_file.read(zip_file.infolist()[0]).decode(UTF_8)
+            processed_text = zip_file.read(zip_file.infolist()[0])
     if decode:
         return processed_text.decode(UTF_8) if isinstance(text, bytes) else text
     return processed_text
 
 
 class JSONDataFrameReader(DataFrameReader):
     """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/parquet.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/parquet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/dataframe/reader_factory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/dataframe/reader_factory.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/adls.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/adls.py`

 * *Files 13% similar despite different names*

```diff
@@ -64,7 +64,27 @@
     def _get_tree(self) -> List[str]:
         """
         We are not implementing this yet. This should
         only be needed for now for the Datalake where we don't need
         to traverse any directories.
         """
         raise NotImplementedError("Not implemented")
+
+    def download(
+        self,
+        path: str,
+        local_file_path: str,
+        *,
+        bucket_name: str = None,
+        verbose: bool = True,
+        **__,
+    ):
+        try:
+            container_client = self.client.get_container_client(bucket_name)
+            with open(local_file_path, "wb") as download_file:
+                download_file.write(
+                    container_client.get_blob_client(path).download_blob().readall()
+                )
+        except Exception as err:
+            if verbose:
+                logger.debug(traceback.format_exc())
+            raise ReadException(f"Error downloading file [{path}] from ADLS: {err}")
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/api_reader.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/api_reader.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/base.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/s3.py`

 * *Files 16% similar despite different names*

```diff
@@ -4,54 +4,61 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Base local reader
+Read files as string from S3
 """
 import traceback
-from abc import ABC, abstractmethod
-from typing import List, Optional, Union
+from typing import List
 
+from metadata.readers.file.base import Reader, ReadException
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-class ReadException(Exception):
-    """
-    To be raised by any errors with the read calls
+class S3Reader(Reader):
+    """S3 Reader
+    Class to read from buckets with prefix as paths
     """
 
+    def __init__(self, client):
+        self.client = client
 
-class Reader(ABC):
-    """
-    Abstract class for all readers
-    """
-
-    @abstractmethod
-    def read(self, path: str, **kwargs) -> Union[str, bytes]:
-        """
-        Given a string, return a string
-        """
-        raise NotImplementedError("Missing read implementation")
+    def read(
+        self, path: str, *, bucket_name: str = None, verbose: bool = True, **__
+    ) -> bytes:
+        try:
+            return self.client.get_object(Bucket=bucket_name, Key=path)["Body"].read()
+        except Exception as err:
+            if verbose:
+                logger.debug(traceback.format_exc())
+            raise ReadException(f"Error fetching file [{path}] from S3: {err}")
 
-    @abstractmethod
     def _get_tree(self) -> List[str]:
         """
-        Return the filenames of the root
-        """
-        raise NotImplementedError("Missing get_tree implementation")
-
-    def get_tree(self) -> Optional[List[str]]:
-        """
-        If something happens, return None
-        """
+        We are not implementing this yet. This should
+        only be needed for now for the Datalake where we don't need
+        to traverse any directories.
+        """
+        raise NotImplementedError("Not implemented")
+
+    def download(
+        self,
+        path: str,
+        local_file_path: str,
+        *,
+        bucket_name: str = None,
+        verbose: bool = True,
+        **__,
+    ):
         try:
-            return self._get_tree()
+            self.client.download_file(bucket_name, path, local_file_path)
         except Exception as err:
-            logger.debug(traceback.format_exc())
-            logger.error(f"Error getting file tree [{err}]")
-        return None
+            if verbose:
+                logger.debug(traceback.format_exc())
+            raise ReadException(f"Error downloading file [{path}] from S3: {err}")
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/bitbucket.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/bitbucket.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/config_source_factory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/config_source_factory.py`

 * *Files 10% similar despite different names*

```diff
@@ -14,14 +14,23 @@
 - Local
 - ADLS
 - S3
 - GCS
 """
 from typing import Any
 
+from metadata.generated.schema.entity.services.connections.dashboard.powerbi.azureConfig import (
+    AzureConfig as PowerBiAzureConfig,
+)
+from metadata.generated.schema.entity.services.connections.dashboard.powerbi.gcsConfig import (
+    GCSConfig as PowerBiGCSConfig,
+)
+from metadata.generated.schema.entity.services.connections.dashboard.powerbi.s3Config import (
+    S3Config as PowerBiS3Config,
+)
 from metadata.generated.schema.entity.services.connections.database.datalake.azureConfig import (
     AzureConfig,
 )
 from metadata.generated.schema.entity.services.connections.database.datalake.gcsConfig import (
     GCSConfig,
 )
 from metadata.generated.schema.entity.services.connections.database.datalake.s3Config import (
@@ -54,14 +63,17 @@
     AzureConfig.__name__: ADLSReader,
     GCSConfig.__name__: GCSReader,
     S3Config.__name__: S3Reader,
     DbtLocalConfig.__name__: LocalReader,
     DbtAzureConfig.__name__: ADLSReader,
     DbtGcsConfig.__name__: GCSReader,
     DbtS3Config.__name__: S3Reader,
+    PowerBiAzureConfig.__name__: ADLSReader,
+    PowerBiS3Config.__name__: S3Reader,
+    PowerBiGCSConfig.__name__: GCSReader,
 }
 
 
 def get_reader(config_source: ConfigSource, client: Any) -> Reader:
     """
     Load the File Reader based on the Config Source
     """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/credentials.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/credentials.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/gcs.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/metrics/static/row_count.py`

 * *Files 24% similar despite different names*

```diff
@@ -6,45 +6,53 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Read files as string from S3
+Table Count Metric definition
 """
-import traceback
-from typing import List
+from typing import Callable
 
-from metadata.readers.file.base import Reader, ReadException
-from metadata.utils.logger import ingestion_logger
+from sqlalchemy import func
 
-logger = ingestion_logger()
+from metadata.generated.schema.configuration.profilerConfiguration import MetricType
+from metadata.generated.schema.entity.data.table import Table
+from metadata.profiler.adaptors.nosql_adaptor import NoSQLAdaptor
+from metadata.profiler.metrics.core import StaticMetric, _label
 
 
-class GCSReader(Reader):
-    """GCS Reader
-    Class to read from buckets with prefix as paths
+class RowCount(StaticMetric):
     """
+    ROW_NUMBER Metric
 
-    def __init__(self, client):
-        self.client = client
+    Count all rows on a table
+    """
 
-    def read(
-        self, path: str, *, bucket_name: str = None, verbose: bool = True, **__
-    ) -> bytes:
-        try:
-            return (
-                self.client.get_bucket(bucket_name).get_blob(path).download_as_string()
-            )
-        except Exception as err:
-            if verbose:
-                logger.debug(traceback.format_exc())
-            raise ReadException(f"Error fetching file [{path}] from GCS: {err}")
+    @classmethod
+    def name(cls):
+        return MetricType.rowCount.value
 
-    def _get_tree(self) -> List[str]:
+    @classmethod
+    def is_col_metric(cls) -> bool:
         """
-        We are not implementing this yet. This should
-        only be needed for now for the Datalake where we don't need
-        to traverse any directories.
+        Mark the class as a Table Metric
         """
-        raise NotImplementedError("Not implemented")
+        return False
+
+    @property
+    def metric_type(self):
+        return int
+
+    @_label
+    def fn(self):
+        """sqlalchemy function"""
+        return func.count()
+
+    def df_fn(self, dfs=None):
+        """pandas function"""
+        return sum(len(df.index) for df in dfs)
+
+    @classmethod
+    def nosql_fn(cls, client: NoSQLAdaptor) -> Callable[[Table], int]:
+        return client.item_count
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/github.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/file/github.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/file/s3.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/test_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,48 +1,65 @@
-#  Copyright 2021 Collate
+#  Copyright 2024 Collate
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Read files as string from S3
+Utility functions for testing
 """
-import traceback
-from typing import List
+from contextlib import contextmanager
 
-from metadata.readers.file.base import Reader, ReadException
-from metadata.utils.logger import ingestion_logger
 
-logger = ingestion_logger()
+class MultipleException(Exception):
+    def __init__(self, exceptions):
+        self.exceptions = exceptions
+        super().__init__(f"Multiple exceptions occurred: {exceptions}")
 
 
-class S3Reader(Reader):
-    """S3 Reader
-    Class to read from buckets with prefix as paths
+class ErrorHandler:
+    """
+    A context manager that accumulates errors and raises them at the end of the block.
+    Useful for cleaning up resources and ensuring that all errors are raised at the end of a test.
+    Example:
+    ```
+    from metadata.utils.test_utils import accumulate_errors
+    with accumulate_errors() as error_handler:
+        error_handler.try_execute(lambda : 1 / 0)
+        error_handler.try_execute(print, "Hello, World!")
+    ```
+
+    ```
+    > Hello, World!
+    > Traceback (most recent call last):
+    >  ...
+    > ZeroDivisionError: division by zero
+    ```
     """
 
-    def __init__(self, client):
-        self.client = client
+    def __init__(self):
+        self.errors = []
 
-    def read(
-        self, path: str, *, bucket_name: str = None, verbose: bool = True, **__
-    ) -> bytes:
+    def try_execute(self, func, *args, **kwargs):
         try:
-            return self.client.get_object(Bucket=bucket_name, Key=path)["Body"].read()
-        except Exception as err:
-            if verbose:
-                logger.debug(traceback.format_exc())
-            raise ReadException(f"Error fetching file [{path}] from S3: {err}")
-
-    def _get_tree(self) -> List[str]:
-        """
-        We are not implementing this yet. This should
-        only be needed for now for the Datalake where we don't need
-        to traverse any directories.
-        """
-        raise NotImplementedError("Not implemented")
+            func(*args, **kwargs)
+        except Exception as exc:
+            self.errors.append(exc)
+
+    def raise_if_errors(self):
+        if len(self.errors) == 1:
+            raise self.errors[0]
+        if len(self.errors) > 1:
+            raise MultipleException(self.errors)
+
+
+@contextmanager
+def accumulate_errors():
+    error_handler = ErrorHandler()
+    try:
+        yield error_handler
+    finally:
+        error_handler.raise_if_errors()
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/readers/models.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/readers/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/timer/repeated_timer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/timer/repeated_timer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/bigquery_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/bigquery_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/class_helper.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/class_helper.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/client_version.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/client_version.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/constants.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/constants.py`

 * *Files 15% similar despite different names*

```diff
@@ -20,14 +20,16 @@
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.mlmodel import MlModel
 from metadata.generated.schema.entity.data.pipeline import Pipeline
 from metadata.generated.schema.entity.data.searchIndex import SearchIndex
 from metadata.generated.schema.entity.data.storedProcedure import StoredProcedure
 from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.data.topic import Topic
+from metadata.generated.schema.entity.domains.dataProduct import DataProduct
+from metadata.generated.schema.entity.domains.domain import Domain
 from metadata.generated.schema.entity.services.dashboardService import DashboardService
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.entity.services.messagingService import MessagingService
 from metadata.generated.schema.entity.services.metadataService import MetadataService
 from metadata.generated.schema.entity.services.mlmodelService import MlModelService
 from metadata.generated.schema.entity.services.pipelineService import PipelineService
 from metadata.generated.schema.entity.services.searchService import SearchService
@@ -66,35 +68,41 @@
 
 AUTHORIZATION_HEADER = "Authorization"
 
 NO_ACCESS_TOKEN = "no_token"
 
 SAMPLE_DATA_DEFAULT_COUNT = 50
 
-# Mainly used for lineage
-ENTITY_REFERENCE_TYPE_MAP = {
+ENTITY_REFERENCE_CLASS_MAP = {
     # Service Entities
-    DatabaseService.__name__: "databaseService",
-    MessagingService.__name__: "messagingService",
-    DashboardService.__name__: "dashboardService",
-    PipelineService.__name__: "pipelineService",
-    StorageService.__name__: "storageService",
-    MlModelService.__name__: "mlmodelService",
-    MetadataService.__name__: "metadataService",
-    SearchService.__name__: "searchService",
+    "databaseService": DatabaseService,
+    "messagingService": MessagingService,
+    "dashboardService": DashboardService,
+    "pipelineService": PipelineService,
+    "storageService": StorageService,
+    "mlmodelService": MlModelService,
+    "metadataService": MetadataService,
+    "searchService": SearchService,
     # Data Asset Entities
-    Table.__name__: "table",
-    StoredProcedure.__name__: "storedProcedure",
-    Database.__name__: "database",
-    DatabaseSchema.__name__: "databaseSchema",
-    Dashboard.__name__: "dashboard",
-    DashboardDataModel.__name__: "dashboardDataModel",
-    Pipeline.__name__: "pipeline",
-    Chart.__name__: "chart",
-    Topic.__name__: "topic",
-    SearchIndex.__name__: "searchIndex",
-    MlModel.__name__: "mlmodel",
-    Container.__name__: "container",
+    "table": Table,
+    "storedProcedure": StoredProcedure,
+    "database": Database,
+    "databaseSchema": DatabaseSchema,
+    "dashboard": Dashboard,
+    "dashboardDataModel": DashboardDataModel,
+    "pipeline": Pipeline,
+    "chart": Chart,
+    "topic": Topic,
+    "searchIndex": SearchIndex,
+    "mlmodel": MlModel,
+    "container": Container,
     # User Entities
-    User.__name__: "user",
-    Team.__name__: "team",
+    "user": User,
+    "team": Team,
+    # Domain
+    "domain": Domain,
+    "dataProduct": DataProduct,
+}
+
+ENTITY_REFERENCE_TYPE_MAP = {
+    value.__name__: key for key, value in ENTITY_REFERENCE_CLASS_MAP.items()
 }
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/credentials.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/credentials.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/custom_thread_pool.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/custom_thread_pool.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/datalake/datalake_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/datalake/datalake_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -175,16 +175,18 @@
     _data_formats = {
         **dict.fromkeys(["int64", "int", "int32"], DataType.INT),
         "dict": DataType.JSON,
         "list": DataType.ARRAY,
         **dict.fromkeys(["float64", "float32", "float"], DataType.FLOAT),
         "bool": DataType.BOOLEAN,
         **dict.fromkeys(
-            ["datetime64", "timedelta[ns]", "datetime64[ns]"], DataType.DATETIME
+            ["datetime64[ns]", "datetime"],
+            DataType.DATETIME,
         ),
+        "timedelta[ns]": DataType.TIME,
         "str": DataType.STRING,
         "bytes": DataType.BYTES,
     }
 
     def __init__(self, data_frame: "DataFrame", raw_data: Any = None):
         self.data_frame = data_frame
         self.raw_data = raw_data
@@ -249,18 +251,52 @@
         data_type = None
         try:
             if data_frame[column_name].dtypes.name == "object" and any(
                 data_frame[column_name].dropna().values
             ):
                 try:
                     # Safely evaluate the input string
-                    df_row_val = data_frame[column_name].dropna().values[0]
-                    parsed_object = ast.literal_eval(str(df_row_val))
+                    df_row_val_list = data_frame[column_name].dropna().values[:1000]
+                    parsed_object_datatype_list = []
+                    for df_row_val in df_row_val_list:
+                        try:
+                            parsed_object_datatype_list.append(
+                                type(ast.literal_eval(str(df_row_val))).__name__.lower()
+                            )
+                        except (ValueError, SyntaxError):
+                            # we try to parse the value as a datetime, if it fails, we fallback to string
+                            # as literal_eval will fail for string
+                            from datetime import datetime
+
+                            from dateutil.parser import ParserError, parse
+
+                            try:
+                                dtype_ = "int64"
+                                if not str(df_row_val).isnumeric():
+                                    # check if the row value is time
+                                    try:
+                                        datetime.strptime(df_row_val, "%H:%M:%S").time()
+                                        dtype_ = "timedelta[ns]"
+                                    except (ValueError, TypeError):
+                                        # check if the row value is date / time / datetime
+                                        type(parse(df_row_val)).__name__.lower()
+                                        dtype_ = "datetime64[ns]"
+                                parsed_object_datatype_list.append(dtype_)
+                            except (ParserError, TypeError):
+                                parsed_object_datatype_list.append("str")
+                        except Exception as err:
+                            logger.debug(
+                                f"Failed to parse datatype for column {column_name}, exc: {err},"
+                                "Falling back to string."
+                            )
+                            parsed_object_datatype_list.append("str")
+
+                    data_type = max(parsed_object_datatype_list)
                     # Determine the data type of the parsed object
-                    data_type = type(parsed_object).__name__.lower()
+
                 except (ValueError, SyntaxError):
                     # Handle any exceptions that may occur
                     data_type = "string"
 
             data_type = cls._data_formats.get(
                 data_type or data_frame[column_name].dtypes.name,
             )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/db_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/db_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/deprecation.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/deprecation.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/dispatch.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/dispatch.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/elasticsearch.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/elasticsearch.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/entity_link.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/entity_link.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/execution_time_tracker.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/execution_time_tracker.py`

 * *Files 21% similar despite different names*

```diff
@@ -9,17 +9,19 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 ExecutionTimeTracker implementation to help track the execution time of different parts
 of the code.
 """
+import threading
+from copy import deepcopy
 from functools import wraps
 from time import perf_counter
-from typing import Dict, List, Optional
+from typing import List, Optional
 
 from pydantic import BaseModel
 
 from metadata.utils.helpers import pretty_print_time_duration
 from metadata.utils.logger import utils_logger
 from metadata.utils.singleton import Singleton
 
@@ -30,14 +32,68 @@
     """Small Model to hold the ExecutionTimeTracker context."""
 
     name: str
     start: float
     stored: bool
 
 
+class ExecutionTimeTrackerContextMap(metaclass=Singleton):
+    """Responsible for managing the ExecutionTimeTracker on different threads."""
+
+    def __init__(self):
+        """Initializes the map."""
+        self.map: dict[int, List[ExecutionTimeTrackerContext]] = {}
+
+    def copy_from_parent(self, parent_thread_id: int, thread_id: Optional[int] = None):
+        """Copy the ExecutionTimeTrackerContext from Parent."""
+        thread_id = thread_id or threading.get_ident()
+
+        self.map[thread_id] = deepcopy(self.map.get(parent_thread_id, []))
+
+    def get_last_stored_context_level(
+        self, thread_id: Optional[int] = None
+    ) -> Optional[str]:
+        """Gets the last stored context level for a given thread."""
+        thread_id = thread_id or threading.get_ident()
+
+        stored_context = [
+            context for context in self.map.get(thread_id, {}) if context.stored
+        ]
+
+        if stored_context:
+            return stored_context[-1].name
+        return None
+
+    def append(
+        self, context: ExecutionTimeTrackerContext, thread_id: Optional[int] = None
+    ):
+        """Appends a new context level for a given thread."""
+        thread_id = thread_id or threading.get_ident()
+        self.map.setdefault(thread_id, []).append(context)
+
+    def pop(self, thread_id: Optional[int] = None) -> ExecutionTimeTrackerContext:
+        """Removes the information of a given thread."""
+        thread_id = thread_id or threading.get_ident()
+        return self.map.get(thread_id, []).pop()
+
+
+class ExecutionTimeTrackerState(metaclass=Singleton):
+    """Tracks the ExecutionTime State across multiple threads."""
+
+    def __init__(self):
+        """Initializes the state and the lock."""
+        self.state = {}
+        self.lock = threading.Lock()
+
+    def add(self, context: ExecutionTimeTrackerContext, elapsed: float):
+        """Updates the State."""
+        with self.lock:
+            self.state[context.name] = self.state.get(context.name, 0) + elapsed
+
+
 class ExecutionTimeTracker(metaclass=Singleton):
     """ExecutionTimeTracker is implemented as a Singleton in order to hold state globally.
 
     It works as a Context Manager in order to track and log execution times.
 
     Example:
 
@@ -56,85 +112,70 @@
         Attrs
         ------
             enabled: Defines if it will be enabled or not.
             context: Keeps track of the context levels and their state.
             state: Keeps track of the global state for the Execution Time Tracker.
         """
         self.enabled: bool = enabled
-        self.context: List[ExecutionTimeTrackerContext] = []
-        self.state: Dict[str, float] = {}
-        self.new_context = None
-        self.store = True
 
-    @property
-    def last_stored_context_level(self) -> Optional[str]:
-        """Returns the last stored context level.
+        self.context_map = ExecutionTimeTrackerContextMap()
+        self.state = ExecutionTimeTrackerState()
 
-        In order to provide better logs and keep track where in the code the time is being
-        measured we keep track of nested contexts.
-
-        If a given context is not stored it will only log to debug but won't be part of the
-        global state.
-        """
-        stored_context = [context for context in self.context if context.stored]
-
-        if stored_context:
-            return stored_context[-1].name
-
-        return None
+        self.new_context = ""
+        self.store = True
 
     def __call__(self, context: str, store: bool = True):
         """At every point we open a new Context Manager we can pass the current 'context' and
         if we want to 'store' it.
 
         Sets the temporary attributes used within the context:
 
             new_context: Full Context name, appending the given context to the last stored context level.
             store: If True, it will take part of the global state. Otherwise it will only log to debug.
         """
         self.new_context = ".".join(
-            [part for part in [self.last_stored_context_level, context] if part]
+            [
+                part
+                for part in [self.context_map.get_last_stored_context_level(), context]
+                if part
+            ]
         )
         self.store = store
 
         return self
 
     def __enter__(self):
         """If enabled, when entering the context, we append a new
         ExecutionTimeTrackerContext to the list.
         """
         if self.enabled:
-            self.context.append(
+            self.context_map.append(
                 ExecutionTimeTrackerContext(
                     name=self.new_context, start=perf_counter(), stored=self.store
                 )
             )
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         """If enabled, when exiting the context, we calculate the elapsed time and log to debug.
         If the context.stored is True, we also save it to the global state."""
         if self.enabled:
             stop = perf_counter()
-            context = self.context.pop(-1)
+            context = self.context_map.pop()
 
             if not context:
                 return
 
             elapsed = stop - context.start
 
             logger.debug(
                 "%s executed in %s", context.name, pretty_print_time_duration(elapsed)
             )
 
             if context.stored:
-                self._save(context, elapsed)
-
-    def _save(self, context: ExecutionTimeTrackerContext, elapsed: float):
-        """Small utility to save the new measure to the global accumulator."""
-        self.state[context.name] = self.state.get(context.name, 0) + elapsed
+                self.state.add(context, elapsed)
 
 
 def calculate_execution_time(context: Optional[str] = None, store: bool = True):
     """Utility decorator to be able to use the ExecutionTimeTracker on a function.
 
     It receives the context and if it should store it.
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/filters.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/filters.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/fqn.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/fqn.py`

 * *Files 6% similar despite different names*

```diff
@@ -188,25 +188,42 @@
     if entity:
         return str(entity.fullyQualifiedName.__root__)
     return None
 
 
 @fqn_build_registry.add(DatabaseSchema)
 def _(
-    _: Optional[OpenMetadata],  # ES Search not enabled for Schemas
+    metadata: Optional[OpenMetadata],  # ES Search not enabled for Schemas
     *,
     service_name: str,
-    database_name: str,
+    database_name: Optional[str],
     schema_name: str,
-) -> str:
-    if not service_name or not database_name or not schema_name:
-        raise FQNBuildingException(
-            f"Args should be informed, but got service=`{service_name}`, db=`{database_name}`, schema=`{schema_name}`"
-        )
-    return _build(service_name, database_name, schema_name)
+    skip_es_search: bool = True,
+    fetch_multiple_entities: bool = False,
+) -> Union[Optional[str], Optional[List[str]]]:
+    entity: Optional[Union[DatabaseSchema, List[DatabaseSchema]]] = None
+
+    if not skip_es_search:
+        entity = search_database_schema_from_es(
+            metadata=metadata,
+            database_name=database_name,
+            schema_name=schema_name,
+            fetch_multiple_entities=fetch_multiple_entities,
+            service_name=service_name,
+        )
+
+    if not entity and database_name:
+        fqn = _build(service_name, database_name, schema_name)
+        return [fqn] if fetch_multiple_entities else fqn
+    if entity and fetch_multiple_entities:
+        return [str(table.fullyQualifiedName.__root__) for table in entity]
+    if entity:
+        return str(entity.fullyQualifiedName.__root__)
+
+    return None
 
 
 @fqn_build_registry.add(Database)
 def _(
     _: Optional[OpenMetadata],  # ES Search not enabled for Databases
     *,
     service_name: str,
@@ -570,14 +587,51 @@
         )
     fqn_search_string = _build(
         service_name or "*", database_name or "*", schema_name or "*", table_name
     )
     return fqn_search_string
 
 
+def search_database_schema_from_es(
+    metadata: OpenMetadata,
+    database_name: str,
+    schema_name: str,
+    service_name: str,
+    fetch_multiple_entities: bool = False,
+    fields: Optional[str] = None,
+):
+    """
+    Find database schema entity in elasticsearch index.
+
+    :param metadata: OM Client
+    :param database_name: name of database in which we are searching for database schema
+    :param schema_name: name of schema we are searching for
+    :param service_name: name of service in which we are searching for database schema
+    :param fetch_multiple_entities: should single match be returned or all matches
+    :param fields: additional fields to return
+    :return: entity / entities matching search criteria
+    """
+    if not schema_name:
+        raise FQNBuildingException(
+            f"Schema Name should be informed, but got schema_name=`{schema_name}`"
+        )
+
+    fqn_search_string = _build(service_name or "*", database_name or "*", schema_name)
+
+    es_result = metadata.es_search_from_fqn(
+        entity_type=DatabaseSchema,
+        fqn_search_string=fqn_search_string,
+        fields=fields,
+    )
+
+    return get_entity_from_es_result(
+        entity_list=es_result, fetch_multiple_entities=fetch_multiple_entities
+    )
+
+
 def search_table_from_es(
     metadata: OpenMetadata,
     database_name: str,
     schema_name: str,
     service_name: str,
     table_name: str,
     fetch_multiple_entities: bool = False,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/helpers.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/powerbi/file_client.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,469 +4,324 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Helpers module for ingestion related methods
+File Client for PowerBi
 """
-
-from __future__ import annotations
-
-import itertools
-import re
+import json
+import os
 import shutil
-import sys
-from datetime import datetime, timedelta
-from math import floor, log
-from pathlib import Path
-from typing import Any, Dict, Iterable, List, Optional, Tuple, Union
-
-import sqlparse
-from sqlparse.sql import Statement
-
-from metadata.generated.schema.entity.data.chart import ChartType
-from metadata.generated.schema.entity.data.table import Column, Table
-from metadata.generated.schema.entity.feed.suggestion import Suggestion, SuggestionType
-from metadata.generated.schema.entity.services.databaseService import DatabaseService
-from metadata.generated.schema.type.basic import EntityLink
-from metadata.generated.schema.type.tagLabel import TagLabel
-from metadata.utils.constants import DEFAULT_DATABASE
+import traceback
+import zipfile
+from collections import defaultdict
+from functools import singledispatch
+from typing import Dict, List, Optional, Tuple
+
+from metadata.clients.aws_client import AWSClient
+from metadata.clients.azure_client import AzureClient
+from metadata.generated.schema.entity.services.connections.dashboard.powerbi.azureConfig import (
+    AzureConfig,
+)
+from metadata.generated.schema.entity.services.connections.dashboard.powerbi.gcsConfig import (
+    GCSConfig,
+)
+from metadata.generated.schema.entity.services.connections.dashboard.powerbi.s3Config import (
+    S3Config,
+)
+from metadata.generated.schema.entity.services.connections.dashboard.powerBIConnection import (
+    LocalConfig,
+    PowerBIConnection,
+)
+from metadata.ingestion.ometa.client import REST
+from metadata.ingestion.source.dashboard.powerbi.models import (
+    ConnectionFile,
+    DataModelSchema,
+    PowerBiTable,
+)
+from metadata.readers.file.config_source_factory import get_reader
+from metadata.readers.file.local import LocalReader
+from metadata.utils.credentials import set_google_credentials
 from metadata.utils.logger import utils_logger
+from metadata.utils.s3_utils import list_s3_objects
 
 logger = utils_logger()
 
 
-class BackupRestoreArgs:
-    def __init__(  # pylint: disable=too-many-arguments
-        self,
-        host: str,
-        user: str,
-        password: str,
-        database: str,
-        port: str,
-        options: List[str],
-        arguments: List[str],
-        schema: Optional[str] = None,
-    ):
-        self.host = host
-        self.user = user
-        self.password = password
-        self.database = database
-        self.port = port
-        self.options = options
-        self.arguments = arguments
-        self.schema = schema
-
-
-class DockerActions:
-    def __init__(
-        self,
-        start: bool,
-        stop: bool,
-        pause: bool,
-        resume: bool,
-        clean: bool,
-        reset_db: bool,
-    ):
-        self.start = start
-        self.stop = stop
-        self.pause = pause
-        self.resume = resume
-        self.clean = clean
-        self.reset_db = reset_db
-
-
-om_chart_type_dict = {
-    "line": ChartType.Line,
-    "big_number": ChartType.Line,
-    "big_number_total": ChartType.Line,
-    "dual_line": ChartType.Line,
-    "line_multi": ChartType.Line,
-    "table": ChartType.Table,
-    "dist_bar": ChartType.Bar,
-    "bar": ChartType.Bar,
-    "box_plot": ChartType.BoxPlot,
-    "boxplot": ChartType.BoxPlot,
-    "histogram": ChartType.Histogram,
-    "treemap": ChartType.Area,
-    "area": ChartType.Area,
-    "pie": ChartType.Pie,
-    "text": ChartType.Text,
-    "scatter": ChartType.Scatter,
-}
-
-
-def pretty_print_time_duration(duration: Union[int, float]) -> str:
-    """
-    Method to format and display the time
-    """
-
-    days = divmod(duration, 86400)[0]
-    hours = divmod(duration, 3600)[0]
-    minutes = divmod(duration, 60)[0]
-    seconds = round(divmod(duration, 60)[1], 2)
-    if days:
-        return f"{days}day(s) {hours}h {minutes}m {seconds}s"
-    if hours:
-        return f"{hours}h {minutes}m {seconds}s"
-    if minutes:
-        return f"{minutes}m {seconds}s"
-    return f"{seconds}s"
-
-
-def get_start_and_end(duration: int = 0):
-    """
-    Method to return start and end time based on duration
-    """
-
-    today = datetime.utcnow()
-    start = (today + timedelta(0 - duration)).replace(
-        hour=0, minute=0, second=0, microsecond=0
-    )
-    # Add one day to make sure we are handling today's queries
-    end = (today + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
-    return start, end
-
-
-def snake_to_camel(snake_str):
-    """
-    Method to convert snake case text to camel case
-    """
-    split_str = snake_str.split("_")
-    split_str[0] = split_str[0].capitalize()
-    if len(split_str) > 1:
-        split_str[1:] = [u.title() for u in split_str[1:]]
-    return "".join(split_str)
-
-
-def datetime_to_ts(date: Optional[datetime]) -> Optional[int]:
-    """
-    Convert a given date to a timestamp as an Int in milliseconds
-    """
-    return int(date.timestamp() * 1_000) if date else None
-
-
-def get_formatted_entity_name(name: str) -> Optional[str]:
-    """
-    Method to get formatted entity name
-    """
-
-    return (
-        name.replace("[", "").replace("]", "").replace("<default>.", "")
-        if name
-        else None
-    )
-
-
-def replace_special_with(raw: str, replacement: str) -> str:
-    """
-    Replace special characters in a string by a hyphen
-    :param raw: raw string to clean
-    :param replacement: string used to replace
-    :return: clean string
-    """
-    return re.sub(r"[^a-zA-Z0-9]", replacement, raw)
-
-
-def get_standard_chart_type(raw_chart_type: str) -> ChartType.Other:
-    """
-    Get standard chart type supported by OpenMetadata based on raw chart type input
-    :param raw_chart_type: raw chart type to be standardize
-    :return: standard chart type
-    """
-    if raw_chart_type is not None:
-        return om_chart_type_dict.get(raw_chart_type.lower(), ChartType.Other)
-    return ChartType.Other
-
-
-def find_in_iter(element: Any, container: Iterable[Any]) -> Optional[Any]:
-    """
-    If the element is in the container, return it.
-    Otherwise, return None
-    :param element: to find
-    :param container: container with element
-    :return: element or None
-    """
-    return next((elem for elem in container if elem == element), None)
-
-
-def find_column_in_table(
-    column_name: str, table: Table, case_sensitive: bool = True
-) -> Optional[Column]:
-    """
-    If the column exists in the table, return it
-    """
-
-    def equals(first: str, second: str) -> bool:
-        if case_sensitive:
-            return first == second
-        return first.lower() == second.lower()
-
-    return next(
-        (col for col in table.columns if equals(col.name.__root__, column_name)), None
-    )
-
-
-def find_suggestion(
-    suggestions: List[Suggestion],
-    suggestion_type: SuggestionType,
-    entity_link: EntityLink,
-) -> Optional[Suggestion]:
-    """Given a list of suggestions, a suggestion type and an entity link, find
-    one suggestion in the list that matches the criteria
-    """
-    return next(
-        (
-            sugg
-            for sugg in suggestions
-            if sugg.type == suggestion_type and sugg.entityLink == entity_link
-        ),
-        None,
-    )
-
-
-def find_column_in_table_with_index(
-    column_name: str, table: Table
-) -> Optional[Tuple[int, Column]]:
-    """Return a column and its index in a Table Entity
-
-    Args:
-         column_name (str): column to find
-         table (Table): Table Entity
-
-    Return:
-          A tuple of Index, Column if the column is found
-    """
-    col_index, col = next(
-        (
-            (col_index, col)
-            for col_index, col in enumerate(table.columns)
-            if str(col.name.__root__).lower() == column_name.lower()
-        ),
-        (None, None),
-    )
-
-    return col_index, col
-
-
-def list_to_dict(original: Optional[List[str]], sep: str = "=") -> Dict[str, str]:
-    """
-    Given a list with strings that have a separator,
-    convert that to a dictionary of key-value pairs
-    """
-    if not original:
-        return {}
-
-    split_original = [
-        (elem.split(sep)[0], elem.split(sep)[1]) for elem in original if sep in elem
-    ]
-    return dict(split_original)
-
-
-def clean_up_starting_ending_double_quotes_in_string(string: str) -> str:
-    """Remove start and ending double quotes in a string
-
-    Args:
-        string (str): a string
-
-    Raises:
-        TypeError: An error occure checking the type of `string`
-
-    Returns:
-        str: a string with no double quotes
-    """
-    if not isinstance(string, str):
-        raise TypeError(f"{string}, must be of type str, instead got `{type(string)}`")
-
-    return string.strip('"')
-
-
-def insensitive_replace(raw_str: str, to_replace: str, replace_by: str) -> str:
-    """Replace `to_replace` by `replace_by` in `raw_str` ignoring the raw_str case.
-
-    Args:
-        raw_str:str: Define the string that will be searched
-        to_replace:str: Specify the string to be replaced
-        replace_by:str: Replace the to_replace:str parameter in the raw_str:str string
-
-    Returns:
-        A string where the given to_replace is replaced by replace_by in raw_str, ignoring case
-    """
-
-    return re.sub(to_replace, replace_by, raw_str, flags=re.IGNORECASE | re.DOTALL)
-
-
-def insensitive_match(raw_str: str, to_match: str) -> bool:
-    """Match `to_match` in `raw_str` ignoring the raw_str case.
-
-    Args:
-        raw_str:str: Define the string that will be searched
-        to_match:str: Specify the string to be matched
-
-    Returns:
-        True if `to_match` matches in `raw_str`, ignoring case. Otherwise, false.
-    """
-
-    return re.match(to_match, raw_str, flags=re.IGNORECASE | re.DOTALL) is not None
-
-
-def get_entity_tier_from_tags(tags: list[TagLabel]) -> Optional[str]:
-    """_summary_
-
-    Args:
-        tags (list[TagLabel]): list of tags
-
-    Returns:
-        Optional[str]
-    """
-    if not tags:
-        return None
-    return next(
-        (
-            tag.tagFQN.__root__
-            for tag in tags
-            if tag.tagFQN.__root__.lower().startswith("tier")
-        ),
-        None,
-    )
-
-
-def format_large_string_numbers(number: Union[float, int]) -> str:
-    """Format large string number to a human readable format.
-    (e.g. 1,000,000 -> 1M, 1,000,000,000 -> 1B, etc)
-
-    Args:
-        number: number
-    """
-    if number == 0:
-        return "0"
-    units = ["", "K", "M", "B", "T"]
-    constant_k = 1000.0
-    magnitude = int(floor(log(abs(number), constant_k)))
-    return f"{number / constant_k**magnitude:.3f}{units[magnitude]}"
-
-
-def clean_uri(uri: str) -> str:
-    """
-    if uri is like http://localhost:9000/
-    then remove the end / and
-    make it http://localhost:9000
-    """
-    return uri[:-1] if uri.endswith("/") else uri
-
-
-def deep_size_of_dict(obj: dict) -> int:
-    """Get deepsize of dict data structure
-
-    Args:
-        obj (dict): dict data structure
-    Returns:
-        int: size of dict data structure
-    """
-    # pylint: disable=unnecessary-lambda-assignment
-    dict_handler = lambda elmt: itertools.chain.from_iterable(elmt.items())
-    handlers = {
-        dict: dict_handler,
-        list: iter,
-    }
-
-    seen = set()
-
-    def sizeof(obj) -> int:
-        if id(obj) in seen:
-            return 0
-
-        seen.add(id(obj))
-        size = sys.getsizeof(obj, 0)
-        for type_, handler in handlers.items():
-            if isinstance(obj, type_):
-                size += sum(map(sizeof, handler(obj)))
-                break
-
-        return size
-
-    return sizeof(obj)
-
-
-def is_safe_sql_query(sql_query: str) -> bool:
-    """Validate SQL query
-    Args:
-        sql_query (str): SQL query
-    Returns:
-        bool
-    """
-
-    forbiden_token = {
-        "CREATE",
-        "ALTER",
-        "DROP",
-        "TRUNCATE",
-        "COMMENT",
-        "RENAME",
-        "INSERT",
-        "UPDATE",
-        "DELETE",
-        "MERGE",
-        "CALL",
-        "EXPLAIN PLAN",
-        "LOCK TABLE",
-        "UNLOCK TABLE",
-        "GRANT",
-        "REVOKE",
-        "COMMIT",
-        "ROLLBACK",
-        "SAVEPOINT",
-        "SET TRANSACTION",
-    }
-
-    if sql_query is None:
-        return True
-
-    parsed_queries: Tuple[Statement] = sqlparse.parse(sql_query)
-    for parsed_query in parsed_queries:
-        validation = [
-            token.normalized in forbiden_token for token in parsed_query.tokens
-        ]
-        if any(validation):
-            return False
-    return True
-
-
-def get_database_name_for_lineage(
-    db_service_entity: DatabaseService, default_db_name: Optional[str]
-) -> Optional[str]:
-    # If the database service supports multiple db or
-    # database service connection details are not available
-    # then pick the database name available from api response
-    if db_service_entity.connection is None or hasattr(
-        db_service_entity.connection.config, "supportsDatabase"
-    ):
-        return default_db_name
-
-    # otherwise if it is an single db source then use "databaseName"
-    # and if databaseName field is not available or is empty then use
-    # "default" as database name
-    return (
-        db_service_entity.connection.config.__dict__.get("databaseName")
-        or DEFAULT_DATABASE
-    )
-
-
-def delete_dir_content(directory: str) -> None:
-    location = Path(directory)
-    if location.is_dir():
-        logger.info("Location exists, cleaning it up")
-        shutil.rmtree(directory)
-
-
-def init_staging_dir(directory: str) -> None:
+def get_prefix_config(config) -> Tuple[Optional[str], Optional[str]]:
     """
-    Prepare the the staging directory
+    Return (bucket, prefix) tuple
     """
-    delete_dir_content(directory=directory)
-    location = Path(directory)
-    logger.info(f"Creating the directory to store staging data in {location}")
-    location.mkdir(parents=True, exist_ok=True)
+    if config.prefixConfig:
+        return (
+            config.prefixConfig.bucketName,
+            config.prefixConfig.objectPrefix,
+        )
+    return None, None
+
+
+def get_blobs_grouped_by_dir(blobs: List[str]) -> Dict[str, List[str]]:
+    """
+    Method to group the objs by the dir
+    """
+    blob_grouped_by_directory = defaultdict(list)
+    for blob in blobs or []:
+        subdirectory = blob.rsplit("/", 1)[0] if "/" in blob else ""
+        blob_file_name = blob.rsplit("/", 1)[1] if "/" in blob else blob
+        if blob_file_name.lower().endswith(".pbit"):
+            blob_grouped_by_directory[subdirectory].append(blob)
+    return blob_grouped_by_directory
+
+
+def download_pbit_files(
+    blob_grouped_by_directory: Dict,
+    config,
+    client,
+    bucket_name: Optional[str],
+    extract_dir: str,
+):
+    """
+    Method to download the files from sources
+    """
+    for (
+        key,
+        blobs,
+    ) in blob_grouped_by_directory.items():
+        kwargs = {}
+        if bucket_name:
+            kwargs = {"bucket_name": bucket_name}
+        try:
+            for blob in blobs:
+                if blob:
+                    reader = get_reader(config_source=config, client=client)
+                    # create the required dir before downloading
+                    os.makedirs(f"{extract_dir}/{key}", exist_ok=True)
+                    reader.download(
+                        path=blob, local_file_path=f"{extract_dir}/{blob}", **kwargs
+                    )
+        except PowerBIFileConfigException as exc:
+            logger.warning(exc)
+
+
+def _get_datamodel_schema_list(path: str) -> Optional[List[DataModelSchema]]:
+    """
+    Method maps the json to datamodel schema model
+    """
+    reader = LocalReader(f"{path}/extracted")
+    connection_files = reader.get_local_files(search_key="Connections")
+    datamodel_schema_list = []
+    for connection_file in connection_files:
+        try:
+            datamodel_schema = DataModelSchema()
+            with open(connection_file, "rb") as file:
+                connection_json_file = json.load(file)
+                datamodel_schema.connectionFile = ConnectionFile(**connection_json_file)
+
+            datamodel_schema_file = connection_file.replace(
+                "Connections", "DataModelSchema"
+            )
+            with open(datamodel_schema_file, "rb") as file:
+                data_model_schema_json_file = json.load(file)
+                datamodel_schema.tables = [
+                    PowerBiTable(**table)
+                    for table in data_model_schema_json_file.get("model")["tables"]
+                    or []
+                ]
+            if datamodel_schema.tables and datamodel_schema.connectionFile:
+                datamodel_schema_list.append(datamodel_schema)
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.error(
+                f"Error reading and mapping the datamodel schema file for {connection_file}: {exc}"
+            )
+    return datamodel_schema_list
+
+
+def get_datamodel_schema_files_from_pbit(path: str) -> Optional[List[DataModelSchema]]:
+    """
+    Method to unzip the locally saved pbit files and get the schema files
+    """
+    try:
+        reader = LocalReader(path)
+        file_paths = reader.get_local_files(search_key=".pbit")
+
+        # Iterate over the file paths
+        for file_path in file_paths:
+            # Open each pbit file
+            with zipfile.ZipFile(file_path, "r") as zip_ref:
+                # Extract all files in the specified folder
+                zip_ref.extractall(
+                    f"{path}/extracted/{file_path.split('/')[-1].split('.')[0]}"
+                )
+
+        return _get_datamodel_schema_list(path)
+
+    except Exception as exc:
+        logger.debug(traceback.format_exc())
+        logger.error(f"Error extracting pbit files: {exc}")
+    return None
+
+
+@singledispatch
+def get_pbit_files(config):
+    """
+    Single dispatch method to get the pbit files from different sources
+    """
+
+    if config:
+        raise NotImplementedError(
+            f"Config not implemented for type {type(config)}: {config}"
+        )
+
+
+@get_pbit_files.register
+def _(config: S3Config):
+    try:
+        bucket_name, prefix = get_prefix_config(config)
+
+        client = AWSClient(config.securityConfig).get_client(service_name="s3")
+
+        if not bucket_name:
+            buckets = client.list_buckets()["Buckets"]
+        else:
+            buckets = [{"Name": bucket_name}]
+        for bucket in buckets:
+            kwargs = {"Bucket": bucket["Name"]}
+            if prefix:
+                kwargs["Prefix"] = prefix if prefix.endswith("/") else f"{prefix}/"
+
+            # Download the pbit files and store them in the local path
+            download_pbit_files(
+                blob_grouped_by_directory=get_blobs_grouped_by_dir(
+                    blobs=[key["Key"] for key in list_s3_objects(client, **kwargs)]
+                ),
+                config=config,
+                client=client,
+                bucket_name=bucket["Name"],
+                extract_dir=config.pbitFilesExtractDir,
+            )
+        # Extract the datamodel schema files from pbit files and return the list of datamodel schema objects
+        return get_datamodel_schema_files_from_pbit(path=config.pbitFilesExtractDir)
+
+    except Exception as exc:
+        logger.debug(traceback.format_exc())
+        raise PowerBIFileConfigException(f"Error fetching .pbit files from s3: {exc}")
+
+
+@get_pbit_files.register
+def _(config: AzureConfig):
+    try:
+        bucket_name, prefix = get_prefix_config(config)
+
+        client = AzureClient(config.securityConfig).create_blob_client()
+
+        if not bucket_name:
+            container_dicts = client.list_containers()
+            containers = [
+                client.get_container_client(container["name"])
+                for container in container_dicts
+            ]
+        else:
+            container_client = client.get_container_client(bucket_name)
+            containers = [container_client]
+        for container_client in containers:
+            if prefix:
+                blob_list = container_client.list_blobs(name_starts_with=prefix)
+            else:
+                blob_list = container_client.list_blobs()
+
+            # Download the pbit files and store them in the local path
+            download_pbit_files(
+                blob_grouped_by_directory=get_blobs_grouped_by_dir(
+                    blobs=[blob.name for blob in blob_list]
+                ),
+                config=config,
+                client=client,
+                bucket_name=container_client.container_name,
+                extract_dir=config.pbitFilesExtractDir,
+            )
+        # Extract the datamodel schema files from pbit files and return the list of datamodel schema objects
+        return get_datamodel_schema_files_from_pbit(path=config.pbitFilesExtractDir)
+
+    except Exception as exc:
+        logger.debug(traceback.format_exc())
+        raise PowerBIFileConfigException(
+            f"Error fetching .pbit files from Azure: {exc}"
+        )
+
+
+@get_pbit_files.register
+def _(config: GCSConfig):
+    try:
+        bucket_name, prefix = get_prefix_config(config)
+        from google.cloud import storage  # pylint: disable=import-outside-toplevel
+
+        set_google_credentials(gcp_credentials=config.securityConfig)
+
+        client = storage.Client()
+        if not bucket_name:
+            buckets = client.list_buckets()
+        else:
+            buckets = [client.get_bucket(bucket_name)]
+        for bucket in buckets:
+            if prefix:
+                obj_list = client.list_blobs(bucket.name, prefix=prefix)
+            else:
+                obj_list = client.list_blobs(bucket.name)
+
+            download_pbit_files(
+                blob_grouped_by_directory=get_blobs_grouped_by_dir(
+                    blobs=[blob.name for blob in obj_list]
+                ),
+                config=config,
+                client=client,
+                bucket_name=bucket.name,
+                extract_dir=config.pbitFilesExtractDir,
+            )
+        # Extract the datamodel schema files from pbit files and return the list of datamodel schema objects
+        return get_datamodel_schema_files_from_pbit(path=config.pbitFilesExtractDir)
+
+    except Exception as exc:
+        logger.debug(traceback.format_exc())
+        raise PowerBIFileConfigException(f"Error fetching .pbit files from GCS: {exc}")
+
+
+@get_pbit_files.register
+def _(config: LocalConfig):
+    try:
+        return get_datamodel_schema_files_from_pbit(path=config.path)
+    except Exception as exc:
+        logger.debug(traceback.format_exc())
+        logger.error(f"Error getting pbit files from local: {exc}")
+    return None
+
+
+class PowerBIFileConfigException(Exception):
+    """
+    Raise when encountering errors while extracting pbit files
+    """
+
+
+class PowerBiFileClient:
+    """
+    File client for PowerBi
+    """
+
+    client: REST
+
+    def __init__(self, config: PowerBIConnection):
+        self.config = config
+
+    def get_data_model_schema_mappings(self) -> Optional[List[DataModelSchema]]:
+        """
+        Get the data model schema mappings
+        """
+        return get_pbit_files(self.config.pbitFilesSource)
+
+    def delete_tmp_files(self):
+        """
+        Method to remove the files after ingestion is completed
+        """
+        shutil.rmtree(self.config.pbitFilesSource.pbitFilesExtractDir)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/importer.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/importer.py`

 * *Files 6% similar despite different names*

```diff
@@ -192,21 +192,30 @@
             f"Cannot get `type` property from connection {connection}. Check the JSON Schema."
         )
 
     service_type: ServiceType = get_service_type_from_source_type(connection_type.value)
 
     # module building strings read better with .format instead of f-strings
     # pylint: disable=consider-using-f-string
-    _connection_fn = import_from_module(
-        "metadata.ingestion.source.{}.{}.connection.{}".format(
-            service_type.name.lower(),
-            connection_type.value.lower(),
-            function_name,
+
+    if connection.type.value.lower().startswith("custom"):
+        python_class_parts = connection.sourcePythonClass.rsplit(".", 1)
+        python_module_path = ".".join(python_class_parts[:-1])
+
+        _connection_fn = import_from_module(
+            "{}.{}".format(python_module_path, function_name)
+        )
+    else:
+        _connection_fn = import_from_module(
+            "metadata.ingestion.source.{}.{}.connection.{}".format(
+                service_type.name.lower(),
+                connection_type.value.lower(),
+                function_name,
+            )
         )
-    )
 
     return _connection_fn
 
 
 def import_test_case_class(
     test_type: str,
     runner_type: str,
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/life_cycle_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/life_cycle_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/logger.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/logger.py`

 * *Files 1% similar despite different names*

```diff
@@ -238,15 +238,15 @@
 
 
 @get_log_name.register
 def _(record: OMetaLifeCycleData) -> str:
     """
     Capture the lifecycle changes of an Entity
     """
-    return f"{type(record.entity).__name__} Lifecycle [{record.entity.name.__root__}]"
+    return f"{record.entity.__name__} Lifecycle [{record.entity_fqn}]"
 
 
 @get_log_name.register
 def _(record: TableAndTests) -> str:
     if record.table:
         return f"Tests for [{record.table.fullyQualifiedName.__root__}]"
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/lru_cache.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/lru_cache.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,14 +11,16 @@
 
 """
 LRU cache
 """
 
 from collections import OrderedDict
 
+LRU_CACHE_SIZE = 4096
+
 
 class LRUCache:
     """Least Recently Used cache"""
 
     def __init__(self, capacity: int) -> None:
         self._cache = OrderedDict()
         self.capacity = capacity
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/metadata_service_helper.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/metadata_service_helper.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/profiler_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/profiler_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/s3_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/s3_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/aws_based_secrets_manager.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/aws_based_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/aws_secrets_manager.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/aws_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/aws_ssm_secrets_manager.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/aws_ssm_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/azure_kv_secrets_manager.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/azure_kv_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/db_secrets_manager.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/db_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/external_secrets_manager.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/external_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/secrets_manager.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/source_hash.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,36 +1,53 @@
-#  Copyright 2022 Collate
+#  Copyright 2021 Collate
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Secrets manager interface
+Source hash utils module
 """
-from abc import abstractmethod
 
-from metadata.utils.logger import ingestion_logger
-from metadata.utils.singleton import Singleton
+import hashlib
+import traceback
+from typing import Dict, Optional
 
-logger = ingestion_logger()
+from metadata.ingestion.ometa.ometa_api import C
+from metadata.utils.logger import utils_logger
 
+logger = utils_logger()
 
-class SecretsManager(metaclass=Singleton):
-    """
-    Abstract class implemented by different secrets' manager providers.
 
-    It contains a set of auxiliary methods for adding missing fields which have been encrypted in the secrets' manager
-    providers.
-    """
+SOURCE_HASH_EXCLUDE_FIELDS = {
+    "sourceHash": True,
+}
+
 
-    @abstractmethod
-    def get_string_value(self, secret_id: str) -> str:
-        """
-        :param secret_id: The secret id to retrieve
-        :return: The value of the secret
-        """
+def generate_source_hash(
+    create_request: C, exclude_fields: Optional[Dict] = None
+) -> Optional[str]:
+    """
+    Given a create_request model convert it to json string and generate a hash value
+    """
+    try:
+        # We always want to exclude the sourceHash when generating the fingerprint
+        exclude_fields = (
+            SOURCE_HASH_EXCLUDE_FIELDS.update(exclude_fields)
+            if exclude_fields
+            else SOURCE_HASH_EXCLUDE_FIELDS
+        )
+
+        create_request_json = create_request.json(exclude=exclude_fields)
+
+        json_bytes = create_request_json.encode("utf-8")
+        return hashlib.md5(json_bytes).hexdigest()
+
+    except Exception as exc:
+        logger.warning(f"Failed to generate source hash due to - {exc}")
+        logger.debug(traceback.format_exc())
+    return None
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/secrets/secrets_manager_factory.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/secrets/secrets_manager_factory.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/singleton.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/singleton.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/source_hash.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/profiler/adaptors/adaptor_factory.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,53 +1,43 @@
-#  Copyright 2021 Collate
+#  Copyright 2024 Collate
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Source hash utils module
+factory for NoSQL adaptors that are used in the NoSQLProfiler.
 """
-
-import hashlib
-import traceback
-from typing import Dict, Optional
-
-from metadata.ingestion.ometa.ometa_api import C
-from metadata.utils.logger import utils_logger
-
-logger = utils_logger()
-
-
-SOURCE_HASH_EXCLUDE_FIELDS = {
-    "sourceHash": True,
+from metadata.generated.schema.entity.services.connections.database.dynamoDBConnection import (
+    DynamoDBConnection,
+)
+from metadata.generated.schema.entity.services.connections.database.mongoDBConnection import (
+    MongoDBConnection,
+)
+from metadata.profiler.adaptors.dynamodb import DynamoDB
+from metadata.profiler.adaptors.mongodb import MongoDB
+from metadata.profiler.factory import Factory
+from metadata.utils.logger import profiler_logger
+
+logger = profiler_logger()
+
+
+class NoSQLAdaptorFactory(Factory):
+    def create(self, interface_type: str, *args, **kwargs) -> any:
+        logger.debug(f"Creating NoSQL client for {interface_type}")
+        client_class = self._interface_type.get(interface_type)
+        if not client_class:
+            raise ValueError(f"Unknown NoSQL source: {interface_type}")
+        logger.debug(f"Using NoSQL client constructor: {client_class.__name__}")
+        return client_class(*args, **kwargs)
+
+
+adaptors = profilers = {
+    MongoDBConnection.__name__: MongoDB,
+    DynamoDBConnection.__name__: DynamoDB,
 }
-
-
-def generate_source_hash(
-    create_request: C, exclude_fields: Optional[Dict] = None
-) -> Optional[str]:
-    """
-    Given a create_request model convert it to json string and generate a hash value
-    """
-    try:
-        # We always want to exclude the sourceHash when generating the fingerprint
-        exclude_fields = (
-            SOURCE_HASH_EXCLUDE_FIELDS.update(exclude_fields)
-            if exclude_fields
-            else SOURCE_HASH_EXCLUDE_FIELDS
-        )
-
-        create_request_json = create_request.json(exclude=exclude_fields)
-
-        json_bytes = create_request_json.encode("utf-8")
-        return hashlib.md5(json_bytes).hexdigest()
-
-    except Exception as exc:
-        logger.warning(f"Failed to generate source hash due to - {exc}")
-        logger.debug(traceback.format_exc())
-    return None
+factory = NoSQLAdaptorFactory()
+factory.register_many(adaptors)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/sqa_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/sqa_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/sqlalchemy_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/sqlalchemy_utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -106,7 +106,22 @@
 ):
     if char_len or (precision is not None and scale is None):
         length = char_len or scale
         return f"{col_type}({str(length)})"
     if scale is not None and precision is not None:
         return f"{col_type}({str(precision)},{str(scale)})"
     return col_type
+
+
+def convert_numpy_to_list(data):
+    """
+    Recursively converts numpy arrays to lists in a nested data structure.
+    """
+    import numpy as np  # pylint: disable=import-outside-toplevel
+
+    if isinstance(data, np.ndarray):
+        return data.tolist()
+    if isinstance(data, list):
+        return [convert_numpy_to_list(item) for item in data]
+    if isinstance(data, dict):
+        return {key: convert_numpy_to_list(value) for key, value in data.items()}
+    return data
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/ssl_registry.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/ssl_registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/storage_metadata_config.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/storage_metadata_config.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/stored_procedures.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/stored_procedures.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/tag_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/tag_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,21 +10,22 @@
 #  limitations under the License.
 """
 Tag utils Module
 """
 
 import functools
 import traceback
-from typing import Iterable, List, Optional
+from typing import Iterable, List, Optional, Union
 
 from metadata.generated.schema.api.classification.createClassification import (
     CreateClassificationRequest,
 )
 from metadata.generated.schema.api.classification.createTag import CreateTagRequest
 from metadata.generated.schema.entity.classification.tag import Tag
+from metadata.generated.schema.entity.data.glossaryTerm import GlossaryTerm
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
 )
 from metadata.generated.schema.type.basic import FullyQualifiedEntityName
 from metadata.generated.schema.type.tagLabel import (
     LabelType,
     State,
@@ -78,58 +79,74 @@
                         stackTrace=traceback.format_exc(),
                     )
                 )
 
 
 @functools.lru_cache(maxsize=512)
 def get_tag_label(
-    metadata: OpenMetadata, tag_name: str, classification_name: str
+    metadata: OpenMetadata,
+    tag_name: str,
+    classification_name: str,
+    tag_type: Union[Tag, GlossaryTerm] = Tag,
 ) -> Optional[TagLabel]:
     """
     Returns the tag label if the tag is created
     """
     try:
-        # Build the tag FQN
-        tag_fqn = fqn.build(
-            metadata,
-            Tag,
-            classification_name=classification_name,
-            tag_name=tag_name,
-        )
+        if tag_type == Tag:
+            # Build the tag FQN
+            tag_fqn = fqn.build(
+                metadata,
+                tag_type,
+                classification_name=classification_name,
+                tag_name=tag_name,
+            )
+            source = TagSource.Classification.value
+
+        if tag_type == GlossaryTerm:
+            tag_fqn = tag_name
+            source = TagSource.Glossary.value
 
         # Check if the tag exists
-        tag = metadata.get_by_name(entity=Tag, fqn=tag_fqn)
+        tag = metadata.get_by_name(entity=tag_type, fqn=tag_fqn)
         if tag:
             return TagLabel(
                 tagFQN=tag_fqn,
                 labelType=LabelType.Automated.value,
                 state=State.Suggested.value,
-                source=TagSource.Classification.value,
+                source=source,
             )
+
+        logger.warning(f"Tag does not exist: {tag_fqn}")
+
     except Exception as err:
         logger.debug(traceback.format_exc())
         logger.error(f"Error processing tag label: {err}")
     return None
 
 
 def get_tag_labels(
     metadata: OpenMetadata,
     tags: List[str],
-    classification_name: str,
+    classification_name: Optional[str],
     include_tags: bool = True,
+    tag_type: Union[Tag, GlossaryTerm] = Tag,
 ) -> Optional[List[TagLabel]]:
     """
     Method to create tag labels from the collected tags
     """
     tag_labels_list = []
     if tags and include_tags:
         for tag in tags:
             try:
                 tag_label = get_tag_label(
-                    metadata, tag_name=tag, classification_name=classification_name
+                    metadata,
+                    tag_name=tag,
+                    classification_name=classification_name,
+                    tag_type=tag_type,
                 )
                 if tag_label:
                     tag_labels_list.append(tag_label)
 
             except Exception as err:
                 logger.debug(traceback.format_exc())
                 logger.error(f"Error processing tag labels: {err}")
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/test_suite.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/test_suite.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/time_utils.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/time_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/timeout.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/utils/timeout.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/utils/uuid_encoder.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/ingestion/source/dashboard/qlikcloud/models.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,26 +4,30 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-UUID Encoder Module
+QlikCloud Models
 """
+from typing import List, Optional
+
+from pydantic import BaseModel, Field
+
+
+# App Models
+class QlikApp(BaseModel):
+    """QlikCloud App model"""
+
+    description: Optional[str]
+    name: Optional[str]
+    id: str
+    app_id: Optional[str] = Field(alias="resourceId", default=None)
+    published: Optional[bool]
 
-import json
-from uuid import UUID
 
+class QlikAppList(BaseModel):
+    """QlikCloud Apps List"""
 
-class UUIDEncoder(json.JSONEncoder):
-    """
-    UUID Encoder class
-    """
-
-    def default(self, o):
-        if isinstance(o, UUID):
-            # if the obj is uuid, we simply return the value of uuid
-            return str(o)
-        return json.JSONEncoder.default(self, o)
+    apps: Optional[List[QlikApp]]
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/application.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/application.py`

 * *Files 2% similar despite different names*

```diff
@@ -64,15 +64,20 @@
         return "AppRunner"
 
     @abstractmethod
     def run(self) -> None:
         """App logic to execute"""
 
     @classmethod
-    def create(cls, config_dict: dict, metadata: OpenMetadata) -> "Step":
+    def create(
+        cls,
+        config_dict: dict,
+        metadata: OpenMetadata,
+        pipeline_name: Optional[str] = None,
+    ) -> "Step":
         config = OpenMetadataApplicationConfig.parse_obj(config_dict)
         return cls(config=config, metadata=metadata)
 
 
 class ApplicationWorkflow(BaseWorkflow, ABC):
     """Base Application Workflow implementation"""
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/application_output_handler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/application_output_handler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/base.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -98,14 +98,15 @@
         )
 
         set_loggers_level(log_level.value)
 
         # We create the ometa client at the workflow level and pass it to the steps
         self.metadata_config = metadata_config
         self.metadata = create_ometa_client(metadata_config)
+        self.set_ingestion_pipeline_status(state=PipelineState.running)
 
         self.post_init()
 
     @property
     def ingestion_pipeline(self):
         """Get or create the Ingestion Pipeline from the configuration"""
         if not self._ingestion_pipeline and self.config.ingestionPipelineFQN:
@@ -176,15 +177,14 @@
         2. Execute the workflow
         3. Validate the pipeline status
         4. Update the pipeline status at the end
         """
         pipeline_state = PipelineState.success
         self.timer.trigger()
         try:
-            self.set_ingestion_pipeline_status(state=PipelineState.running)
             self.execute_internal()
 
             if SUCCESS_THRESHOLD_VALUE <= self.calculate_success() < 100:
                 pipeline_state = PipelineState.partialSuccess
 
         # Any unhandled exception breaking the workflow should update the status
         except Exception as err:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/data_insight.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/data_insight.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/data_quality.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/data_quality.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/ingestion.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/ingestion.py`

 * *Files 5% similar despite different names*

```diff
@@ -37,14 +37,15 @@
 from metadata.generated.schema.metadataIngestion.workflow import (
     OpenMetadataWorkflowConfig,
 )
 from metadata.ingestion.api.parser import parse_workflow_config_gracefully
 from metadata.ingestion.api.step import Step, Summary
 from metadata.ingestion.api.steps import BulkSink, Processor, Sink, Source, Stage
 from metadata.ingestion.models.custom_types import ServiceWithConnectionType
+from metadata.profiler.api.models import ProfilerProcessorConfig
 from metadata.utils.class_helper import (
     get_service_class_from_service_type,
     get_service_type_from_source_type,
 )
 from metadata.utils.logger import ingestion_logger
 from metadata.workflow.base import BaseWorkflow, InvalidWorkflowJSONException
 from metadata.workflow.workflow_status_mixin import SUCCESS_THRESHOLD_VALUE
@@ -197,7 +198,25 @@
                 raise exc
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.error(
                     f"Unknown error getting service connection for service name [{service_name}]"
                     f" using the secrets manager provider [{self.metadata.config.secretsManagerProvider}]: {exc}"
                 )
+
+    def validate(self):
+        try:
+            if (
+                not self.config.source.serviceConnection.__root__.config.supportsProfiler
+            ):
+                raise AttributeError()
+        except AttributeError:
+            if ProfilerProcessorConfig.parse_obj(
+                self.config.processor.dict().get("config")
+            ).ignoreValidation:
+                logger.debug(
+                    f"Profiler is not supported for the service connection: {self.config.source.serviceConnection}"
+                )
+                return
+            raise WorkflowExecutionError(
+                f"Profiler is not supported for the service connection: {self.config.source.serviceConnection}"
+            )
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/metadata.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -53,15 +53,23 @@
             )
             if source_type.startswith("custom")
             else import_source_class(
                 service_type=self.service_type, source_type=source_type
             )
         )
 
-        source: Source = source_class.create(self.config.source.dict(), self.metadata)
+        pipeline_name = (
+            self.ingestion_pipeline.fullyQualifiedName.__root__
+            if self.ingestion_pipeline
+            else None
+        )
+
+        source: Source = source_class.create(
+            self.config.source.dict(), self.metadata, pipeline_name
+        )
         logger.debug(f"Source type:{source_type},{source_class} configured")
         source.prepare()
         logger.debug(f"Source type:{source_type},{source_class}  prepared")
 
         return source
 
     def _get_sink(self) -> Sink:
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/output_handler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/output_handler.py`

 * *Files 4% similar despite different names*

```diff
@@ -22,14 +22,15 @@
 from tabulate import tabulate
 
 from metadata.generated.schema.entity.services.ingestionPipelines.status import (
     StackTraceError,
 )
 from metadata.generated.schema.metadataIngestion.workflow import LogLevels
 from metadata.ingestion.api.step import Summary
+from metadata.ingestion.lineage.models import QueryParsingFailures
 from metadata.utils.execution_time_tracker import ExecutionTimeTracker
 from metadata.utils.helpers import pretty_print_time_duration
 from metadata.utils.logger import ANSI, log_ansi_encoded_string
 
 WORKFLOW_FAILURE_MESSAGE = "Workflow finished with failures"
 WORKFLOW_WARNING_MESSAGE = "Workflow finished with warnings"
 WORKFLOW_SUCCESS_MESSAGE = "Workflow finished successfully"
@@ -144,36 +145,57 @@
     tracker = ExecutionTimeTracker()
 
     summary_table = {
         "Context": [],
         "Execution Time Aggregate": [],
     }
 
-    for key in sorted(tracker.state.keys()):
+    for key in sorted(tracker.state.state.keys()):
         summary_table["Context"].append(key)
         summary_table["Execution Time Aggregate"].append(
-            pretty_print_time_duration(tracker.state[key])
+            pretty_print_time_duration(tracker.state.state[key])
         )
 
     log_ansi_encoded_string(bold=True, message="Execution Time Summary")
     log_ansi_encoded_string(message=f"\n{tabulate(summary_table, tablefmt='grid')}")
 
 
+def print_query_parsing_issues():
+    """Log the QueryParsingFailures Summary."""
+    query_failures = QueryParsingFailures()
+
+    summary_table = {
+        "Query": [],
+        "Error": [],
+    }
+
+    for failure in query_failures:
+        summary_table["Query"].append(failure.query)
+        summary_table["Error"].append(failure.error)
+
+    if summary_table["Query"]:
+        log_ansi_encoded_string(bold=True, message="Query Parsing Error Summary")
+        log_ansi_encoded_string(
+            message=f"\n{tabulate(summary_table, tablefmt='grid', headers=summary_table.keys())}"
+        )
+
+
 def print_workflow_summary(workflow: "BaseWorkflow") -> None:
     """
     Args:
         workflow: the workflow status to be printed
 
     Returns:
         Print Workflow status when the workflow logger level is DEBUG
     """
 
     if is_debug_enabled(workflow):
         print_workflow_status_debug(workflow)
         print_execution_time_summary()
+        print_query_parsing_issues()
 
     failures = []
     total_records = 0
     total_errors = 0
     for step in workflow.workflow_steps():
         step_summary = Summary.from_step(step)
         total_records += step_summary.records
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/profiler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/profiler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/usage.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/workflow_output_handler.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/workflow_output_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 #  limitations under the License.
 
 """
 Module handles the output messages from different workflows
 """
 
 import time
+import traceback
 from typing import Type, Union
 
 from metadata.config.common import ConfigurationError
 from metadata.ingestion.api.parser import (
     InvalidWorkflowException,
     ParsingConfigurationError,
 )
@@ -106,14 +107,15 @@
         exc, (ParsingConfigurationError, ConfigurationError, InvalidWorkflowException)
     ):
         print_error_msg(f"Error loading {workflow_type.name} configuration: {exc}")
         print_file_example(source_type_name, workflow_type)
         print_more_info(workflow_type)
     else:
         print_error_msg(f"\nError initializing {workflow_type.name}: {exc}")
+        print_error_msg(traceback.format_exc())
         print_more_info(workflow_type)
 
 
 def print_status(workflow: "IngestionWorkflow") -> None:
     """
     Print the workflow results
     """
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/metadata/workflow/workflow_status_mixin.py` & `openmetadata-ingestion-1.4.0.0rc1/src/metadata/workflow/workflow_status_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/PKG-INFO` & `openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: openmetadata-ingestion
-Version: 1.3.4.0
+Version: 1.4.0.0rc1
 Summary: Ingestion Framework for OpenMetadata
 Author: OpenMetadata Committers
 License:                                  Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
            TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
@@ -206,246 +206,233 @@
            limitations under the License.
 Project-URL: Homepage, https://open-metadata.org/
 Project-URL: Documentation, https://docs.open-metadata.org/
 Project-URL: Source, https://github.com/open-metadata/OpenMetadata
 Requires-Python: >=3.8
 Description-Content-Type: text/x-rst
 License-File: LICENSE
-Requires-Dist: jsonschema
-Requires-Dist: pydantic~=1.10
-Requires-Dist: cached-property==1.5.2
-Requires-Dist: mypy_extensions>=0.4.3
-Requires-Dist: requests-aws4auth~=1.1
+Requires-Dist: boto3<2.0,>=1.20
 Requires-Dist: sqlalchemy<2,>=1.4.0
-Requires-Dist: collate-sqllineage~=1.3.0
-Requires-Dist: azure-keyvault-secrets
-Requires-Dist: avro~=1.11
+Requires-Dist: tabulate==0.9.0
+Requires-Dist: requests>=2.23
 Requires-Dist: setuptools~=66.0.0
-Requires-Dist: antlr4-python3-runtime==4.9.2
+Requires-Dist: typing-inspect
+Requires-Dist: requests-aws4auth~=1.1
+Requires-Dist: memory-profiler
+Requires-Dist: importlib-metadata>=4.13.0
+Requires-Dist: cached-property==1.5.2
 Requires-Dist: email-validator>=1.0.3
+Requires-Dist: mypy_extensions>=0.4.3
 Requires-Dist: azure-identity~=1.12
-Requires-Dist: croniter~=1.3.0
-Requires-Dist: requests>=2.23
-Requires-Dist: google-auth>=1.33.0
-Requires-Dist: Jinja2>=2.11.3
-Requires-Dist: jsonpatch<2.0,>=1.24
-Requires-Dist: cryptography
+Requires-Dist: antlr4-python3-runtime==4.9.2
+Requires-Dist: azure-keyvault-secrets
+Requires-Dist: cryptography>=42.0.0
+Requires-Dist: python-dateutil>=2.8.1
 Requires-Dist: chardet==4.0.0
-Requires-Dist: google>=3.0.0
-Requires-Dist: grpcio-tools>=1.47.2
 Requires-Dist: pymysql>=1.0.2
-Requires-Dist: python-jose~=3.3
-Requires-Dist: wheel~=0.38.4
-Requires-Dist: idna<3,>=2.5
-Requires-Dist: python-dateutil>=2.8.1
-Requires-Dist: importlib-metadata>=4.13.0
-Requires-Dist: boto3<2.0,>=1.20
-Requires-Dist: typing-inspect
-Requires-Dist: memory-profiler
+Requires-Dist: pydantic~=1.10
+Requires-Dist: collate-sqllineage~=1.3.0
+Requires-Dist: jsonpatch<2.0,>=1.24
+Requires-Dist: Jinja2>=2.11.3
 Requires-Dist: PyYAML~=6.0
-Requires-Dist: tabulate==0.9.0
 Provides-Extra: base
-Requires-Dist: jsonschema; extra == "base"
-Requires-Dist: pydantic~=1.10; extra == "base"
-Requires-Dist: cached-property==1.5.2; extra == "base"
-Requires-Dist: mypy_extensions>=0.4.3; extra == "base"
-Requires-Dist: requests-aws4auth~=1.1; extra == "base"
+Requires-Dist: boto3<2.0,>=1.20; extra == "base"
 Requires-Dist: sqlalchemy<2,>=1.4.0; extra == "base"
-Requires-Dist: collate-sqllineage~=1.3.0; extra == "base"
-Requires-Dist: azure-keyvault-secrets; extra == "base"
-Requires-Dist: avro~=1.11; extra == "base"
+Requires-Dist: tabulate==0.9.0; extra == "base"
+Requires-Dist: requests>=2.23; extra == "base"
 Requires-Dist: setuptools~=66.0.0; extra == "base"
-Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "base"
+Requires-Dist: typing-inspect; extra == "base"
+Requires-Dist: requests-aws4auth~=1.1; extra == "base"
+Requires-Dist: memory-profiler; extra == "base"
+Requires-Dist: importlib-metadata>=4.13.0; extra == "base"
+Requires-Dist: cached-property==1.5.2; extra == "base"
 Requires-Dist: email-validator>=1.0.3; extra == "base"
+Requires-Dist: mypy_extensions>=0.4.3; extra == "base"
 Requires-Dist: azure-identity~=1.12; extra == "base"
-Requires-Dist: croniter~=1.3.0; extra == "base"
-Requires-Dist: requests>=2.23; extra == "base"
-Requires-Dist: google-auth>=1.33.0; extra == "base"
-Requires-Dist: Jinja2>=2.11.3; extra == "base"
-Requires-Dist: jsonpatch<2.0,>=1.24; extra == "base"
-Requires-Dist: cryptography; extra == "base"
+Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "base"
+Requires-Dist: azure-keyvault-secrets; extra == "base"
+Requires-Dist: cryptography>=42.0.0; extra == "base"
+Requires-Dist: python-dateutil>=2.8.1; extra == "base"
 Requires-Dist: chardet==4.0.0; extra == "base"
-Requires-Dist: google>=3.0.0; extra == "base"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "base"
 Requires-Dist: pymysql>=1.0.2; extra == "base"
-Requires-Dist: python-jose~=3.3; extra == "base"
-Requires-Dist: wheel~=0.38.4; extra == "base"
-Requires-Dist: idna<3,>=2.5; extra == "base"
-Requires-Dist: python-dateutil>=2.8.1; extra == "base"
-Requires-Dist: importlib-metadata>=4.13.0; extra == "base"
-Requires-Dist: boto3<2.0,>=1.20; extra == "base"
-Requires-Dist: typing-inspect; extra == "base"
-Requires-Dist: memory-profiler; extra == "base"
+Requires-Dist: pydantic~=1.10; extra == "base"
+Requires-Dist: collate-sqllineage~=1.3.0; extra == "base"
+Requires-Dist: jsonpatch<2.0,>=1.24; extra == "base"
+Requires-Dist: Jinja2>=2.11.3; extra == "base"
 Requires-Dist: PyYAML~=6.0; extra == "base"
-Requires-Dist: tabulate==0.9.0; extra == "base"
 Provides-Extra: dev
+Requires-Dist: build; extra == "dev"
+Requires-Dist: boto3-stubs[essential]; extra == "dev"
+Requires-Dist: isort; extra == "dev"
 Requires-Dist: twine; extra == "dev"
-Requires-Dist: black==22.3.0; extra == "dev"
-Requires-Dist: pycln; extra == "dev"
 Requires-Dist: pylint~=3.0.0; extra == "dev"
-Requires-Dist: build; extra == "dev"
-Requires-Dist: pre-commit; extra == "dev"
 Requires-Dist: datamodel-code-generator==0.24.2; extra == "dev"
-Requires-Dist: isort; extra == "dev"
+Requires-Dist: pre-commit; extra == "dev"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "dev"
+Requires-Dist: pycln; extra == "dev"
+Requires-Dist: black==22.3.0; extra == "dev"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "dev"
 Provides-Extra: test
-Requires-Dist: lkml~=1.3; extra == "test"
-Requires-Dist: spacy==3.5.0; extra == "test"
-Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "test"
-Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "test"
-Requires-Dist: pyarrow~=14.0; extra == "test"
-Requires-Dist: scikit-learn~=1.0; extra == "test"
-Requires-Dist: pymongo~=4.3; extra == "test"
-Requires-Dist: elasticsearch8~=8.9.0; extra == "test"
-Requires-Dist: google>=3.0.0; extra == "test"
 Requires-Dist: pytest==7.0.0; extra == "test"
-Requires-Dist: pytest-cov; extra == "test"
-Requires-Dist: great-expectations~=0.18.0; extra == "test"
 Requires-Dist: trino[sqlalchemy]; extra == "test"
-Requires-Dist: moto==4.0.8; extra == "test"
 Requires-Dist: looker-sdk>=22.20.0; extra == "test"
-Requires-Dist: coverage; extra == "test"
-Requires-Dist: pytest-order; extra == "test"
-Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "test"
+Requires-Dist: lkml~=1.3; extra == "test"
 Requires-Dist: giturlparse; extra == "test"
+Requires-Dist: testcontainers==3.7.1; python_version < "3.9" and extra == "test"
 Requires-Dist: pydomo~=0.3; extra == "test"
-Requires-Dist: apache-airflow==2.7.3; extra == "test"
-Requires-Dist: dbt-artifacts-parser; extra == "test"
+Requires-Dist: minio==7.2.5; extra == "test"
+Requires-Dist: scikit-learn~=1.0; extra == "test"
+Requires-Dist: pytest-cov; extra == "test"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "test"
 Requires-Dist: sqlalchemy-databricks~=0.1; extra == "test"
+Requires-Dist: great-expectations~=0.18.0; extra == "test"
+Requires-Dist: coverage; extra == "test"
+Requires-Dist: pyarrow~=14.0; extra == "test"
+Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "test"
+Requires-Dist: pymongo~=4.3; extra == "test"
+Requires-Dist: elasticsearch8~=8.9.0; extra == "test"
 Requires-Dist: tableau-api-lib~=0.1; extra == "test"
+Requires-Dist: apache-airflow==2.7.3; extra == "test"
 Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "test"
+Requires-Dist: spacy==3.5.0; extra == "test"
+Requires-Dist: dbt-artifacts-parser; extra == "test"
+Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "test"
+Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "test"
+Requires-Dist: boto3-stubs[boto3]; extra == "test"
+Requires-Dist: pytest-order; extra == "test"
+Requires-Dist: testcontainers==4.4.0; python_version >= "3.9" and extra == "test"
+Requires-Dist: moto==4.0.8; extra == "test"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "test"
 Provides-Extra: e2e-test
 Requires-Dist: pytest-playwright; extra == "e2e-test"
 Requires-Dist: pytest-base-url; extra == "e2e-test"
 Provides-Extra: extended-testing
 Requires-Dist: Faker; extra == "extended-testing"
 Provides-Extra: data-insight
 Requires-Dist: elasticsearch8~=8.9.0; extra == "data-insight"
-Requires-Dist: elasticsearch==7.13.1; extra == "data-insight"
 Provides-Extra: airflow
-Requires-Dist: attrs; extra == "airflow"
 Requires-Dist: apache-airflow==2.7.3; extra == "airflow"
+Requires-Dist: attrs; extra == "airflow"
 Provides-Extra: amundsen
 Requires-Dist: neo4j~=5.3.0; extra == "amundsen"
 Provides-Extra: athena
 Requires-Dist: pyathena==3.0.8; extra == "athena"
 Provides-Extra: atlas
 Provides-Extra: azuresql
 Requires-Dist: pyodbc<5,>=4.0.35; extra == "azuresql"
 Provides-Extra: azure-sso
 Requires-Dist: msal~=1.2; extra == "azure-sso"
 Provides-Extra: backup
-Requires-Dist: azure-identity~=1.12; extra == "backup"
-Requires-Dist: azure-storage-blob; extra == "backup"
 Requires-Dist: boto3<2.0,>=1.20; extra == "backup"
+Requires-Dist: azure-storage-blob; extra == "backup"
+Requires-Dist: azure-identity~=1.12; extra == "backup"
 Provides-Extra: bigquery
-Requires-Dist: google-cloud-logging; extra == "bigquery"
-Requires-Dist: pyarrow~=14.0; extra == "bigquery"
 Requires-Dist: cachetools; extra == "bigquery"
+Requires-Dist: google-cloud-logging; extra == "bigquery"
 Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "bigquery"
+Requires-Dist: pyarrow~=14.0; extra == "bigquery"
 Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "bigquery"
 Provides-Extra: bigtable
 Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "bigtable"
 Requires-Dist: pandas~=2.0.0; extra == "bigtable"
 Provides-Extra: clickhouse
-Requires-Dist: clickhouse-driver~=0.2; extra == "clickhouse"
 Requires-Dist: clickhouse-sqlalchemy~=0.2; extra == "clickhouse"
+Requires-Dist: clickhouse-driver~=0.2; extra == "clickhouse"
 Provides-Extra: dagster
-Requires-Dist: GeoAlchemy2~=0.12; extra == "dagster"
 Requires-Dist: psycopg2-binary; extra == "dagster"
-Requires-Dist: dagster_graphql~=1.1; extra == "dagster"
 Requires-Dist: pymysql>=1.0.2; extra == "dagster"
+Requires-Dist: dagster_graphql~=1.1; extra == "dagster"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "dagster"
 Provides-Extra: dbt
+Requires-Dist: google-cloud; extra == "dbt"
 Requires-Dist: boto3<2.0,>=1.20; extra == "dbt"
 Requires-Dist: azure-storage-blob~=12.14; extra == "dbt"
-Requires-Dist: google-cloud; extra == "dbt"
-Requires-Dist: azure-identity~=1.12; extra == "dbt"
 Requires-Dist: google-cloud-storage==1.43.0; extra == "dbt"
 Requires-Dist: dbt-artifacts-parser; extra == "dbt"
+Requires-Dist: azure-identity~=1.12; extra == "dbt"
 Provides-Extra: db2
 Requires-Dist: ibm-db-sa~=0.3; extra == "db2"
 Provides-Extra: db2-ibmi
 Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "db2-ibmi"
 Provides-Extra: databricks
 Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "databricks"
+Requires-Dist: sqlalchemy-databricks~=0.1; extra == "databricks"
+Requires-Dist: ndg-httpsclient~=0.5.1; extra == "databricks"
 Requires-Dist: pyasn1~=0.6.0; extra == "databricks"
 Requires-Dist: pyOpenSSL~=24.1.0; extra == "databricks"
-Requires-Dist: ndg-httpsclient~=0.5.1; extra == "databricks"
-Requires-Dist: sqlalchemy-databricks~=0.1; extra == "databricks"
 Provides-Extra: datalake-azure
-Requires-Dist: adlfs~=2022.11; extra == "datalake-azure"
+Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-azure"
 Requires-Dist: azure-storage-blob~=12.14; extra == "datalake-azure"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "datalake-azure"
+Requires-Dist: adlfs~=2022.11; extra == "datalake-azure"
 Requires-Dist: cramjam~=2.7; extra == "datalake-azure"
+Requires-Dist: pyarrow~=14.0; extra == "datalake-azure"
 Requires-Dist: pandas~=2.0.0; extra == "datalake-azure"
-Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-azure"
 Requires-Dist: azure-identity~=1.12; extra == "datalake-azure"
-Requires-Dist: pyarrow~=14.0; extra == "datalake-azure"
 Provides-Extra: datalake-gcs
-Requires-Dist: google-cloud-storage==1.43.0; extra == "datalake-gcs"
+Requires-Dist: gcsfs~=2022.11; extra == "datalake-gcs"
 Requires-Dist: cramjam~=2.7; extra == "datalake-gcs"
+Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-gcs"
+Requires-Dist: google-cloud-storage==1.43.0; extra == "datalake-gcs"
 Requires-Dist: pandas~=2.0.0; extra == "datalake-gcs"
 Requires-Dist: pyarrow~=14.0; extra == "datalake-gcs"
-Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-gcs"
-Requires-Dist: gcsfs~=2022.11; extra == "datalake-gcs"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "datalake-gcs"
 Provides-Extra: datalake-s3
 Requires-Dist: s3fs==0.4.2; extra == "datalake-s3"
 Requires-Dist: cramjam~=2.7; extra == "datalake-s3"
-Requires-Dist: pandas~=2.0.0; extra == "datalake-s3"
 Requires-Dist: boto3<2.0,>=1.20; extra == "datalake-s3"
 Requires-Dist: pyarrow~=14.0; extra == "datalake-s3"
+Requires-Dist: pandas~=2.0.0; extra == "datalake-s3"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "datalake-s3"
 Provides-Extra: deltalake
 Requires-Dist: delta-spark<=2.3.0; extra == "deltalake"
-Provides-Extra: docker
-Requires-Dist: python_on_whales==0.55.0; extra == "docker"
 Provides-Extra: domo
 Requires-Dist: pydomo~=0.3; extra == "domo"
 Provides-Extra: doris
 Requires-Dist: pydoris==1.0.2; extra == "doris"
 Provides-Extra: druid
 Requires-Dist: pydruid>=0.6.5; extra == "druid"
 Provides-Extra: dynamodb
 Requires-Dist: boto3<2.0,>=1.20; extra == "dynamodb"
 Provides-Extra: elasticsearch
 Requires-Dist: elasticsearch8~=8.9.0; extra == "elasticsearch"
-Requires-Dist: elasticsearch==7.13.1; extra == "elasticsearch"
 Provides-Extra: glue
 Requires-Dist: boto3<2.0,>=1.20; extra == "glue"
 Provides-Extra: great-expectations
 Requires-Dist: great-expectations~=0.18.0; extra == "great-expectations"
 Provides-Extra: hive
 Requires-Dist: impyla~=0.18.0; extra == "hive"
 Requires-Dist: thrift-sasl~=0.4; extra == "hive"
-Requires-Dist: thrift<1,>=0.13; extra == "hive"
+Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "hive"
 Requires-Dist: pure-sasl; extra == "hive"
+Requires-Dist: thrift<1,>=0.13; extra == "hive"
 Requires-Dist: presto-types-parser>=0.0.2; extra == "hive"
-Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "hive"
 Provides-Extra: iceberg
-Requires-Dist: pyiceberg; extra == "iceberg"
+Requires-Dist: gcsfs~=2022.11; extra == "iceberg"
+Requires-Dist: adlfs~=2022.11; extra == "iceberg"
 Requires-Dist: pydantic~=1.10; extra == "iceberg"
 Requires-Dist: pyarrow~=14.0; extra == "iceberg"
-Requires-Dist: adlfs~=2022.11; extra == "iceberg"
-Requires-Dist: gcsfs~=2022.11; extra == "iceberg"
+Requires-Dist: pyiceberg<1; extra == "iceberg"
 Provides-Extra: impala
-Requires-Dist: presto-types-parser>=0.0.2; extra == "impala"
+Requires-Dist: impyla[kerberos]~=0.18.0; extra == "impala"
 Requires-Dist: thrift-sasl~=0.4; extra == "impala"
-Requires-Dist: thrift<1,>=0.13; extra == "impala"
 Requires-Dist: pure-sasl; extra == "impala"
-Requires-Dist: impyla[kerberos]~=0.18.0; extra == "impala"
+Requires-Dist: thrift<1,>=0.13; extra == "impala"
+Requires-Dist: presto-types-parser>=0.0.2; extra == "impala"
 Provides-Extra: kafka
-Requires-Dist: avro~=1.11; extra == "kafka"
-Requires-Dist: protobuf; extra == "kafka"
 Requires-Dist: fastavro>=1.2.0; extra == "kafka"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "kafka"
 Requires-Dist: confluent_kafka==2.1.1; extra == "kafka"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "kafka"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "kafka"
+Requires-Dist: protobuf; extra == "kafka"
 Provides-Extra: kinesis
 Requires-Dist: boto3<2.0,>=1.20; extra == "kinesis"
-Provides-Extra: ldap-users
-Requires-Dist: ldap3==2.9.1; extra == "ldap-users"
 Provides-Extra: looker
-Requires-Dist: giturlparse; extra == "looker"
 Requires-Dist: gitpython~=3.1.34; extra == "looker"
+Requires-Dist: giturlparse; extra == "looker"
 Requires-Dist: looker-sdk>=22.20.0; extra == "looker"
 Requires-Dist: lkml~=1.3; extra == "looker"
 Provides-Extra: mlflow
 Requires-Dist: mlflow-skinny>=2.3.0; extra == "mlflow"
 Requires-Dist: alembic~=1.10.2; extra == "mlflow"
 Provides-Extra: mongo
 Requires-Dist: pymongo~=4.3; extra == "mongo"
@@ -455,313 +442,316 @@
 Provides-Extra: mssql
 Requires-Dist: sqlalchemy-pytds~=0.3; extra == "mssql"
 Provides-Extra: mssql-odbc
 Requires-Dist: pyodbc<5,>=4.0.35; extra == "mssql-odbc"
 Provides-Extra: mysql
 Requires-Dist: pymysql>=1.0.2; extra == "mysql"
 Provides-Extra: nifi
-Provides-Extra: okta
-Requires-Dist: okta~=2.3; extra == "okta"
+Provides-Extra: openlineage
+Requires-Dist: fastavro>=1.2.0; extra == "openlineage"
+Requires-Dist: confluent_kafka==2.1.1; extra == "openlineage"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "openlineage"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "openlineage"
+Requires-Dist: protobuf; extra == "openlineage"
 Provides-Extra: oracle
-Requires-Dist: oracledb~=1.2; extra == "oracle"
 Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "oracle"
+Requires-Dist: oracledb~=1.2; extra == "oracle"
 Provides-Extra: pgspider
 Requires-Dist: psycopg2-binary; extra == "pgspider"
 Requires-Dist: sqlalchemy-pgspider; extra == "pgspider"
 Provides-Extra: pinotdb
 Requires-Dist: pinotdb~=0.3; extra == "pinotdb"
 Provides-Extra: postgres
-Requires-Dist: GeoAlchemy2~=0.12; extra == "postgres"
 Requires-Dist: psycopg2-binary; extra == "postgres"
-Requires-Dist: packaging==21.3; extra == "postgres"
 Requires-Dist: pymysql>=1.0.2; extra == "postgres"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "postgres"
+Requires-Dist: packaging==21.3; extra == "postgres"
 Provides-Extra: powerbi
+Requires-Dist: boto3<2.0,>=1.20; extra == "powerbi"
+Requires-Dist: azure-storage-blob~=12.14; extra == "powerbi"
+Requires-Dist: google-cloud-storage==1.43.0; extra == "powerbi"
 Requires-Dist: msal~=1.2; extra == "powerbi"
+Requires-Dist: azure-identity~=1.12; extra == "powerbi"
 Provides-Extra: qliksense
 Requires-Dist: websocket-client~=1.6.1; extra == "qliksense"
 Provides-Extra: presto
-Requires-Dist: presto-types-parser>=0.0.2; extra == "presto"
 Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "presto"
+Requires-Dist: presto-types-parser>=0.0.2; extra == "presto"
 Provides-Extra: pymssql
 Requires-Dist: pymssql~=2.2.0; extra == "pymssql"
 Provides-Extra: quicksight
 Requires-Dist: boto3<2.0,>=1.20; extra == "quicksight"
 Provides-Extra: redash
 Requires-Dist: packaging==21.3; extra == "redash"
 Provides-Extra: redpanda
-Requires-Dist: avro~=1.11; extra == "redpanda"
-Requires-Dist: protobuf; extra == "redpanda"
 Requires-Dist: fastavro>=1.2.0; extra == "redpanda"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "redpanda"
 Requires-Dist: confluent_kafka==2.1.1; extra == "redpanda"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "redpanda"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "redpanda"
+Requires-Dist: protobuf; extra == "redpanda"
 Provides-Extra: redshift
-Requires-Dist: GeoAlchemy2~=0.12; extra == "redshift"
 Requires-Dist: psycopg2-binary; extra == "redshift"
 Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "redshift"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "redshift"
 Provides-Extra: sagemaker
 Requires-Dist: boto3<2.0,>=1.20; extra == "sagemaker"
 Provides-Extra: salesforce
 Requires-Dist: simple_salesforce==1.11.4; extra == "salesforce"
+Provides-Extra: sample-data
+Requires-Dist: grpcio-tools>=1.47.2; extra == "sample-data"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "sample-data"
 Provides-Extra: sap-hana
-Requires-Dist: sqlalchemy-hana; extra == "sap-hana"
 Requires-Dist: hdbcli; extra == "sap-hana"
+Requires-Dist: sqlalchemy-hana; extra == "sap-hana"
 Provides-Extra: sas
 Provides-Extra: singlestore
 Requires-Dist: pymysql>=1.0.2; extra == "singlestore"
 Provides-Extra: sklearn
 Requires-Dist: scikit-learn~=1.0; extra == "sklearn"
 Provides-Extra: snowflake
 Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "snowflake"
 Provides-Extra: superset
 Provides-Extra: tableau
+Requires-Dist: tableau-api-lib~=0.1; extra == "tableau"
 Requires-Dist: validators~=0.22.0; extra == "tableau"
 Requires-Dist: packaging==21.3; extra == "tableau"
-Requires-Dist: tableau-api-lib~=0.1; extra == "tableau"
 Provides-Extra: trino
 Requires-Dist: trino[sqlalchemy]; extra == "trino"
 Provides-Extra: vertica
 Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "vertica"
 Provides-Extra: pii-processor
 Requires-Dist: presidio-analyzer==2.2.32; extra == "pii-processor"
 Requires-Dist: spacy==3.5.0; extra == "pii-processor"
 Requires-Dist: pandas~=2.0.0; extra == "pii-processor"
 Provides-Extra: all
-Requires-Dist: jsonschema; extra == "all"
-Requires-Dist: presidio-analyzer==2.2.32; extra == "all"
-Requires-Dist: elasticsearch==7.13.1; extra == "all"
-Requires-Dist: cramjam~=2.7; extra == "all"
+Requires-Dist: trino[sqlalchemy]; extra == "all"
+Requires-Dist: looker-sdk>=22.20.0; extra == "all"
 Requires-Dist: lkml~=1.3; extra == "all"
-Requires-Dist: oracledb~=1.2; extra == "all"
-Requires-Dist: pymssql~=2.2.0; extra == "all"
-Requires-Dist: mypy_extensions>=0.4.3; extra == "all"
-Requires-Dist: google-cloud; extra == "all"
-Requires-Dist: ldap3==2.9.1; extra == "all"
-Requires-Dist: python_on_whales==0.55.0; extra == "all"
-Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "all"
-Requires-Dist: requests-aws4auth~=1.1; extra == "all"
-Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "all"
-Requires-Dist: simple_salesforce==1.11.4; extra == "all"
-Requires-Dist: spacy==3.5.0; extra == "all"
+Requires-Dist: boto3<2.0,>=1.20; extra == "all"
 Requires-Dist: sqlalchemy<2,>=1.4.0; extra == "all"
-Requires-Dist: collate-sqllineage~=1.3.0; extra == "all"
-Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "all"
-Requires-Dist: azure-keyvault-secrets; extra == "all"
-Requires-Dist: neo4j~=5.3.0; extra == "all"
-Requires-Dist: packaging==21.3; extra == "all"
-Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "all"
-Requires-Dist: azure-storage-blob; extra == "all"
-Requires-Dist: pyathena==3.0.8; extra == "all"
-Requires-Dist: pyarrow~=14.0; extra == "all"
-Requires-Dist: croniter~=1.3.0; extra == "all"
-Requires-Dist: ndg-httpsclient~=0.5.1; extra == "all"
-Requires-Dist: s3fs==0.4.2; extra == "all"
-Requires-Dist: Jinja2>=2.11.3; extra == "all"
-Requires-Dist: jsonpatch<2.0,>=1.24; extra == "all"
-Requires-Dist: pymongo~=4.3; extra == "all"
-Requires-Dist: scikit-learn~=1.0; extra == "all"
-Requires-Dist: elasticsearch8~=8.9.0; extra == "all"
-Requires-Dist: gitpython~=3.1.34; extra == "all"
 Requires-Dist: pyodbc<5,>=4.0.35; extra == "all"
-Requires-Dist: gcsfs~=2022.11; extra == "all"
-Requires-Dist: pyiceberg; extra == "all"
-Requires-Dist: pymysql>=1.0.2; extra == "all"
-Requires-Dist: impyla~=0.18.0; extra == "all"
 Requires-Dist: tabulate==0.9.0; extra == "all"
-Requires-Dist: pure-sasl; extra == "all"
-Requires-Dist: trino[sqlalchemy]; extra == "all"
-Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "all"
-Requires-Dist: msal~=1.2; extra == "all"
-Requires-Dist: looker-sdk>=22.20.0; extra == "all"
-Requires-Dist: wheel~=0.38.4; extra == "all"
-Requires-Dist: idna<3,>=2.5; extra == "all"
-Requires-Dist: python-dateutil>=2.8.1; extra == "all"
-Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "all"
-Requires-Dist: boto3<2.0,>=1.20; extra == "all"
-Requires-Dist: typing-inspect; extra == "all"
-Requires-Dist: giturlparse; extra == "all"
-Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "all"
 Requires-Dist: fastavro>=1.2.0; extra == "all"
-Requires-Dist: PyYAML~=6.0; extra == "all"
-Requires-Dist: google-cloud-storage==1.43.0; extra == "all"
-Requires-Dist: confluent_kafka==2.1.1; extra == "all"
-Requires-Dist: hdbcli; extra == "all"
-Requires-Dist: tableau-api-lib~=0.1; extra == "all"
-Requires-Dist: impyla[kerberos]~=0.18.0; extra == "all"
-Requires-Dist: pydantic~=1.10; extra == "all"
+Requires-Dist: scikit-learn~=1.0; extra == "all"
+Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "all"
+Requires-Dist: setuptools~=66.0.0; extra == "all"
 Requires-Dist: dagster_graphql~=1.1; extra == "all"
-Requires-Dist: cached-property==1.5.2; extra == "all"
+Requires-Dist: sqlalchemy-pytds~=0.3; extra == "all"
 Requires-Dist: thrift<1,>=0.13; extra == "all"
-Requires-Dist: protobuf; extra == "all"
-Requires-Dist: mlflow-skinny>=2.3.0; extra == "all"
-Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "all"
-Requires-Dist: sqlalchemy-hana; extra == "all"
-Requires-Dist: avro~=1.11; extra == "all"
-Requires-Dist: setuptools~=66.0.0; extra == "all"
-Requires-Dist: psycopg2-binary; extra == "all"
-Requires-Dist: couchbase~=4.1; extra == "all"
 Requires-Dist: pinotdb~=0.3; extra == "all"
+Requires-Dist: packaging==21.3; extra == "all"
+Requires-Dist: requests-aws4auth~=1.1; extra == "all"
+Requires-Dist: neo4j~=5.3.0; extra == "all"
+Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "all"
+Requires-Dist: memory-profiler; extra == "all"
+Requires-Dist: importlib-metadata>=4.13.0; extra == "all"
+Requires-Dist: impyla[kerberos]~=0.18.0; extra == "all"
+Requires-Dist: couchbase~=4.1; extra == "all"
 Requires-Dist: email-validator>=1.0.3; extra == "all"
-Requires-Dist: sqlalchemy-pgspider; extra == "all"
-Requires-Dist: pyOpenSSL~=24.1.0; extra == "all"
-Requires-Dist: thrift-sasl~=0.4; extra == "all"
-Requires-Dist: azure-identity~=1.12; extra == "all"
-Requires-Dist: google-cloud-logging; extra == "all"
-Requires-Dist: azure-storage-blob~=12.14; extra == "all"
-Requires-Dist: requests>=2.23; extra == "all"
-Requires-Dist: google-auth>=1.33.0; extra == "all"
-Requires-Dist: cryptography; extra == "all"
-Requires-Dist: validators~=0.22.0; extra == "all"
+Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "all"
+Requires-Dist: mypy_extensions>=0.4.3; extra == "all"
+Requires-Dist: google-cloud; extra == "all"
+Requires-Dist: pymssql~=2.2.0; extra == "all"
+Requires-Dist: presidio-analyzer==2.2.32; extra == "all"
+Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "all"
+Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "all"
+Requires-Dist: azure-keyvault-secrets; extra == "all"
+Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "all"
+Requires-Dist: python-dateutil>=2.8.1; extra == "all"
+Requires-Dist: pymongo~=4.3; extra == "all"
 Requires-Dist: chardet==4.0.0; extra == "all"
-Requires-Dist: google>=3.0.0; extra == "all"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "all"
+Requires-Dist: azure-storage-blob; extra == "all"
+Requires-Dist: pymysql>=1.0.2; extra == "all"
+Requires-Dist: pydantic~=1.10; extra == "all"
+Requires-Dist: sqlalchemy-pgspider; extra == "all"
+Requires-Dist: hdbcli; extra == "all"
+Requires-Dist: confluent_kafka==2.1.1; extra == "all"
+Requires-Dist: Jinja2>=2.11.3; extra == "all"
+Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "all"
 Requires-Dist: pandas~=2.0.0; extra == "all"
-Requires-Dist: pyasn1~=0.6.0; extra == "all"
-Requires-Dist: delta-spark<=2.3.0; extra == "all"
+Requires-Dist: websocket-client~=1.6.1; extra == "all"
+Requires-Dist: validators~=0.22.0; extra == "all"
+Requires-Dist: protobuf; extra == "all"
+Requires-Dist: gcsfs~=2022.11; extra == "all"
+Requires-Dist: PyYAML~=6.0; extra == "all"
+Requires-Dist: oracledb~=1.2; extra == "all"
+Requires-Dist: google-cloud-logging; extra == "all"
+Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "all"
+Requires-Dist: pyOpenSSL~=24.1.0; extra == "all"
 Requires-Dist: clickhouse-driver~=0.2; extra == "all"
+Requires-Dist: presto-types-parser>=0.0.2; extra == "all"
+Requires-Dist: s3fs==0.4.2; extra == "all"
+Requires-Dist: mlflow-skinny>=2.3.0; extra == "all"
+Requires-Dist: psycopg2-binary; extra == "all"
+Requires-Dist: giturlparse; extra == "all"
+Requires-Dist: pydomo~=0.3; extra == "all"
 Requires-Dist: alembic~=1.10.2; extra == "all"
-Requires-Dist: websocket-client~=1.6.1; extra == "all"
-Requires-Dist: python-jose~=3.3; extra == "all"
-Requires-Dist: okta~=2.3; extra == "all"
-Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "all"
-Requires-Dist: importlib-metadata>=4.13.0; extra == "all"
-Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "all"
+Requires-Dist: requests>=2.23; extra == "all"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "all"
+Requires-Dist: typing-inspect; extra == "all"
+Requires-Dist: simple_salesforce==1.11.4; extra == "all"
+Requires-Dist: sqlalchemy-databricks~=0.1; extra == "all"
+Requires-Dist: pyathena==3.0.8; extra == "all"
 Requires-Dist: clickhouse-sqlalchemy~=0.2; extra == "all"
+Requires-Dist: cached-property==1.5.2; extra == "all"
 Requires-Dist: pydoris==1.0.2; extra == "all"
-Requires-Dist: sqlalchemy-pytds~=0.3; extra == "all"
-Requires-Dist: GeoAlchemy2~=0.12; extra == "all"
+Requires-Dist: thrift-sasl~=0.4; extra == "all"
+Requires-Dist: cramjam~=2.7; extra == "all"
+Requires-Dist: pyarrow~=14.0; extra == "all"
+Requires-Dist: google-cloud-storage==1.43.0; extra == "all"
+Requires-Dist: impyla~=0.18.0; extra == "all"
+Requires-Dist: azure-identity~=1.12; extra == "all"
+Requires-Dist: gitpython~=3.1.34; extra == "all"
 Requires-Dist: cachetools; extra == "all"
-Requires-Dist: pydomo~=0.3; extra == "all"
+Requires-Dist: cryptography>=42.0.0; extra == "all"
+Requires-Dist: ndg-httpsclient~=0.5.1; extra == "all"
+Requires-Dist: tableau-api-lib~=0.1; extra == "all"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "all"
 Requires-Dist: adlfs~=2022.11; extra == "all"
-Requires-Dist: memory-profiler; extra == "all"
-Requires-Dist: presto-types-parser>=0.0.2; extra == "all"
-Requires-Dist: pydruid>=0.6.5; extra == "all"
+Requires-Dist: collate-sqllineage~=1.3.0; extra == "all"
+Requires-Dist: jsonpatch<2.0,>=1.24; extra == "all"
+Requires-Dist: pure-sasl; extra == "all"
+Requires-Dist: msal~=1.2; extra == "all"
+Requires-Dist: spacy==3.5.0; extra == "all"
 Requires-Dist: dbt-artifacts-parser; extra == "all"
-Requires-Dist: sqlalchemy-databricks~=0.1; extra == "all"
-Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "all"
+Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "all"
+Requires-Dist: delta-spark<=2.3.0; extra == "all"
+Requires-Dist: azure-storage-blob~=12.14; extra == "all"
+Requires-Dist: sqlalchemy-hana; extra == "all"
+Requires-Dist: pyasn1~=0.6.0; extra == "all"
+Requires-Dist: pydruid>=0.6.5; extra == "all"
+Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "all"
+Requires-Dist: elasticsearch8~=8.9.0; extra == "all"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "all"
+Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "all"
+Requires-Dist: pyiceberg<1; extra == "all"
 Provides-Extra: slim
-Requires-Dist: jsonschema; extra == "slim"
-Requires-Dist: presidio-analyzer==2.2.32; extra == "slim"
-Requires-Dist: elasticsearch==7.13.1; extra == "slim"
-Requires-Dist: cramjam~=2.7; extra == "slim"
+Requires-Dist: trino[sqlalchemy]; extra == "slim"
+Requires-Dist: looker-sdk>=22.20.0; extra == "slim"
 Requires-Dist: lkml~=1.3; extra == "slim"
-Requires-Dist: oracledb~=1.2; extra == "slim"
-Requires-Dist: pymssql~=2.2.0; extra == "slim"
-Requires-Dist: mypy_extensions>=0.4.3; extra == "slim"
-Requires-Dist: google-cloud; extra == "slim"
-Requires-Dist: ldap3==2.9.1; extra == "slim"
-Requires-Dist: python_on_whales==0.55.0; extra == "slim"
-Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "slim"
-Requires-Dist: requests-aws4auth~=1.1; extra == "slim"
-Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "slim"
-Requires-Dist: simple_salesforce==1.11.4; extra == "slim"
-Requires-Dist: spacy==3.5.0; extra == "slim"
+Requires-Dist: boto3<2.0,>=1.20; extra == "slim"
 Requires-Dist: sqlalchemy<2,>=1.4.0; extra == "slim"
-Requires-Dist: collate-sqllineage~=1.3.0; extra == "slim"
-Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "slim"
-Requires-Dist: azure-keyvault-secrets; extra == "slim"
-Requires-Dist: neo4j~=5.3.0; extra == "slim"
-Requires-Dist: packaging==21.3; extra == "slim"
-Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "slim"
-Requires-Dist: azure-storage-blob; extra == "slim"
-Requires-Dist: pyathena==3.0.8; extra == "slim"
-Requires-Dist: pyarrow~=14.0; extra == "slim"
-Requires-Dist: croniter~=1.3.0; extra == "slim"
-Requires-Dist: ndg-httpsclient~=0.5.1; extra == "slim"
-Requires-Dist: s3fs==0.4.2; extra == "slim"
-Requires-Dist: Jinja2>=2.11.3; extra == "slim"
-Requires-Dist: jsonpatch<2.0,>=1.24; extra == "slim"
-Requires-Dist: pymongo~=4.3; extra == "slim"
-Requires-Dist: elasticsearch8~=8.9.0; extra == "slim"
-Requires-Dist: gitpython~=3.1.34; extra == "slim"
 Requires-Dist: pyodbc<5,>=4.0.35; extra == "slim"
-Requires-Dist: gcsfs~=2022.11; extra == "slim"
-Requires-Dist: pyiceberg; extra == "slim"
-Requires-Dist: pymysql>=1.0.2; extra == "slim"
-Requires-Dist: impyla~=0.18.0; extra == "slim"
 Requires-Dist: tabulate==0.9.0; extra == "slim"
-Requires-Dist: pure-sasl; extra == "slim"
-Requires-Dist: trino[sqlalchemy]; extra == "slim"
-Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "slim"
-Requires-Dist: msal~=1.2; extra == "slim"
-Requires-Dist: looker-sdk>=22.20.0; extra == "slim"
-Requires-Dist: wheel~=0.38.4; extra == "slim"
-Requires-Dist: idna<3,>=2.5; extra == "slim"
-Requires-Dist: python-dateutil>=2.8.1; extra == "slim"
-Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "slim"
-Requires-Dist: boto3<2.0,>=1.20; extra == "slim"
-Requires-Dist: typing-inspect; extra == "slim"
-Requires-Dist: giturlparse; extra == "slim"
-Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "slim"
 Requires-Dist: fastavro>=1.2.0; extra == "slim"
-Requires-Dist: PyYAML~=6.0; extra == "slim"
-Requires-Dist: google-cloud-storage==1.43.0; extra == "slim"
-Requires-Dist: confluent_kafka==2.1.1; extra == "slim"
-Requires-Dist: hdbcli; extra == "slim"
-Requires-Dist: tableau-api-lib~=0.1; extra == "slim"
-Requires-Dist: impyla[kerberos]~=0.18.0; extra == "slim"
-Requires-Dist: pydantic~=1.10; extra == "slim"
+Requires-Dist: sqlalchemy-vertica[vertica-python]>=0.0.5; extra == "slim"
+Requires-Dist: setuptools~=66.0.0; extra == "slim"
 Requires-Dist: dagster_graphql~=1.1; extra == "slim"
-Requires-Dist: cached-property==1.5.2; extra == "slim"
+Requires-Dist: sqlalchemy-pytds~=0.3; extra == "slim"
 Requires-Dist: thrift<1,>=0.13; extra == "slim"
-Requires-Dist: protobuf; extra == "slim"
-Requires-Dist: mlflow-skinny>=2.3.0; extra == "slim"
-Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "slim"
-Requires-Dist: sqlalchemy-hana; extra == "slim"
-Requires-Dist: avro~=1.11; extra == "slim"
-Requires-Dist: setuptools~=66.0.0; extra == "slim"
-Requires-Dist: psycopg2-binary; extra == "slim"
-Requires-Dist: couchbase~=4.1; extra == "slim"
 Requires-Dist: pinotdb~=0.3; extra == "slim"
+Requires-Dist: packaging==21.3; extra == "slim"
+Requires-Dist: requests-aws4auth~=1.1; extra == "slim"
+Requires-Dist: neo4j~=5.3.0; extra == "slim"
+Requires-Dist: sqlalchemy-ibmi~=0.9.3; extra == "slim"
+Requires-Dist: memory-profiler; extra == "slim"
+Requires-Dist: importlib-metadata>=4.13.0; extra == "slim"
+Requires-Dist: impyla[kerberos]~=0.18.0; extra == "slim"
+Requires-Dist: couchbase~=4.1; extra == "slim"
 Requires-Dist: email-validator>=1.0.3; extra == "slim"
-Requires-Dist: sqlalchemy-pgspider; extra == "slim"
-Requires-Dist: pyOpenSSL~=24.1.0; extra == "slim"
-Requires-Dist: thrift-sasl~=0.4; extra == "slim"
-Requires-Dist: azure-identity~=1.12; extra == "slim"
-Requires-Dist: google-cloud-logging; extra == "slim"
-Requires-Dist: azure-storage-blob~=12.14; extra == "slim"
-Requires-Dist: requests>=2.23; extra == "slim"
-Requires-Dist: google-auth>=1.33.0; extra == "slim"
-Requires-Dist: cryptography; extra == "slim"
-Requires-Dist: validators~=0.22.0; extra == "slim"
+Requires-Dist: google-cloud-datacatalog>=3.6.2; extra == "slim"
+Requires-Dist: mypy_extensions>=0.4.3; extra == "slim"
+Requires-Dist: google-cloud; extra == "slim"
+Requires-Dist: pymssql~=2.2.0; extra == "slim"
+Requires-Dist: presidio-analyzer==2.2.32; extra == "slim"
+Requires-Dist: antlr4-python3-runtime==4.9.2; extra == "slim"
+Requires-Dist: google-cloud-bigtable>=2.0.0; extra == "slim"
+Requires-Dist: azure-keyvault-secrets; extra == "slim"
+Requires-Dist: sqlalchemy-redshift==0.8.12; extra == "slim"
+Requires-Dist: python-dateutil>=2.8.1; extra == "slim"
+Requires-Dist: pymongo~=4.3; extra == "slim"
 Requires-Dist: chardet==4.0.0; extra == "slim"
-Requires-Dist: google>=3.0.0; extra == "slim"
-Requires-Dist: grpcio-tools>=1.47.2; extra == "slim"
+Requires-Dist: azure-storage-blob; extra == "slim"
+Requires-Dist: pymysql>=1.0.2; extra == "slim"
+Requires-Dist: pydantic~=1.10; extra == "slim"
+Requires-Dist: sqlalchemy-pgspider; extra == "slim"
+Requires-Dist: hdbcli; extra == "slim"
+Requires-Dist: confluent_kafka==2.1.1; extra == "slim"
+Requires-Dist: Jinja2>=2.11.3; extra == "slim"
+Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "slim"
 Requires-Dist: pandas~=2.0.0; extra == "slim"
-Requires-Dist: pyasn1~=0.6.0; extra == "slim"
+Requires-Dist: websocket-client~=1.6.1; extra == "slim"
+Requires-Dist: validators~=0.22.0; extra == "slim"
+Requires-Dist: protobuf; extra == "slim"
+Requires-Dist: gcsfs~=2022.11; extra == "slim"
+Requires-Dist: PyYAML~=6.0; extra == "slim"
+Requires-Dist: oracledb~=1.2; extra == "slim"
+Requires-Dist: google-cloud-logging; extra == "slim"
+Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "slim"
+Requires-Dist: pyOpenSSL~=24.1.0; extra == "slim"
 Requires-Dist: clickhouse-driver~=0.2; extra == "slim"
+Requires-Dist: presto-types-parser>=0.0.2; extra == "slim"
+Requires-Dist: s3fs==0.4.2; extra == "slim"
+Requires-Dist: mlflow-skinny>=2.3.0; extra == "slim"
+Requires-Dist: psycopg2-binary; extra == "slim"
+Requires-Dist: giturlparse; extra == "slim"
+Requires-Dist: pydomo~=0.3; extra == "slim"
 Requires-Dist: alembic~=1.10.2; extra == "slim"
-Requires-Dist: websocket-client~=1.6.1; extra == "slim"
-Requires-Dist: python-jose~=3.3; extra == "slim"
-Requires-Dist: okta~=2.3; extra == "slim"
-Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "slim"
-Requires-Dist: importlib-metadata>=4.13.0; extra == "slim"
-Requires-Dist: databricks-sdk<0.20.0,>=0.18.0; extra == "slim"
+Requires-Dist: requests>=2.23; extra == "slim"
+Requires-Dist: grpcio-tools>=1.47.2; extra == "slim"
+Requires-Dist: typing-inspect; extra == "slim"
+Requires-Dist: simple_salesforce==1.11.4; extra == "slim"
+Requires-Dist: sqlalchemy-databricks~=0.1; extra == "slim"
+Requires-Dist: pyathena==3.0.8; extra == "slim"
 Requires-Dist: clickhouse-sqlalchemy~=0.2; extra == "slim"
+Requires-Dist: cached-property==1.5.2; extra == "slim"
 Requires-Dist: pydoris==1.0.2; extra == "slim"
-Requires-Dist: sqlalchemy-pytds~=0.3; extra == "slim"
-Requires-Dist: GeoAlchemy2~=0.12; extra == "slim"
+Requires-Dist: thrift-sasl~=0.4; extra == "slim"
+Requires-Dist: cramjam~=2.7; extra == "slim"
+Requires-Dist: pyarrow~=14.0; extra == "slim"
+Requires-Dist: google-cloud-storage==1.43.0; extra == "slim"
+Requires-Dist: impyla~=0.18.0; extra == "slim"
+Requires-Dist: azure-identity~=1.12; extra == "slim"
+Requires-Dist: gitpython~=3.1.34; extra == "slim"
 Requires-Dist: cachetools; extra == "slim"
-Requires-Dist: pydomo~=0.3; extra == "slim"
+Requires-Dist: cryptography>=42.0.0; extra == "slim"
+Requires-Dist: ndg-httpsclient~=0.5.1; extra == "slim"
+Requires-Dist: tableau-api-lib~=0.1; extra == "slim"
+Requires-Dist: GeoAlchemy2~=0.12; extra == "slim"
 Requires-Dist: adlfs~=2022.11; extra == "slim"
-Requires-Dist: memory-profiler; extra == "slim"
-Requires-Dist: presto-types-parser>=0.0.2; extra == "slim"
-Requires-Dist: pydruid>=0.6.5; extra == "slim"
+Requires-Dist: collate-sqllineage~=1.3.0; extra == "slim"
+Requires-Dist: jsonpatch<2.0,>=1.24; extra == "slim"
+Requires-Dist: pure-sasl; extra == "slim"
+Requires-Dist: msal~=1.2; extra == "slim"
+Requires-Dist: spacy==3.5.0; extra == "slim"
 Requires-Dist: dbt-artifacts-parser; extra == "slim"
-Requires-Dist: sqlalchemy-databricks~=0.1; extra == "slim"
-Requires-Dist: pyhive[hive_pure_sasl]~=0.7; extra == "slim"
+Requires-Dist: snowflake-sqlalchemy~=1.4; extra == "slim"
+Requires-Dist: azure-storage-blob~=12.14; extra == "slim"
+Requires-Dist: sqlalchemy-hana; extra == "slim"
+Requires-Dist: pyasn1~=0.6.0; extra == "slim"
+Requires-Dist: pydruid>=0.6.5; extra == "slim"
+Requires-Dist: cx_Oracle<9,>=8.3.0; extra == "slim"
+Requires-Dist: elasticsearch8~=8.9.0; extra == "slim"
+Requires-Dist: avro<1.12,>=1.11.3; extra == "slim"
+Requires-Dist: sqlalchemy-bigquery>=1.2.2; extra == "slim"
+Requires-Dist: pyiceberg<1; extra == "slim"
 
 ---
 This guide will help you setup the Ingestion framework and connectors
 ---
 
-![Python version 3.7+](https://img.shields.io/badge/python-3.7%2B-blue)
+![Python version 3.8+](https://img.shields.io/badge/python-3.8%2B-blue)
 
 OpenMetadata Ingestion is a simple framework to build connectors and ingest metadata of various systems through OpenMetadata APIs. It could be used in an orchestration framework(e.g. Apache Airflow) to ingest metadata.
 **Prerequisites**
 
-- Python &gt;= 3.7.x
+- Python &gt;= 3.8.x
 
 ### Docs
 
 Please refer to the documentation here https://docs.open-metadata.org/connectors
 
 <img referrerpolicy="no-referrer-when-downgrade" src="https://static.scarf.sh/a.png?x-pxid=c1a30c7c-6dc7-4928-95bf-6ee08ca6aa6a" />
+
+### TopologyRunner
+
+All the Ingestion Workflows run through the TopologyRunner.
+
+The flow is depicted in the images below.
+
+**TopologyRunner Standard Flow**
+
+![image](../openmetadata-docs/images/v1.4/features/ingestion/workflows/metadata/multithreading/single-thread-flow.png)
+
+**TopologyRunner Multithread Flow**
+
+![image](../openmetadata-docs/images/v1.4/features/ingestion/workflows/metadata/multithreading/multi-thread-flow.png)
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/SOURCES.txt` & `openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -22,20 +22,17 @@
 ./src/metadata/applications/auto_tagger.py
 ./src/metadata/automations/runner.py
 ./src/metadata/cli/__init__.py
 ./src/metadata/cli/app.py
 ./src/metadata/cli/backup.py
 ./src/metadata/cli/dataquality.py
 ./src/metadata/cli/db_dump.py
-./src/metadata/cli/docker.py
 ./src/metadata/cli/ingest.py
 ./src/metadata/cli/insight.py
 ./src/metadata/cli/lineage.py
-./src/metadata/cli/openmetadata_dag_config_migration.py
-./src/metadata/cli/openmetadata_imports_migration.py
 ./src/metadata/cli/profile.py
 ./src/metadata/cli/restore.py
 ./src/metadata/cli/usage.py
 ./src/metadata/cli/utils.py
 ./src/metadata/clients/aws_client.py
 ./src/metadata/clients/azure_client.py
 ./src/metadata/clients/domo_client.py
@@ -210,14 +207,15 @@
 ./src/metadata/examples/workflows/pinotdb.yaml
 ./src/metadata/examples/workflows/postgres.yaml
 ./src/metadata/examples/workflows/postgres_lineage.yaml
 ./src/metadata/examples/workflows/postgres_usage.yaml
 ./src/metadata/examples/workflows/powerbi.yaml
 ./src/metadata/examples/workflows/presto.yaml
 ./src/metadata/examples/workflows/qlik_sense.yaml
+./src/metadata/examples/workflows/qlikcloud.yaml
 ./src/metadata/examples/workflows/query_log_usage.yaml
 ./src/metadata/examples/workflows/quicksight.yaml
 ./src/metadata/examples/workflows/redash.yaml
 ./src/metadata/examples/workflows/redpanda.yaml
 ./src/metadata/examples/workflows/redshift.yaml
 ./src/metadata/examples/workflows/redshift_lineage.yaml
 ./src/metadata/examples/workflows/redshift_profiler.yaml
@@ -377,17 +375,20 @@
 ./src/metadata/generated/schema/configuration/fernetConfiguration.py
 ./src/metadata/generated/schema/configuration/jwtTokenConfiguration.py
 ./src/metadata/generated/schema/configuration/kafkaEventConfiguration.py
 ./src/metadata/generated/schema/configuration/ldapConfiguration.py
 ./src/metadata/generated/schema/configuration/loginConfiguration.py
 ./src/metadata/generated/schema/configuration/logoConfiguration.py
 ./src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py
+./src/metadata/generated/schema/configuration/profilerConfiguration.py
 ./src/metadata/generated/schema/configuration/slackAppConfiguration.py
 ./src/metadata/generated/schema/configuration/taskNotificationConfiguration.py
 ./src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py
+./src/metadata/generated/schema/configuration/themeConfiguration.py
+./src/metadata/generated/schema/configuration/uiThemePreference.py
 ./src/metadata/generated/schema/configuration/ldapTrustStoreConfig/__init__.py
 ./src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py
 ./src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py
 ./src/metadata/generated/schema/configuration/ldapTrustStoreConfig/jvmDefaultConfig.py
 ./src/metadata/generated/schema/configuration/ldapTrustStoreConfig/trustAllConfig.py
 ./src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py
 ./src/metadata/generated/schema/dataInsight/__init__.py
@@ -424,16 +425,29 @@
 ./src/metadata/generated/schema/entity/applications/createAppRequest.py
 ./src/metadata/generated/schema/entity/applications/jobStatus.py
 ./src/metadata/generated/schema/entity/applications/liveExecutionContext.py
 ./src/metadata/generated/schema/entity/applications/scheduledExecutionContext.py
 ./src/metadata/generated/schema/entity/applications/configuration/__init__.py
 ./src/metadata/generated/schema/entity/applications/configuration/applicationConfig.py
 ./src/metadata/generated/schema/entity/applications/configuration/external/__init__.py
-./src/metadata/generated/schema/entity/applications/configuration/external/autoTaggerAppConfig.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automatorAppConfig.py
 ./src/metadata/generated/schema/entity/applications/configuration/external/metaPilotAppConfig.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/__init__.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/addDescriptionAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/addDomainAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/addOwnerAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/addTagsAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/addTierAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/lineagePropagationAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/mlTaggingAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/removeDescriptionAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/removeDomainAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/removeOwnerAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/removeTagsAction.py
+./src/metadata/generated/schema/entity/applications/configuration/external/automator/removeTierAction.py
 ./src/metadata/generated/schema/entity/applications/configuration/internal/__init__.py
 ./src/metadata/generated/schema/entity/applications/configuration/internal/dataInsightsAppConfig.py
 ./src/metadata/generated/schema/entity/applications/configuration/internal/dataInsightsReportAppConfig.py
 ./src/metadata/generated/schema/entity/applications/configuration/internal/searchIndexingAppConfig.py
 ./src/metadata/generated/schema/entity/applications/configuration/private/external/__init__.py
 ./src/metadata/generated/schema/entity/applications/configuration/private/external/metaPilotAppPrivateConfig.py
 ./src/metadata/generated/schema/entity/applications/marketplace/__init__.py
@@ -467,15 +481,23 @@
 ./src/metadata/generated/schema/entity/docStore/document.py
 ./src/metadata/generated/schema/entity/domains/__init__.py
 ./src/metadata/generated/schema/entity/domains/dataProduct.py
 ./src/metadata/generated/schema/entity/domains/domain.py
 ./src/metadata/generated/schema/entity/events/__init__.py
 ./src/metadata/generated/schema/entity/events/webhook.py
 ./src/metadata/generated/schema/entity/feed/__init__.py
+./src/metadata/generated/schema/entity/feed/assets.py
+./src/metadata/generated/schema/entity/feed/customProperty.py
+./src/metadata/generated/schema/entity/feed/description.py
+./src/metadata/generated/schema/entity/feed/domain.py
+./src/metadata/generated/schema/entity/feed/entityInfo.py
+./src/metadata/generated/schema/entity/feed/owner.py
 ./src/metadata/generated/schema/entity/feed/suggestion.py
+./src/metadata/generated/schema/entity/feed/tag.py
+./src/metadata/generated/schema/entity/feed/testCaseResult.py
 ./src/metadata/generated/schema/entity/feed/thread.py
 ./src/metadata/generated/schema/entity/policies/__init__.py
 ./src/metadata/generated/schema/entity/policies/filters.py
 ./src/metadata/generated/schema/entity/policies/policy.py
 ./src/metadata/generated/schema/entity/policies/accessControl/__init__.py
 ./src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py
 ./src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py
@@ -504,19 +526,25 @@
 ./src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/lightdashConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/mstrConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py
+./src/metadata/generated/schema/entity/services/connections/dashboard/qlikCloudConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/qlikSenseConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py
 ./src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py
+./src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/__init__.py
+./src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/azureConfig.py
+./src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/bucketDetails.py
+./src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/gcsConfig.py
+./src/metadata/generated/schema/entity/services/connections/dashboard/powerbi/s3Config.py
 ./src/metadata/generated/schema/entity/services/connections/database/__init__.py
 ./src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py
 ./src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py
 ./src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py
 ./src/metadata/generated/schema/entity/services/connections/database/bigTableConnection.py
 ./src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py
 ./src/metadata/generated/schema/entity/services/connections/database/couchbaseConnection.py
@@ -595,15 +623,17 @@
 ./src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py
 ./src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py
 ./src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py
 ./src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py
 ./src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py
 ./src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py
 ./src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py
+./src/metadata/generated/schema/entity/services/connections/pipeline/kafkaConnectConnection.py
 ./src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py
+./src/metadata/generated/schema/entity/services/connections/pipeline/openLineageConnection.py
 ./src/metadata/generated/schema/entity/services/connections/pipeline/sparkConnection.py
 ./src/metadata/generated/schema/entity/services/connections/pipeline/splineConnection.py
 ./src/metadata/generated/schema/entity/services/connections/search/__init__.py
 ./src/metadata/generated/schema/entity/services/connections/search/customSearchConnection.py
 ./src/metadata/generated/schema/entity/services/connections/search/elasticSearchConnection.py
 ./src/metadata/generated/schema/entity/services/connections/search/openSearchConnection.py
 ./src/metadata/generated/schema/entity/services/connections/search/elasticSearch/__init__.py
@@ -735,14 +765,15 @@
 ./src/metadata/generated/schema/type/csvDocumentation.py
 ./src/metadata/generated/schema/type/csvErrorType.py
 ./src/metadata/generated/schema/type/csvFile.py
 ./src/metadata/generated/schema/type/csvImportResult.py
 ./src/metadata/generated/schema/type/customProperty.py
 ./src/metadata/generated/schema/type/dailyCount.py
 ./src/metadata/generated/schema/type/databaseConnectionConfig.py
+./src/metadata/generated/schema/type/entityHierarchy.py
 ./src/metadata/generated/schema/type/entityHistory.py
 ./src/metadata/generated/schema/type/entityLineage.py
 ./src/metadata/generated/schema/type/entityReference.py
 ./src/metadata/generated/schema/type/entityReferenceList.py
 ./src/metadata/generated/schema/type/entityRelationship.py
 ./src/metadata/generated/schema/type/entityUsage.py
 ./src/metadata/generated/schema/type/filterPattern.py
@@ -759,14 +790,15 @@
 ./src/metadata/generated/schema/type/tableQuery.py
 ./src/metadata/generated/schema/type/tableUsageCount.py
 ./src/metadata/generated/schema/type/tagLabel.py
 ./src/metadata/generated/schema/type/usageDetails.py
 ./src/metadata/generated/schema/type/usageRequest.py
 ./src/metadata/generated/schema/type/votes.py
 ./src/metadata/generated/schema/type/customProperties/__init__.py
+./src/metadata/generated/schema/type/customProperties/complexTypes.py
 ./src/metadata/generated/schema/type/customProperties/enumConfig.py
 ./src/metadata/great_expectations/__init__.py
 ./src/metadata/great_expectations/action.py
 ./src/metadata/great_expectations/utils/__init__.py
 ./src/metadata/great_expectations/utils/ometa_config_handler.py
 ./src/metadata/ingestion/api/closeable.py
 ./src/metadata/ingestion/api/common.py
@@ -870,16 +902,23 @@
 ./src/metadata/ingestion/source/dashboard/mstr/client.py
 ./src/metadata/ingestion/source/dashboard/mstr/connection.py
 ./src/metadata/ingestion/source/dashboard/mstr/metadata.py
 ./src/metadata/ingestion/source/dashboard/mstr/models.py
 ./src/metadata/ingestion/source/dashboard/powerbi/__init__.py
 ./src/metadata/ingestion/source/dashboard/powerbi/client.py
 ./src/metadata/ingestion/source/dashboard/powerbi/connection.py
+./src/metadata/ingestion/source/dashboard/powerbi/file_client.py
 ./src/metadata/ingestion/source/dashboard/powerbi/metadata.py
 ./src/metadata/ingestion/source/dashboard/powerbi/models.py
+./src/metadata/ingestion/source/dashboard/qlikcloud/__init__.py
+./src/metadata/ingestion/source/dashboard/qlikcloud/client.py
+./src/metadata/ingestion/source/dashboard/qlikcloud/connection.py
+./src/metadata/ingestion/source/dashboard/qlikcloud/constants.py
+./src/metadata/ingestion/source/dashboard/qlikcloud/metadata.py
+./src/metadata/ingestion/source/dashboard/qlikcloud/models.py
 ./src/metadata/ingestion/source/dashboard/qliksense/client.py
 ./src/metadata/ingestion/source/dashboard/qliksense/connection.py
 ./src/metadata/ingestion/source/dashboard/qliksense/constants.py
 ./src/metadata/ingestion/source/dashboard/qliksense/metadata.py
 ./src/metadata/ingestion/source/dashboard/qliksense/models.py
 ./src/metadata/ingestion/source/dashboard/quicksight/__init__.py
 ./src/metadata/ingestion/source/dashboard/quicksight/connection.py
@@ -906,14 +945,16 @@
 ./src/metadata/ingestion/source/dashboard/tableau/queries.py
 ./src/metadata/ingestion/source/database/column_helpers.py
 ./src/metadata/ingestion/source/database/column_type_parser.py
 ./src/metadata/ingestion/source/database/common_db_source.py
 ./src/metadata/ingestion/source/database/common_nosql_source.py
 ./src/metadata/ingestion/source/database/database_service.py
 ./src/metadata/ingestion/source/database/extended_sample_data.py
+./src/metadata/ingestion/source/database/external_table_lineage_mixin.py
+./src/metadata/ingestion/source/database/incremental_metadata_extraction.py
 ./src/metadata/ingestion/source/database/life_cycle_query_mixin.py
 ./src/metadata/ingestion/source/database/lineage_source.py
 ./src/metadata/ingestion/source/database/multi_db_source.py
 ./src/metadata/ingestion/source/database/query_parser_source.py
 ./src/metadata/ingestion/source/database/sample_data.py
 ./src/metadata/ingestion/source/database/sample_usage.py
 ./src/metadata/ingestion/source/database/sql_column_handler.py
@@ -934,14 +975,15 @@
 ./src/metadata/ingestion/source/database/azuresql/metadata.py
 ./src/metadata/ingestion/source/database/azuresql/queries.py
 ./src/metadata/ingestion/source/database/azuresql/query_parser.py
 ./src/metadata/ingestion/source/database/azuresql/usage.py
 ./src/metadata/ingestion/source/database/bigquery/__init__.py
 ./src/metadata/ingestion/source/database/bigquery/connection.py
 ./src/metadata/ingestion/source/database/bigquery/helper.py
+./src/metadata/ingestion/source/database/bigquery/incremental_table_processor.py
 ./src/metadata/ingestion/source/database/bigquery/lineage.py
 ./src/metadata/ingestion/source/database/bigquery/metadata.py
 ./src/metadata/ingestion/source/database/bigquery/models.py
 ./src/metadata/ingestion/source/database/bigquery/queries.py
 ./src/metadata/ingestion/source/database/bigquery/query_parser.py
 ./src/metadata/ingestion/source/database/bigquery/usage.py
 ./src/metadata/ingestion/source/database/bigtable/__init__.py
@@ -1087,14 +1129,15 @@
 ./src/metadata/ingestion/source/database/presto/metadata.py
 ./src/metadata/ingestion/source/database/presto/queries.py
 ./src/metadata/ingestion/source/database/query/__init__.py
 ./src/metadata/ingestion/source/database/query/lineage.py
 ./src/metadata/ingestion/source/database/query/usage.py
 ./src/metadata/ingestion/source/database/redshift/__init__.py
 ./src/metadata/ingestion/source/database/redshift/connection.py
+./src/metadata/ingestion/source/database/redshift/incremental_table_processor.py
 ./src/metadata/ingestion/source/database/redshift/lineage.py
 ./src/metadata/ingestion/source/database/redshift/metadata.py
 ./src/metadata/ingestion/source/database/redshift/models.py
 ./src/metadata/ingestion/source/database/redshift/queries.py
 ./src/metadata/ingestion/source/database/redshift/query_parser.py
 ./src/metadata/ingestion/source/database/redshift/usage.py
 ./src/metadata/ingestion/source/database/redshift/utils.py
@@ -1204,14 +1247,19 @@
 ./src/metadata/ingestion/source/pipeline/gluepipeline/__init__.py
 ./src/metadata/ingestion/source/pipeline/gluepipeline/connection.py
 ./src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py
 ./src/metadata/ingestion/source/pipeline/nifi/__init__.py
 ./src/metadata/ingestion/source/pipeline/nifi/client.py
 ./src/metadata/ingestion/source/pipeline/nifi/connection.py
 ./src/metadata/ingestion/source/pipeline/nifi/metadata.py
+./src/metadata/ingestion/source/pipeline/openlineage/__init__.py
+./src/metadata/ingestion/source/pipeline/openlineage/connection.py
+./src/metadata/ingestion/source/pipeline/openlineage/metadata.py
+./src/metadata/ingestion/source/pipeline/openlineage/models.py
+./src/metadata/ingestion/source/pipeline/openlineage/utils.py
 ./src/metadata/ingestion/source/pipeline/spline/__init__.py
 ./src/metadata/ingestion/source/pipeline/spline/client.py
 ./src/metadata/ingestion/source/pipeline/spline/connection.py
 ./src/metadata/ingestion/source/pipeline/spline/metadata.py
 ./src/metadata/ingestion/source/pipeline/spline/models.py
 ./src/metadata/ingestion/source/pipeline/spline/utils.py
 ./src/metadata/ingestion/source/search/search_service.py
@@ -1232,18 +1280,27 @@
 ./src/metadata/parsers/schema_parsers.py
 ./src/metadata/pii/constants.py
 ./src/metadata/pii/models.py
 ./src/metadata/pii/ner.py
 ./src/metadata/pii/processor.py
 ./src/metadata/pii/scanners/column_name_scanner.py
 ./src/metadata/pii/scanners/ner_scanner.py
+./src/metadata/profiler/__init__.py
+./src/metadata/profiler/factory.py
 ./src/metadata/profiler/registry.py
+./src/metadata/profiler/adaptors/__init__.py
+./src/metadata/profiler/adaptors/adaptor_factory.py
+./src/metadata/profiler/adaptors/dynamodb.py
+./src/metadata/profiler/adaptors/factory.py
+./src/metadata/profiler/adaptors/mongodb.py
+./src/metadata/profiler/adaptors/nosql_adaptor.py
 ./src/metadata/profiler/api/models.py
 ./src/metadata/profiler/interface/profiler_interface.py
 ./src/metadata/profiler/interface/profiler_interface_factory.py
+./src/metadata/profiler/interface/nosql/profiler_interface.py
 ./src/metadata/profiler/interface/pandas/profiler_interface.py
 ./src/metadata/profiler/interface/sqlalchemy/profiler_interface.py
 ./src/metadata/profiler/interface/sqlalchemy/bigquery/profiler_interface.py
 ./src/metadata/profiler/interface/sqlalchemy/databricks/profiler_interface.py
 ./src/metadata/profiler/interface/sqlalchemy/db2/__init__.py
 ./src/metadata/profiler/interface/sqlalchemy/db2/profiler_interface.py
 ./src/metadata/profiler/interface/sqlalchemy/mariadb/profiler_interface.py
@@ -1276,14 +1333,15 @@
 ./src/metadata/profiler/metrics/static/max_length.py
 ./src/metadata/profiler/metrics/static/mean.py
 ./src/metadata/profiler/metrics/static/min.py
 ./src/metadata/profiler/metrics/static/min_length.py
 ./src/metadata/profiler/metrics/static/not_like_count.py
 ./src/metadata/profiler/metrics/static/not_regexp_match_count.py
 ./src/metadata/profiler/metrics/static/null_count.py
+./src/metadata/profiler/metrics/static/null_missing_count.py
 ./src/metadata/profiler/metrics/static/regexp_match_count.py
 ./src/metadata/profiler/metrics/static/row_count.py
 ./src/metadata/profiler/metrics/static/stddev.py
 ./src/metadata/profiler/metrics/static/sum.py
 ./src/metadata/profiler/metrics/static/unique_count.py
 ./src/metadata/profiler/metrics/system/dml_operation.py
 ./src/metadata/profiler/metrics/system/system.py
@@ -1296,14 +1354,15 @@
 ./src/metadata/profiler/metrics/window/third_quartile.py
 ./src/metadata/profiler/orm/registry.py
 ./src/metadata/profiler/orm/converter/base.py
 ./src/metadata/profiler/orm/converter/common.py
 ./src/metadata/profiler/orm/converter/converter_registry.py
 ./src/metadata/profiler/orm/converter/bigquery/converter.py
 ./src/metadata/profiler/orm/converter/mssql/converter.py
+./src/metadata/profiler/orm/converter/redshift/converter.py
 ./src/metadata/profiler/orm/converter/snowflake/converter.py
 ./src/metadata/profiler/orm/functions/concat.py
 ./src/metadata/profiler/orm/functions/conn_test.py
 ./src/metadata/profiler/orm/functions/count.py
 ./src/metadata/profiler/orm/functions/datetime.py
 ./src/metadata/profiler/orm/functions/length.py
 ./src/metadata/profiler/orm/functions/median.py
@@ -1319,20 +1378,22 @@
 ./src/metadata/profiler/orm/types/custom_image.py
 ./src/metadata/profiler/orm/types/custom_ip.py
 ./src/metadata/profiler/orm/types/custom_timestamp.py
 ./src/metadata/profiler/orm/types/uuid.py
 ./src/metadata/profiler/processor/core.py
 ./src/metadata/profiler/processor/default.py
 ./src/metadata/profiler/processor/handle_partition.py
+./src/metadata/profiler/processor/metric_filter.py
 ./src/metadata/profiler/processor/models.py
 ./src/metadata/profiler/processor/processor.py
 ./src/metadata/profiler/processor/runner.py
 ./src/metadata/profiler/processor/sample_data_handler.py
 ./src/metadata/profiler/processor/sampler/sampler_factory.py
 ./src/metadata/profiler/processor/sampler/sampler_interface.py
+./src/metadata/profiler/processor/sampler/nosql/sampler.py
 ./src/metadata/profiler/processor/sampler/pandas/sampler.py
 ./src/metadata/profiler/processor/sampler/sqlalchemy/sampler.py
 ./src/metadata/profiler/processor/sampler/sqlalchemy/bigquery/sampler.py
 ./src/metadata/profiler/processor/sampler/sqlalchemy/trino/sampler.py
 ./src/metadata/profiler/source/metadata.py
 ./src/metadata/profiler/source/metadata_ext.py
 ./src/metadata/profiler/source/profiler_source_factory.py
@@ -1396,19 +1457,21 @@
 ./src/metadata/utils/profiler_utils.py
 ./src/metadata/utils/s3_utils.py
 ./src/metadata/utils/singleton.py
 ./src/metadata/utils/source_hash.py
 ./src/metadata/utils/sqa_like_column.py
 ./src/metadata/utils/sqa_utils.py
 ./src/metadata/utils/sqlalchemy_utils.py
+./src/metadata/utils/ssl_manager.py
 ./src/metadata/utils/ssl_registry.py
 ./src/metadata/utils/storage_metadata_config.py
 ./src/metadata/utils/stored_procedures.py
 ./src/metadata/utils/tag_utils.py
 ./src/metadata/utils/test_suite.py
+./src/metadata/utils/test_utils.py
 ./src/metadata/utils/time_utils.py
 ./src/metadata/utils/timeout.py
 ./src/metadata/utils/uuid_encoder.py
 ./src/metadata/utils/datalake/__init__.py
 ./src/metadata/utils/datalake/datalake_utils.py
 ./src/metadata/utils/secrets/__init__.py
 ./src/metadata/utils/secrets/aws_based_secrets_manager.py
```

### Comparing `openmetadata-ingestion-1.3.4.0/src/openmetadata_ingestion.egg-info/requires.txt` & `openmetadata-ingestion-1.4.0.0rc1/src/openmetadata_ingestion.egg-info/requires.txt`

 * *Files 12% similar despite different names*

```diff
@@ -1,158 +1,138 @@
-jsonschema
-pydantic~=1.10
-cached-property==1.5.2
-mypy_extensions>=0.4.3
-requests-aws4auth~=1.1
+boto3<2.0,>=1.20
 sqlalchemy<2,>=1.4.0
-collate-sqllineage~=1.3.0
-azure-keyvault-secrets
-avro~=1.11
+tabulate==0.9.0
+requests>=2.23
 setuptools~=66.0.0
-antlr4-python3-runtime==4.9.2
+typing-inspect
+requests-aws4auth~=1.1
+memory-profiler
+importlib-metadata>=4.13.0
+cached-property==1.5.2
 email-validator>=1.0.3
+mypy_extensions>=0.4.3
 azure-identity~=1.12
-croniter~=1.3.0
-requests>=2.23
-google-auth>=1.33.0
-Jinja2>=2.11.3
-jsonpatch<2.0,>=1.24
-cryptography
+antlr4-python3-runtime==4.9.2
+azure-keyvault-secrets
+cryptography>=42.0.0
+python-dateutil>=2.8.1
 chardet==4.0.0
-google>=3.0.0
-grpcio-tools>=1.47.2
 pymysql>=1.0.2
-python-jose~=3.3
-wheel~=0.38.4
-idna<3,>=2.5
-python-dateutil>=2.8.1
-importlib-metadata>=4.13.0
-boto3<2.0,>=1.20
-typing-inspect
-memory-profiler
+pydantic~=1.10
+collate-sqllineage~=1.3.0
+jsonpatch<2.0,>=1.24
+Jinja2>=2.11.3
 PyYAML~=6.0
-tabulate==0.9.0
 
 [airflow]
-attrs
 apache-airflow==2.7.3
+attrs
 
 [all]
-jsonschema
-presidio-analyzer==2.2.32
-elasticsearch==7.13.1
-cramjam~=2.7
+trino[sqlalchemy]
+looker-sdk>=22.20.0
 lkml~=1.3
-oracledb~=1.2
-pymssql~=2.2.0
-mypy_extensions>=0.4.3
-google-cloud
-ldap3==2.9.1
-python_on_whales==0.55.0
-sqlalchemy-redshift==0.8.12
-requests-aws4auth~=1.1
-snowflake-sqlalchemy~=1.4
-simple_salesforce==1.11.4
-spacy==3.5.0
+boto3<2.0,>=1.20
 sqlalchemy<2,>=1.4.0
-collate-sqllineage~=1.3.0
-google-cloud-bigtable>=2.0.0
-azure-keyvault-secrets
-neo4j~=5.3.0
-packaging==21.3
-antlr4-python3-runtime==4.9.2
-azure-storage-blob
-pyathena==3.0.8
-pyarrow~=14.0
-croniter~=1.3.0
-ndg-httpsclient~=0.5.1
-s3fs==0.4.2
-Jinja2>=2.11.3
-jsonpatch<2.0,>=1.24
-pymongo~=4.3
-scikit-learn~=1.0
-elasticsearch8~=8.9.0
-gitpython~=3.1.34
 pyodbc<5,>=4.0.35
-gcsfs~=2022.11
-pyiceberg
-pymysql>=1.0.2
-impyla~=0.18.0
 tabulate==0.9.0
-pure-sasl
-trino[sqlalchemy]
-sqlalchemy-vertica[vertica-python]>=0.0.5
-msal~=1.2
-looker-sdk>=22.20.0
-wheel~=0.38.4
-idna<3,>=2.5
-python-dateutil>=2.8.1
-sqlalchemy-bigquery>=1.2.2
-boto3<2.0,>=1.20
-typing-inspect
-giturlparse
-sqlalchemy-ibmi~=0.9.3
 fastavro>=1.2.0
-PyYAML~=6.0
-google-cloud-storage==1.43.0
-confluent_kafka==2.1.1
-hdbcli
-tableau-api-lib~=0.1
-impyla[kerberos]~=0.18.0
-pydantic~=1.10
+scikit-learn~=1.0
+sqlalchemy-vertica[vertica-python]>=0.0.5
+setuptools~=66.0.0
 dagster_graphql~=1.1
-cached-property==1.5.2
+sqlalchemy-pytds~=0.3
 thrift<1,>=0.13
-protobuf
-mlflow-skinny>=2.3.0
-google-cloud-datacatalog>=3.6.2
-sqlalchemy-hana
-avro~=1.11
-setuptools~=66.0.0
-psycopg2-binary
-couchbase~=4.1
 pinotdb~=0.3
+packaging==21.3
+requests-aws4auth~=1.1
+neo4j~=5.3.0
+sqlalchemy-ibmi~=0.9.3
+memory-profiler
+importlib-metadata>=4.13.0
+impyla[kerberos]~=0.18.0
+couchbase~=4.1
 email-validator>=1.0.3
-sqlalchemy-pgspider
-pyOpenSSL~=24.1.0
-thrift-sasl~=0.4
-azure-identity~=1.12
-google-cloud-logging
-azure-storage-blob~=12.14
-requests>=2.23
-google-auth>=1.33.0
-cryptography
-validators~=0.22.0
+google-cloud-datacatalog>=3.6.2
+mypy_extensions>=0.4.3
+google-cloud
+pymssql~=2.2.0
+presidio-analyzer==2.2.32
+antlr4-python3-runtime==4.9.2
+google-cloud-bigtable>=2.0.0
+azure-keyvault-secrets
+sqlalchemy-redshift==0.8.12
+python-dateutil>=2.8.1
+pymongo~=4.3
 chardet==4.0.0
-google>=3.0.0
-grpcio-tools>=1.47.2
+azure-storage-blob
+pymysql>=1.0.2
+pydantic~=1.10
+sqlalchemy-pgspider
+hdbcli
+confluent_kafka==2.1.1
+Jinja2>=2.11.3
+databricks-sdk<0.20.0,>=0.18.0
 pandas~=2.0.0
-pyasn1~=0.6.0
-delta-spark<=2.3.0
+websocket-client~=1.6.1
+validators~=0.22.0
+protobuf
+gcsfs~=2022.11
+PyYAML~=6.0
+oracledb~=1.2
+google-cloud-logging
+pyhive[hive_pure_sasl]~=0.7
+pyOpenSSL~=24.1.0
 clickhouse-driver~=0.2
+presto-types-parser>=0.0.2
+s3fs==0.4.2
+mlflow-skinny>=2.3.0
+psycopg2-binary
+giturlparse
+pydomo~=0.3
 alembic~=1.10.2
-websocket-client~=1.6.1
-python-jose~=3.3
-okta~=2.3
-cx_Oracle<9,>=8.3.0
-importlib-metadata>=4.13.0
-databricks-sdk<0.20.0,>=0.18.0
+requests>=2.23
+grpcio-tools>=1.47.2
+typing-inspect
+simple_salesforce==1.11.4
+sqlalchemy-databricks~=0.1
+pyathena==3.0.8
 clickhouse-sqlalchemy~=0.2
+cached-property==1.5.2
 pydoris==1.0.2
-sqlalchemy-pytds~=0.3
-GeoAlchemy2~=0.12
+thrift-sasl~=0.4
+cramjam~=2.7
+pyarrow~=14.0
+google-cloud-storage==1.43.0
+impyla~=0.18.0
+azure-identity~=1.12
+gitpython~=3.1.34
 cachetools
-pydomo~=0.3
+cryptography>=42.0.0
+ndg-httpsclient~=0.5.1
+tableau-api-lib~=0.1
+GeoAlchemy2~=0.12
 adlfs~=2022.11
-memory-profiler
-presto-types-parser>=0.0.2
-pydruid>=0.6.5
+collate-sqllineage~=1.3.0
+jsonpatch<2.0,>=1.24
+pure-sasl
+msal~=1.2
+spacy==3.5.0
 dbt-artifacts-parser
-sqlalchemy-databricks~=0.1
-pyhive[hive_pure_sasl]~=0.7
+snowflake-sqlalchemy~=1.4
+delta-spark<=2.3.0
+azure-storage-blob~=12.14
+sqlalchemy-hana
+pyasn1~=0.6.0
+pydruid>=0.6.5
+cx_Oracle<9,>=8.3.0
+elasticsearch8~=8.9.0
+avro<1.12,>=1.11.3
+sqlalchemy-bigquery>=1.2.2
+pyiceberg<1
 
 [amundsen]
 neo4j~=5.3.0
 
 [athena]
 pyathena==3.0.8
 
@@ -161,141 +141,134 @@
 [azure-sso]
 msal~=1.2
 
 [azuresql]
 pyodbc<5,>=4.0.35
 
 [backup]
-azure-identity~=1.12
-azure-storage-blob
 boto3<2.0,>=1.20
+azure-storage-blob
+azure-identity~=1.12
 
 [base]
-jsonschema
-pydantic~=1.10
-cached-property==1.5.2
-mypy_extensions>=0.4.3
-requests-aws4auth~=1.1
+boto3<2.0,>=1.20
 sqlalchemy<2,>=1.4.0
-collate-sqllineage~=1.3.0
-azure-keyvault-secrets
-avro~=1.11
+tabulate==0.9.0
+requests>=2.23
 setuptools~=66.0.0
-antlr4-python3-runtime==4.9.2
+typing-inspect
+requests-aws4auth~=1.1
+memory-profiler
+importlib-metadata>=4.13.0
+cached-property==1.5.2
 email-validator>=1.0.3
+mypy_extensions>=0.4.3
 azure-identity~=1.12
-croniter~=1.3.0
-requests>=2.23
-google-auth>=1.33.0
-Jinja2>=2.11.3
-jsonpatch<2.0,>=1.24
-cryptography
+antlr4-python3-runtime==4.9.2
+azure-keyvault-secrets
+cryptography>=42.0.0
+python-dateutil>=2.8.1
 chardet==4.0.0
-google>=3.0.0
-grpcio-tools>=1.47.2
 pymysql>=1.0.2
-python-jose~=3.3
-wheel~=0.38.4
-idna<3,>=2.5
-python-dateutil>=2.8.1
-importlib-metadata>=4.13.0
-boto3<2.0,>=1.20
-typing-inspect
-memory-profiler
+pydantic~=1.10
+collate-sqllineage~=1.3.0
+jsonpatch<2.0,>=1.24
+Jinja2>=2.11.3
 PyYAML~=6.0
-tabulate==0.9.0
 
 [bigquery]
-google-cloud-logging
-pyarrow~=14.0
 cachetools
+google-cloud-logging
 google-cloud-datacatalog>=3.6.2
+pyarrow~=14.0
 sqlalchemy-bigquery>=1.2.2
 
 [bigtable]
 google-cloud-bigtable>=2.0.0
 pandas~=2.0.0
 
 [clickhouse]
-clickhouse-driver~=0.2
 clickhouse-sqlalchemy~=0.2
+clickhouse-driver~=0.2
 
 [couchbase]
 couchbase~=4.1
 
 [dagster]
-GeoAlchemy2~=0.12
 psycopg2-binary
-dagster_graphql~=1.1
 pymysql>=1.0.2
+dagster_graphql~=1.1
+GeoAlchemy2~=0.12
 
 [data-insight]
 elasticsearch8~=8.9.0
-elasticsearch==7.13.1
 
 [databricks]
 databricks-sdk<0.20.0,>=0.18.0
+sqlalchemy-databricks~=0.1
+ndg-httpsclient~=0.5.1
 pyasn1~=0.6.0
 pyOpenSSL~=24.1.0
-ndg-httpsclient~=0.5.1
-sqlalchemy-databricks~=0.1
 
 [datalake-azure]
-adlfs~=2022.11
+boto3<2.0,>=1.20
 azure-storage-blob~=12.14
+avro<1.12,>=1.11.3
+adlfs~=2022.11
 cramjam~=2.7
+pyarrow~=14.0
 pandas~=2.0.0
-boto3<2.0,>=1.20
 azure-identity~=1.12
-pyarrow~=14.0
 
 [datalake-gcs]
-google-cloud-storage==1.43.0
+gcsfs~=2022.11
 cramjam~=2.7
+boto3<2.0,>=1.20
+google-cloud-storage==1.43.0
 pandas~=2.0.0
 pyarrow~=14.0
-boto3<2.0,>=1.20
-gcsfs~=2022.11
+avro<1.12,>=1.11.3
 
 [datalake-s3]
 s3fs==0.4.2
 cramjam~=2.7
-pandas~=2.0.0
 boto3<2.0,>=1.20
 pyarrow~=14.0
+pandas~=2.0.0
+avro<1.12,>=1.11.3
 
 [db2]
 ibm-db-sa~=0.3
 
 [db2-ibmi]
 sqlalchemy-ibmi~=0.9.3
 
 [dbt]
+google-cloud
 boto3<2.0,>=1.20
 azure-storage-blob~=12.14
-google-cloud
-azure-identity~=1.12
 google-cloud-storage==1.43.0
 dbt-artifacts-parser
+azure-identity~=1.12
 
 [deltalake]
 delta-spark<=2.3.0
 
 [dev]
+build
+boto3-stubs[essential]
+isort
 twine
-black==22.3.0
-pycln
 pylint~=3.0.0
-build
-pre-commit
 datamodel-code-generator==0.24.2
-isort
-
-[docker]
-python_on_whales==0.55.0
+pre-commit
+grpcio-tools>=1.47.2
+pycln
+black==22.3.0
+avro<1.12,>=1.11.3
 
 [domo]
 pydomo~=0.3
 
 [doris]
 pydoris==1.0.2
 
@@ -307,63 +280,59 @@
 
 [e2e_test]
 pytest-playwright
 pytest-base-url
 
 [elasticsearch]
 elasticsearch8~=8.9.0
-elasticsearch==7.13.1
 
 [extended_testing]
 Faker
 
 [glue]
 boto3<2.0,>=1.20
 
 [great-expectations]
 great-expectations~=0.18.0
 
 [hive]
 impyla~=0.18.0
 thrift-sasl~=0.4
-thrift<1,>=0.13
+pyhive[hive_pure_sasl]~=0.7
 pure-sasl
+thrift<1,>=0.13
 presto-types-parser>=0.0.2
-pyhive[hive_pure_sasl]~=0.7
 
 [iceberg]
-pyiceberg
+gcsfs~=2022.11
+adlfs~=2022.11
 pydantic~=1.10
 pyarrow~=14.0
-adlfs~=2022.11
-gcsfs~=2022.11
+pyiceberg<1
 
 [impala]
-presto-types-parser>=0.0.2
+impyla[kerberos]~=0.18.0
 thrift-sasl~=0.4
-thrift<1,>=0.13
 pure-sasl
-impyla[kerberos]~=0.18.0
+thrift<1,>=0.13
+presto-types-parser>=0.0.2
 
 [kafka]
-avro~=1.11
-protobuf
 fastavro>=1.2.0
-grpcio-tools>=1.47.2
 confluent_kafka==2.1.1
+grpcio-tools>=1.47.2
+avro<1.12,>=1.11.3
+protobuf
 
 [kinesis]
 boto3<2.0,>=1.20
 
-[ldap-users]
-ldap3==2.9.1
-
 [looker]
-giturlparse
 gitpython~=3.1.34
+giturlparse
 looker-sdk>=22.20.0
 lkml~=1.3
 
 [mlflow]
 mlflow-skinny>=2.3.0
 alembic~=1.10.2
 
@@ -378,235 +347,245 @@
 pyodbc<5,>=4.0.35
 
 [mysql]
 pymysql>=1.0.2
 
 [nifi]
 
-[okta]
-okta~=2.3
+[openlineage]
+fastavro>=1.2.0
+confluent_kafka==2.1.1
+grpcio-tools>=1.47.2
+avro<1.12,>=1.11.3
+protobuf
 
 [oracle]
-oracledb~=1.2
 cx_Oracle<9,>=8.3.0
+oracledb~=1.2
 
 [pgspider]
 psycopg2-binary
 sqlalchemy-pgspider
 
 [pii-processor]
 presidio-analyzer==2.2.32
 spacy==3.5.0
 pandas~=2.0.0
 
 [pinotdb]
 pinotdb~=0.3
 
 [postgres]
-GeoAlchemy2~=0.12
 psycopg2-binary
-packaging==21.3
 pymysql>=1.0.2
+GeoAlchemy2~=0.12
+packaging==21.3
 
 [powerbi]
+boto3<2.0,>=1.20
+azure-storage-blob~=12.14
+google-cloud-storage==1.43.0
 msal~=1.2
+azure-identity~=1.12
 
 [presto]
-presto-types-parser>=0.0.2
 pyhive[hive_pure_sasl]~=0.7
+presto-types-parser>=0.0.2
 
 [pymssql]
 pymssql~=2.2.0
 
 [qliksense]
 websocket-client~=1.6.1
 
 [quicksight]
 boto3<2.0,>=1.20
 
 [redash]
 packaging==21.3
 
 [redpanda]
-avro~=1.11
-protobuf
 fastavro>=1.2.0
-grpcio-tools>=1.47.2
 confluent_kafka==2.1.1
+grpcio-tools>=1.47.2
+avro<1.12,>=1.11.3
+protobuf
 
 [redshift]
-GeoAlchemy2~=0.12
 psycopg2-binary
 sqlalchemy-redshift==0.8.12
+GeoAlchemy2~=0.12
 
 [sagemaker]
 boto3<2.0,>=1.20
 
 [salesforce]
 simple_salesforce==1.11.4
 
+[sample-data]
+grpcio-tools>=1.47.2
+avro<1.12,>=1.11.3
+
 [sap-hana]
-sqlalchemy-hana
 hdbcli
+sqlalchemy-hana
 
 [sas]
 
 [singlestore]
 pymysql>=1.0.2
 
 [sklearn]
 scikit-learn~=1.0
 
 [slim]
-jsonschema
-presidio-analyzer==2.2.32
-elasticsearch==7.13.1
-cramjam~=2.7
+trino[sqlalchemy]
+looker-sdk>=22.20.0
 lkml~=1.3
-oracledb~=1.2
-pymssql~=2.2.0
-mypy_extensions>=0.4.3
-google-cloud
-ldap3==2.9.1
-python_on_whales==0.55.0
-sqlalchemy-redshift==0.8.12
-requests-aws4auth~=1.1
-snowflake-sqlalchemy~=1.4
-simple_salesforce==1.11.4
-spacy==3.5.0
+boto3<2.0,>=1.20
 sqlalchemy<2,>=1.4.0
-collate-sqllineage~=1.3.0
-google-cloud-bigtable>=2.0.0
-azure-keyvault-secrets
-neo4j~=5.3.0
-packaging==21.3
-antlr4-python3-runtime==4.9.2
-azure-storage-blob
-pyathena==3.0.8
-pyarrow~=14.0
-croniter~=1.3.0
-ndg-httpsclient~=0.5.1
-s3fs==0.4.2
-Jinja2>=2.11.3
-jsonpatch<2.0,>=1.24
-pymongo~=4.3
-elasticsearch8~=8.9.0
-gitpython~=3.1.34
 pyodbc<5,>=4.0.35
-gcsfs~=2022.11
-pyiceberg
-pymysql>=1.0.2
-impyla~=0.18.0
 tabulate==0.9.0
-pure-sasl
-trino[sqlalchemy]
-sqlalchemy-vertica[vertica-python]>=0.0.5
-msal~=1.2
-looker-sdk>=22.20.0
-wheel~=0.38.4
-idna<3,>=2.5
-python-dateutil>=2.8.1
-sqlalchemy-bigquery>=1.2.2
-boto3<2.0,>=1.20
-typing-inspect
-giturlparse
-sqlalchemy-ibmi~=0.9.3
 fastavro>=1.2.0
-PyYAML~=6.0
-google-cloud-storage==1.43.0
-confluent_kafka==2.1.1
-hdbcli
-tableau-api-lib~=0.1
-impyla[kerberos]~=0.18.0
-pydantic~=1.10
+sqlalchemy-vertica[vertica-python]>=0.0.5
+setuptools~=66.0.0
 dagster_graphql~=1.1
-cached-property==1.5.2
+sqlalchemy-pytds~=0.3
 thrift<1,>=0.13
-protobuf
-mlflow-skinny>=2.3.0
-google-cloud-datacatalog>=3.6.2
-sqlalchemy-hana
-avro~=1.11
-setuptools~=66.0.0
-psycopg2-binary
-couchbase~=4.1
 pinotdb~=0.3
+packaging==21.3
+requests-aws4auth~=1.1
+neo4j~=5.3.0
+sqlalchemy-ibmi~=0.9.3
+memory-profiler
+importlib-metadata>=4.13.0
+impyla[kerberos]~=0.18.0
+couchbase~=4.1
 email-validator>=1.0.3
-sqlalchemy-pgspider
-pyOpenSSL~=24.1.0
-thrift-sasl~=0.4
-azure-identity~=1.12
-google-cloud-logging
-azure-storage-blob~=12.14
-requests>=2.23
-google-auth>=1.33.0
-cryptography
-validators~=0.22.0
+google-cloud-datacatalog>=3.6.2
+mypy_extensions>=0.4.3
+google-cloud
+pymssql~=2.2.0
+presidio-analyzer==2.2.32
+antlr4-python3-runtime==4.9.2
+google-cloud-bigtable>=2.0.0
+azure-keyvault-secrets
+sqlalchemy-redshift==0.8.12
+python-dateutil>=2.8.1
+pymongo~=4.3
 chardet==4.0.0
-google>=3.0.0
-grpcio-tools>=1.47.2
+azure-storage-blob
+pymysql>=1.0.2
+pydantic~=1.10
+sqlalchemy-pgspider
+hdbcli
+confluent_kafka==2.1.1
+Jinja2>=2.11.3
+databricks-sdk<0.20.0,>=0.18.0
 pandas~=2.0.0
-pyasn1~=0.6.0
+websocket-client~=1.6.1
+validators~=0.22.0
+protobuf
+gcsfs~=2022.11
+PyYAML~=6.0
+oracledb~=1.2
+google-cloud-logging
+pyhive[hive_pure_sasl]~=0.7
+pyOpenSSL~=24.1.0
 clickhouse-driver~=0.2
+presto-types-parser>=0.0.2
+s3fs==0.4.2
+mlflow-skinny>=2.3.0
+psycopg2-binary
+giturlparse
+pydomo~=0.3
 alembic~=1.10.2
-websocket-client~=1.6.1
-python-jose~=3.3
-okta~=2.3
-cx_Oracle<9,>=8.3.0
-importlib-metadata>=4.13.0
-databricks-sdk<0.20.0,>=0.18.0
+requests>=2.23
+grpcio-tools>=1.47.2
+typing-inspect
+simple_salesforce==1.11.4
+sqlalchemy-databricks~=0.1
+pyathena==3.0.8
 clickhouse-sqlalchemy~=0.2
+cached-property==1.5.2
 pydoris==1.0.2
-sqlalchemy-pytds~=0.3
-GeoAlchemy2~=0.12
+thrift-sasl~=0.4
+cramjam~=2.7
+pyarrow~=14.0
+google-cloud-storage==1.43.0
+impyla~=0.18.0
+azure-identity~=1.12
+gitpython~=3.1.34
 cachetools
-pydomo~=0.3
+cryptography>=42.0.0
+ndg-httpsclient~=0.5.1
+tableau-api-lib~=0.1
+GeoAlchemy2~=0.12
 adlfs~=2022.11
-memory-profiler
-presto-types-parser>=0.0.2
-pydruid>=0.6.5
+collate-sqllineage~=1.3.0
+jsonpatch<2.0,>=1.24
+pure-sasl
+msal~=1.2
+spacy==3.5.0
 dbt-artifacts-parser
-sqlalchemy-databricks~=0.1
-pyhive[hive_pure_sasl]~=0.7
+snowflake-sqlalchemy~=1.4
+azure-storage-blob~=12.14
+sqlalchemy-hana
+pyasn1~=0.6.0
+pydruid>=0.6.5
+cx_Oracle<9,>=8.3.0
+elasticsearch8~=8.9.0
+avro<1.12,>=1.11.3
+sqlalchemy-bigquery>=1.2.2
+pyiceberg<1
 
 [snowflake]
 snowflake-sqlalchemy~=1.4
 
 [superset]
 
 [tableau]
+tableau-api-lib~=0.1
 validators~=0.22.0
 packaging==21.3
-tableau-api-lib~=0.1
 
 [test]
-lkml~=1.3
-spacy==3.5.0
-sqlalchemy-redshift==0.8.12
-snowflake-sqlalchemy~=1.4
-pyarrow~=14.0
-scikit-learn~=1.0
-pymongo~=4.3
-elasticsearch8~=8.9.0
-google>=3.0.0
 pytest==7.0.0
-pytest-cov
-great-expectations~=0.18.0
 trino[sqlalchemy]
-moto==4.0.8
 looker-sdk>=22.20.0
-coverage
-pytest-order
-databricks-sdk<0.20.0,>=0.18.0
+lkml~=1.3
 giturlparse
 pydomo~=0.3
-apache-airflow==2.7.3
-dbt-artifacts-parser
+minio==7.2.5
+scikit-learn~=1.0
+pytest-cov
+grpcio-tools>=1.47.2
 sqlalchemy-databricks~=0.1
+great-expectations~=0.18.0
+coverage
+pyarrow~=14.0
+sqlalchemy-redshift==0.8.12
+pymongo~=4.3
+elasticsearch8~=8.9.0
 tableau-api-lib~=0.1
+apache-airflow==2.7.3
 pyhive[hive_pure_sasl]~=0.7
+spacy==3.5.0
+dbt-artifacts-parser
+databricks-sdk<0.20.0,>=0.18.0
+snowflake-sqlalchemy~=1.4
+boto3-stubs[boto3]
+pytest-order
+moto==4.0.8
+avro<1.12,>=1.11.3
+
+[test:python_version < "3.9"]
+testcontainers==3.7.1
+
+[test:python_version >= "3.9"]
+testcontainers==4.4.0
 
 [trino]
 trino[sqlalchemy]
 
 [vertica]
 sqlalchemy-vertica[vertica-python]>=0.0.5
```

