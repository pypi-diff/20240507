# Comparing `tmp/nvidia_modelopt-0.11.0-cp39-cp39-win_amd64.whl.zip` & `tmp/nvidia_modelopt-0.11.1-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,127 +1,130 @@
-Zip file size: 465264 bytes, number of entries: 125
--rw-rw-rw-  2.0 fat      694 b- defN 24-Apr-29 12:03 modelopt/__init__.py
--rw-rw-rw-  2.0 fat      604 b- defN 24-Apr-29 12:03 modelopt/deploy/__init__.py
--rw-rw-rw-  2.0 fat     2390 b- defN 24-Apr-29 12:03 modelopt/deploy/llm/__init__.py
--rw-rw-rw-  2.0 fat     5213 b- defN 24-Apr-29 12:03 modelopt/deploy/llm/generate.py
--rw-rw-rw-  2.0 fat    12337 b- defN 24-Apr-29 12:03 modelopt/deploy/llm/model_config_trt.py
--rw-rw-rw-  2.0 fat     6892 b- defN 24-Apr-29 12:03 modelopt/deploy/llm/nemo_utils.py
--rw-rw-rw-  2.0 fat      791 b- defN 24-Apr-29 12:03 modelopt/onnx/__init__.py
--rw-rw-rw-  2.0 fat     7880 b- defN 24-Apr-29 12:03 modelopt/onnx/op_types.py
--rw-rw-rw-  2.0 fat    19455 b- defN 24-Apr-29 12:03 modelopt/onnx/utils.py
--rw-rw-rw-  2.0 fat      619 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/__init__.py
--rw-rw-rw-  2.0 fat     4605 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/__main__.py
--rw-rw-rw-  2.0 fat     4605 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/calib_utils.py
--rw-rw-rw-  2.0 fat    19230 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/graph_utils.py
--rw-rw-rw-  2.0 fat     4118 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/gs_patching.py
--rw-rw-rw-  2.0 fat    16443 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/int4.py
--rw-rw-rw-  2.0 fat     4306 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/operators.py
--rw-rw-rw-  2.0 fat     8217 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/ort_patching.py
--rw-rw-rw-  2.0 fat     1053 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/ort_utils.py
--rw-rw-rw-  2.0 fat    12547 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/partitioning.py
--rw-rw-rw-  2.0 fat     7440 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/qdq_utils.py
--rw-rw-rw-  2.0 fat    20977 b- defN 24-Apr-29 12:03 modelopt/onnx/quantization/quantize.py
--rw-rw-rw-  2.0 fat      805 b- defN 24-Apr-30 05:32 modelopt/torch/__init__.py
--rw-rw-rw-  2.0 fat      876 b- defN 24-Apr-30 04:56 modelopt/torch/_deploy/__init__.py
--rw-rw-rw-  2.0 fat     4824 b- defN 24-Apr-30 04:56 modelopt/torch/_deploy/compilation.py
--rw-rw-rw-  2.0 fat     5522 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/device_model.py
--rw-rw-rw-  2.0 fat     7588 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/profiling.py
--rw-rw-rw-  2.0 fat      957 b- defN 24-Apr-30 04:56 modelopt/torch/_deploy/_runtime/__init__.py
--rw-rw-rw-  2.0 fat     2327 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/_runtime/common.py
--rw-rw-rw-  2.0 fat     7266 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/_runtime/ort_client.py
--rw-rw-rw-  2.0 fat     2224 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/_runtime/registry.py
--rw-rw-rw-  2.0 fat     5865 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/_runtime/runtime_client.py
--rw-rw-rw-  2.0 fat     7730 b- defN 24-Apr-30 04:56 modelopt/torch/_deploy/_runtime/trt_client.py
--rw-rw-rw-  2.0 fat     2594 b- defN 24-Apr-30 04:56 modelopt/torch/_deploy/_runtime/tensorrt/constants.py
--rw-rw-rw-  2.0 fat   119296 b- defN 24-Apr-30 05:34 modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     1763 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py
--rw-rw-rw-  2.0 fat     6972 b- defN 24-Apr-30 04:56 modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py
--rw-rw-rw-  2.0 fat     5691 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py
--rw-rw-rw-  2.0 fat     6524 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py
--rw-rw-rw-  2.0 fat      633 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/utils/__init__.py
--rw-rw-rw-  2.0 fat     4730 b- defN 24-Apr-29 12:03 modelopt/torch/_deploy/utils/onnx_optimizer.py
--rw-rw-rw-  2.0 fat    19935 b- defN 24-Apr-30 04:56 modelopt/torch/_deploy/utils/torch_onnx.py
--rw-rw-rw-  2.0 fat      848 b- defN 24-Apr-29 12:03 modelopt/torch/export/__init__.py
--rw-rw-rw-  2.0 fat    12668 b- defN 24-Apr-29 12:03 modelopt/torch/export/distribute.py
--rw-rw-rw-  2.0 fat    52001 b- defN 24-Apr-29 12:03 modelopt/torch/export/layer_utils.py
--rw-rw-rw-  2.0 fat    13738 b- defN 24-Apr-29 12:03 modelopt/torch/export/model_config.py
--rw-rw-rw-  2.0 fat    17373 b- defN 24-Apr-30 04:56 modelopt/torch/export/model_config_export.py
--rw-rw-rw-  2.0 fat    18360 b- defN 24-Apr-29 12:03 modelopt/torch/export/model_config_utils.py
--rw-rw-rw-  2.0 fat    29334 b- defN 24-Apr-29 12:03 modelopt/torch/export/postprocess.py
--rw-rw-rw-  2.0 fat     3182 b- defN 24-Apr-29 12:03 modelopt/torch/export/scaling_factor_utils.py
--rw-rw-rw-  2.0 fat    11215 b- defN 24-Apr-29 12:03 modelopt/torch/export/tensorrt_llm_utils.py
--rw-rw-rw-  2.0 fat     4088 b- defN 24-Apr-29 12:03 modelopt/torch/export/transformer_engine.py
--rw-rw-rw-  2.0 fat     1360 b- defN 24-Apr-29 12:03 modelopt/torch/opt/__init__.py
--rw-rw-rw-  2.0 fat     3137 b- defN 24-Apr-29 12:03 modelopt/torch/opt/_hooks.py
--rw-rw-rw-  2.0 fat    15227 b- defN 24-Apr-29 12:03 modelopt/torch/opt/config.py
--rw-rw-rw-  2.0 fat    24050 b- defN 24-Apr-29 12:03 modelopt/torch/opt/conversion.py
--rw-rw-rw-  2.0 fat    57369 b- defN 24-Apr-29 12:03 modelopt/torch/opt/dynamic.py
--rw-rw-rw-  2.0 fat     9461 b- defN 24-Apr-29 12:03 modelopt/torch/opt/hparam.py
--rw-rw-rw-  2.0 fat    12723 b- defN 24-Apr-29 12:03 modelopt/torch/opt/mode.py
--rw-rw-rw-  2.0 fat     9570 b- defN 24-Apr-29 12:03 modelopt/torch/opt/searcher.py
--rw-rw-rw-  2.0 fat     1955 b- defN 24-Apr-29 12:03 modelopt/torch/opt/utils.py
--rw-rw-rw-  2.0 fat      846 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/__init__.py
--rw-rw-rw-  2.0 fat    13912 b- defN 24-Apr-30 05:32 modelopt/torch/quantization/config.py
--rw-rw-rw-  2.0 fat    11437 b- defN 24-Apr-30 04:56 modelopt/torch/quantization/conversion.py
--rw-rw-rw-  2.0 fat     1109 b- defN 24-Apr-30 05:32 modelopt/torch/quantization/extensions.py
--rw-rw-rw-  2.0 fat     4062 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/mode.py
--rw-rw-rw-  2.0 fat   320000 b- defN 24-Apr-30 05:34 modelopt/torch/quantization/model_calib.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     6302 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/model_quant.py
--rw-rw-rw-  2.0 fat     1393 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/optim.py
--rw-rw-rw-  2.0 fat     1725 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/quant_modules.py
--rw-rw-rw-  2.0 fat    29725 b- defN 24-Apr-30 05:32 modelopt/torch/quantization/tensor_quant.py
--rw-rw-rw-  2.0 fat     5506 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/utils.py
--rw-rw-rw-  2.0 fat      816 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/calib/__init__.py
--rw-rw-rw-  2.0 fat     1832 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/calib/calibrator.py
--rw-rw-rw-  2.0 fat    17445 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/calib/histogram.py
--rw-rw-rw-  2.0 fat     3436 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/calib/max.py
--rw-rw-rw-  2.0 fat      863 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/__init__.py
--rw-rw-rw-  2.0 fat     2822 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/functional.py
--rw-rw-rw-  2.0 fat      601 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/modules/__init__.py
--rw-rw-rw-  2.0 fat     8338 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/modules/_utils.py
--rw-rw-rw-  2.0 fat     2317 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/modules/clip.py
--rw-rw-rw-  2.0 fat     3986 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/modules/quant_conv.py
--rw-rw-rw-  2.0 fat     1556 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/modules/quant_instancenorm.py
--rw-rw-rw-  2.0 fat     2762 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/modules/quant_linear.py
--rw-rw-rw-  2.0 fat     5675 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/modules/quant_module.py
--rw-rw-rw-  2.0 fat     3556 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/nn/modules/quant_pooling.py
--rw-rw-rw-  2.0 fat    34738 b- defN 24-Apr-30 05:32 modelopt/torch/quantization/nn/modules/tensor_quantizer.py
--rw-rw-rw-  2.0 fat     1892 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/plugins/__init__.py
--rw-rw-rw-  2.0 fat     1484 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/plugins/apex.py
--rw-rw-rw-  2.0 fat     4226 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/plugins/custom.py
--rw-rw-rw-  2.0 fat     3453 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/plugins/diffusers.py
--rw-rw-rw-  2.0 fat     2489 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/plugins/huggingface.py
--rw-rw-rw-  2.0 fat     1220 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/plugins/megatron.py
--rw-rw-rw-  2.0 fat     2472 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/plugins/nemo.py
--rw-rw-rw-  2.0 fat     3061 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/src/tensor_quant.cpp
--rw-rw-rw-  2.0 fat     1320 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/src/tensor_quant_fp8.cpp
--rw-rw-rw-  2.0 fat     5557 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/src/tensor_quant_gpu.cu
--rw-rw-rw-  2.0 fat     2082 b- defN 24-Apr-29 12:03 modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu
--rw-rw-rw-  2.0 fat      671 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/__init__.py
--rw-rw-rw-  2.0 fat     1641 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/config.py
--rw-rw-rw-  2.0 fat     5352 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/magnitude.py
--rw-rw-rw-  2.0 fat     6847 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/mode.py
--rw-rw-rw-  2.0 fat     3588 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/module.py
--rw-rw-rw-  2.0 fat     3199 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/searcher.py
--rw-rw-rw-  2.0 fat    10791 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/sparsegpt.py
--rw-rw-rw-  2.0 fat     5740 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/sparsification.py
--rw-rw-rw-  2.0 fat      990 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/plugins/__init__.py
--rw-rw-rw-  2.0 fat     1609 b- defN 24-Apr-29 12:03 modelopt/torch/sparsity/plugins/megatron.py
--rw-rw-rw-  2.0 fat      804 b- defN 24-Apr-29 12:03 modelopt/torch/utils/__init__.py
--rw-rw-rw-  2.0 fat     5582 b- defN 24-Apr-29 12:03 modelopt/torch/utils/_pytree.py
--rw-rw-rw-  2.0 fat     2419 b- defN 24-Apr-29 12:03 modelopt/torch/utils/cpp_extension.py
--rw-rw-rw-  2.0 fat     7448 b- defN 24-Apr-29 12:03 modelopt/torch/utils/dataset_utils.py
--rw-rw-rw-  2.0 fat     8262 b- defN 24-Apr-29 12:03 modelopt/torch/utils/distributed.py
--rw-rw-rw-  2.0 fat     3894 b- defN 24-Apr-29 12:03 modelopt/torch/utils/graph.py
--rw-rw-rw-  2.0 fat     1951 b- defN 24-Apr-29 12:03 modelopt/torch/utils/list.py
--rw-rw-rw-  2.0 fat     2991 b- defN 24-Apr-29 12:03 modelopt/torch/utils/logging.py
--rw-rw-rw-  2.0 fat    22299 b- defN 24-Apr-29 12:03 modelopt/torch/utils/network.py
--rw-rw-rw-  2.0 fat     2816 b- defN 24-Apr-29 12:03 modelopt/torch/utils/perf.py
--rw-rw-rw-  2.0 fat     5706 b- defN 24-Apr-29 12:03 modelopt/torch/utils/random.py
--rw-rw-rw-  2.0 fat     1999 b- defN 24-Apr-29 12:03 modelopt/torch/utils/tensor.py
--rw-rw-rw-  2.0 fat      540 b- defN 24-Apr-30 05:34 nvidia_modelopt-0.11.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     2589 b- defN 24-Apr-30 05:34 nvidia_modelopt-0.11.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-30 05:34 nvidia_modelopt-0.11.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       72 b- defN 24-Apr-30 05:33 nvidia_modelopt-0.11.0.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat        9 b- defN 24-Apr-30 05:33 nvidia_modelopt-0.11.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat    12107 b- defN 24-Apr-30 05:34 nvidia_modelopt-0.11.0.dist-info/RECORD
-125 files, 1310229 bytes uncompressed, 445682 bytes compressed:  66.0%
+Zip file size: 472606 bytes, number of entries: 128
+-rw-rw-rw-  2.0 fat      694 b- defN 24-May-06 08:47 modelopt/__init__.py
+-rw-rw-rw-  2.0 fat      604 b- defN 24-May-06 08:47 modelopt/deploy/__init__.py
+-rw-rw-rw-  2.0 fat     2390 b- defN 24-May-06 08:47 modelopt/deploy/llm/__init__.py
+-rw-rw-rw-  2.0 fat     5213 b- defN 24-May-06 08:47 modelopt/deploy/llm/generate.py
+-rw-rw-rw-  2.0 fat    12337 b- defN 24-May-06 08:47 modelopt/deploy/llm/model_config_trt.py
+-rw-rw-rw-  2.0 fat     6892 b- defN 24-May-06 08:47 modelopt/deploy/llm/nemo_utils.py
+-rw-rw-rw-  2.0 fat      791 b- defN 24-May-06 08:47 modelopt/onnx/__init__.py
+-rw-rw-rw-  2.0 fat     8003 b- defN 24-May-06 08:47 modelopt/onnx/op_types.py
+-rw-rw-rw-  2.0 fat    19828 b- defN 24-May-06 08:47 modelopt/onnx/utils.py
+-rw-rw-rw-  2.0 fat      619 b- defN 24-May-06 08:47 modelopt/onnx/quantization/__init__.py
+-rw-rw-rw-  2.0 fat     4605 b- defN 24-May-06 08:47 modelopt/onnx/quantization/__main__.py
+-rw-rw-rw-  2.0 fat     4605 b- defN 24-May-06 08:47 modelopt/onnx/quantization/calib_utils.py
+-rw-rw-rw-  2.0 fat    18754 b- defN 24-May-06 08:47 modelopt/onnx/quantization/graph_utils.py
+-rw-rw-rw-  2.0 fat     4132 b- defN 24-May-06 08:47 modelopt/onnx/quantization/gs_patching.py
+-rw-rw-rw-  2.0 fat    18389 b- defN 24-May-07 07:17 modelopt/onnx/quantization/int4.py
+-rw-rw-rw-  2.0 fat     4306 b- defN 24-May-06 08:47 modelopt/onnx/quantization/operators.py
+-rw-rw-rw-  2.0 fat     8217 b- defN 24-May-06 08:47 modelopt/onnx/quantization/ort_patching.py
+-rw-rw-rw-  2.0 fat     1053 b- defN 24-May-06 08:47 modelopt/onnx/quantization/ort_utils.py
+-rw-rw-rw-  2.0 fat    14046 b- defN 24-May-06 08:47 modelopt/onnx/quantization/partitioning.py
+-rw-rw-rw-  2.0 fat     7440 b- defN 24-May-06 08:47 modelopt/onnx/quantization/qdq_utils.py
+-rw-rw-rw-  2.0 fat     2475 b- defN 24-May-06 08:47 modelopt/onnx/quantization/quant_utils.py
+-rw-rw-rw-  2.0 fat    21116 b- defN 24-May-06 08:47 modelopt/onnx/quantization/quantize.py
+-rw-rw-rw-  2.0 fat      805 b- defN 24-May-07 07:17 modelopt/torch/__init__.py
+-rw-rw-rw-  2.0 fat      876 b- defN 24-May-06 08:47 modelopt/torch/_deploy/__init__.py
+-rw-rw-rw-  2.0 fat     4824 b- defN 24-May-06 08:47 modelopt/torch/_deploy/compilation.py
+-rw-rw-rw-  2.0 fat     5522 b- defN 24-May-06 08:47 modelopt/torch/_deploy/device_model.py
+-rw-rw-rw-  2.0 fat     7588 b- defN 24-May-06 08:47 modelopt/torch/_deploy/profiling.py
+-rw-rw-rw-  2.0 fat      957 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/__init__.py
+-rw-rw-rw-  2.0 fat     2327 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/common.py
+-rw-rw-rw-  2.0 fat     7266 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/ort_client.py
+-rw-rw-rw-  2.0 fat     2224 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/registry.py
+-rw-rw-rw-  2.0 fat     5865 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/runtime_client.py
+-rw-rw-rw-  2.0 fat     7730 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/trt_client.py
+-rw-rw-rw-  2.0 fat     2594 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/constants.py
+-rw-rw-rw-  2.0 fat   119296 b- defN 24-May-07 07:19 modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     1763 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py
+-rw-rw-rw-  2.0 fat     6972 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py
+-rw-rw-rw-  2.0 fat     5691 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py
+-rw-rw-rw-  2.0 fat     6524 b- defN 24-May-06 08:47 modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py
+-rw-rw-rw-  2.0 fat      633 b- defN 24-May-06 08:47 modelopt/torch/_deploy/utils/__init__.py
+-rw-rw-rw-  2.0 fat     4730 b- defN 24-May-06 08:47 modelopt/torch/_deploy/utils/onnx_optimizer.py
+-rw-rw-rw-  2.0 fat    20946 b- defN 24-May-06 08:47 modelopt/torch/_deploy/utils/torch_onnx.py
+-rw-rw-rw-  2.0 fat      848 b- defN 24-May-06 08:47 modelopt/torch/export/__init__.py
+-rw-rw-rw-  2.0 fat    12668 b- defN 24-May-06 08:47 modelopt/torch/export/distribute.py
+-rw-rw-rw-  2.0 fat    52089 b- defN 24-May-07 07:17 modelopt/torch/export/layer_utils.py
+-rw-rw-rw-  2.0 fat    13780 b- defN 24-May-06 08:47 modelopt/torch/export/model_config.py
+-rw-rw-rw-  2.0 fat    17778 b- defN 24-May-06 08:47 modelopt/torch/export/model_config_export.py
+-rw-rw-rw-  2.0 fat    18360 b- defN 24-May-06 08:47 modelopt/torch/export/model_config_utils.py
+-rw-rw-rw-  2.0 fat    29334 b- defN 24-May-06 08:47 modelopt/torch/export/postprocess.py
+-rw-rw-rw-  2.0 fat     3182 b- defN 24-May-06 08:47 modelopt/torch/export/scaling_factor_utils.py
+-rw-rw-rw-  2.0 fat    11289 b- defN 24-May-06 08:47 modelopt/torch/export/tensorrt_llm_utils.py
+-rw-rw-rw-  2.0 fat     4088 b- defN 24-May-06 08:47 modelopt/torch/export/transformer_engine.py
+-rw-rw-rw-  2.0 fat     1360 b- defN 24-May-06 08:47 modelopt/torch/opt/__init__.py
+-rw-rw-rw-  2.0 fat     3137 b- defN 24-May-06 08:47 modelopt/torch/opt/_hooks.py
+-rw-rw-rw-  2.0 fat    15227 b- defN 24-May-06 08:47 modelopt/torch/opt/config.py
+-rw-rw-rw-  2.0 fat    24050 b- defN 24-May-06 08:47 modelopt/torch/opt/conversion.py
+-rw-rw-rw-  2.0 fat    57369 b- defN 24-May-06 08:47 modelopt/torch/opt/dynamic.py
+-rw-rw-rw-  2.0 fat     9461 b- defN 24-May-06 08:47 modelopt/torch/opt/hparam.py
+-rw-rw-rw-  2.0 fat    12723 b- defN 24-May-06 08:47 modelopt/torch/opt/mode.py
+-rw-rw-rw-  2.0 fat     9570 b- defN 24-May-06 08:47 modelopt/torch/opt/searcher.py
+-rw-rw-rw-  2.0 fat     1955 b- defN 24-May-06 08:47 modelopt/torch/opt/utils.py
+-rw-rw-rw-  2.0 fat      846 b- defN 24-May-06 08:47 modelopt/torch/quantization/__init__.py
+-rw-rw-rw-  2.0 fat    13912 b- defN 24-May-07 07:17 modelopt/torch/quantization/config.py
+-rw-rw-rw-  2.0 fat    10771 b- defN 24-May-06 08:47 modelopt/torch/quantization/conversion.py
+-rw-rw-rw-  2.0 fat     1109 b- defN 24-May-07 07:17 modelopt/torch/quantization/extensions.py
+-rw-rw-rw-  2.0 fat     4062 b- defN 24-May-06 08:47 modelopt/torch/quantization/mode.py
+-rw-rw-rw-  2.0 fat   322560 b- defN 24-May-07 07:19 modelopt/torch/quantization/model_calib.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     6302 b- defN 24-May-06 08:47 modelopt/torch/quantization/model_quant.py
+-rw-rw-rw-  2.0 fat     1393 b- defN 24-May-06 08:47 modelopt/torch/quantization/optim.py
+-rw-rw-rw-  2.0 fat     1725 b- defN 24-May-06 08:47 modelopt/torch/quantization/quant_modules.py
+-rw-rw-rw-  2.0 fat    29725 b- defN 24-May-07 07:17 modelopt/torch/quantization/tensor_quant.py
+-rw-rw-rw-  2.0 fat     5703 b- defN 24-May-06 08:47 modelopt/torch/quantization/utils.py
+-rw-rw-rw-  2.0 fat      816 b- defN 24-May-06 08:47 modelopt/torch/quantization/calib/__init__.py
+-rw-rw-rw-  2.0 fat     1832 b- defN 24-May-06 08:47 modelopt/torch/quantization/calib/calibrator.py
+-rw-rw-rw-  2.0 fat    17445 b- defN 24-May-06 08:47 modelopt/torch/quantization/calib/histogram.py
+-rw-rw-rw-  2.0 fat     3436 b- defN 24-May-06 08:47 modelopt/torch/quantization/calib/max.py
+-rw-rw-rw-  2.0 fat      945 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/__init__.py
+-rw-rw-rw-  2.0 fat     2822 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/functional.py
+-rw-rw-rw-  2.0 fat      601 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/__init__.py
+-rw-rw-rw-  2.0 fat     8338 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/_utils.py
+-rw-rw-rw-  2.0 fat     2317 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/clip.py
+-rw-rw-rw-  2.0 fat     1389 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_activations.py
+-rw-rw-rw-  2.0 fat     1568 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_batchnorm.py
+-rw-rw-rw-  2.0 fat     3986 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_conv.py
+-rw-rw-rw-  2.0 fat     1556 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_instancenorm.py
+-rw-rw-rw-  2.0 fat     2762 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_linear.py
+-rw-rw-rw-  2.0 fat     7849 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_module.py
+-rw-rw-rw-  2.0 fat     3556 b- defN 24-May-06 08:47 modelopt/torch/quantization/nn/modules/quant_pooling.py
+-rw-rw-rw-  2.0 fat    34362 b- defN 24-May-07 07:17 modelopt/torch/quantization/nn/modules/tensor_quantizer.py
+-rw-rw-rw-  2.0 fat     1892 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/__init__.py
+-rw-rw-rw-  2.0 fat     1484 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/apex.py
+-rw-rw-rw-  2.0 fat     4226 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/custom.py
+-rw-rw-rw-  2.0 fat     3453 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/diffusers.py
+-rw-rw-rw-  2.0 fat     2489 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/huggingface.py
+-rw-rw-rw-  2.0 fat     4804 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/megatron.py
+-rw-rw-rw-  2.0 fat     2472 b- defN 24-May-06 08:47 modelopt/torch/quantization/plugins/nemo.py
+-rw-rw-rw-  2.0 fat     3061 b- defN 24-May-06 08:47 modelopt/torch/quantization/src/tensor_quant.cpp
+-rw-rw-rw-  2.0 fat     1320 b- defN 24-May-06 08:47 modelopt/torch/quantization/src/tensor_quant_fp8.cpp
+-rw-rw-rw-  2.0 fat     5557 b- defN 24-May-06 08:47 modelopt/torch/quantization/src/tensor_quant_gpu.cu
+-rw-rw-rw-  2.0 fat     2082 b- defN 24-May-06 08:47 modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu
+-rw-rw-rw-  2.0 fat      671 b- defN 24-May-06 08:47 modelopt/torch/sparsity/__init__.py
+-rw-rw-rw-  2.0 fat     1641 b- defN 24-May-06 08:47 modelopt/torch/sparsity/config.py
+-rw-rw-rw-  2.0 fat     5352 b- defN 24-May-06 08:47 modelopt/torch/sparsity/magnitude.py
+-rw-rw-rw-  2.0 fat     6847 b- defN 24-May-06 08:47 modelopt/torch/sparsity/mode.py
+-rw-rw-rw-  2.0 fat     3588 b- defN 24-May-06 08:47 modelopt/torch/sparsity/module.py
+-rw-rw-rw-  2.0 fat     3199 b- defN 24-May-06 08:47 modelopt/torch/sparsity/searcher.py
+-rw-rw-rw-  2.0 fat    10791 b- defN 24-May-06 08:47 modelopt/torch/sparsity/sparsegpt.py
+-rw-rw-rw-  2.0 fat     5740 b- defN 24-May-06 08:47 modelopt/torch/sparsity/sparsification.py
+-rw-rw-rw-  2.0 fat      990 b- defN 24-May-06 08:47 modelopt/torch/sparsity/plugins/__init__.py
+-rw-rw-rw-  2.0 fat     2762 b- defN 24-May-06 08:47 modelopt/torch/sparsity/plugins/megatron.py
+-rw-rw-rw-  2.0 fat      804 b- defN 24-May-06 08:47 modelopt/torch/utils/__init__.py
+-rw-rw-rw-  2.0 fat     5582 b- defN 24-May-06 08:47 modelopt/torch/utils/_pytree.py
+-rw-rw-rw-  2.0 fat     2419 b- defN 24-May-06 08:47 modelopt/torch/utils/cpp_extension.py
+-rw-rw-rw-  2.0 fat     7448 b- defN 24-May-06 08:47 modelopt/torch/utils/dataset_utils.py
+-rw-rw-rw-  2.0 fat     8262 b- defN 24-May-06 08:47 modelopt/torch/utils/distributed.py
+-rw-rw-rw-  2.0 fat     3894 b- defN 24-May-06 08:47 modelopt/torch/utils/graph.py
+-rw-rw-rw-  2.0 fat     1951 b- defN 24-May-06 08:47 modelopt/torch/utils/list.py
+-rw-rw-rw-  2.0 fat     2991 b- defN 24-May-06 08:47 modelopt/torch/utils/logging.py
+-rw-rw-rw-  2.0 fat    22299 b- defN 24-May-06 08:47 modelopt/torch/utils/network.py
+-rw-rw-rw-  2.0 fat     2816 b- defN 24-May-06 08:47 modelopt/torch/utils/perf.py
+-rw-rw-rw-  2.0 fat     5706 b- defN 24-May-06 08:47 modelopt/torch/utils/random.py
+-rw-rw-rw-  2.0 fat     1999 b- defN 24-May-06 08:47 modelopt/torch/utils/tensor.py
+-rw-rw-rw-  2.0 fat      540 b- defN 24-May-07 07:19 nvidia_modelopt-0.11.1.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     2589 b- defN 24-May-07 07:19 nvidia_modelopt-0.11.1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 24-May-07 07:19 nvidia_modelopt-0.11.1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       72 b- defN 24-May-07 07:18 nvidia_modelopt-0.11.1.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat        9 b- defN 24-May-07 07:18 nvidia_modelopt-0.11.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    12435 b- defN 24-May-07 07:19 nvidia_modelopt-0.11.1.dist-info/RECORD
+128 files, 1329935 bytes uncompressed, 452482 bytes compressed:  66.0%
```

## zipnote {}

```diff
@@ -54,14 +54,17 @@
 
 Filename: modelopt/onnx/quantization/partitioning.py
 Comment: 
 
 Filename: modelopt/onnx/quantization/qdq_utils.py
 Comment: 
 
+Filename: modelopt/onnx/quantization/quant_utils.py
+Comment: 
+
 Filename: modelopt/onnx/quantization/quantize.py
 Comment: 
 
 Filename: modelopt/torch/__init__.py
 Comment: 
 
 Filename: modelopt/torch/_deploy/__init__.py
@@ -234,14 +237,20 @@
 
 Filename: modelopt/torch/quantization/nn/modules/_utils.py
 Comment: 
 
 Filename: modelopt/torch/quantization/nn/modules/clip.py
 Comment: 
 
+Filename: modelopt/torch/quantization/nn/modules/quant_activations.py
+Comment: 
+
+Filename: modelopt/torch/quantization/nn/modules/quant_batchnorm.py
+Comment: 
+
 Filename: modelopt/torch/quantization/nn/modules/quant_conv.py
 Comment: 
 
 Filename: modelopt/torch/quantization/nn/modules/quant_instancenorm.py
 Comment: 
 
 Filename: modelopt/torch/quantization/nn/modules/quant_linear.py
@@ -351,26 +360,26 @@
 
 Filename: modelopt/torch/utils/random.py
 Comment: 
 
 Filename: modelopt/torch/utils/tensor.py
 Comment: 
 
-Filename: nvidia_modelopt-0.11.0.dist-info/LICENSE
+Filename: nvidia_modelopt-0.11.1.dist-info/LICENSE
 Comment: 
 
-Filename: nvidia_modelopt-0.11.0.dist-info/METADATA
+Filename: nvidia_modelopt-0.11.1.dist-info/METADATA
 Comment: 
 
-Filename: nvidia_modelopt-0.11.0.dist-info/WHEEL
+Filename: nvidia_modelopt-0.11.1.dist-info/WHEEL
 Comment: 
 
-Filename: nvidia_modelopt-0.11.0.dist-info/entry_points.txt
+Filename: nvidia_modelopt-0.11.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: nvidia_modelopt-0.11.0.dist-info/top_level.txt
+Filename: nvidia_modelopt-0.11.1.dist-info/top_level.txt
 Comment: 
 
-Filename: nvidia_modelopt-0.11.0.dist-info/RECORD
+Filename: nvidia_modelopt-0.11.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## modelopt/onnx/op_types.py

```diff
@@ -41,14 +41,15 @@
         "Elu",
         "Tanh",
         "Sigmoid",
         # "BatchNormalization",
         "Softmax",
         "Softplus",
         "InstanceNormalization",
+        "CumSum",
     ]
 
 
 def is_binary_op(op_type: str):
     """Returns whether the given op is a binary operator or not."""
     return op_type in [
         "Add",
@@ -74,15 +75,15 @@
     """Returns whether the given op type is of reduction category and fusible by Myelin."""
     return op_type in [
         "ReduceMax",
         "ReduceMin",
         "ReduceMean",
         "ReduceProd",
         "ReduceSum",
-        "TopK",  # Tranformed to BottomK based on `largest` param
+        "TopK",  # Transformed to BottomK based on `largest` param
     ]
 
 
 def is_copy_op(op_type: str):
     """Returns whether the given op is a copy operator or not."""
     return op_type in [
         "Flatten",
@@ -143,15 +144,15 @@
         "GroupNormalization",
         "LayerNormalization",
     ]
 
 
 def is_conversion_op(op_type: str):
     """Returns whether the given op type is of Conversion category or not."""
-    return op_type in ["Cast", "Quantize", "Dequantize"]
+    return op_type in ["Cast", "QuantizeLinear", "DequantizeLinear"]
 
 
 def is_non_reshape_copy_op(op_type: str):
     """Returns whether the given op is a non-reshape copy op or not."""
     return is_copy_op(op_type) and (op_type != "Reshape")
 
 
@@ -181,15 +182,24 @@
         "RandomUniform",
         "Bernoulli",
     ]
 
 
 def is_modifier_op(op_type: str):
     """Returns whether the given op type is of Modifier category or not."""
-    return op_type in ["Identity", "Trilu", "Expand", "Pad", "TileDropout", "Col2Im", "MaxUnpool"]
+    return op_type in [
+        "Identity",
+        "Trilu",
+        "Expand",
+        "Pad",
+        "Dropout",
+        "TileDropout",
+        "Col2Im",
+        "MaxUnpool",
+    ]
 
 
 def is_sequence_op(op_type: str):
     """Returns whether the given op type is of Sequence category or not."""
     return op_type in [
         "SequenceAt",
         "SequenceConstruct",
```

## modelopt/onnx/utils.py

```diff
@@ -14,15 +14,15 @@
 from collections import defaultdict
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import onnx
 import onnx.onnx_cpp2py_export.checker as C  # noqa: N812
 from onnx import numpy_helper
-from onnx_graphsurgeon.ir.node import Node
+from onnx_graphsurgeon import Node, Variable
 
 
 def get_input_names_from_bytes(model_bytes: bytes, external_inputs_only: bool = True) -> List[str]:
     """This function returns the inputs names of the given onnx model in bytes.
 
     Args:
         model_bytes: Onnx model in bytes.
@@ -550,14 +550,24 @@
     for tensor in node.outputs:
         for consumer in tensor.outputs:  # Traverse all consumer of the tensor
             children.append(consumer)
 
     return children
 
 
+def get_variable_inputs(node: Node) -> List[Variable]:
+    """Returns the variable inputs of the given Node."""
+    var_inputs = []
+    for tensor in node.inputs:
+        if isinstance(tensor, Variable):
+            if not tensor.inputs or (tensor.inputs and tensor.inputs[0].op != "Constant"):
+                var_inputs.append(tensor)
+    return var_inputs
+
+
 def save_onnx(
     onnx_model: onnx.onnx_ml_pb2.ModelProto, onnx_path: str, save_as_external_data: bool = False
 ):
     """Save an ONNX model to given path."""
     onnx.save_model(
         onnx_model,
         onnx_path,
```

## modelopt/onnx/quantization/graph_utils.py

```diff
@@ -92,15 +92,15 @@
     # Note, matching path with Add/Mul type nodes with const input will fail
     node_type = node.op
     if node_type == "Add" and has_const_input(node):
         node_type = "BiasAdd"
     elif node_type == "Mul" and has_const_input(node):
         node_type = "ConstMul"
 
-    # Check if current non-wild node type does not matche the expected path type
+    # Check if current non-wild node type does not match the expected path type
     # And if path type is not optional (ex. BiasAdd)
     is_match = (node_type == path_type[0]) or (node.op == path_type[0])
     is_wild_match = node_type in wild_card_types
     if not is_match and not is_wild_match and (path_type[0] not in optional_path_types):
         return False
 
     # Add current node name in the path
@@ -176,16 +176,15 @@
         ["BiasAdd", "ConstMul", "Conv"],
         ["Relu", "BiasAdd", "ConstMul", "Conv"],
         ["BatchNormalization", "BiasAdd", "Conv"],
         ["Relu", "BatchNormalization", "BiasAdd", "Conv"],
     ]
     for idx, path_type in enumerate(fusible_linear_path_types):
         if has_path_type(node, graph, path_type, is_forward=False, wild_card_types=[]):
-            # Examples of models requiring the outputs check:  FILF_v1_1, Convnext-tiny_opset13 and 17.
-            return _get_backbone(node) if len(node.outputs[0].outputs) == 1 else None
+            return _get_backbone(node)
 
     return None
 
 
 def filter_quantizable_kgen_heads(
     cask_fusible_partitions: List[List[Node]],
     kgen_partitions: List[List[Node]],
@@ -194,54 +193,46 @@
     """Returns the list of kgen head names if it follows a CASK partition."""
     cask_partition_nodes = set()
     for partition in cask_fusible_partitions:
         cask_partition_nodes.update([node.name for node in partition])
 
     cask_partition_heads = [partition[0] for partition in cask_fusible_partitions]
 
-    def _is_followed_by_cask_partition(node: Node):
+    def _is_following_cask_partition(node: Node):
         # Checking if cask fusible partition can be reached backward
         # ignoring the copy ops
         if node.name in cask_partition_nodes:
             return True
 
         if not is_copy_op(node.op):
             return False
 
         for parent in get_parent_nodes(node):
-            if _is_followed_by_cask_partition(parent):
+            if _is_following_cask_partition(parent):
                 return True
 
         return False
 
     def _has_other_quantizable_consumer(
         tensor: Tensor, quantizable_kgen_heads: List[Node], head_name: str
     ):
         # Note. this is kinda approximate analysis,
         # all quantizable kgen heads may haven't got discovered yet
         quantizable_ops = [node.name for node in cask_partition_heads + quantizable_kgen_heads]
 
-        # Look for other quantizable consumer than the currnet kgen head
+        # Look for other quantizable consumer than the current kgen head
         if head_name in quantizable_ops:
             quantizable_ops.remove(head_name)
 
         for consumer in tensor.outputs:
             if consumer.name in quantizable_ops:
                 return True
 
         return False
 
-    def _has_non_quantizable_consumer(tensor: Tensor):
-        for consumer in tensor.outputs:
-            # Ex. coatnet_0.onnx to avoid mul quantization in BN -> sigmoid -> mul pattern
-            # Ideally, this should be fixed by _is_cask_fusible logic which is not complete yet
-            if consumer.op in ["Sigmoid"]:
-                return True
-        return False
-
     quantizable_kgen_heads = []
     no_quantize_inputs = []  # list of tuple [(src_node_name, dst_node_name, input_name), ...]
     output_quantization_candidates = [
         "AveragePool",
         "BatchNormalization",
         "GlobalAveragePool",
         "MaxPool",
@@ -261,32 +252,26 @@
         no_quantize_inputs_of_head = []
         has_quantizable_input = False
 
         # Check each of the parent (input producer for partition head)
         # or predecessor nodes and see if output quantization is needed for them
         # and decide which input of kgen head needs quantization
         for parent in head_parents:
-            if _has_non_quantizable_consumer(parent.outputs[0]):
-                continue
-
-            # If the head is followed by quantizable ops/groups
-            if (
-                _is_followed_by_cask_partition(parent)
-                or parent.op in output_quantization_candidates
-            ):
+            # If the head is consuming output of any quantizable op, then it is quantizable
+            if _is_following_cask_partition(parent) or parent.op in output_quantization_candidates:
                 quantizable_kgen_heads.append(partition[0])
                 has_quantizable_input = True
-            # If has no quantizable consumer of the input from parent
+            # If the input from the current parent has no other quantizable consumer, do not quantize that input
             elif not _has_other_quantizable_consumer(
                 parent.outputs[0], quantizable_kgen_heads, head_node.name
             ):
                 no_quantize_inputs_of_head.append((parent, partition[0], parent.outputs[0].name))
 
-        # If at least one input is quantizable, collect if there is any non-quantizable inputs
-        if has_quantizable_input:
+        # If at least one input of Add is quantizable, collect if there is any non-quantizable inputs
+        if head_node.op == "Add" and has_quantizable_input:
             no_quantize_inputs.extend(no_quantize_inputs_of_head)
 
     return quantizable_kgen_heads, no_quantize_inputs
 
 
 def classify_partition_nodes(
     partitions: List[List[Node]],
@@ -418,15 +403,19 @@
     """
     logging.info("Deleting QDQ nodes from marked inputs to make certain operations fusible ...")
     graph_nodes = {node.name: node for node in graph.nodes}
     for source, target, non_qdq_input_name in no_quantize_inputs:
         # Note. no_quantize_inputs objects are from non-quantized input graph
         # we are deleting some QDQ from the new quantized output graph
         source_node = graph_nodes[source.name]
-        dq_node = source_node.o().o()
+        try:
+            dq_node = source_node.o().o()
+        except Exception:
+            # Reached end of the graph
+            continue
         if dq_node.op == "DequantizeLinear":
             dq_node = dq_node.outputs[0]  # source_node->Q->DQ->target_node
             while len(dq_node.outputs):
                 # Find the input index in the target connecting with source_node
                 target_input_idx_arr = [
                     idx
                     for idx, inp in enumerate(dq_node.outputs[0].inputs)
```

## modelopt/onnx/quantization/gs_patching.py

```diff
@@ -14,26 +14,24 @@
 
 import numpy as np
 import onnx
 import onnx_graphsurgeon as gs
 from onnx.mapping import TENSOR_TYPE_MAP
 from onnx_graphsurgeon.ir.tensor import LazyValues
 
+from modelopt.onnx.quantization.quant_utils import pack_float32_to_4bit_optimized
+
 GS_LOGGER = gs.logger.logger.G_LOGGER
 ONNX_MAJOR, ONNX_MINOR, ONNX_REV = tuple(map(int, onnx.__version__.split(".")[:3]))
 
 
 def _onnx_supports_int4():
     return ONNX_MAJOR > 1 or ONNX_MAJOR == 1 and ONNX_MINOR >= 16
 
 
-if _onnx_supports_int4():
-    from onnx.helper import pack_float32_to_4bit
-
-
 def _make_constant(
     name: str, values: Union[np.ndarray, LazyValues], dtype: onnx.TensorProto.DataType
 ) -> gs.Constant:
     """Creates a constant with a specified dtype."""
     converted_dtype = (
         dtype if isinstance(values, LazyValues) else TENSOR_TYPE_MAP[int(dtype)].np_dtype
     )
@@ -65,15 +63,15 @@
             tensor, "explicit_dtype", onnx.helper.np_dtype_to_tensor_dtype(tensor.values.dtype)
         )
 
         vals = tensor.values
         if _onnx_supports_int4() and dtype in [onnx.TensorProto.INT4, onnx.TensorProto.UINT4]:
             signed = dtype == onnx.TensorProto.INT4
             np_dtype = onnx.helper.tensor_dtype_to_np_dtype(dtype)
-            vals = pack_float32_to_4bit(tensor.values, signed=signed).astype(np_dtype)
+            vals = pack_float32_to_4bit_optimized(tensor.values, signed=signed).astype(np_dtype)
 
         onnx_tensor = onnx.helper.make_tensor(
             tensor.name,
             dtype,
             dims=tensor.values.shape,
             vals=vals.tobytes(),
             raw=True,
```

## modelopt/onnx/quantization/int4.py

```diff
@@ -35,14 +35,15 @@
 import numpy
 import onnx
 import onnx.numpy_helper as numpy_helper
 import onnx_graphsurgeon as gs
 from onnxruntime.quantization.calibrate import CalibrationDataReader
 
 import modelopt.onnx.quantization.qdq_utils as qdq
+from modelopt.onnx.quantization.gs_patching import patch_gs_modules
 from modelopt.onnx.quantization.ort_utils import create_inference_session
 from modelopt.onnx.utils import save_onnx
 
 # Set logging level to info
 logging.getLogger().setLevel(logging.INFO)
 
 BLOCK_SIZE = 128
@@ -105,52 +106,64 @@
     num_blocks = w_padded.shape[0] // s.shape[0]
     w_padded = w_padded * s.repeat(num_blocks, axis=0)
     return _depad(w_padded, w.shape)
 
 
 def quantize_int4_rtn(
     onnx_model: onnx.onnx_pb.ModelProto,
-    scale_type: onnx.TensorProto.DataType,
+    gemm_io_type: onnx.TensorProto.DataType,
     dq_only: bool = False,
 ) -> onnx.onnx_pb.ModelProto:
     """Quantizes `onnx_model` using the RTN (Round-to-Nearest) algorithm.
 
     This algorithm computes scale factors by computing s = max(abs(block)) / 8, for each block. The
     quantized weights are computed via Q(w) = round_to_even(w / s), where `round_to_even` denotes
     rounding ties to the nearest even integer (i.e. 1.5, 2.5 both round to 2).
 
     Always selects the first dimension (0) to block over. This is because we must batch over the Cin
     dimension, and in ONNX, weights are always plugged into the RHS (i.e. y = x @ W).
     """
     graph = gs.import_onnx(onnx_model)
     gemm_nodes = [node for node in graph.nodes if node.op in ["Gemm", "MatMul"]]
     gemm_tensors = {}
+    act_tensors = []
     for gemm in gemm_nodes:
         for in_tensor in gemm.inputs:
             if not isinstance(in_tensor, gs.Constant):
                 continue
             if len(in_tensor.values.shape) == 1:
                 # 1D blocked quantization not supported.
                 continue
             gemm_tensors[in_tensor.name] = in_tensor
+            act_tensors.append(gemm.inputs[0])
 
     gemm_weights = {name: tensor.values for name, tensor in gemm_tensors.items()}
     scales = {name: find_scales(w, BLOCK_SIZE) for name, w in gemm_weights.items()}
     logging.info("Computed scales.")
 
+    # Change the scale type to the expected type, fp16 by default
+    for name, w in scales.items():
+        s = scales[name]
+        scales[name] = s.astype(onnx.mapping.TENSOR_TYPE_MAP[gemm_io_type].np_dtype)
+
+    # Change the input activation type to the expected type, fp16 by default
+    for act_tensor in act_tensors:
+        _change_input_type(onnx_model.graph, act_tensor.name, gemm_io_type)
+
+    # Import the update graph
+    graph = gs.import_onnx(onnx_model)
+
     if dq_only:
         # Calculate actual quantized weights.
         gemm_weights_quantized = {}
         for name, w in gemm_weights.items():
-            s = scales[name]
-            scales[name] = s.astype(onnx.mapping.TENSOR_TYPE_MAP[scale_type].np_dtype)
-            gemm_weights_quantized[name] = rtn(w, s, BLOCK_SIZE)
+            gemm_weights_quantized[name] = rtn(w, scales[name], BLOCK_SIZE)
+
         qdq.insert_dq_nodes(graph, scales, quantized_weights=gemm_weights_quantized)
     else:
-        # TODO: with both Q and DQ nodes, the DQ node scales are in float32, need change?
         qdq.insert_qdq_nodes(graph, scales, weight_map=gemm_tensors)
 
     logging.info(f"Inserted {'DQ' if dq_only else 'Q/DQ'} nodes.")
     return gs.export_onnx(graph)
 
 
 def quant_tensor(w: np.ndarray, block_size: int, alpha: float = 1.0):
@@ -252,52 +265,80 @@
 
     # Update the best alpha value for the weight blocks
     awq_clip.update_best_params()
 
 
 def _find_quantizable_weights(
     graph: onnx.onnx_pb.GraphProto,
-) -> List[Tuple[onnx.onnx_pb.ValueInfoProto, onnx.onnx_pb.ValueInfoProto]]:
+) -> List[Tuple[onnx.onnx_pb.ValueInfoProto, onnx.onnx_pb.ValueInfoProto, bool]]:
     """Finds the quantizable weights from the graph."""
     wa_pack = []
-    gemm_nodes = [node for node in graph.node if node.op_type in ["MatMul"]]
+    gemm_nodes = [node for node in graph.node if node.op_type in ["Gemm", "MatMul"]]
     initializer_idxs = {initializer.name: idx for idx, initializer in enumerate(graph.initializer)}
     for gemm in gemm_nodes:
         if gemm.input[0] in initializer_idxs:
             # Ex. two const input to MatMul_115 in fastvit0.onnx
+            # Note. RTN algorithm will quantize these weights though
             continue
 
         if gemm.input[1] not in initializer_idxs:
             continue
 
         weight_tensor = graph.initializer[initializer_idxs[gemm.input[1]]]
         if len(weight_tensor.dims) == 1:  # 1D blocked quantization not supported
             continue
 
         act_tensor = onnx.helper.ValueInfoProto()
         act_tensor.name = gemm.input[0]
-        wa_pack.append((act_tensor, weight_tensor))
+
+        # TODO: support transA by transposing activation tensors in _clip_search
+        do_transpose = gemm.op_type == "Gemm" and any(
+            [attr.name == "transB" and attr.i > 0 for attr in gemm.attribute]
+        )
+
+        wa_pack.append((act_tensor, weight_tensor, do_transpose))
 
     return wa_pack
 
 
-def _augment_graph(graph: onnx.onnx_pb.GraphProto, wa_pack: List[Tuple[gs.Tensor, gs.Tensor]]):
+def _augment_graph(
+    graph: onnx.onnx_pb.GraphProto, wa_pack: List[Tuple[gs.Tensor, gs.Tensor, bool]]
+):
     """Extend graph outputs with MatMuls activation input."""
     augmented_outputs = set([tensor.name for tensor in graph.output])
-    for act_tensor, _ in wa_pack:
+    for act_tensor, _, _ in wa_pack:
         if act_tensor.name not in augmented_outputs:
             graph.output.append(act_tensor)
             augmented_outputs.add(act_tensor.name)
 
 
+def _change_input_type(
+    graph: onnx.onnx_pb.GraphProto, input_name: str, gemm_io_type: onnx.TensorProto.DataType
+):
+    # Find the corresponding value info in the graph
+    done = False
+    for value_info in graph.value_info:
+        if value_info.name == input_name:
+            value_info.type.tensor_type.elem_type = gemm_io_type
+            done = True
+            break
+
+    if not done:
+        # If input not in value_info, it must be a graph input
+        for input_info in graph.input:
+            if input_info.name == input_name:
+                input_info.type.tensor_type.elem_type = gemm_io_type
+                break
+
+
 def quantize_int4_awq_clip(
     onnx_model: onnx.onnx_pb.ModelProto,
     data_reader: CalibrationDataReader,
     use_external_data_format: bool,
-    scale_type: onnx.TensorProto.DataType,
+    gemm_io_type: onnx.TensorProto.DataType,
 ) -> onnx.onnx_pb.ModelProto:
     """Quantizes `onnx_model` using the Activation aware quantization a.k.a AWQ algorithm."""
     logging.info("Finding quantizable weights and augmenting graph output with input activations")
     t = time.time()
     augmented_model = copy.deepcopy(onnx_model)
     graph = augmented_model.graph
 
@@ -329,45 +370,59 @@
         inputs.append(inp_d)
         assert isinstance(inp_d, dict)
 
     # Apply AWQ clip on selected weights
     logging.info("Started clip search ...")
     t = time.time()
     alphas = {}
-    for act_tensor, weight_tensor in wa_pack:
+    for act_tensor, weight_tensor, do_transpose in wa_pack:
         # First capture all the  activation values after calibration data sweep
         output_dicts = {}
         for inp_d in inputs:
             np_inp_d = {name: numpy.asarray(tensor) for name, tensor in inp_d.items()}
             output = session.run([act_tensor.name], np_inp_d)
             output_dicts.setdefault(act_tensor.name, []).append(output[0])
 
         # Concatenating the activation tensors over all calib data
         x = np.concatenate(output_dicts[act_tensor.name], axis=0)  # n_token, ci
         w = numpy_helper.to_array(
             weight_tensor, base_dir=os.path.dirname(augmented_onnx_path)
         ).copy()
+        if do_transpose:
+            w = w.T
 
         awq_clip = AWQClipHelper(w, BLOCK_SIZE)
         _clip_search(x, w, awq_clip)
         alphas[weight_tensor.name] = awq_clip.best_alpha
 
     logging.info(f"Clip search for all weights took {time.time() - t} seconds")
 
     # Compute quantized weights and scales which are needed for DQ nodes
     logging.info("Quantizing the actual weights ...")
     t = time.time()
-    for _, weight_tensor in wa_pack:
+    for act_tensor, weight_tensor, do_transpose in wa_pack:
         w = numpy_helper.to_array(
             weight_tensor, base_dir=os.path.dirname(augmented_onnx_path)
         ).copy()
+        if do_transpose:
+            w = w.T
+
         alpha = alphas.get(weight_tensor.name, 1)
         qw, scale = quant_tensor(w, BLOCK_SIZE, alpha)
-        scales[weight_tensor.name] = scale.astype(onnx.mapping.TENSOR_TYPE_MAP[scale_type].np_dtype)
+        if do_transpose:
+            qw = qw.T
+            scale = scale.T
+        scales[weight_tensor.name] = scale.astype(
+            onnx.mapping.TENSOR_TYPE_MAP[gemm_io_type].np_dtype
+        )
         gemm_weights_quantized[weight_tensor.name] = numpy.asarray(qw).astype(numpy.int8)
+
+        # Change the input activation type to the expected type, fp16 by default
+        # TODO: cast input C for Gemm
+        _change_input_type(onnx_model.graph, act_tensor.name, gemm_io_type)
     logging.info(f"Quantizing actual weights took {time.time() - t} seconds")
 
     logging.info("Inserting DQ nodes using quantized weights and scales ...")
     t = time.time()
     graph_gs = gs.import_onnx(onnx_model)
     qdq.insert_dq_nodes(graph_gs, scales, quantized_weights=gemm_weights_quantized)
     logging.info(f"Inserting DQ nodes took {time.time() - t} seconds")
@@ -389,25 +444,29 @@
 
 
 def quantize_int4(
     quantize_mode: str,
     onnx_model: onnx.onnx_pb.ModelProto,
     calibration_data_reader: CalibrationDataReader = None,
     use_external_data_format: bool = True,
-    scale_type: onnx.TensorProto.DataType = onnx.TensorProto.FLOAT16,
+    gemm_io_type: onnx.TensorProto.DataType = onnx.TensorProto.FLOAT16,
 ) -> onnx.onnx_pb.ModelProto:
     """Applies INT4 WoQ (Weight-Only-Quantization) to an ONNX file.
 
     Currently only GEMM quantization is supported.
     """
     logging.info(f"Quantization Mode: {quantize_mode}")
+
+    # Patch GS modules to support INT4.
+    patch_gs_modules()
+
     if "trt" in quantize_mode:
         qdq.use_trt_qdq_ops()
 
     if quantize_mode in ["int4_rtn", "int4_rtn_dq", "int4_rtn_trt", "int4_rtn_trt_dq"]:
-        return quantize_int4_rtn(onnx_model, scale_type, dq_only="dq" in quantize_mode)
+        return quantize_int4_rtn(onnx_model, gemm_io_type, dq_only="dq" in quantize_mode)
     elif quantize_mode in ["int4_awq_clip", "int4_awq_clip_trt"]:
         return quantize_int4_awq_clip(
-            onnx_model, calibration_data_reader, use_external_data_format, scale_type
+            onnx_model, calibration_data_reader, use_external_data_format, gemm_io_type
         )
     else:
         raise RuntimeError(f"Unsupported quant mode: '{quantize_mode}'")
```

## modelopt/onnx/quantization/partitioning.py

```diff
@@ -22,29 +22,32 @@
 )
 from modelopt.onnx.quantization.graph_utils import (
     get_fusible_backbone,
     has_const_input,
     has_path_type,
     is_const_input,
 )
-from modelopt.onnx.utils import get_child_nodes
+from modelopt.onnx.utils import (
+    get_child_nodes,
+    get_variable_inputs,
+)
 
 
 def _build_fusible_partition(
     cur_node: Node,
     fusible_partition: List[Node],
     partitioned_nodes: Set[str],
     non_residual_inputs: Dict[str, str],
     graph: Graph,
 ) -> None:
     """Traverses the graph starting from cur_node and updates the fusible_partition list.
 
     Add a nodes to the partition if any of these holds:
     1. The node is a unary or binary pointwise operation and fusible by cask
-    2. The node is BN or Relu and fusible with precedding Conv op
+    2. The node is BN and/or Relu and fusible with preceding Conv op
     3. The node is a residual Add and fusible with current partition
 
     Args:
         cur_node: Current candidate node for the partition.
         fusible_partition: Current fusible partition.
         partitioned_nodes: Set of already partitioned nodes.
         non_residual_inputs: Non-residual input map.
@@ -78,14 +81,39 @@
     def _is_cask_fusible(node: Node, partition_node_outputs: List[str]) -> bool:
         for tensor in node.inputs:
             if tensor.name not in partition_node_outputs:
                 if not is_const_input(tensor):
                     return False
         return True
 
+    def _is_fusible_mul(mul_node: Node) -> bool:
+        # Don't consider Mul as fusible if it has the indicated ancestors.
+        # Otherwise, this causes regressions in:
+        #  - densenet-12 and inception-v2-9 (dangling constants): [mul_node.op, "Unsqueeze"]
+        #  - faster_vit: ["Mul", "Add", "Tanh", "Mul", "Add", "Mul", "Pow"]
+        #  This improves perf in various models:  mobilenet_v3, vovnet19b, coatnet_0, regnety_040.
+
+        var_inps = get_variable_inputs(mul_node)
+        if len(var_inps) <= 1:
+            return True
+
+        # Conv-Sigmoid-Mul chain is fusible
+        if has_path_type(mul_node, graph, ["Mul", "Sigmoid", "Conv"], is_forward=False):
+            return True
+
+        non_fusible_patterns = [["Mul", "Sigmoid"], ["Mul", "HardSigmoid"]]
+        if any([has_path_type(mul_node, graph, p, is_forward=False) for p in non_fusible_patterns]):
+            return False
+
+        return True
+
+    # Check the Mul nodes for their fusion compatibility
+    if cur_node.op == "Mul" and not _is_fusible_mul(cur_node):
+        return
+
     # Add current node to the partition
     fusible_partition.append(cur_node)
     partitioned_nodes.add(cur_node.name)
 
     # If on non-residual path, return after adding the node to the partition
     # TODO: can Myelin fuse pointwise ops followed by residual Add?
     if cur_node.op == "Add" and non_residual_inputs[cur_node.name]:
@@ -155,37 +183,41 @@
                 fusible_partition,
                 partitioned_nodes,
                 non_residual_inputs,
                 graph,
             )
 
             # Gather the non-empty partitions
-            assert fusible_partition
-            partitioned_nodes.update([node.name for node in fusible_partition])
-            all_fusible_partitions.append(fusible_partition)
+            if fusible_partition:
+                partitioned_nodes.update([node.name for node in fusible_partition])
+                all_fusible_partitions.append(fusible_partition)
 
         return all_fusible_partitions
 
     cask_fusible_partitions = _partition_helper(is_linear_op)
     kgen_partitions = _partition_helper(is_pointwise_or_elementwise_op)
 
     return cask_fusible_partitions, kgen_partitions
 
 
-def get_skiped_output_layers(graph: Graph) -> List[str]:
+def get_skiped_output_layers(graph: Graph, paritially_quantizable_nodes: List[Node]) -> List[str]:
     """Returns the name of the non-quantizable output layers."""
     # TODO: see if input producer is already quantized or not
     # TODO: filter out input layers if consumer is not quantized already
     output_layers = []
+    paritially_quantizable_node_names = set([node.name for node in paritially_quantizable_nodes])
     graph_output_names = [tensor.name for tensor in graph.outputs]
 
     for node in graph.nodes:
         for tensor in node.outputs:
             if tensor.name in graph_output_names:
-                if node.op not in ["Conv", "Gemm", "MatMul"]:
+                if (
+                    node.op not in ["Conv", "Gemm", "MatMul"]
+                    and node.name not in paritially_quantizable_node_names
+                ):
                     output_layers.append(node.name)
 
     return output_layers
 
 
 def find_quantizable_nodes(
     graph: Graph,
@@ -239,19 +271,19 @@
 
 def find_hardcoded_patterns(graph: Graph) -> List[List[Node]]:
     """Finds some non-quantizable pre-defined patterns!.
 
     Note. matching this tail pattern causes MTL_v1 -5.5%
     ["ReduceSum", "Add", "Div", "Mul", "ReduceSum", "Sub", "Pow", "Mul", "ReduceSum", "Sqrt"]
     """
-    p1 = ["MatMul", "Transpose", "BatchNormalization", "Transpose", "Relu", "ReduceMax"]
+    gelu = ["Div", "Erf", "Add", "Mul", "Mul"]
 
     matched_node_names = []
     for node in graph.nodes:
-        for path_type in [p1]:
+        for path_type in [gelu]:
             path_nodes = []
             if has_path_type(
                 node,
                 graph,
                 path_type,
                 is_forward=True,
                 wild_card_types=[],
@@ -275,53 +307,44 @@
         "ReduceMean",
         "Add",
         "Sqrt",
         "Div",
         "Mul",
         "Add",
     ]
+    mean_var_norm_chain_types = layer_norm_chain_types[:-2]
     wild_card_types = ["Cast"]
     layer_norm_partitions = []
 
     for node in graph.nodes:
         layer_norm_partition = []
-        if node.op == "ReduceMean" and has_path_type(
-            node,
-            graph,
-            layer_norm_chain_types,
-            True,
-            wild_card_types,
-            layer_norm_partition,
+        if node.op == "LayerNormalization":
+            layer_norm_partitions.append([node])
+        elif node.op == layer_norm_chain_types[0] and has_path_type(
+            node, graph, layer_norm_chain_types, True, wild_card_types, layer_norm_partition
+        ):
+            layer_norm_partitions.append(layer_norm_partition)
+        elif node.op == mean_var_norm_chain_types[0] and has_path_type(
+            node, graph, mean_var_norm_chain_types, True, wild_card_types, layer_norm_partition
         ):
             layer_norm_partitions.append(layer_norm_partition)
 
     return layer_norm_partitions
 
 
 def find_mha_partitions(graph: Graph) -> List[List[Node]]:
-    """Finds the MHA patterns in the graph.
+    """Finds the MHA patterns in the graph that should not be quantized.
 
-    The most common MHA inplementation looks like this:
-    t -> MatMul -> "Add" -> Mul (Optional) -> Sub (Optional) -> Softmax -> MatMul -> output
-    Note, we do not match the optional nodes yet.
+    A common MHA implementation looks like this:
+    t -> MatMul -> (optional) Pointwise ops (such as Add, Mul, Sub) -> Softmax -> MatMul -> output
+    Patterns that do not look like that should not be quantized (at least for now).
     """
     mha_chain_types = [
         [
             "MatMul",
-            "Add",
-            "Softmax",
-            "MatMul",
-        ],
-        [
-            "MatMul",
-            "Add",
-            "Reshape",
-            "Add",
-            "Reshape",
-            "Softmax",
             "MatMul",
         ],
     ]
     mha_partitions = []
 
     for node in graph.nodes:
         if node.op == "MatMul":
@@ -329,23 +352,22 @@
                 mha_partition = []
                 if has_path_type(node, graph, chain_type, True, [], mha_partition):
                     mha_partitions.append(mha_partition)
 
     return mha_partitions
 
 
-def find_partitions_from_patterns(graph: Graph) -> List[List[str]]:
+def find_non_quantizable_partitions_from_patterns(graph: Graph) -> List[List[str]]:
     """Finds fusible partition from fixed patterns.
 
     Certain fused kernel counterpart is often a subgraph of native ops in onnx.
     Those patterns are identified here and quantized to match compiler expectation.
     """
     hard_coded_partitions = find_hardcoded_patterns(graph)
     layer_norm_partitions = find_layer_norm_partitions(graph)
-    # TODO: QDQ pattern for mha versions
-    # mha_partitions = find_mha_partitions(graph, node_children, tensor_producers)
+    mha_partitions = find_mha_partitions(graph)
 
     partitions = []
-    for partition_nodes in hard_coded_partitions + layer_norm_partitions:
+    for partition_nodes in hard_coded_partitions + layer_norm_partitions + mha_partitions:
         partitions.append([node.name for node in partition_nodes])
 
     return partitions
```

## modelopt/onnx/quantization/quantize.py

```diff
@@ -49,21 +49,20 @@
 from modelopt.onnx.quantization.graph_utils import (
     build_non_residual_input_map,
     classify_partition_nodes,
     filter_quantizable_kgen_heads,
     print_stat,
     remove_partial_input_qdq,
 )
-from modelopt.onnx.quantization.gs_patching import patch_gs_modules
 from modelopt.onnx.quantization.int4 import quantize_int4
 from modelopt.onnx.quantization.operators import QDQConvTranspose, QDQNormalization
 from modelopt.onnx.quantization.ort_patching import patch_ort_modules
 from modelopt.onnx.quantization.partitioning import (
     find_fusible_partitions,
-    find_partitions_from_patterns,
+    find_non_quantizable_partitions_from_patterns,
     find_quantizable_nodes,
     get_skiped_output_layers,
 )
 from modelopt.onnx.utils import (
     duplicate_shared_linear_weights,
     name_onnx_nodes,
     save_onnx,
@@ -148,15 +147,14 @@
     return matched_node_names
 
 
 def _configure_ort(op_types: List[str], op_types_to_quantize: List[str]):
     # Register some new QDQ operators on top of ORT
     QDQRegistry["BatchNormalization"] = QDQNormalization
     QDQRegistry["ConvTranspose"] = QDQConvTranspose
-    QDQRegistry["InstanceNormalization"] = QDQNormalization
     QDQRegistry["LRN"] = QDQNormalization  # Example: caffenet-12.onnx
     QDQRegistry["HardSwish"] = (
         QDQOperatorBase  # Example: mobilenet_v3_opset17, efficientvit_b3_opset17
     )
 
     # Patch ORT modules to fix bugs and support some edge cases
     patch_ort_modules()
@@ -213,21 +211,23 @@
     quantizable_op_types: List[str],
     verbose: bool,
 ) -> Tuple[List[Node], List[Tuple[Node, Node, str]]]:
     # Build a map of add nodes to their non-residual inputs, i.e. fusible with Conv group
     logging.info("Building non-residual Add input map ...")
     non_residual_inputs = build_non_residual_input_map(graph)
 
-    logging.info("Searching for hard-coded patterns like MHA, LayerNorm etc. ...")
-    hard_coded_partitions = find_partitions_from_patterns(graph)
+    logging.info(
+        "Searching for hard-coded patterns like MHA, LayerNorm, etc. to avoid quantization."
+    )
+    non_quantizable_hard_coded_partitions = find_non_quantizable_partitions_from_patterns(graph)
 
     logging.info("Building KGEN/CASK targeted partitions ...")
     # partitioned_nodes keeps track of nodes that are already part of some partition.
     # Certain nodes of those partitions are quantizable. For example, heads.
-    partitioned_nodes = set(sum(hard_coded_partitions, []))
+    partitioned_nodes = set(sum(non_quantizable_hard_coded_partitions, []))
     cask_fusible_partitions, kgen_partitions = find_fusible_partitions(
         graph,
         partitioned_nodes,
         non_residual_inputs,
     )
     if verbose:
         logging.info(
@@ -246,20 +246,25 @@
     quantizable_kgen_heads, no_quantize_kgen_inputs = filter_quantizable_kgen_heads(
         cask_fusible_partitions,
         kgen_partitions,
         quantizable_op_types,
     )
 
     quantizable_nodes = quantizable_kgen_heads + quantizable_partition_nodes
-    quantizable_nodes.extend([dst for _, dst, _ in no_quantize_inputs])
+    paritially_quantizable_nodes = [dst for _, dst, _ in no_quantize_inputs]
+
+    # Quantize all inputs of partially quantizable nodes by ORT
+    # but remove QDQ from non-quantizable inputs in the post-processing step
+    quantizable_nodes.extend(paritially_quantizable_nodes)
+
     quantizable_nodes.extend(
         find_quantizable_nodes(graph, quantizable_nodes, partitioned_nodes, quantizable_op_types)
     )
 
-    skip_list = get_skiped_output_layers(graph)
+    skip_list = get_skiped_output_layers(graph, paritially_quantizable_nodes)
     quantizable_nodes = [node for node in quantizable_nodes if node.name not in skip_list]
 
     return quantizable_nodes, no_quantize_inputs + no_quantize_kgen_inputs
 
 
 def _find_nodes_to_exclude(
     graph: Graph, nodes_to_exclude: List[str], op_types_to_exclude: List[str]
@@ -453,16 +458,14 @@
                 extra_options=trt_guided_options,
                 activation_type=QuantType.QFLOAT8E4M3FN,
                 weight_type=QuantType.QFLOAT8E4M3FN,
             )
             # Load the quantized output model for printing stat etc.
             onnx_model = onnx.load(output_path)
     else:
-        # Patch GS modules to support INT4.
-        patch_gs_modules()
         onnx_model = quantize_int4(
             quantize_mode,
             onnx_model,
             calibration_data_reader,
             use_external_data_format,
         )
```

## modelopt/torch/_deploy/utils/torch_onnx.py

```diff
@@ -44,31 +44,47 @@
 ONNX_EXPORT_OUT_PREFIX = "out"
 TWO_GB = 2147483648
 
 
 class OnnxBytes:
     """A class to save and load onnx models as bytes."""
 
-    def __init__(self, onnx_load_path: str) -> None:
+    def __init__(self, onnx_load_path: str, external_data_format: bool = False) -> None:
         """Loads the model from the specified path.
 
-        All the files inside the path are saved as a dictionary where the key is the file name and
-        the value is the file bytes.
-        The filename of the .onnx file will be saved as the name of the model.
+        If the model is loaded without external data format, then it is saved as a dictionary where
+        the key is the model name and the value is the model bytes.
+        If the model is loaded with external data format, then the model is saved as a dictionary
+        where the keys include all the file names in the model directory and the value are the corresponding file bytes.
+        For external data format, we assume that the external data for the model is saved in the same directory
+        as the model file.
+
+        Args:
+            onnx_load_path: The path to load the .onnx model file.
+            external_data_format: If True, the onnx model is loaded from the external data format.
         """
         self.onnx_model = {}
         self.model_name = ""
         print("Loading onnx model from path:", onnx_load_path)
-        for onnx_model_file in os.listdir(onnx_load_path):
-            with open(os.path.join(onnx_load_path, onnx_model_file), "rb") as f:
+        if external_data_format:
+            onnx_load_path = os.path.dirname(onnx_load_path)
+            for onnx_model_file in os.listdir(onnx_load_path):
+                with open(os.path.join(onnx_load_path, onnx_model_file), "rb") as f:
+                    self.onnx_model[onnx_model_file] = f.read()
+                if onnx_model_file.endswith(".onnx"):
+                    if self.model_name != "":
+                        raise ValueError("Multiple onnx files found in the directory")
+                    self.model_name = onnx_model_file.replace(".onnx", "")
+        else:
+            onnx_model_file = os.path.basename(onnx_load_path)
+            if not onnx_model_file.endswith(".onnx"):
+                raise ValueError("The file should be a .onnx file")
+            with open(onnx_load_path, "rb") as f:
                 self.onnx_model[onnx_model_file] = f.read()
-            if onnx_model_file.endswith(".onnx"):
-                if self.model_name != "":
-                    raise ValueError("Multiple onnx files found in the directory")
-                self.model_name = onnx_model_file.replace(".onnx", "")
+            self.model_name = onnx_model_file.replace(".onnx", "")
 
     def write_to_disk(self, onnx_save_path: str) -> None:
         """Writes the onnx model to the specified path."""
         if os.path.exists(onnx_save_path):
             print(f"Removing existing directory: {onnx_save_path}")
             shutil.rmtree(onnx_save_path)
         os.makedirs(onnx_save_path)
@@ -262,27 +278,27 @@
     flat_output, tree_spec_output = flatten_tree(output, prefix=ONNX_EXPORT_OUT_PREFIX)
 
     # output names are the names of the flattened input tree spec but without None values
     output_names = [k for k, v in zip(tree_spec_output.names, flat_output) if v is not None]
 
     model_name = model.__class__.__name__
     onnx_build_folder = "./build/onnx/"
-    onnx_path = f"{onnx_build_folder}{model_name}"
+    onnx_path = os.path.join(onnx_build_folder, model_name)
     os.makedirs(onnx_path, exist_ok=True)
     onnx_save_path = os.path.join(onnx_path, f"{model_name}.onnx")
 
     # If the onnx_load path is specified by the user or if an onnx model exists at the default path
     # then the model is loaded from this path and returned along with the metadata
     if os.path.exists(onnx_save_path):
-        print(f"Overriding onnx load path to {onnx_path}")
-        onnx_load_path = onnx_path
+        print(f"Overriding onnx load path to {onnx_save_path}")
+        onnx_load_path = onnx_save_path
 
     if onnx_load_path != "":
         onnx_model = OnnxBytes(onnx_load_path)
-        onnx_model_graph = onnx.load(os.path.join(onnx_load_path, onnx_model.model_name + ".onnx"))
+        onnx_model_graph = onnx.load(os.path.join(onnx_load_path))
         model_metadata = create_model_metadata(
             tree_spec_input, tree_spec_output, input_none_names, onnx_model_graph, model
         )
         return onnx_model.to_bytes(), model_metadata
 
     # Export onnx model from pytorch model
     # As the maximum size of protobuf is 2GB, we cannot use io.BytesIO() buffer during export.
@@ -323,15 +339,15 @@
             onnx_save_path,
             save_as_external_data=True,
             all_tensors_to_one_file=True,
             convert_attribute=False,
         )
     else:
         onnx.save(onnx_opt_graph, onnx_save_path)
-    onnx_bytes = OnnxBytes(os.path.dirname(onnx_save_path))
+    onnx_bytes = OnnxBytes(onnx_save_path)
 
     if remove_exported_model:
         shutil.rmtree(os.path.dirname(onnx_build_folder))
     return onnx_bytes.to_bytes(), model_metadata
 
 
 def create_onnx_model_dict(
```

## modelopt/torch/export/layer_utils.py

```diff
@@ -212,15 +212,14 @@
         if hasattr(module, "zero_centered_gamma") and module.zero_centered_gamma:
             return True
 
         return False
 
     if _weights_plus_one(module):
         # megatron layernorm's weight needs to be updated.
-        print("Layernorm weights = weights + 1")
         weight = weight.float() + 1.0
 
     config = LayernormConfig(
         weight=weight.type(dtype),
         bias=(
             module.bias.detach().type(dtype)
             if hasattr(module, "bias") and module.bias is not None
@@ -1055,14 +1054,18 @@
         if k in model_metadata_config:
             config.rotary_pct = model_metadata_config[k]
 
     for k in ["alibi"]:
         if k in model_metadata_config:
             config.use_alibi = model_metadata_config[k]
 
+    for k in ["alibi_bias_max"]:
+        if k in model_metadata_config:
+            config.alibi_bias_max = model_metadata_config[k]
+
     for k in ["new_decoder_architecture"]:
         if k in model_metadata_config:
             config.new_decoder_architecture = model_metadata_config[k]
 
     for k in ["apply_residual_connection_post_layernorm"]:
         if k in model_metadata_config:
             config.apply_residual_connection_post_layernorm = model_metadata_config[k]
```

## modelopt/torch/export/model_config.py

```diff
@@ -344,14 +344,17 @@
 
     # Mixture of Experts
     moe_num_experts: int = 0
     moe_top_k: int = 0
     moe_tp_mode: int = 0
     moe_renorm_mode: int = 0
 
+    # MPT
+    alibi_bias_max: int = 0
+
     # Arctic variants
     residual_layernorm: LayernormConfig = None
     residual_mlp: MLPConfig = None
 
     @property
     def hidden_size(self):
         """Returns the hidden size of the transformer model."""
```

## modelopt/torch/export/model_config_export.py

```diff
@@ -223,22 +223,30 @@
         elif is_layernorm(module):
             if has_embedding_layernorm and config.ln_embed is None:
                 # Assume embedding_layernorm is placed before the ln_f.
                 config.ln_embed = build_layernorm_config(module, dtype)
             else:
                 config.ln_f = build_layernorm_config(module, dtype)
         elif is_linear(module):
-            config.lm_head = build_linear_config(module, "column", dtype)
+            # TRT LLM forces the embedding table to be shared for the following models.
+            force_share_embedding_table = decoder_type in ["gemma"]
+            if force_share_embedding_table and torch.equal(
+                module.weight.to(dtype), config.vocab_embedding.weight
+            ):
+                config.share_embedding_table = True
+            else:
+                config.lm_head = build_linear_config(module, "column", dtype)
 
     # For the training time PP, not all ranks will have the lm_head layer.
     if config.lm_head is None and training_pipeline_parallel == 1:
         # Models that share weights for lm_head and vocab_embedding
         assert decoder_type in [
             "mpt",
             "gpt2",
+            "gemma",
         ], f"lm_head not available for decoder {decoder_type}"
         config.share_embedding_table = True
 
     config.quantization = config.layers[0].quantization
     if config.quantization in [QUANTIZATION_INT4_AWQ, QUANTIZATION_W4A8_AWQ]:
         if config.vocab_size % 64 != 0:
             # TODO: Check if this works for Mixtral
```

## modelopt/torch/export/tensorrt_llm_utils.py

```diff
@@ -205,14 +205,15 @@
                 "moe_top_k": model_config.layers[0].moe_top_k,
             }
         )
     elif decoder_type == "mpt":
         config.update(
             {
                 "clip_qkv": model_config.layers[0].attention.clip_qkv,
+                "alibi_bias_max": model_config.layers[0].alibi_bias_max,
             }
         )
     elif decoder_type == "qwen":
         config.update(
             {
                 "intermediate_size": model_config.layers[0].ffn_hidden_size_local * 2 * tp_size,
                 "seq_length": model_config.layers[0].seq_length,
```

## modelopt/torch/quantization/conversion.py

```diff
@@ -6,28 +6,27 @@
 # documentation and any modifications thereto. Any use, reproduction,
 # disclosure or distribution of this material and related documentation
 # without an express license agreement from NVIDIA CORPORATION or
 # its affiliates is strictly prohibited.
 """Quantization conversion/restore utilities."""
 
 import fnmatch
-import warnings
 from typing import Any, Callable, Dict, Union
 
 import torch.nn as nn
 
 from modelopt.torch.opt.conversion import ApplyModeError, ModelLikeModule
 from modelopt.torch.opt.dynamic import DynamicModule
 from modelopt.torch.opt.mode import ConvertReturnType, MetadataDict
 
 from .config import QuantizeConfig, _QuantizeExportConfig
-from .nn import QuantModuleRegistry, SequentialQuantizer, TensorQuantizer
+from .nn import QuantLinearConvBase, QuantModuleRegistry, SequentialQuantizer, TensorQuantizer
 from .plugins.custom import register_custom_model_plugins_on_the_fly
 from .tensor_quant import QuantDescriptor
-from .utils import is_quantized
+from .utils import is_quantized, is_quantized_layer_with_weight
 
 __all__ = [
     "replace_quant_module",
     "set_quantizer_by_cfg",
     "set_quantizer_attribute",
     "register",
     "unregister",
@@ -50,60 +49,46 @@
 def restore_quantized_model(
     model: nn.Module, config: QuantizeConfig, metadata: MetadataDict
 ) -> nn.Module:
     """Restores the quantizer states from the given state dict."""
     # initialize the true module if necessary
     model = model.init_modellike() if isinstance(model, ModelLikeModule) else model
 
-    def _get_parent_device(child_name):
-        parent_module = model.get_submodule(child_name.rpartition(".")[0])
-        # If the parent module is a sequential quantizer, get the device of the parent of the parent
-        if isinstance(parent_module, SequentialQuantizer):
-            return _get_parent_device(child_name.rpartition(".")[0].rpartition(".")[0])
-
-        try:
-            return next(parent_module.parameters()).device
-        except StopIteration:
-            # For modules without parameters
-            return None
-
     assert not is_quantized(model), "Model must not be quantized!"
 
     quantizer_state_dict = metadata["quantizer_state"]
 
     replace_quant_module(model)
-    SequentialQuantizer.restore_sequential_quantizers(model, quantizer_state_dict)
+    set_quantizer_by_cfg(model, config["quant_cfg"])
 
     unmatched_keys = quantizer_state_dict.keys() - quantizer_state(model).keys()
     extra_keys = quantizer_state(model).keys() - quantizer_state_dict.keys()
 
     if unmatched_keys:
         raise ApplyModeError(f"Unmatched keys in quantizer state_dict: {unmatched_keys}")
     if extra_keys:
         raise ApplyModeError(f"Extra keys in quantizer state_dict: {extra_keys}")
 
     for name, module in model.named_modules():
         if isinstance(module, TensorQuantizer):
-            device = _get_parent_device(name)
-            module.set_from_modelopt_state(quantizer_state_dict[name], name, device)
-            if device is None:
-                warnings.warn(
-                    f"Restoring quantizer {name} from state dict. Could not look up parent"
-                    " module device. Please move the model to the correct device after this. Model"
-                    " forward might throw error otherwise."
-                )
+            module.set_from_modelopt_state(quantizer_state_dict[name], name)
+
+    for name, module in model.named_modules():
+        if is_quantized_layer_with_weight(module):
+            QuantLinearConvBase.initialize_quantizer_with_dummy_states(module)
+        if isinstance(module, TensorQuantizer):
+            module.clean_up_after_set_from_modelopt_state(name)
 
     return model
 
 
 def update_quantize_metadata(
     model: nn.Module, config: QuantizeConfig, metadata: MetadataDict
 ) -> None:
     """Update the quantizer state in the metadata dict."""
-    config.quant_cfg = {}
     metadata["quantizer_state"] = quantizer_state(model)
 
 
 def quantizer_state(model: nn.Module) -> Dict[str, Any]:
     """Returns the quantizer state dict describing the quantizer states in the model."""
     return {
         n: m.get_modelopt_state()
```

## modelopt/torch/quantization/utils.py

```diff
@@ -13,15 +13,15 @@
 from contextlib import contextmanager
 
 import torch
 
 __all__ = [
     "reduce_amax",
     "is_quantized",
-    "is_quantized_linear",
+    "is_quantized_layer_with_weight",
     "is_quantized_column_parallel_linear",
     "is_quantized_row_parallel_linear",
     "replace_function",
     "EXPORT_MODE",
     "export_torch_mode",
     "is_torch_library_supported",
 ]
@@ -74,14 +74,19 @@
 
     for _module in module.modules():
         if isinstance(_module, TensorQuantizer):
             return True
     return False
 
 
+def is_quantized_layer_with_weight(module):
+    """Check if a module is quantized with weights."""
+    return is_quantized(module) and getattr(module, "weight", None) is not None
+
+
 def is_quantized_linear(module):
     """Check if a module is a quantized linear module."""
     return (
         hasattr(module, "input_quantizer")
         and hasattr(module, "weight_quantizer")
         and getattr(module, "weight", None) is not None
         and module.weight.dim() == 2
```

## modelopt/torch/quantization/nn/__init__.py

```diff
@@ -7,13 +7,15 @@
 # disclosure or distribution of this material and related documentation
 # without an express license agreement from NVIDIA CORPORATION or
 # its affiliates is strictly prohibited.
 
 """Modules with quantization support."""
 
 from .modules.clip import *
+from .modules.quant_activations import *
+from .modules.quant_batchnorm import *
 from .modules.quant_conv import *
 from .modules.quant_instancenorm import *
 from .modules.quant_linear import *
 from .modules.quant_module import *
 from .modules.quant_pooling import *
 from .modules.tensor_quantizer import *
```

## modelopt/torch/quantization/nn/modules/quant_module.py

```diff
@@ -7,33 +7,34 @@
 # disclosure or distribution of this material and related documentation
 # without an express license agreement from NVIDIA CORPORATION or
 # its affiliates is strictly prohibited.
 
 """Base class for quantization modules."""
 import contextlib
 import copy
+from typing import Union
 
 import torch
 
 from modelopt.torch.opt.dynamic import DynamicModule, _DMRegistryCls
 from modelopt.torch.quantization.utils import is_torch_export_mode
 
 from ...tensor_quant import QUANT_DESC_8BIT_PER_TENSOR, QuantDescriptor
-from .tensor_quantizer import TensorQuantizer
+from .tensor_quantizer import SequentialQuantizer, TensorQuantizer
 
 __all__ = ["QuantInputBase", "QuantLinearConvBase", "QuantModuleRegistry"]
 
 QuantModuleRegistry = _DMRegistryCls("Quant")
 
 
 class QuantInputBase(DynamicModule):
     """Base class for modules where the input is quantized."""
 
-    input_quantizer: TensorQuantizer
-    output_quantizer: TensorQuantizer
+    input_quantizer: Union[TensorQuantizer, SequentialQuantizer]
+    output_quantizer: Union[TensorQuantizer, SequentialQuantizer]
     default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
     default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
 
     def forward(self, input, *args, **kwargs):
         """Quantize the input before calling the original forward method."""
         input = self.input_quantizer(input)
         output = super().forward(input, *args, **kwargs)
@@ -54,15 +55,15 @@
 
 class QuantLinearConvBase(QuantInputBase):
     """Base class for quantized linear modules.
 
     Quantized linear modules are modules where both the input and the weight are quantized.
     """
 
-    weight_quantizer: TensorQuantizer
+    weight_quantizer: Union[TensorQuantizer, SequentialQuantizer]
     _enable_weight_quantization: bool
     default_quant_desc_weight = QUANT_DESC_8BIT_PER_TENSOR
 
     @contextlib.contextmanager
     def quantize_weight(self):
         """Context in which `self.weight` is quantized."""
         self._enable_weight_quantization = True
@@ -87,14 +88,58 @@
         super()._setup()
         self._register_temp_attribute(
             "weight_quantizer", TensorQuantizer(self.default_quant_desc_weight)
         )
         self._register_temp_attribute("_enable_weight_quantization", False)
         self._register_dynamic_attribute("weight", self._get_quantized_weight)
 
+    @staticmethod
+    def initialize_quantizer_with_dummy_states(module):
+        """Initialize the quantizer states with dummy values with the correct type and device."""
+        # Lets do a local import; nn modules should not import from model_calib
+        from modelopt.torch.quantization.model_calib import max_calibrate
+
+        def _initialize_activation_quantizer_amax(quantizer, device, dtype):
+            if not getattr(quantizer, "_has_amax", False):
+                return
+            # We need the outputs to initialize the amax in this case; Weights alone are not enough
+            if (
+                quantizer.block_sizes is not None
+                and quantizer.block_sizes.get("type", None) != "dynamic"
+            ):
+                return
+            if quantizer.axis is not None:
+                return
+            quantizer.amax = torch.tensor(1, device=device, dtype=dtype)
+
+        device, dtype = module.weight.device, module.weight.dtype
+
+        for input_quantizer in SequentialQuantizer.tensor_quantizer_iterator(
+            getattr(module, "input_quantizer", None)
+        ):
+            if getattr(input_quantizer, "_has_pre_quant_scale", False):
+                input_quantizer.pre_quant_scale = torch.ones(
+                    module.weight.shape[1], device=device, dtype=dtype
+                )
+
+            _initialize_activation_quantizer_amax(input_quantizer, device, dtype)
+
+        for output_quantizer in SequentialQuantizer.tensor_quantizer_iterator(
+            getattr(module, "output_quantizer", None)
+        ):
+            _initialize_activation_quantizer_amax(output_quantizer, device, dtype)
+
+        for weight_quantizer in SequentialQuantizer.tensor_quantizer_iterator(
+            getattr(module, "weight_quantizer", None)
+        ):
+            if getattr(weight_quantizer, "_has_amax", False):
+                max_calibrate(
+                    weight_quantizer, lambda weight_quantizer: weight_quantizer(module.weight)
+                )
+
 
 class _LegacyQuantInputBaseMixin:
     """A mixin to support legacy quantized modules which needs to have an __init__ method."""
 
     _quantized_cls = QuantInputBase
     default_quant_desc_input = QUANT_DESC_8BIT_PER_TENSOR
     default_quant_desc_output = QUANT_DESC_8BIT_PER_TENSOR
```

## modelopt/torch/quantization/nn/modules/tensor_quantizer.py

```diff
@@ -103,29 +103,22 @@
             self.clip = Clip(-init_amax, init_amax, learn_min=True, learn_max=True)
             # It makes more sense to enable clip stage (which learns amax) if learn_amax is true
             self.enable_clip()
         if if_clip:
             self.enable_clip()
 
         if quant_desc.calib_method == "histogram":
-            self._calib_constructor_args = (
-                calib.HistogramCalibrator,
-                (),
-                dict(num_bits=self._num_bits, axis=self._axis, unsigned=self._unsigned),
-            )
+            calib_cls = calib.HistogramCalibrator
         elif quant_desc.calib_method == "max":
-            self._calib_constructor_args = (
-                calib.MaxCalibrator,
-                (),
-                dict(num_bits=self._num_bits, axis=self._axis, unsigned=self._unsigned),
-            )
+            calib_cls = calib.MaxCalibrator
         else:
             raise ValueError(f"Unknown calib_method: {quant_desc.calib_method}")
-        calib_cls, args, kwargs = self._calib_constructor_args
-        self._calibrator = calib_cls(*args, **kwargs)
+        self._calibrator = calib_cls(
+            num_bits=self._num_bits, axis=self._axis, unsigned=self._unsigned
+        )
 
     @property
     def num_bits(self):
         """Return num_bits for quantization."""
         return self._num_bits
 
     @num_bits.setter
@@ -644,14 +637,26 @@
         elif src_has_amax and not dst_has_amax:
             warnings.warn(
                 f"{prefix[:-1]}: No '_amax' buffer to load amax into."
                 " '_amax` will be created as WAR for now. "
                 "This behavior will change in future."
             )
             self.register_buffer("_amax", state_dict[prefix + "_amax"].data.to(device))
+        elif (
+            src_has_amax
+            and dst_has_amax
+            and (self._amax.shape != state_dict[prefix + "_amax"].shape)
+        ):
+            # This is a workaround to support the sharded checkpoint loading in Megatron.
+            # The sharded checkpoint has the amax in a different shape.
+            assert self._amax.numel() == state_dict[prefix + "_amax"].numel(), (
+                f" {prefix[:-1]}: amax state cannot be loaded. Expected {self._amax.shape}, got"
+                f" {state_dict[prefix + '_amax'].shape}"
+            )
+            state_dict[prefix + "_amax"] = state_dict[prefix + "_amax"].view(self._amax.shape)
 
         dst_has_pre_quant_scale = "_pre_quant_scale" in self._buffers
         src_has_pre_quant_scale = prefix + "_pre_quant_scale" in state_dict
 
         if not src_has_pre_quant_scale and dst_has_pre_quant_scale:
             warnings.warn(f"{prefix[:-1]}: No pre_quant_scale in state_dict.")
         elif src_has_pre_quant_scale and not dst_has_pre_quant_scale:
@@ -662,77 +667,76 @@
             )
             self.register_buffer(
                 "_pre_quant_scale", state_dict[prefix + "_pre_quant_scale"].data.to(device)
             )
 
         super(TensorQuantizer, self)._load_from_state_dict(state_dict, prefix, *args, **kwargs)
 
+    def _get_skip_properties_for_modelopt_state(self):
+        return {"clip", "_calibrator", "_original_shape", "_block_reshape_size", "_padding"}
+
     def _get_properties_for_modelopt_state(self):
-        return self.__dict__.keys() - nn.Module().__dict__.keys() - {"clip", "_calibrator"}
+        return (
+            self.__dict__.keys()
+            - nn.Module().__dict__.keys()
+            - self._get_skip_properties_for_modelopt_state()
+        )
 
     def get_modelopt_state(self) -> Dict[str, Any]:
         """Get meta state to be saved in checkpoint."""
         modelopt_state = {}
         for k in self._get_properties_for_modelopt_state():
             modelopt_state[k] = getattr(self, k)
 
-        if self.amax is not None:
-            modelopt_state["_amax"] = self.amax
+        if hasattr(self, "_amax"):
+            modelopt_state["_has_amax"] = True
 
-        if self.pre_quant_scale is not None:
-            modelopt_state["_pre_quant_scale"] = self.pre_quant_scale
+        if hasattr(self, "_pre_quant_scale"):
+            modelopt_state["_has_pre_quant_scale"] = True
 
         if hasattr(self, "clip"):
             modelopt_state["_init_clip"] = True
 
         return modelopt_state
 
-    def set_from_modelopt_state(self, modelopt_state, prefix="", device=None):
+    def set_from_modelopt_state(self, modelopt_state, prefix=""):
         """Set meta state from checkpoint."""
-        for key in self._get_properties_for_modelopt_state():
-            if key in modelopt_state:
-                setattr(self, key, modelopt_state[key])
+        # Set all properties except the skip properties; this is done for backward compatibility
+        for key in modelopt_state.keys() - self._get_skip_properties_for_modelopt_state():
+            setattr(self, key, modelopt_state[key])
 
         if "_init_clip" in modelopt_state:
             # clip min and max parameters will be loaded from checkpoint
             self.clip = Clip(-1.0, 1.0, learn_min=True, learn_max=True)
 
-        if "_amax" in modelopt_state:
-            self.amax = modelopt_state["_amax"]
-
-        if "_pre_quant_scale" in modelopt_state:
-            self.pre_quant_scale = modelopt_state["_pre_quant_scale"]
+        # Create a temporary variable to indicate if the quantizer had amax in the checkpoint
+        self._has_amax = modelopt_state.get("_has_amax", "_amax" in modelopt_state)
 
-        # For backward compatibility
-        if "_calibrator_type" in modelopt_state:
-            warnings.warn(
-                "Loading ``TensorQuantizer`` state with old state. Support for this will be removed"
-                " in future."
-            )
-            assert "_calib_constructor_args" not in modelopt_state
-            if "MaxCalibrator" in modelopt_state["_calibrator_type"]:
-                calib_cls = calib.MaxCalibrator
-            elif "HistogramCalibrator" in modelopt_state["_calibrator_type"]:
-                calib_cls = calib.HistogramCalibrator
-            else:
-                raise RuntimeError(
-                    f"{prefix[:-1]}: Unknown calibrator type: {modelopt_state['_calibrator_type']}"
-                )
-            self._calib_constructor_args = (
-                calib_cls,
-                (),
-                dict(num_bits=self._num_bits, axis=self._axis, unsigned=self._unsigned),
-            )
+        # Create a temporary variable to indicate if the quantizer had pre_quant_scale in the checkpoint
+        self._has_pre_quant_scale = modelopt_state.get(
+            "_has_pre_quant_scale", "_pre_quant_scale" in modelopt_state
+        )
 
-        # NOTE: We are not saving the calibrator state, is that needed?
-        calib_cls, calib_args, calib_kwargs = self._calib_constructor_args
-        self._calibrator = calib_cls(*calib_args, **calib_kwargs)
+    def clean_up_after_set_from_modelopt_state(self, prefix=""):
+        """Clean up temporary variables created during set_from_modelopt_state."""
+        warning_msg = (
+            f"Could not initialize the quantizer states for {prefix}. The quantizer"
+            " states after `load_state_dict` could be in the wrong device. Please move"
+            " the modules to the correct device after loading the state dict."
+        )
 
-        if device is not None:
-            self.to(device)
+        if hasattr(self, "_has_amax"):
+            if self._has_amax and self.amax is None:
+                warnings.warn(warning_msg, UserWarning)
+            delattr(self, "_has_amax")
+
+        if hasattr(self, "_has_pre_quant_scale"):
+            if self._has_pre_quant_scale and self.pre_quant_scale is None:
+                warnings.warn(warning_msg, UserWarning)
+            delattr(self, "_has_pre_quant_scale")
 
     # TODO: [OMNIML-823] type specification & validation for attributes
     def set_from_attribute_dict(self, attribute_dict: Dict[str, Any]):
         """Set quantizer attributes from attribute_dict."""
         if "num_bits" in attribute_dict:
             self.num_bits = attribute_dict["num_bits"]
         if "axis" in attribute_dict:
@@ -750,18 +754,15 @@
                             attribute_dict["block_sizes"]
                         )
                     )
                     == 1
                 ), "Dynamic block quantization only supports quantization last axis."
             self.block_sizes = attribute_dict["block_sizes"]
         if "calibrator" in attribute_dict:
-            self._calib_constructor_args = standardize_constructor_args(
-                attribute_dict["calibrator"]
-            )
-            calib_cls, args, kwargs = self._calib_constructor_args
+            calib_cls, args, kwargs = standardize_constructor_args(attribute_dict["calibrator"])
             self._calibrator = calib_cls(*args, **kwargs)
         if "enable" in attribute_dict:
             if attribute_dict["enable"]:
                 self.enable()
             else:
                 self.disable()
 
@@ -800,31 +801,14 @@
 
     def set_from_attribute_dict(self, attributes: List[Dict[str, Any]]):
         """Set the attributes of contained quantizers from a list of attribute_dicts."""
         for attribute, quantizer in zip(attributes, self):
             quantizer.set_from_attribute_dict(attribute)
 
     @staticmethod
-    def restore_sequential_quantizers(model, modelopt_state: Dict[str, Any]):
-        """Restore sequential quantizers from checkpoint."""
-        for name, modelopt_state in modelopt_state.items():
-            if (
-                "is_sequential_quantizer" in modelopt_state
-                and modelopt_state["is_sequential_quantizer"]
-            ):
-                sequential_quantizer = SequentialQuantizer(
-                    *(
-                        TensorQuantizer(QuantDescriptor())
-                        for _ in range(modelopt_state["num_quantizers"])
-                    )
-                )
-                parent_module = model.get_submodule(name.rpartition(".")[0])
-                setattr(parent_module, name.rpartition(".")[-1], sequential_quantizer)
-
-    @staticmethod
     @contextlib.contextmanager
     def replace_sequential_quantizer_with_single_quantizer(model, indx: int = 0):
         """Replace instances of :class:`SequentialQuantizer` in the model with single quantizers.
 
         The quantizer indexed by ``indx`` from the sequential quantizer is used to replace it.
         This method is useful for individually calibrating the quantizers in a sequential quantizer.
         """
@@ -843,14 +827,16 @@
                 original_name = name.rpartition(".")[-1].replace("_original_", "")
                 setattr(parent_module, original_name, module)
                 delattr(parent_module, name.rpartition(".")[-1])
 
     @staticmethod
     def tensor_quantizer_iterator(quantizers):
         """Iterator for the quantizers in the container (but yield itself if its a TensorQuantizer)."""
+        if quantizers is None:
+            return
         if isinstance(quantizers, TensorQuantizer):
             yield quantizers
         elif isinstance(quantizers, SequentialQuantizer):
             for quantizer in quantizers:
                 yield quantizer
         else:
             raise ValueError("Invalid quantizer type.")
```

## modelopt/torch/quantization/plugins/megatron.py

```diff
@@ -8,25 +8,92 @@
 # without an express license agreement from NVIDIA CORPORATION or
 # its affiliates is strictly prohibited.
 
 """Support quantization for megatron linear layers."""
 
 
 import megatron.core.tensor_parallel.layers as megatron_parallel
+from megatron.core.transformer.utils import make_sharded_tensors_for_checkpoint
 
 from ..nn import QuantModuleRegistry
+from ..tensor_quant import QuantDescriptor
 from .custom import _ParallelLinear
 
 __all__ = []
 
 
-@QuantModuleRegistry.register(
-    {
-        megatron_parallel.ColumnParallelLinear: "megatron_ColumnParallelLinear",
-        megatron_parallel.RowParallelLinear: "megatron_RowParallelLinear",
-    }
-)
 class _MegatronParallelLinear(_ParallelLinear):
     _functionals_to_replace = [
         (megatron_parallel, "linear_with_grad_accumulation_and_async_allreduce"),
         (megatron_parallel, "linear_with_frozen_weight"),
     ]
+
+    def _process_weight_quantizer_amax(self, k, v, quantizer_state_dict):
+        if v.ndim == 0:
+            quantizer_state_dict[k] = v.view(-1)
+        elif v.ndim == 2:
+            quantizer_state_dict[k] = v.view(self.weight.shape[0], -1)
+        else:
+            raise ValueError(f"Invalid weight quantizer {k} amax: {v}, {v.shape}")
+
+    def _process_activation_quantizer_amax(self, k, v, quantizer_state_dict):
+        assert v.ndim == 0, f"Invalid activation quantizer amax: {v}, {v.shape}"
+        quantizer_state_dict[k] = v.view(-1)
+
+    def _process_activation_quantizer_pre_quant_scale(self, k, v, quantizer_state_dict):
+        assert v.ndim == 1, f"Invalid activation quantizer pre_quant_scale: {v}, {v.shape}"
+        quantizer_state_dict[k] = v
+
+    def _get_shard_axis_dict(self):
+        raise NotImplementedError
+
+    def sharded_state_dict(self, prefix="", sharded_offsets=(), metadata=None):
+        sharded_state_dict = super().sharded_state_dict(prefix, sharded_offsets)
+
+        quantizer_state_dict, sharded_axis_dict = {}, self._get_shard_axis_dict()
+        for k, v in self.state_dict(prefix="", keep_vars=True).items():
+            if "weight_quantizer." in k:
+                assert k.endswith("._amax"), f"Invalid weight quantizer state: {k}"
+                self._process_weight_quantizer_amax(k, v, quantizer_state_dict)
+            elif ("input_quantizer" in k or "output_quantizer" in k) and k.endswith("._amax"):
+                self._process_activation_quantizer_amax(k, v, quantizer_state_dict)
+            elif k.endswith("input_quantizer._pre_quant_scale"):
+                self._process_activation_quantizer_pre_quant_scale(k, v, quantizer_state_dict)
+
+        sharded_state_dict.update(
+            **make_sharded_tensors_for_checkpoint(
+                quantizer_state_dict, prefix, sharded_axis_dict, sharded_offsets
+            )
+        )
+        return sharded_state_dict
+
+
+@QuantModuleRegistry.register(
+    {megatron_parallel.ColumnParallelLinear: "megatron_ColumnParallelLinear"}
+)
+class _MegatronColumnParallelLinear(_MegatronParallelLinear):
+    def _get_shard_axis_dict(self):
+        shard_axis_dict = {}
+        for k, v in self.state_dict(prefix="", keep_vars=True).items():
+            if "weight_quantizer." in k and v.ndim != 0:
+                shard_axis_dict[k] = 0
+        return shard_axis_dict
+
+
+@QuantModuleRegistry.register({megatron_parallel.RowParallelLinear: "megatron_RowParallelLinear"})
+class _MegatronRowParallelLinear(_MegatronParallelLinear):
+    def _get_shard_axis_dict(self):
+        shard_axis_dict = {}
+        for k, v in self.state_dict(prefix="", keep_vars=True).items():
+            if "weight_quantizer" in k:
+                assert "._amax" in k, f"Invalid weight quantizer state: {k}"
+                submodule_name = k.split("weight_quantizer")[-1].split("._amax")[0].split(".")[-1]
+                quantizer = self.weight_quantizer.get_submodule(submodule_name)
+                # The weights are split across dim -1; Only static block quantization requires sharding
+                if quantizer.block_sizes and quantizer.block_sizes.get("type", None) != "dynamic":
+                    assert (-1 in quantizer.block_sizes or 1 in quantizer.block_sizes) and len(
+                        QuantDescriptor.get_block_quant_axes_and_sizes(quantizer.block_sizes)
+                    ) == 1, f"Invalid block sizes: {quantizer.block_sizes}"
+                    shard_axis_dict[k] = 1
+            elif "input_quantizer._pre_quant_scale" in k:
+                shard_axis_dict[k] = 0
+        return shard_axis_dict
```

## modelopt/torch/sparsity/plugins/megatron.py

```diff
@@ -8,24 +8,55 @@
 # without an express license agreement from NVIDIA CORPORATION or
 # its affiliates is strictly prohibited.
 
 
 """Support megatron parallel linear."""
 
 from megatron.core.tensor_parallel.layers import ColumnParallelLinear, RowParallelLinear
+from megatron.core.transformer.utils import make_sharded_tensors_for_checkpoint
 
 from modelopt.torch.sparsity.config import SparseGPTConfig, SparseMagnitudeConfig
 from modelopt.torch.sparsity.module import SparseModule, SpDMRegistry
 
-SpDMRegistry.register(
-    {
-        ColumnParallelLinear: "megatron.core.tensor_parallel.layers.ColumnParallelLinear",
-        RowParallelLinear: "megatron.core.tensor_parallel.layers.RowParallelLinear",
-    }
-)(SparseModule)
+
+class _MegatronParallelLinear(SparseModule):
+    def _get_shard_axis_dict(self):
+        raise NotImplementedError
+
+    def sharded_state_dict(self, prefix="", sharded_offsets=(), metadata=None):
+        sharded_state_dict = super().sharded_state_dict(prefix, sharded_offsets)
+
+        sparse_state_dict, sharded_axis_dict = {}, self._get_shard_axis_dict()
+        for k, v in self.state_dict(prefix="", keep_vars=True).items():
+            if k == "_weight_mask":
+                sparse_state_dict[k] = v
+        if sparse_state_dict:
+            sharded_state_dict.update(
+                **make_sharded_tensors_for_checkpoint(
+                    sparse_state_dict, prefix, sharded_axis_dict, sharded_offsets
+                )
+            )
+
+        return sharded_state_dict
+
+
+@SpDMRegistry.register(
+    {ColumnParallelLinear: "megatron.core.tensor_parallel.layers.ColumnParallelLinear"}
+)
+class _MegatronColumnParallelLinear(_MegatronParallelLinear):
+    def _get_shard_axis_dict(self):
+        return {"_weight_mask": 0}
+
+
+@SpDMRegistry.register(
+    {RowParallelLinear: "megatron.core.tensor_parallel.layers.RowParallelLinear"}
+)
+class _MegatronRowParallelLinear(_MegatronParallelLinear):
+    def _get_shard_axis_dict(self):
+        return {"_weight_mask": 1}
 
 
 def _get_extra_rules():
     """Get the extra rules for megatron."""
     return {
         "megatron.core.tensor_parallel.layers.ColumnParallelLinear": {
             "*": {},
```

## Comparing `nvidia_modelopt-0.11.0.dist-info/LICENSE` & `nvidia_modelopt-0.11.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `nvidia_modelopt-0.11.0.dist-info/METADATA` & `nvidia_modelopt-0.11.1.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: nvidia-modelopt
-Version: 0.11.0
+Version: 0.11.1
 Summary: Nvidia TensorRT Model Optimizer: a unified model optimization and deployment toolkit.
 Author-email: "Nvidia, Inc." <ammo-support@exchange.nvidia.com>
 License: NVIDIA Proprietary Software
 Project-URL: Homepage, https://nvidia.com
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
```

## Comparing `nvidia_modelopt-0.11.0.dist-info/RECORD` & `nvidia_modelopt-0.11.1.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,125 +1,128 @@
 modelopt/__init__.py,sha256=t4cVfYKBeLUteTaxDlZl2sIXnM4AaAfKFFRfUA0XbsA,694
 modelopt/deploy/__init__.py,sha256=kPLOuoLCaV0xwzl2Me4kXwJECUXDIa_YR2ozADCnYbQ,604
 modelopt/deploy/llm/__init__.py,sha256=qczpeDFe-TSphksKq1BWHqMQLCUwLKgVPadw05b-fYY,2390
 modelopt/deploy/llm/generate.py,sha256=DC8o4xl_lH7BqP0_swcLWQDG5Mr6UIa0GStkt0iFEGY,5213
 modelopt/deploy/llm/model_config_trt.py,sha256=svfuyx2PH1pZBCT3VXYvjs9Ozv2LiT382m12cgFMJXo,12337
 modelopt/deploy/llm/nemo_utils.py,sha256=VEN1dz02mwiD8gj-zAG1cRcW-mdDxqfnE-BJxeBaZ3U,6892
 modelopt/onnx/__init__.py,sha256=2FUcmfR-kxw22Gd3a7cFiQrE6BjiUHZ7ZtXlYW0INE4,791
-modelopt/onnx/op_types.py,sha256=Uxs876rHkrxJwhtX9KXVPCNCHqfQJUbemZrpWh4Wg8Y,7880
-modelopt/onnx/utils.py,sha256=q0MxFZzIKNnVcSyktiy7ajHaiPoEXjpH-A0uC4kVhWM,19455
+modelopt/onnx/op_types.py,sha256=KXitAMSzXYlQvvtL9u4553Ov7Ga2yC5Lq3bmTjor9Tk,8003
+modelopt/onnx/utils.py,sha256=GfJDKUxg2kse2NtyKrjwVrwmUQ_PJjwwJBf_pte3DKQ,19828
 modelopt/onnx/quantization/__init__.py,sha256=X4YfnosBlCqdsAjIuYHOYKVdrrGrzSFcouCwn9yMocM,619
 modelopt/onnx/quantization/__main__.py,sha256=x7m_ORCfBwB06MKmBJcczSQD5ohD5ozlqNaWGURCZQU,4605
 modelopt/onnx/quantization/calib_utils.py,sha256=YgzjMcagg-Mp9PNSxVIBxkjzt5UoTppNILEvdBp6La8,4605
-modelopt/onnx/quantization/graph_utils.py,sha256=O5KGRpyDiU8Nj3Vu6ntm9kaYEzNrZoZdF7B7eJlSviQ,19230
-modelopt/onnx/quantization/gs_patching.py,sha256=GzGx5_QcAmwFy-7Ms-ajY14v2bJNWBQ_mzOB-YJyynI,4118
-modelopt/onnx/quantization/int4.py,sha256=Qigfj9_SJ7Vx6EPh4-M0g6iMbQD58Zu-WXT_CiSR0LY,16443
+modelopt/onnx/quantization/graph_utils.py,sha256=zcREuVYFLcxKVT8kmS_pYyiFPcEMEfxko-7PHFVJy9Q,18754
+modelopt/onnx/quantization/gs_patching.py,sha256=EqE3DtVsUsaZFNGamPsYLZcYDpeYv_Bdh4NaJYvCEgg,4132
+modelopt/onnx/quantization/int4.py,sha256=S-yVyhxaMrFrid_fqh8XljUQRtGTU9AXI4i_-p1yiLM,18389
 modelopt/onnx/quantization/operators.py,sha256=sP8VMx8tMcjz-WEBvjzAC87oPognbt6c_5lV68agDDY,4306
 modelopt/onnx/quantization/ort_patching.py,sha256=TtCECorZIuknwADi4IUVFgikmru2Fzq1CjFx3WTVH6M,8217
 modelopt/onnx/quantization/ort_utils.py,sha256=YMy7gD7U0kijeA-3-DRxQzk0wr9iOR0-AI4P7y467RE,1053
-modelopt/onnx/quantization/partitioning.py,sha256=KDROAbkiuTKTVwsyEHtzUppCiP8g7Y7vZJMjeRXTRDE,12547
+modelopt/onnx/quantization/partitioning.py,sha256=D2b16rIOZyNLKCPOZATGZyoxu16a9UrgVR5E2kjkXtI,14046
 modelopt/onnx/quantization/qdq_utils.py,sha256=AzScMs3y6MKs7_Oy63_XSAk-zwwWTgeMBf1YYjEvE6A,7440
-modelopt/onnx/quantization/quantize.py,sha256=dHASh8QNqaaHoikr4c6YxyJ4fP6WRnu9lxHfQ9Nia1o,20977
+modelopt/onnx/quantization/quant_utils.py,sha256=-pWAKMONfs8y0m54LuCvb6RwSbYfjyKENKsMExiyr7M,2475
+modelopt/onnx/quantization/quantize.py,sha256=jJLm_W_0Di2759e_pX4QEvtFTJFEl5BEkGL9dghyNl4,21116
 modelopt/torch/__init__.py,sha256=jlb5BBtkY93z1952iYPpx-qrVp1iP-2TL4fYl7-4BOg,805
 modelopt/torch/_deploy/__init__.py,sha256=wLswmGXJx9LEBt7qspaRgfZ85WV3PhlOJgZErB5l1K8,876
 modelopt/torch/_deploy/compilation.py,sha256=qc6kZNAiyLr3JGmTHceYxzv782zhFx_a3esv_aSRNnM,4824
 modelopt/torch/_deploy/device_model.py,sha256=Erk23_sh8R9y60J_YDchQyNS6KC0b5q3B3JFA00ovAQ,5522
 modelopt/torch/_deploy/profiling.py,sha256=aO-T_jtNHEk0M9whkFw-aWaq4_bRnIf0jeeTsGLrhcs,7588
 modelopt/torch/_deploy/_runtime/__init__.py,sha256=xVnt845HcVeoBK9RxvivmyEXkROiDrx6gIELsXDEJAE,957
 modelopt/torch/_deploy/_runtime/common.py,sha256=vzqRT2DHbTchiJddXdcrm3TTeYwJ3ZjwWDFeFYKXbIY,2327
 modelopt/torch/_deploy/_runtime/ort_client.py,sha256=wVLcgNPFE5xTCDY77gd4bvWYS72QeZIaiEiR4qIPTys,7266
 modelopt/torch/_deploy/_runtime/registry.py,sha256=BRzIT-Qqrz1bSiIx7OwU7Umf627n_PSByaghL5m8830,2224
 modelopt/torch/_deploy/_runtime/runtime_client.py,sha256=DQpJfwPHdXCB6EiSHbbepvkCqP0aP7yqyGRzCKG0q7k,5865
 modelopt/torch/_deploy/_runtime/trt_client.py,sha256=ClOVYKRyl-n7m_DH1FQkcq7n3jQxtW5w6pAC2AXZ8Lc,7730
 modelopt/torch/_deploy/_runtime/tensorrt/constants.py,sha256=_vqNBMtba0jV9BfBT06F5TcjD0qmQGznXsCJaohopBI,2594
-modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cp39-win_amd64.pyd,sha256=1XnEPuCLsnIAY3ftEuu8WBfesi4oo6u8pWVLH6qdptw,119296
+modelopt/torch/_deploy/_runtime/tensorrt/engine_builder.cp39-win_amd64.pyd,sha256=f5dx7ZEQd8CTnYGBHgaWeZNrRB7DhpYNiJ6A25dFG-Y,119296
 modelopt/torch/_deploy/_runtime/tensorrt/hw_param_config.py,sha256=Om3X9ryKTsqRFWPN0vkdoSDq9DV7wO0-ErVeyFZIWQI,1763
 modelopt/torch/_deploy/_runtime/tensorrt/layerwise_profiling.py,sha256=s2-8XGsrlzTtkWVSfZlFNptlyEGQ3gLL0vH8tiJEEn0,6972
 modelopt/torch/_deploy/_runtime/tensorrt/parse_trtexec_log.py,sha256=k2DcHME2FYj61QYDuI0-hdEaAYWGm0jDgdShNW0jOp8,5691
 modelopt/torch/_deploy/_runtime/tensorrt/tensorrt_utils.py,sha256=lWQpy4eFQ7v4wQIT9aPQdIrM2aC2hg6zJasoFUxymcc,6524
 modelopt/torch/_deploy/utils/__init__.py,sha256=IsziBUwYpHo5BDrPdakSq7qYp-UJbiEPZ6i957NSf6A,633
 modelopt/torch/_deploy/utils/onnx_optimizer.py,sha256=i36vl83-Np_WLrpzh28whxSqWf2LFt246NvXtQolo_c,4730
-modelopt/torch/_deploy/utils/torch_onnx.py,sha256=wuSQp2A6J6tvCIc8uUAR6iMEFhW7Upan2abQZgoeU84,19935
+modelopt/torch/_deploy/utils/torch_onnx.py,sha256=UZNtj8cvfW3gOW7JPUv1yah6mvah2OVnq0zjT-EeBBo,20946
 modelopt/torch/export/__init__.py,sha256=sbUwLSbjSx9mWji8-RFcM6RQuNElqEeAJHfGaxwlykY,848
 modelopt/torch/export/distribute.py,sha256=DWx7kSyo4_0GIM4pspGOuhsczOg8NYec3cCrzg6KLC8,12668
-modelopt/torch/export/layer_utils.py,sha256=so-X8gT0OsjvJFAEYA70PbMxE7i5GX9ihy3lfBl6c_U,52001
-modelopt/torch/export/model_config.py,sha256=3eD57lalT8jcSdN7zstiTt-8bMJX-ZRMu5i6ei9LeVo,13738
-modelopt/torch/export/model_config_export.py,sha256=xa_-5y2t6fOpdzFUq6nty_PrmoYg8Xbvl24l9wp4CVA,17373
+modelopt/torch/export/layer_utils.py,sha256=Z-Q-kxhfSarp32FquZ5EGmmHRBX5YQtgtvFojQ-kLrg,52089
+modelopt/torch/export/model_config.py,sha256=7U9UwAS2S8MC1k4fQtPCQW9yKIi9Ts66Nvo9qjuv2TA,13780
+modelopt/torch/export/model_config_export.py,sha256=Nx_Jtdyi-2W7jAGJWeC2244nimWcdEarzYLJoj6FPYM,17778
 modelopt/torch/export/model_config_utils.py,sha256=9WPNMtbyB8oqWhLR_0Tzy-7dAtF7BTc2Vp2FixXc7Io,18360
 modelopt/torch/export/postprocess.py,sha256=j-vywTNk2RnmHoYfY0sBIwh1yTMLhzwKJdtWI_V8qr0,29334
 modelopt/torch/export/scaling_factor_utils.py,sha256=Rvie1EqsSt5vhWnQcQ6yR78US_4NK7d7zdDgHin09b0,3182
-modelopt/torch/export/tensorrt_llm_utils.py,sha256=YzWY0d0L5pvHsA0EoPoGAugRz3vStjQ-3qeXkuSf82M,11215
+modelopt/torch/export/tensorrt_llm_utils.py,sha256=OIgPd7VdaWgK3XYpF0aimq1FK81Ube-NF5HYRuz2w5Y,11289
 modelopt/torch/export/transformer_engine.py,sha256=FXbHlEY5jkzLpWaQeeHqqBhdie7GZvdO0yFJ22FJDMw,4088
 modelopt/torch/opt/__init__.py,sha256=ftGL2EIBmHUHbG15ZwHZGF9HzIr08NwW6NffxlVm6hE,1360
 modelopt/torch/opt/_hooks.py,sha256=PVO8kSihdf46nc-8zU5oqFU3lwm1V68CkNOlgrEFKHQ,3137
 modelopt/torch/opt/config.py,sha256=CBeNoDb6qFw5mgFu4Nl-BLaPCLWsfVFK30sOjeTpz-M,15227
 modelopt/torch/opt/conversion.py,sha256=-ZGqMKTYEulQbLfQmfqhm3K2tLHt8bOGDJtYRK-qaSE,24050
 modelopt/torch/opt/dynamic.py,sha256=Q_iakZ3JHE5ikIcl-LflkWCs4e318rn7VKEsH56WvN8,57369
 modelopt/torch/opt/hparam.py,sha256=DDkdVsC_1LwjIp1aUNf5uq-nOFzREsBKCKqRR8UezJw,9461
 modelopt/torch/opt/mode.py,sha256=9ys9j_HQ0uzUv7rWMQ4MvswMcRTyEDHap2iyNd_mpbI,12723
 modelopt/torch/opt/searcher.py,sha256=H4rv0yHFkMQuZA0kOwRqNmSs-oHmSaLoWlAOJ8ISQoY,9570
 modelopt/torch/opt/utils.py,sha256=XMLCLaOKyZxjdCkWv0jlqf4kNUyEZyocqoLdzWZCh3s,1955
 modelopt/torch/quantization/__init__.py,sha256=ja68_nADm7sL-YuGpElmuu-Ve9DN6uKtONEUpFROYe8,846
 modelopt/torch/quantization/config.py,sha256=y4_JdUB4svxqm8mv6wYQTlNbcX-0NnBqQDYqim23Os0,13912
-modelopt/torch/quantization/conversion.py,sha256=adRBIwOYkcXfTDY9qfxYYyFscNH4MkGUitaRGgJmFvA,11437
+modelopt/torch/quantization/conversion.py,sha256=0qoiIAczFtZKJREhVMpTNjwBduXV-g0_ZlkglZl6-pM,10771
 modelopt/torch/quantization/extensions.py,sha256=cYxeCB9z33ooN8XoCuCZTFvl-YLgjNCJSVIe5sI1tcg,1109
 modelopt/torch/quantization/mode.py,sha256=OQcfuY9j8lJRmuMWKGca00HLpOz9ufgyTGoZaZf22j8,4062
-modelopt/torch/quantization/model_calib.cp39-win_amd64.pyd,sha256=7r4p0FJ0S2ef5GXp_gXy8D2D1UIizItlefY27NTcfPw,320000
+modelopt/torch/quantization/model_calib.cp39-win_amd64.pyd,sha256=DGD25I9CTXFYcVkSDJ9fJpmSEyzTwitNRrQevDh2bHQ,322560
 modelopt/torch/quantization/model_quant.py,sha256=0sm586IppQmULxuGRc4QiyLyy1JZIwKLRAXZFVAmdkg,6302
 modelopt/torch/quantization/optim.py,sha256=SBYwxlDU8YQH8QyEq_GRG_8_NsqdOMoSKcAw3e00sMw,1393
 modelopt/torch/quantization/quant_modules.py,sha256=CI4kw5-cXgQBWotSOM36q3ZxJmlDzAWkT05k4tR5HKI,1725
 modelopt/torch/quantization/tensor_quant.py,sha256=AFHsMkMQgxPDjxHhpIudICQsS4NnheAKT4uCmkeVYlQ,29725
-modelopt/torch/quantization/utils.py,sha256=eXovxj95kBAeRiI_twiFyp2VEwQ39QFf1vk5MQly5FQ,5506
+modelopt/torch/quantization/utils.py,sha256=OAmRGhxYzk-WfXa_vYc5PO9P0ebGk7eREU_GJph75z4,5703
 modelopt/torch/quantization/calib/__init__.py,sha256=QC2CDk13q2ZbfENLeWk4s3ckC8FJBd7pTolMc44Sf4A,816
 modelopt/torch/quantization/calib/calibrator.py,sha256=mOtasRBGoPfYFh4aw2XeqQZmCsj__dHq-5P-3Fxm7s0,1832
 modelopt/torch/quantization/calib/histogram.py,sha256=IqcxhdIFKsMuaAp95GMYu-RzwHL0iJCdOX65T6wO6YQ,17445
 modelopt/torch/quantization/calib/max.py,sha256=ws0NnHv4EeiLr3iVOPoajQM3zmeI2jQ7REj76B3psUI,3436
-modelopt/torch/quantization/nn/__init__.py,sha256=ui_-9k0eBDa0_G0FSeq21bvtC8pdoghGy_aGtvYnkj4,863
+modelopt/torch/quantization/nn/__init__.py,sha256=_6vmgxHTsy7UY7aKLJlExoGQ3x8AEjiTh9RNDHbz1kk,945
 modelopt/torch/quantization/nn/functional.py,sha256=1J5SB4T3DpC-KBn9AR473kSwa0zCTKGyT9xxb3eKrlU,2822
 modelopt/torch/quantization/nn/modules/__init__.py,sha256=hiV4Q5QP0y1rl1zOgawSoknb6I-drlZ-Xr6EUmiwyeE,601
 modelopt/torch/quantization/nn/modules/_utils.py,sha256=yWWFcCIz09SKYDIPLusfZ2vJckdC4IiDQF0nuOzwDDM,8338
 modelopt/torch/quantization/nn/modules/clip.py,sha256=-8NWLJoX2kUt4FEvfikX9y5UlL1lDtpFMSQ1VxCH4so,2317
+modelopt/torch/quantization/nn/modules/quant_activations.py,sha256=9qxIwQcQj3oftQR84LfAEs9oOCVdDeBQzVLM3vyZIPM,1389
+modelopt/torch/quantization/nn/modules/quant_batchnorm.py,sha256=oUGuPEjzJIN3LzJlx7BEnLG63QbW-hLNHJXVVq1ZF3w,1568
 modelopt/torch/quantization/nn/modules/quant_conv.py,sha256=QMNBQRk1tPD6YbgiDMfAyWI7bWXfg-jAiGW9fij3bjo,3986
 modelopt/torch/quantization/nn/modules/quant_instancenorm.py,sha256=R2Qr4OFDdCaZLzRX8jlcJGL_kmVujLW01hlJDwopfHg,1556
 modelopt/torch/quantization/nn/modules/quant_linear.py,sha256=oGGrOR75vGSLxPZ_-GEVYeT_IPARnzI7Vjd6_Y2TGRM,2762
-modelopt/torch/quantization/nn/modules/quant_module.py,sha256=kub0f0BFgLK2CD4Di5Suy7gm79qM8TtkB-uyo6C9VVM,5675
+modelopt/torch/quantization/nn/modules/quant_module.py,sha256=meEhgENukyaaU-UBAN3X8JHhbUDWu68fmleJ7C274IY,7849
 modelopt/torch/quantization/nn/modules/quant_pooling.py,sha256=mQfjb0Hao7wKxh4SgrE6_ll4aeVuqE8MA2nM_5IAE8g,3556
-modelopt/torch/quantization/nn/modules/tensor_quantizer.py,sha256=78Ac_V4fb0OGNpE35zoP6emdZ3o7spNDVBJakCK2RI8,34738
+modelopt/torch/quantization/nn/modules/tensor_quantizer.py,sha256=468W-LhHIE_cYwAdu6Cp72hypruTK-M5hMcbE6Wyl7Y,34362
 modelopt/torch/quantization/plugins/__init__.py,sha256=loDBZvUrrpz-xZAuPTix28x9Y7szaB4L23C8iNZnNT4,1892
 modelopt/torch/quantization/plugins/apex.py,sha256=GhgQFLYUW1fWSZ8PPRASs_HYLhITh2woUkNft5Ry_Yo,1484
 modelopt/torch/quantization/plugins/custom.py,sha256=Q-WprnXxeP1rZ3UlMmKgo6a9uiHZXezysFLM1mryx0Q,4226
 modelopt/torch/quantization/plugins/diffusers.py,sha256=18IaJCbQPX84tjWhmApPDPrSPRpB7SPfvaB9uvnQl2A,3453
 modelopt/torch/quantization/plugins/huggingface.py,sha256=sQRyRWua3r5B7-Mi1HcIAxZwfjETf7Ib7j23a6vX5WE,2489
-modelopt/torch/quantization/plugins/megatron.py,sha256=OXCzvP-nGrmGdVb48gICaJ3qLUbzT-pPP-miE3qvHDI,1220
+modelopt/torch/quantization/plugins/megatron.py,sha256=h7nyRTH9rKCBe1MsrraKo30wj5SmRBWyDBXAlGlKDpY,4804
 modelopt/torch/quantization/plugins/nemo.py,sha256=EN21xNTtlY0EVQeeZomMrpSI7V_V-twt0Hg8odB6YzQ,2472
 modelopt/torch/quantization/src/tensor_quant.cpp,sha256=iyjrAdeIlQLI7QTSU2ynpC6l7R04ZyzJ5L8IqixlIBA,3061
 modelopt/torch/quantization/src/tensor_quant_fp8.cpp,sha256=3sZMuzS9Rh92-_sVwos9aPEqRs8-vHULIriOmQSMWYI,1320
 modelopt/torch/quantization/src/tensor_quant_gpu.cu,sha256=ZjyZLEJKzElXsGJMpq5I9qdrK--NT9EPRjgu8P2MAYs,5557
 modelopt/torch/quantization/src/tensor_quant_gpu_fp8.cu,sha256=ONDjED7AlcwWFRGKzPuuqMKqYODiJrVlzjBQdbY4_aQ,2082
 modelopt/torch/sparsity/__init__.py,sha256=HZ_QgP_YXBYrek8UTJ_25p5FLEV8QsfADksqI-6V1Ic,671
 modelopt/torch/sparsity/config.py,sha256=lndNOuurv-YHJTu-15W8YgvFwqxSmdZahaTAZ2OvWBE,1641
 modelopt/torch/sparsity/magnitude.py,sha256=GQC2-XHvKbLAjoC8z_6pyLQMBBIrN0sbDhGZCnWOg4M,5352
 modelopt/torch/sparsity/mode.py,sha256=9RJivfnNG-yF9b6lsn91r-0Zc_u37K8jRWmsq_oXmx4,6847
 modelopt/torch/sparsity/module.py,sha256=LVB7t5nNN0Fdh-2HcnzhgH8m6AaPx9s8B5NgId_jN4g,3588
 modelopt/torch/sparsity/searcher.py,sha256=xipaaEWc79FXu0ovpR9O3prLqSVTVT4LptvXx7WX1J4,3199
 modelopt/torch/sparsity/sparsegpt.py,sha256=kRHqKje2DF4kZb4FOrIi6roG0QWJGGfc-8YpJ4cR3jI,10791
 modelopt/torch/sparsity/sparsification.py,sha256=R-oFV0mvzfpB-oaSLX0HMKq92knMgowgjB5mruzBJRY,5740
 modelopt/torch/sparsity/plugins/__init__.py,sha256=YSIFMOUX0q6gzG82Rf9xN6TOP7mTHwrbthWT86FSwnE,990
-modelopt/torch/sparsity/plugins/megatron.py,sha256=gIL7KOP8kI9ASmSKWh50btKGTxpQf_Lj4YJywJfn-nU,1609
+modelopt/torch/sparsity/plugins/megatron.py,sha256=Pd77yaBAWzwpHRn2LwhFg1zYlH_gT4vP1vSx-4qCVsY,2762
 modelopt/torch/utils/__init__.py,sha256=_3Fi4SbTPAi3wRkkfZ8WCo7Jj-SkA5XfUnK9hCcEMxg,804
 modelopt/torch/utils/_pytree.py,sha256=AJC3qOtD4ljmGcUjIboGxmI9zmIvyC7sOIMlDsLRL2o,5582
 modelopt/torch/utils/cpp_extension.py,sha256=aDxgP_sml9rLkoFRsirSNNbvNJqPDU_eTNe6YxcPwVo,2419
 modelopt/torch/utils/dataset_utils.py,sha256=U9Qe6mG6Em2ZkmVOIaGb5otlfHjmjzTSXViITEQXPaQ,7448
 modelopt/torch/utils/distributed.py,sha256=onQe2CnrpHkw7X34QBmBB_iJP3TSQHk_Vi9t2Gm7WFs,8262
 modelopt/torch/utils/graph.py,sha256=udBDNDEpv9TPPEsYZemll8-5sksCJthE99XPPAcWWUQ,3894
 modelopt/torch/utils/list.py,sha256=Gof1ysHiU5ZuYQdefa-XZBsfR1233sPj-2k58jutAuk,1951
 modelopt/torch/utils/logging.py,sha256=9TSEoce7ZFYNV72DjcdeI4JP6QU9TymZXxh2EbnoSVg,2991
 modelopt/torch/utils/network.py,sha256=Jfc-9QZhSBJMY8PhF1hYFuX7C6Gg6aYr6XY_CYdJP-U,22299
 modelopt/torch/utils/perf.py,sha256=Mha-bppNORq-VlxdS12ttfm6c7Ca3z_dTxBpPT_0UqQ,2816
 modelopt/torch/utils/random.py,sha256=lIg_8WdFH8plq57tXMK0DYxNoBzR8W2nOR-Gqhu6PWA,5706
 modelopt/torch/utils/tensor.py,sha256=Zxcrtp5Y2wrjHN0tukVz1GDfggRZxI2QNldWrbWcCKU,1999
-nvidia_modelopt-0.11.0.dist-info/LICENSE,sha256=xBMtI6murYnjWr29V05XL75sviTU9Sq1XfxrjhWJ6TI,540
-nvidia_modelopt-0.11.0.dist-info/METADATA,sha256=wHcZbuoLoWlffDeH_grM8KDsyhmzwiHPNMIW1Y1Yeyc,2589
-nvidia_modelopt-0.11.0.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
-nvidia_modelopt-0.11.0.dist-info/entry_points.txt,sha256=bRFL6MJQ9MybG1Ulo2LVylrwaZGnOPt4liQ_5c5Vt6g,72
-nvidia_modelopt-0.11.0.dist-info/top_level.txt,sha256=IMo1VR4nTJCILUd5BAb4Pd5DmXMRn_4W9wobCeTfn5Y,9
-nvidia_modelopt-0.11.0.dist-info/RECORD,,
+nvidia_modelopt-0.11.1.dist-info/LICENSE,sha256=xBMtI6murYnjWr29V05XL75sviTU9Sq1XfxrjhWJ6TI,540
+nvidia_modelopt-0.11.1.dist-info/METADATA,sha256=UPLsbsmztu7gFIVyqoljXvzbgd8EcE4P9PIIVlpWucg,2589
+nvidia_modelopt-0.11.1.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
+nvidia_modelopt-0.11.1.dist-info/entry_points.txt,sha256=bRFL6MJQ9MybG1Ulo2LVylrwaZGnOPt4liQ_5c5Vt6g,72
+nvidia_modelopt-0.11.1.dist-info/top_level.txt,sha256=IMo1VR4nTJCILUd5BAb4Pd5DmXMRn_4W9wobCeTfn5Y,9
+nvidia_modelopt-0.11.1.dist-info/RECORD,,
```

