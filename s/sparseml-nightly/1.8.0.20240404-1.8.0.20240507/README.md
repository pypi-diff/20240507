# Comparing `tmp/sparseml_nightly-1.8.0.20240404-py3-none-any.whl.zip` & `tmp/sparseml_nightly-1.8.0.20240507-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,569 +1,563 @@
-Zip file size: 1319581 bytes, number of entries: 567
--rw-r--r--  2.0 unx     1489 b- defN 24-Apr-04 11:59 sparseml/__init__.py
--rw-r--r--  2.0 unx      898 b- defN 24-Apr-04 11:59 sparseml/analytics.py
--rw-r--r--  2.0 unx    10284 b- defN 24-Apr-04 11:59 sparseml/base.py
--rw-r--r--  2.0 unx     7288 b- defN 24-Apr-04 11:59 sparseml/integration_helper_functions.py
--rw-r--r--  2.0 unx     2483 b- defN 24-Apr-04 11:59 sparseml/log.py
--rw-r--r--  2.0 unx     1607 b- defN 24-Apr-04 11:59 sparseml/version.py
--rw-r--r--  2.0 unx      758 b- defN 24-Apr-04 11:59 sparseml/benchmark/__init__.py
--rw-r--r--  2.0 unx    17763 b- defN 24-Apr-04 11:59 sparseml/benchmark/info.py
--rw-r--r--  2.0 unx    10778 b- defN 24-Apr-04 11:59 sparseml/benchmark/serialization.py
--rw-r--r--  2.0 unx      915 b- defN 24-Apr-04 11:59 sparseml/core/__init__.py
--rw-r--r--  2.0 unx     6842 b- defN 24-Apr-04 11:59 sparseml/core/event.py
--rw-r--r--  2.0 unx     6492 b- defN 24-Apr-04 11:59 sparseml/core/factory.py
--rw-r--r--  2.0 unx     2850 b- defN 24-Apr-04 11:59 sparseml/core/framework.py
--rw-r--r--  2.0 unx     2855 b- defN 24-Apr-04 11:59 sparseml/core/framework_object.py
--rw-r--r--  2.0 unx     3780 b- defN 24-Apr-04 11:59 sparseml/core/helpers.py
--rw-r--r--  2.0 unx    23720 b- defN 24-Apr-04 11:59 sparseml/core/session.py
--rw-r--r--  2.0 unx     9034 b- defN 24-Apr-04 11:59 sparseml/core/state.py
--rw-r--r--  2.0 unx      667 b- defN 24-Apr-04 11:59 sparseml/core/data/__init__.py
--rw-r--r--  2.0 unx     1565 b- defN 24-Apr-04 11:59 sparseml/core/data/base.py
--rw-r--r--  2.0 unx     6415 b- defN 24-Apr-04 11:59 sparseml/core/data/pytorch.py
--rw-r--r--  2.0 unx      678 b- defN 24-Apr-04 11:59 sparseml/core/lifecycle/__init__.py
--rw-r--r--  2.0 unx    11954 b- defN 24-Apr-04 11:59 sparseml/core/lifecycle/event.py
--rw-r--r--  2.0 unx     8242 b- defN 24-Apr-04 11:59 sparseml/core/lifecycle/session.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/core/logger/__init__.py
--rw-r--r--  2.0 unx    44703 b- defN 24-Apr-04 11:59 sparseml/core/logger/logger.py
--rw-r--r--  2.0 unx      667 b- defN 24-Apr-04 11:59 sparseml/core/logger/utils/__init__.py
--rw-r--r--  2.0 unx    12725 b- defN 24-Apr-04 11:59 sparseml/core/logger/utils/frequency_manager.py
--rw-r--r--  2.0 unx      693 b- defN 24-Apr-04 11:59 sparseml/core/model/__init__.py
--rw-r--r--  2.0 unx     5895 b- defN 24-Apr-04 11:59 sparseml/core/model/base.py
--rw-r--r--  2.0 unx     5641 b- defN 24-Apr-04 11:59 sparseml/core/model/pytorch.py
--rw-r--r--  2.0 unx      699 b- defN 24-Apr-04 11:59 sparseml/core/modifier/__init__.py
--rw-r--r--  2.0 unx     3326 b- defN 24-Apr-04 11:59 sparseml/core/modifier/base.py
--rw-r--r--  2.0 unx    10810 b- defN 24-Apr-04 11:59 sparseml/core/modifier/modifier.py
--rw-r--r--  2.0 unx     6093 b- defN 24-Apr-04 11:59 sparseml/core/modifier/stage.py
--rw-r--r--  2.0 unx      672 b- defN 24-Apr-04 11:59 sparseml/core/optimizer/__init__.py
--rw-r--r--  2.0 unx     3409 b- defN 24-Apr-04 11:59 sparseml/core/optimizer/base.py
--rw-r--r--  2.0 unx     4584 b- defN 24-Apr-04 11:59 sparseml/core/optimizer/pytorch.py
--rw-r--r--  2.0 unx      790 b- defN 24-Apr-04 11:59 sparseml/core/recipe/__init__.py
--rw-r--r--  2.0 unx     6872 b- defN 24-Apr-04 11:59 sparseml/core/recipe/args.py
--rw-r--r--  2.0 unx     1599 b- defN 24-Apr-04 11:59 sparseml/core/recipe/base.py
--rw-r--r--  2.0 unx     6313 b- defN 24-Apr-04 11:59 sparseml/core/recipe/container.py
--rw-r--r--  2.0 unx     2932 b- defN 24-Apr-04 11:59 sparseml/core/recipe/metadata.py
--rw-r--r--  2.0 unx     4224 b- defN 24-Apr-04 11:59 sparseml/core/recipe/modifier.py
--rw-r--r--  2.0 unx    24978 b- defN 24-Apr-04 11:59 sparseml/core/recipe/recipe.py
--rw-r--r--  2.0 unx     7629 b- defN 24-Apr-04 11:59 sparseml/core/recipe/stage.py
--rw-r--r--  2.0 unx      663 b- defN 24-Apr-04 11:59 sparseml/core/utils/__init__.py
--rw-r--r--  2.0 unx     1025 b- defN 24-Apr-04 11:59 sparseml/core/utils/session_helpers.py
--rw-r--r--  2.0 unx      863 b- defN 24-Apr-04 11:59 sparseml/deepsparse/__init__.py
--rw-r--r--  2.0 unx     3516 b- defN 24-Apr-04 11:59 sparseml/deepsparse/base.py
--rw-r--r--  2.0 unx      801 b- defN 24-Apr-04 11:59 sparseml/deepsparse/framework/__init__.py
--rw-r--r--  2.0 unx     6032 b- defN 24-Apr-04 11:59 sparseml/deepsparse/framework/info.py
--rw-r--r--  2.0 unx      813 b- defN 24-Apr-04 11:59 sparseml/deepsparse/sparsification/__init__.py
--rw-r--r--  2.0 unx     1348 b- defN 24-Apr-04 11:59 sparseml/deepsparse/sparsification/info.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/evaluation/__init__.py
--rw-r--r--  2.0 unx     5689 b- defN 24-Apr-04 11:59 sparseml/evaluation/cli.py
--rw-r--r--  2.0 unx     2140 b- defN 24-Apr-04 11:59 sparseml/evaluation/evaluator.py
--rw-r--r--  2.0 unx      940 b- defN 24-Apr-04 11:59 sparseml/evaluation/integrations_config.yaml
--rw-r--r--  2.0 unx     5571 b- defN 24-Apr-04 11:59 sparseml/evaluation/registry.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/evaluation/integrations/__init__.py
--rw-r--r--  2.0 unx     6000 b- defN 24-Apr-04 11:59 sparseml/evaluation/integrations/lm_evaluation_harness.py
--rw-r--r--  2.0 unx    10987 b- defN 24-Apr-04 11:59 sparseml/evaluation/integrations/perplexity.py
--rw-r--r--  2.0 unx      661 b- defN 24-Apr-04 11:59 sparseml/export/__init__.py
--rw-r--r--  2.0 unx    21172 b- defN 24-Apr-04 11:59 sparseml/export/export.py
--rw-r--r--  2.0 unx     8049 b- defN 24-Apr-04 11:59 sparseml/export/export_data.py
--rw-r--r--  2.0 unx     2681 b- defN 24-Apr-04 11:59 sparseml/export/export_torch_model.py
--rw-r--r--  2.0 unx    13052 b- defN 24-Apr-04 11:59 sparseml/export/helpers.py
--rw-r--r--  2.0 unx     8868 b- defN 24-Apr-04 11:59 sparseml/export/validators.py
--rw-r--r--  2.0 unx      786 b- defN 24-Apr-04 11:59 sparseml/exporters/__init__.py
--rw-r--r--  2.0 unx     1477 b- defN 24-Apr-04 11:59 sparseml/exporters/base_exporter.py
--rw-r--r--  2.0 unx     6576 b- defN 24-Apr-04 11:59 sparseml/exporters/kv_cache_injector.py
--rw-r--r--  2.0 unx     5522 b- defN 24-Apr-04 11:59 sparseml/exporters/onnx_to_deepsparse.py
--rw-r--r--  2.0 unx     2350 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/__init__.py
--rw-r--r--  2.0 unx     2333 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/base_transform.py
--rw-r--r--  2.0 unx     1388 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/constants_to_initializers.py
--rw-r--r--  2.0 unx     4630 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/conv_to_convinteger_add_cast_mul.py
--rw-r--r--  2.0 unx     5973 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/conv_to_qlinearconv.py
--rw-r--r--  2.0 unx     2440 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/delete_repeated_qdq.py
--rw-r--r--  2.0 unx     1842 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/delete_trivial_onnx_adds.py
--rw-r--r--  2.0 unx     2241 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/flatten_qparams.py
--rw-r--r--  2.0 unx     3553 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/fold_conv_div_bn.py
--rw-r--r--  2.0 unx     1669 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/fold_identity_initializers.py
--rw-r--r--  2.0 unx     2070 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/fold_relu_quants.py
--rw-r--r--  2.0 unx     4418 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/gemm_to_matmulinteger_add_cast_mul.py
--rw-r--r--  2.0 unx     7629 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/gemm_to_qlinearmatmul.py
--rw-r--r--  2.0 unx     1645 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/initializers_to_uint8.py
--rw-r--r--  2.0 unx     6297 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/matmul_add_to_matmulinteger_add_cast_mul.py
--rw-r--r--  2.0 unx     4681 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/matmul_to_matmulinteger_cast_mul.py
--rw-r--r--  2.0 unx     4156 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/matmul_to_qlinearmatmul.py
--rw-r--r--  2.0 unx     3770 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/onnx_transform.py
--rw-r--r--  2.0 unx     3433 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/propagate_dequant_through_split.py
--rw-r--r--  2.0 unx     4801 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/propagate_embedding_quantization.py
--rw-r--r--  2.0 unx     4464 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/quantize_qat_embedding.py
--rw-r--r--  2.0 unx     3869 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/quantize_residuals.py
--rw-r--r--  2.0 unx     3331 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/remove_duplicate_qconv_weights.py
--rw-r--r--  2.0 unx     2545 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/remove_duplicate_quantize_ops.py
--rw-r--r--  2.0 unx     3210 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/skip_input_quantize.py
--rw-r--r--  2.0 unx     1373 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/unwrap_batchnorms.py
--rw-r--r--  2.0 unx      911 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/__init__.py
--rw-r--r--  2.0 unx    30558 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/cache_keys_and_values.py
--rw-r--r--  2.0 unx    10686 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/configs.py
--rw-r--r--  2.0 unx     7027 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_base.py
--rw-r--r--  2.0 unx     4974 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_codegen.py
--rw-r--r--  2.0 unx     8801 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_llama.py
--rw-r--r--  2.0 unx     4261 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_mpt.py
--rw-r--r--  2.0 unx     6066 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_opt.py
--rw-r--r--  2.0 unx      730 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/utils/__init__.py
--rw-r--r--  2.0 unx    11112 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/utils/add_quantized_conv_matmul_add_ops.py
--rw-r--r--  2.0 unx     6809 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/utils/helpers.py
--rw-r--r--  2.0 unx    14429 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/utils/matching.py
--rw-r--r--  2.0 unx      790 b- defN 24-Apr-04 11:59 sparseml/framework/__init__.py
--rw-r--r--  2.0 unx     9479 b- defN 24-Apr-04 11:59 sparseml/framework/info.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/evaluator.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/trainer.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/data/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/metrics/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/model/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/optim/__init__.py
--rw-r--r--  2.0 unx     1144 b- defN 24-Apr-04 11:59 sparseml/keras/__init__.py
--rw-r--r--  2.0 unx     8054 b- defN 24-Apr-04 11:59 sparseml/keras/base.py
--rw-r--r--  2.0 unx      943 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/__init__.py
--rw-r--r--  2.0 unx     3297 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/dataset.py
--rw-r--r--  2.0 unx     2423 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/helpers.py
--rw-r--r--  2.0 unx     2761 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/registry.py
--rw-r--r--  2.0 unx      786 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/classification/__init__.py
--rw-r--r--  2.0 unx     8369 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/classification/imagefolder.py
--rw-r--r--  2.0 unx     4301 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/classification/imagenet.py
--rw-r--r--  2.0 unx     2727 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/classification/imagenette.py
--rw-r--r--  2.0 unx      793 b- defN 24-Apr-04 11:59 sparseml/keras/framework/__init__.py
--rw-r--r--  2.0 unx     5906 b- defN 24-Apr-04 11:59 sparseml/keras/framework/info.py
--rw-r--r--  2.0 unx      921 b- defN 24-Apr-04 11:59 sparseml/keras/models/__init__.py
--rw-r--r--  2.0 unx    11814 b- defN 24-Apr-04 11:59 sparseml/keras/models/registry.py
--rw-r--r--  2.0 unx      656 b- defN 24-Apr-04 11:59 sparseml/keras/models/classification/__init__.py
--rw-r--r--  2.0 unx    17932 b- defN 24-Apr-04 11:59 sparseml/keras/models/classification/resnet.py
--rw-r--r--  2.0 unx      768 b- defN 24-Apr-04 11:59 sparseml/keras/models/external/__init__.py
--rw-r--r--  2.0 unx     4402 b- defN 24-Apr-04 11:59 sparseml/keras/models/external/keras_applications.py
--rw-r--r--  2.0 unx     1166 b- defN 24-Apr-04 11:59 sparseml/keras/optim/__init__.py
--rw-r--r--  2.0 unx     5677 b- defN 24-Apr-04 11:59 sparseml/keras/optim/manager.py
--rw-r--r--  2.0 unx    14777 b- defN 24-Apr-04 11:59 sparseml/keras/optim/mask_pruning.py
--rw-r--r--  2.0 unx    19740 b- defN 24-Apr-04 11:59 sparseml/keras/optim/mask_pruning_creator.py
--rw-r--r--  2.0 unx     9183 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier.py
--rw-r--r--  2.0 unx     1676 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier_epoch.py
--rw-r--r--  2.0 unx    14736 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier_lr.py
--rw-r--r--  2.0 unx     5477 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier_params.py
--rw-r--r--  2.0 unx    24031 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier_pruning.py
--rw-r--r--  2.0 unx     1133 b- defN 24-Apr-04 11:59 sparseml/keras/optim/utils.py
--rw-r--r--  2.0 unx      808 b- defN 24-Apr-04 11:59 sparseml/keras/sparsification/__init__.py
--rw-r--r--  2.0 unx     1356 b- defN 24-Apr-04 11:59 sparseml/keras/sparsification/info.py
--rw-r--r--  2.0 unx      962 b- defN 24-Apr-04 11:59 sparseml/keras/utils/__init__.py
--rw-r--r--  2.0 unx     8202 b- defN 24-Apr-04 11:59 sparseml/keras/utils/callbacks.py
--rw-r--r--  2.0 unx     1022 b- defN 24-Apr-04 11:59 sparseml/keras/utils/compat.py
--rw-r--r--  2.0 unx     5737 b- defN 24-Apr-04 11:59 sparseml/keras/utils/exporter.py
--rw-r--r--  2.0 unx     6087 b- defN 24-Apr-04 11:59 sparseml/keras/utils/logger.py
--rw-r--r--  2.0 unx     1738 b- defN 24-Apr-04 11:59 sparseml/keras/utils/model.py
--rw-r--r--  2.0 unx      800 b- defN 24-Apr-04 11:59 sparseml/modifiers/__init__.py
--rw-r--r--  2.0 unx      656 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/__init__.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/output/__init__.py
--rw-r--r--  2.0 unx     1343 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/output/base.py
--rw-r--r--  2.0 unx     7612 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/output/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/__init__.py
--rw-r--r--  2.0 unx      715 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/pytorch/__init__.py
--rw-r--r--  2.0 unx    13320 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/pytorch/kd_factory.py
--rw-r--r--  2.0 unx     3921 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/pytorch/kd_wrapper.py
--rw-r--r--  2.0 unx     4527 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/pytorch/model_wrapper.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/experimental/__init__.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/logarithmic_equalization/__init__.py
--rw-r--r--  2.0 unx     2764 b- defN 24-Apr-04 11:59 sparseml/modifiers/logarithmic_equalization/base.py
--rw-r--r--  2.0 unx     1947 b- defN 24-Apr-04 11:59 sparseml/modifiers/logarithmic_equalization/pytorch.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/__init__.py
--rw-r--r--  2.0 unx     5181 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/base.py
--rw-r--r--  2.0 unx     3543 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/utils/__init__.py
--rw-r--r--  2.0 unx     6686 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/utils/helpers.py
--rw-r--r--  2.0 unx     6753 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/utils/sgpt_wrapper.py
--rw-r--r--  2.0 unx      683 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/__init__.py
--rw-r--r--  2.0 unx     5725 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/helpers.py
--rw-r--r--  2.0 unx      676 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/constant/__init__.py
--rw-r--r--  2.0 unx      951 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/constant/base.py
--rw-r--r--  2.0 unx     3103 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/constant/pytorch.py
--rw-r--r--  2.0 unx      677 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/magnitude/__init__.py
--rw-r--r--  2.0 unx     1168 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/magnitude/base.py
--rw-r--r--  2.0 unx     5262 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/magnitude/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/utils/__init__.py
--rw-r--r--  2.0 unx      688 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/utils/pytorch/__init__.py
--rw-r--r--  2.0 unx     5984 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/utils/pytorch/layer_mask.py
--rw-r--r--  2.0 unx     5622 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/utils/pytorch/mask_factory.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/__init__.py
--rw-r--r--  2.0 unx     3905 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/base.py
--rw-r--r--  2.0 unx    10346 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/pytorch.py
--rw-r--r--  2.0 unx      633 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/utils/__init__.py
--rw-r--r--  2.0 unx     4469 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/utils/wanda_wrapper.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/__init__.py
--rw-r--r--  2.0 unx     5512 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/base.py
--rw-r--r--  2.0 unx     8924 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/__init__.py
--rw-r--r--  2.0 unx     2220 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/constants.py
--rw-r--r--  2.0 unx     2906 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/fake_quant_wrapper.py
--rw-r--r--  2.0 unx    32720 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/helpers.py
--rw-r--r--  2.0 unx    13545 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/quantization_scheme.py
--rw-r--r--  2.0 unx    20463 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/quantize.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/smoothquant/__init__.py
--rw-r--r--  2.0 unx     7535 b- defN 24-Apr-04 11:59 sparseml/modifiers/smoothquant/base.py
--rw-r--r--  2.0 unx     8167 b- defN 24-Apr-04 11:59 sparseml/modifiers/smoothquant/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/utils/__init__.py
--rw-r--r--  2.0 unx     3944 b- defN 24-Apr-04 11:59 sparseml/modifiers/utils/compression_wrapper.py
--rw-r--r--  2.0 unx     5257 b- defN 24-Apr-04 11:59 sparseml/modifiers/utils/layer_compressor.py
--rw-r--r--  2.0 unx     3265 b- defN 24-Apr-04 11:59 sparseml/modifiers/utils/pytorch_helpers.py
--rw-r--r--  2.0 unx     1036 b- defN 24-Apr-04 11:59 sparseml/onnx/__init__.py
--rw-r--r--  2.0 unx     6202 b- defN 24-Apr-04 11:59 sparseml/onnx/base.py
--rw-r--r--  2.0 unx      743 b- defN 24-Apr-04 11:59 sparseml/onnx/benchmark/__init__.py
--rw-r--r--  2.0 unx    15366 b- defN 24-Apr-04 11:59 sparseml/onnx/benchmark/info.py
--rw-r--r--  2.0 unx      823 b- defN 24-Apr-04 11:59 sparseml/onnx/framework/__init__.py
--rw-r--r--  2.0 unx     6116 b- defN 24-Apr-04 11:59 sparseml/onnx/framework/info.py
--rw-r--r--  2.0 unx      820 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/__init__.py
--rw-r--r--  2.0 unx    13285 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/analyzer_model.py
--rw-r--r--  2.0 unx    19639 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/sensitivity_pruning.py
--rw-r--r--  2.0 unx     6470 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/structured_pruning.py
--rw-r--r--  2.0 unx      815 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/quantization/__init__.py
--rw-r--r--  2.0 unx    14753 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/quantization/calibration.py
--rw-r--r--  2.0 unx    73551 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/quantization/quantize.py
--rw-r--r--  2.0 unx     4552 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/quantization/quantize_model_post_training.py
--rw-r--r--  2.0 unx      869 b- defN 24-Apr-04 11:59 sparseml/onnx/sparsification/__init__.py
--rw-r--r--  2.0 unx    10209 b- defN 24-Apr-04 11:59 sparseml/onnx/sparsification/analyzer.py
--rw-r--r--  2.0 unx     1363 b- defN 24-Apr-04 11:59 sparseml/onnx/sparsification/info.py
--rw-r--r--  2.0 unx     8009 b- defN 24-Apr-04 11:59 sparseml/onnx/sparsification/model_info.py
--rw-r--r--  2.0 unx      867 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/__init__.py
--rw-r--r--  2.0 unx    13013 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/data.py
--rw-r--r--  2.0 unx    20691 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/graph_editor.py
--rw-r--r--  2.0 unx     8133 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/graph_optimizer.py
--rw-r--r--  2.0 unx    40230 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/helpers.py
--rw-r--r--  2.0 unx     1958 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/loss.py
--rw-r--r--  2.0 unx    31591 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/model.py
--rw-r--r--  2.0 unx     5437 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/sparse_tensor.py
--rw-r--r--  2.0 unx      931 b- defN 24-Apr-04 11:59 sparseml/openpifpaf/__init__.py
--rw-r--r--  2.0 unx     3713 b- defN 24-Apr-04 11:59 sparseml/openpifpaf/export.py
--rw-r--r--  2.0 unx    10950 b- defN 24-Apr-04 11:59 sparseml/openpifpaf/train.py
--rw-r--r--  2.0 unx     4211 b- defN 24-Apr-04 11:59 sparseml/openpifpaf/trainer.py
--rw-r--r--  2.0 unx      882 b- defN 24-Apr-04 11:59 sparseml/optim/__init__.py
--rw-r--r--  2.0 unx     6302 b- defN 24-Apr-04 11:59 sparseml/optim/analyzer.py
--rw-r--r--  2.0 unx    25563 b- defN 24-Apr-04 11:59 sparseml/optim/helpers.py
--rw-r--r--  2.0 unx    25984 b- defN 24-Apr-04 11:59 sparseml/optim/manager.py
--rw-r--r--  2.0 unx    30708 b- defN 24-Apr-04 11:59 sparseml/optim/modifier.py
--rw-r--r--  2.0 unx    26315 b- defN 24-Apr-04 11:59 sparseml/optim/sensitivity.py
--rw-r--r--  2.0 unx     2190 b- defN 24-Apr-04 11:59 sparseml/pytorch/__init__.py
--rw-r--r--  2.0 unx     6154 b- defN 24-Apr-04 11:59 sparseml/pytorch/base.py
--rw-r--r--  2.0 unx      933 b- defN 24-Apr-04 11:59 sparseml/pytorch/opset.py
--rw-r--r--  2.0 unx    12086 b- defN 24-Apr-04 11:59 sparseml/pytorch/torch_to_onnx_exporter.py
--rw-r--r--  2.0 unx      998 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/__init__.py
--rw-r--r--  2.0 unx     4193 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/generic.py
--rw-r--r--  2.0 unx     3014 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/registry.py
--rw-r--r--  2.0 unx      828 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/__init__.py
--rw-r--r--  2.0 unx     4457 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/cifar.py
--rw-r--r--  2.0 unx     3669 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/imagefolder.py
--rw-r--r--  2.0 unx     4000 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/imagenet.py
--rw-r--r--  2.0 unx     6491 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/imagenette.py
--rw-r--r--  2.0 unx     2434 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/mnist.py
--rw-r--r--  2.0 unx      767 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/detection/__init__.py
--rw-r--r--  2.0 unx    16159 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/detection/coco.py
--rw-r--r--  2.0 unx     5705 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/detection/helpers.py
--rw-r--r--  2.0 unx    10759 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/detection/voc.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/image_classification/__init__.py
--rw-r--r--  2.0 unx     9512 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/image_classification/ffcv_dataset.py
--rw-r--r--  2.0 unx      684 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/recommendation/__init__.py
--rw-r--r--  2.0 unx      693 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/video/__init__.py
--rw-r--r--  2.0 unx      814 b- defN 24-Apr-04 11:59 sparseml/pytorch/framework/__init__.py
--rw-r--r--  2.0 unx     5580 b- defN 24-Apr-04 11:59 sparseml/pytorch/framework/info.py
--rw-r--r--  2.0 unx      753 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/__init__.py
--rw-r--r--  2.0 unx    18265 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/export.py
--rw-r--r--  2.0 unx     4991 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/integration_helper_functions.py
--rw-r--r--  2.0 unx    15494 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/lr_analysis.py
--rw-r--r--  2.0 unx    14444 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/pr_sensitivity.py
--rw-r--r--  2.0 unx    29287 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/train.py
--rw-r--r--  2.0 unx      682 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/__init__.py
--rw-r--r--  2.0 unx     4278 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/cli_helpers.py
--rw-r--r--  2.0 unx     1257 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/constants.py
--rw-r--r--  2.0 unx    23894 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/helpers.py
--rw-r--r--  2.0 unx    12056 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/trainer.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/pytorch/model_load/__init__.py
--rw-r--r--  2.0 unx    11440 b- defN 24-Apr-04 11:59 sparseml/pytorch/model_load/helpers.py
--rw-r--r--  2.0 unx      976 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/__init__.py
--rw-r--r--  2.0 unx    14753 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/registry.py
--rw-r--r--  2.0 unx      901 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/__init__.py
--rw-r--r--  2.0 unx    11658 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/darknet.py
--rw-r--r--  2.0 unx    40293 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/efficientnet.py
--rw-r--r--  2.0 unx    16512 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/inception_v3.py
--rw-r--r--  2.0 unx     4164 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/mnist.py
--rw-r--r--  2.0 unx     9546 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/mobilenet.py
--rw-r--r--  2.0 unx    13014 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/mobilenet_v2.py
--rw-r--r--  2.0 unx    40800 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/resnet.py
--rw-r--r--  2.0 unx    16649 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/vgg.py
--rw-r--r--  2.0 unx      824 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/__init__.py
--rw-r--r--  2.0 unx     6820 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/ssd.py
--rw-r--r--  2.0 unx     8116 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/ssd_lite.py
--rw-r--r--  2.0 unx     4046 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/ssd_mobilenet.py
--rw-r--r--  2.0 unx     9069 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/ssd_resnet.py
--rw-r--r--  2.0 unx    10188 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/yolo_v3.py
--rw-r--r--  2.0 unx      763 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/external/__init__.py
--rw-r--r--  2.0 unx     6759 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/external/torchvision.py
--rw-r--r--  2.0 unx      676 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/recommendation/__init__.py
--rw-r--r--  2.0 unx      925 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/__init__.py
--rw-r--r--  2.0 unx     8673 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/activations.py
--rw-r--r--  2.0 unx    11854 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/fatrelu.py
--rw-r--r--  2.0 unx     1690 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/identity.py
--rw-r--r--  2.0 unx     2828 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/se.py
--rw-r--r--  2.0 unx     1243 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/__init__.py
--rw-r--r--  2.0 unx    13638 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/analyzer_as.py
--rw-r--r--  2.0 unx    15582 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/analyzer_module.py
--rw-r--r--  2.0 unx     3955 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/analyzer_pruning.py
--rw-r--r--  2.0 unx    26838 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/manager.py
--rw-r--r--  2.0 unx    36844 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/mask_creator_pruning.py
--rw-r--r--  2.0 unx    23085 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/mask_pruning.py
--rw-r--r--  2.0 unx    10449 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/mask_pruning_scorer.py
--rw-r--r--  2.0 unx     6605 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/optimizer.py
--rw-r--r--  2.0 unx    14879 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/sensitivity_as.py
--rw-r--r--  2.0 unx     6101 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/sensitivity_lr.py
--rw-r--r--  2.0 unx     9324 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/sensitivity_pruning.py
--rw-r--r--  2.0 unx      633 b- defN 24-Apr-04 11:59 sparseml/pytorch/recipe_template/__init__.py
--rw-r--r--  2.0 unx     4534 b- defN 24-Apr-04 11:59 sparseml/pytorch/recipe_template/cli.py
--rw-r--r--  2.0 unx     1559 b- defN 24-Apr-04 11:59 sparseml/pytorch/recipe_template/description.py
--rw-r--r--  2.0 unx    15943 b- defN 24-Apr-04 11:59 sparseml/pytorch/recipe_template/main.py
--rw-r--r--  2.0 unx      992 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/__init__.py
--rw-r--r--  2.0 unx     1366 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/info.py
--rw-r--r--  2.0 unx    32159 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/modifier.py
--rw-r--r--  2.0 unx    18952 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/modifier_thinning.py
--rw-r--r--  2.0 unx      705 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/distillation/__init__.py
--rw-r--r--  2.0 unx     4741 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/distillation/modifier_distillation.py
--rw-r--r--  2.0 unx    14742 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/distillation/modifier_distillation_base.py
--rw-r--r--  2.0 unx    19177 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/distillation/modifier_per_layer.py
--rw-r--r--  2.0 unx     1276 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/__init__.py
--rw-r--r--  2.0 unx    29250 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/mask_creator.py
--rw-r--r--  2.0 unx    24209 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/mask_params.py
--rw-r--r--  2.0 unx    13389 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_as.py
--rw-r--r--  2.0 unx    12006 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_powerpropagation.py
--rw-r--r--  2.0 unx    10455 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_acdc.py
--rw-r--r--  2.0 unx    33219 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_base.py
--rw-r--r--  2.0 unx     5757 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_constant.py
--rw-r--r--  2.0 unx     8857 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_layer.py
--rw-r--r--  2.0 unx    15595 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_magnitude.py
--rw-r--r--  2.0 unx    63519 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_mfac.py
--rw-r--r--  2.0 unx     8774 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_movement.py
--rw-r--r--  2.0 unx    24121 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_obs.py
--rw-r--r--  2.0 unx    23735 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_rigl.py
--rw-r--r--  2.0 unx    18245 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_structured.py
--rw-r--r--  2.0 unx    17683 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_topkast.py
--rw-r--r--  2.0 unx     4644 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/scorer.py
--rw-r--r--  2.0 unx      813 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/__init__.py
--rw-r--r--  2.0 unx     2220 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/constants.py
--rw-r--r--  2.0 unx    34914 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/helpers.py
--rw-r--r--  2.0 unx    33626 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/legacy_modifier_quantization.py
--rw-r--r--  2.0 unx    26778 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/modifier_quantization.py
--rw-r--r--  2.0 unx    13482 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/quantization_scheme.py
--rw-r--r--  2.0 unx    18047 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/quantize.py
--rw-r--r--  2.0 unx    76796 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/quantize_qat_export.py
--rw-r--r--  2.0 unx      790 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/__init__.py
--rw-r--r--  2.0 unx     1778 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_epoch.py
--rw-r--r--  2.0 unx     2883 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_logging.py
--rw-r--r--  2.0 unx    24287 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_lr.py
--rw-r--r--  2.0 unx    21497 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_params.py
--rw-r--r--  2.0 unx     6690 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_regularizer.py
--rw-r--r--  2.0 unx      943 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/__init__.py
--rw-r--r--  2.0 unx     6490 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/export_onnx.py
--rw-r--r--  2.0 unx     2870 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/presets.py
--rw-r--r--  2.0 unx     2530 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/sampler.py
--rw-r--r--  2.0 unx    43917 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/train.py
--rw-r--r--  2.0 unx     7128 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/transforms.py
--rw-r--r--  2.0 unx    16675 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/utils.py
--rw-r--r--  2.0 unx     1160 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/__init__.py
--rw-r--r--  2.0 unx     9706 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/benchmarker.py
--rw-r--r--  2.0 unx     2846 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/callbacks.py
--rw-r--r--  2.0 unx     1061 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/distributed.py
--rw-r--r--  2.0 unx    30884 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/exporter.py
--rw-r--r--  2.0 unx    42811 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/helpers.py
--rw-r--r--  2.0 unx     1663 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/log_sparsification_info.py
--rw-r--r--  2.0 unx    31374 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/logger.py
--rw-r--r--  2.0 unx    27048 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/loss.py
--rw-r--r--  2.0 unx    11754 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/model.py
--rw-r--r--  2.0 unx    39117 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/module.py
--rw-r--r--  2.0 unx     9180 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification.py
--rw-r--r--  2.0 unx    30059 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/ssd_helpers.py
--rw-r--r--  2.0 unx    12337 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/yolo_helpers.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification_info/__init__.py
--rw-r--r--  2.0 unx    15081 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification_info/configs.py
--rw-r--r--  2.0 unx     4336 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification_info/helpers.py
--rw-r--r--  2.0 unx     2788 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification_info/module_sparsification_info.py
--rw-r--r--  2.0 unx      655 b- defN 24-Apr-04 11:59 sparseml/recipe_template/__init__.py
--rw-r--r--  2.0 unx     4788 b- defN 24-Apr-04 11:59 sparseml/recipe_template/utils.py
--rw-r--r--  2.0 unx     1058 b- defN 24-Apr-04 11:59 sparseml/sparsification/__init__.py
--rw-r--r--  2.0 unx     9387 b- defN 24-Apr-04 11:59 sparseml/sparsification/analyzer.py
--rw-r--r--  2.0 unx     9065 b- defN 24-Apr-04 11:59 sparseml/sparsification/info.py
--rw-r--r--  2.0 unx    15565 b- defN 24-Apr-04 11:59 sparseml/sparsification/model_info.py
--rw-r--r--  2.0 unx     2002 b- defN 24-Apr-04 11:59 sparseml/sparsification/modifier_epoch.py
--rw-r--r--  2.0 unx    10117 b- defN 24-Apr-04 11:59 sparseml/sparsification/modifier_lr.py
--rw-r--r--  2.0 unx     5505 b- defN 24-Apr-04 11:59 sparseml/sparsification/modifier_params.py
--rw-r--r--  2.0 unx    12845 b- defN 24-Apr-04 11:59 sparseml/sparsification/modifier_pruning.py
--rw-r--r--  2.0 unx     3700 b- defN 24-Apr-04 11:59 sparseml/sparsification/oracle.py
--rw-r--r--  2.0 unx    18570 b- defN 24-Apr-04 11:59 sparseml/sparsification/recipe_builder.py
--rw-r--r--  2.0 unx    14413 b- defN 24-Apr-04 11:59 sparseml/sparsification/recipe_editor.py
--rw-r--r--  2.0 unx     1250 b- defN 24-Apr-04 11:59 sparseml/sparsification/types.py
--rw-r--r--  2.0 unx     1169 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/__init__.py
--rw-r--r--  2.0 unx     7272 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/base.py
--rw-r--r--  2.0 unx      925 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/__init__.py
--rw-r--r--  2.0 unx     8121 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/dataset.py
--rw-r--r--  2.0 unx     5600 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/helpers.py
--rw-r--r--  2.0 unx     2768 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/registry.py
--rw-r--r--  2.0 unx      807 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/__init__.py
--rw-r--r--  2.0 unx    12686 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/cifar.py
--rw-r--r--  2.0 unx     8690 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/imagefolder.py
--rw-r--r--  2.0 unx     2032 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/imagenet.py
--rw-r--r--  2.0 unx     4695 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/imagenette.py
--rw-r--r--  2.0 unx      805 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/framework/__init__.py
--rw-r--r--  2.0 unx     5859 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/framework/info.py
--rw-r--r--  2.0 unx      925 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/__init__.py
--rw-r--r--  2.0 unx    19752 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/estimator.py
--rw-r--r--  2.0 unx    14774 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/registry.py
--rw-r--r--  2.0 unx      822 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/__init__.py
--rw-r--r--  2.0 unx     3540 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/mnist.py
--rw-r--r--  2.0 unx    11161 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/mobilenet.py
--rw-r--r--  2.0 unx    18359 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/mobilenet_v2.py
--rw-r--r--  2.0 unx    28103 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/resnet.py
--rw-r--r--  2.0 unx    26886 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/vgg.py
--rw-r--r--  2.0 unx      865 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/nn/__init__.py
--rw-r--r--  2.0 unx    18670 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/nn/layers.py
--rw-r--r--  2.0 unx     1238 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/__init__.py
--rw-r--r--  2.0 unx     8607 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/analyzer_module.py
--rw-r--r--  2.0 unx     9591 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/manager.py
--rw-r--r--  2.0 unx    19683 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/mask_creator_pruning.py
--rw-r--r--  2.0 unx    33919 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/mask_pruning.py
--rw-r--r--  2.0 unx    15955 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier.py
--rw-r--r--  2.0 unx     1715 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier_epoch.py
--rw-r--r--  2.0 unx    10685 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier_lr.py
--rw-r--r--  2.0 unx     7092 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier_params.py
--rw-r--r--  2.0 unx    15702 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier_pruning.py
--rw-r--r--  2.0 unx     5682 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/schedule_lr.py
--rw-r--r--  2.0 unx     9232 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/sensitivity_pruning.py
--rw-r--r--  2.0 unx      801 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/sparsification/__init__.py
--rw-r--r--  2.0 unx     1385 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/sparsification/info.py
--rw-r--r--  2.0 unx      967 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/__init__.py
--rw-r--r--  2.0 unx    10913 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/exporter.py
--rw-r--r--  2.0 unx      996 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/helpers.py
--rw-r--r--  2.0 unx     1974 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/loss.py
--rw-r--r--  2.0 unx     8119 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/nets_utils.py
--rw-r--r--  2.0 unx     1327 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/summary.py
--rw-r--r--  2.0 unx    12536 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/variable.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/tools/__init__.py
--rw-r--r--  2.0 unx     1193 b- defN 24-Apr-04 11:59 sparseml/transformers/__init__.py
--rw-r--r--  2.0 unx      985 b- defN 24-Apr-04 11:59 sparseml/transformers/base.py
--rw-r--r--  2.0 unx    23904 b- defN 24-Apr-04 11:59 sparseml/transformers/export.py
--rw-r--r--  2.0 unx     9485 b- defN 24-Apr-04 11:59 sparseml/transformers/integration_helper_functions.py
--rw-r--r--  2.0 unx    30795 b- defN 24-Apr-04 11:59 sparseml/transformers/masked_language_modeling.py
--rw-r--r--  2.0 unx    37002 b- defN 24-Apr-04 11:59 sparseml/transformers/question_answering.py
--rw-r--r--  2.0 unx    40360 b- defN 24-Apr-04 11:59 sparseml/transformers/text_classification.py
--rw-r--r--  2.0 unx      818 b- defN 24-Apr-04 11:59 sparseml/transformers/text_generation.py
--rw-r--r--  2.0 unx    34389 b- defN 24-Apr-04 11:59 sparseml/transformers/token_classification.py
--rw-r--r--  2.0 unx      683 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/__init__.py
--rw-r--r--  2.0 unx      749 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/compressors/__init__.py
--rw-r--r--  2.0 unx     3052 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/compressors/base.py
--rw-r--r--  2.0 unx     1160 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/compressors/dense.py
--rw-r--r--  2.0 unx     8382 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/compressors/sparse_bitmask.py
--rw-r--r--  2.0 unx      751 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/config/__init__.py
--rw-r--r--  2.0 unx     3846 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/config/base.py
--rw-r--r--  2.0 unx     1263 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/config/dense.py
--rw-r--r--  2.0 unx     1236 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/config/sparse_bitmask.py
--rw-r--r--  2.0 unx      718 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/utils/__init__.py
--rw-r--r--  2.0 unx     6092 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/utils/compress_save.py
--rw-r--r--  2.0 unx     1778 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/utils/helpers.py
--rw-r--r--  2.0 unx     6966 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/utils/safetensors_load.py
--rw-r--r--  2.0 unx      701 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/__init__.py
--rw-r--r--  2.0 unx     4265 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/callbacks.py
--rw-r--r--  2.0 unx     2519 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/model_args.py
--rw-r--r--  2.0 unx    11988 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/runner.py
--rw-r--r--  2.0 unx    22620 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/session_mixin.py
--rw-r--r--  2.0 unx    13101 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/text_generation.py
--rw-r--r--  2.0 unx     4126 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/trainer.py
--rw-r--r--  2.0 unx     3005 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/training_args.py
--rw-r--r--  2.0 unx     1021 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/__init__.py
--rw-r--r--  2.0 unx     8736 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/base.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/c4.py
--rw-r--r--  2.0 unx     2537 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/cnn_dailymail.py
--rw-r--r--  2.0 unx     4281 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/custom.py
--rw-r--r--  2.0 unx     5399 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/data_args.py
--rw-r--r--  2.0 unx     8961 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/data_helpers.py
--rw-r--r--  2.0 unx     2892 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/evolcodealpaca.py
--rw-r--r--  2.0 unx     2597 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/gsm8k.py
--rw-r--r--  2.0 unx     3435 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/open_platypus.py
--rw-r--r--  2.0 unx     1369 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/ptb.py
--rw-r--r--  2.0 unx     3521 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/ultrachat_200k.py
--rw-r--r--  2.0 unx     1237 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/wikitext.py
--rw-r--r--  2.0 unx      922 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/__init__.py
--rw-r--r--  2.0 unx    19272 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/question_answering.py
--rw-r--r--  2.0 unx     1874 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/sparse_config.py
--rw-r--r--  2.0 unx    20264 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/sparse_model.py
--rw-r--r--  2.0 unx     2327 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/sparse_tokenizer.py
--rw-r--r--  2.0 unx    40305 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/trainer.py
--rw-r--r--  2.0 unx     1890 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/training_args.py
--rw-r--r--  2.0 unx      866 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/__init__.py
--rw-r--r--  2.0 unx     2429 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/base.py
--rw-r--r--  2.0 unx     3922 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modification_objects.py
--rw-r--r--  2.0 unx     2434 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modify_model.py
--rw-r--r--  2.0 unx     9148 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_bert.py
--rw-r--r--  2.0 unx     5883 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_distilbert.py
--rw-r--r--  2.0 unx     8457 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_llama.py
--rw-r--r--  2.0 unx     7689 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_mistral.py
--rw-r--r--  2.0 unx     2549 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_mobilebert.py
--rw-r--r--  2.0 unx     9862 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_opt.py
--rw-r--r--  2.0 unx     1486 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/registry.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/__init__.py
--rw-r--r--  2.0 unx    19683 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/export.py
--rw-r--r--  2.0 unx     7695 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/obcq.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/utils/__init__.py
--rw-r--r--  2.0 unx     3671 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/utils/helpers.py
--rw-r--r--  2.0 unx      805 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/__init__.py
--rw-r--r--  2.0 unx    20055 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/helpers.py
--rw-r--r--  2.0 unx     6711 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/initializers.py
--rw-r--r--  2.0 unx     5081 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/load_task_dataset.py
--rw-r--r--  2.0 unx     2764 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/load_task_model.py
--rw-r--r--  2.0 unx     2536 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/metrics.py
--rw-r--r--  2.0 unx     1972 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/optimizations.py
--rw-r--r--  2.0 unx     1037 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/preprocessing_functions.py
--rw-r--r--  2.0 unx      844 b- defN 24-Apr-04 11:59 sparseml/utils/__init__.py
--rw-r--r--  2.0 unx      886 b- defN 24-Apr-04 11:59 sparseml/utils/frameworks.py
--rw-r--r--  2.0 unx    31604 b- defN 24-Apr-04 11:59 sparseml/utils/helpers.py
--rw-r--r--  2.0 unx     3983 b- defN 24-Apr-04 11:59 sparseml/utils/restricted_eval.py
--rw-r--r--  2.0 unx     1083 b- defN 24-Apr-04 11:59 sparseml/utils/singleton.py
--rw-r--r--  2.0 unx     6312 b- defN 24-Apr-04 11:59 sparseml/utils/worker.py
--rw-r--r--  2.0 unx     2952 b- defN 24-Apr-04 11:59 sparseml/utils/wrapper.py
--rw-r--r--  2.0 unx      819 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/__init__.py
--rw-r--r--  2.0 unx      833 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/cifar.py
--rw-r--r--  2.0 unx     3750 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/coco.py
--rw-r--r--  2.0 unx     1217 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/helpers.py
--rw-r--r--  2.0 unx    23366 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/imagenet.py
--rw-r--r--  2.0 unx     8967 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/imagenette.py
--rw-r--r--  2.0 unx     1009 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/voc.py
--rw-r--r--  2.0 unx      633 b- defN 24-Apr-04 11:59 sparseml/utils/fsdp/__init__.py
--rw-r--r--  2.0 unx     2096 b- defN 24-Apr-04 11:59 sparseml/utils/fsdp/context.py
--rw-r--r--  2.0 unx     6631 b- defN 24-Apr-04 11:59 sparseml/utils/fsdp/helpers.py
--rw-r--r--  2.0 unx      656 b- defN 24-Apr-04 11:59 sparseml/utils/pytorch/__init__.py
--rw-r--r--  2.0 unx    10897 b- defN 24-Apr-04 11:59 sparseml/utils/pytorch/module.py
--rw-r--r--  2.0 unx     1696 b- defN 24-Apr-04 11:59 sparseml/utils/pytorch/utils.py
--rw-r--r--  2.0 unx      680 b- defN 24-Apr-04 11:59 sparseml/utils/pytorch/pruning/__init__.py
--rw-r--r--  2.0 unx     1875 b- defN 24-Apr-04 11:59 sparseml/yolact/COCO.sh
--rw-r--r--  2.0 unx     1418 b- defN 24-Apr-04 11:59 sparseml/yolact/COCO_test.sh
--rw-r--r--  2.0 unx     4020 b- defN 24-Apr-04 11:59 sparseml/yolact/__init__.py
--rw-r--r--  2.0 unx     1784 b- defN 24-Apr-04 11:59 sparseml/yolact/scripts.py
--rw-r--r--  2.0 unx     1440 b- defN 24-Apr-04 11:59 sparseml/yolov5/__init__.py
--rw-r--r--  2.0 unx     4505 b- defN 24-Apr-04 11:59 sparseml/yolov5/helpers.py
--rw-r--r--  2.0 unx     1609 b- defN 24-Apr-04 11:59 sparseml/yolov5/scripts.py
--rw-r--r--  2.0 unx     1220 b- defN 24-Apr-04 11:59 sparseml/yolov5/yolov5.status.yaml
--rw-r--r--  2.0 unx     1117 b- defN 24-Apr-04 11:59 sparseml/yolov8/__init__.py
--rw-r--r--  2.0 unx     6061 b- defN 24-Apr-04 11:59 sparseml/yolov8/default.yaml
--rw-r--r--  2.0 unx     2815 b- defN 24-Apr-04 11:59 sparseml/yolov8/export.py
--rw-r--r--  2.0 unx     2259 b- defN 24-Apr-04 11:59 sparseml/yolov8/modules.py
--rw-r--r--  2.0 unx     7394 b- defN 24-Apr-04 11:59 sparseml/yolov8/train.py
--rw-r--r--  2.0 unx    37860 b- defN 24-Apr-04 11:59 sparseml/yolov8/trainers.py
--rw-r--r--  2.0 unx     2748 b- defN 24-Apr-04 11:59 sparseml/yolov8/val.py
--rw-r--r--  2.0 unx     8459 b- defN 24-Apr-04 11:59 sparseml/yolov8/validators.py
--rw-r--r--  2.0 unx      685 b- defN 24-Apr-04 11:59 sparseml/yolov8/utils/__init__.py
--rw-r--r--  2.0 unx     6683 b- defN 24-Apr-04 11:59 sparseml/yolov8/utils/export_samples.py
--rw-r--r--  2.0 unx     4041 b- defN 24-Apr-04 11:59 sparseml/yolov8/utils/helpers.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/LICENSE
--rw-r--r--  2.0 unx    13747 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/LICENSE-ULTRALYTICS
--rw-r--r--  2.0 unx    23627 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/METADATA
--rw-r--r--  2.0 unx     2085 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/NOTICE
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/WHEEL
--rw-r--r--  2.0 unx     3122 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        9 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    56768 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/RECORD
-567 files, 4551890 bytes uncompressed, 1227493 bytes compressed:  73.0%
+Zip file size: 1313479 bytes, number of entries: 561
+-rw-r--r--  2.0 unx     1489 b- defN 24-May-07 00:45 sparseml/__init__.py
+-rw-r--r--  2.0 unx      898 b- defN 24-May-07 00:45 sparseml/analytics.py
+-rw-r--r--  2.0 unx    10284 b- defN 24-May-07 00:45 sparseml/base.py
+-rw-r--r--  2.0 unx     8182 b- defN 24-May-07 00:45 sparseml/integration_helper_functions.py
+-rw-r--r--  2.0 unx     2483 b- defN 24-May-07 00:45 sparseml/log.py
+-rw-r--r--  2.0 unx     1607 b- defN 24-May-07 00:45 sparseml/version.py
+-rw-r--r--  2.0 unx      758 b- defN 24-May-07 00:45 sparseml/benchmark/__init__.py
+-rw-r--r--  2.0 unx    17763 b- defN 24-May-07 00:45 sparseml/benchmark/info.py
+-rw-r--r--  2.0 unx    10778 b- defN 24-May-07 00:45 sparseml/benchmark/serialization.py
+-rw-r--r--  2.0 unx      915 b- defN 24-May-07 00:45 sparseml/core/__init__.py
+-rw-r--r--  2.0 unx     6842 b- defN 24-May-07 00:45 sparseml/core/event.py
+-rw-r--r--  2.0 unx     6492 b- defN 24-May-07 00:45 sparseml/core/factory.py
+-rw-r--r--  2.0 unx     2850 b- defN 24-May-07 00:45 sparseml/core/framework.py
+-rw-r--r--  2.0 unx     2855 b- defN 24-May-07 00:45 sparseml/core/framework_object.py
+-rw-r--r--  2.0 unx     3780 b- defN 24-May-07 00:45 sparseml/core/helpers.py
+-rw-r--r--  2.0 unx    23720 b- defN 24-May-07 00:45 sparseml/core/session.py
+-rw-r--r--  2.0 unx     9034 b- defN 24-May-07 00:45 sparseml/core/state.py
+-rw-r--r--  2.0 unx      667 b- defN 24-May-07 00:45 sparseml/core/data/__init__.py
+-rw-r--r--  2.0 unx     1565 b- defN 24-May-07 00:45 sparseml/core/data/base.py
+-rw-r--r--  2.0 unx     6415 b- defN 24-May-07 00:45 sparseml/core/data/pytorch.py
+-rw-r--r--  2.0 unx      678 b- defN 24-May-07 00:45 sparseml/core/lifecycle/__init__.py
+-rw-r--r--  2.0 unx    11954 b- defN 24-May-07 00:45 sparseml/core/lifecycle/event.py
+-rw-r--r--  2.0 unx     8242 b- defN 24-May-07 00:45 sparseml/core/lifecycle/session.py
+-rw-r--r--  2.0 unx      654 b- defN 24-May-07 00:45 sparseml/core/logger/__init__.py
+-rw-r--r--  2.0 unx    44703 b- defN 24-May-07 00:45 sparseml/core/logger/logger.py
+-rw-r--r--  2.0 unx      667 b- defN 24-May-07 00:45 sparseml/core/logger/utils/__init__.py
+-rw-r--r--  2.0 unx    12725 b- defN 24-May-07 00:45 sparseml/core/logger/utils/frequency_manager.py
+-rw-r--r--  2.0 unx      693 b- defN 24-May-07 00:45 sparseml/core/model/__init__.py
+-rw-r--r--  2.0 unx     5895 b- defN 24-May-07 00:45 sparseml/core/model/base.py
+-rw-r--r--  2.0 unx     5641 b- defN 24-May-07 00:45 sparseml/core/model/pytorch.py
+-rw-r--r--  2.0 unx      699 b- defN 24-May-07 00:45 sparseml/core/modifier/__init__.py
+-rw-r--r--  2.0 unx     3326 b- defN 24-May-07 00:45 sparseml/core/modifier/base.py
+-rw-r--r--  2.0 unx    10840 b- defN 24-May-07 00:45 sparseml/core/modifier/modifier.py
+-rw-r--r--  2.0 unx     6093 b- defN 24-May-07 00:45 sparseml/core/modifier/stage.py
+-rw-r--r--  2.0 unx      672 b- defN 24-May-07 00:45 sparseml/core/optimizer/__init__.py
+-rw-r--r--  2.0 unx     3409 b- defN 24-May-07 00:45 sparseml/core/optimizer/base.py
+-rw-r--r--  2.0 unx     4584 b- defN 24-May-07 00:45 sparseml/core/optimizer/pytorch.py
+-rw-r--r--  2.0 unx      790 b- defN 24-May-07 00:45 sparseml/core/recipe/__init__.py
+-rw-r--r--  2.0 unx     6872 b- defN 24-May-07 00:45 sparseml/core/recipe/args.py
+-rw-r--r--  2.0 unx     1702 b- defN 24-May-07 00:45 sparseml/core/recipe/base.py
+-rw-r--r--  2.0 unx     6313 b- defN 24-May-07 00:45 sparseml/core/recipe/container.py
+-rw-r--r--  2.0 unx     2932 b- defN 24-May-07 00:45 sparseml/core/recipe/metadata.py
+-rw-r--r--  2.0 unx     4248 b- defN 24-May-07 00:45 sparseml/core/recipe/modifier.py
+-rw-r--r--  2.0 unx    26428 b- defN 24-May-07 00:45 sparseml/core/recipe/recipe.py
+-rw-r--r--  2.0 unx     7726 b- defN 24-May-07 00:45 sparseml/core/recipe/stage.py
+-rw-r--r--  2.0 unx      663 b- defN 24-May-07 00:45 sparseml/core/utils/__init__.py
+-rw-r--r--  2.0 unx     1025 b- defN 24-May-07 00:45 sparseml/core/utils/session_helpers.py
+-rw-r--r--  2.0 unx      863 b- defN 24-May-07 00:45 sparseml/deepsparse/__init__.py
+-rw-r--r--  2.0 unx     3516 b- defN 24-May-07 00:45 sparseml/deepsparse/base.py
+-rw-r--r--  2.0 unx      801 b- defN 24-May-07 00:45 sparseml/deepsparse/framework/__init__.py
+-rw-r--r--  2.0 unx     6032 b- defN 24-May-07 00:45 sparseml/deepsparse/framework/info.py
+-rw-r--r--  2.0 unx      813 b- defN 24-May-07 00:45 sparseml/deepsparse/sparsification/__init__.py
+-rw-r--r--  2.0 unx     1348 b- defN 24-May-07 00:45 sparseml/deepsparse/sparsification/info.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/evaluation/__init__.py
+-rw-r--r--  2.0 unx     5689 b- defN 24-May-07 00:45 sparseml/evaluation/cli.py
+-rw-r--r--  2.0 unx     2140 b- defN 24-May-07 00:45 sparseml/evaluation/evaluator.py
+-rw-r--r--  2.0 unx      940 b- defN 24-May-07 00:45 sparseml/evaluation/integrations_config.yaml
+-rw-r--r--  2.0 unx     5571 b- defN 24-May-07 00:45 sparseml/evaluation/registry.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/evaluation/integrations/__init__.py
+-rw-r--r--  2.0 unx     6000 b- defN 24-May-07 00:45 sparseml/evaluation/integrations/lm_evaluation_harness.py
+-rw-r--r--  2.0 unx    10987 b- defN 24-May-07 00:45 sparseml/evaluation/integrations/perplexity.py
+-rw-r--r--  2.0 unx      661 b- defN 24-May-07 00:45 sparseml/export/__init__.py
+-rw-r--r--  2.0 unx    21753 b- defN 24-May-07 00:45 sparseml/export/export.py
+-rw-r--r--  2.0 unx     8049 b- defN 24-May-07 00:45 sparseml/export/export_data.py
+-rw-r--r--  2.0 unx     2681 b- defN 24-May-07 00:45 sparseml/export/export_torch_model.py
+-rw-r--r--  2.0 unx    13656 b- defN 24-May-07 00:45 sparseml/export/helpers.py
+-rw-r--r--  2.0 unx     8868 b- defN 24-May-07 00:45 sparseml/export/validators.py
+-rw-r--r--  2.0 unx      786 b- defN 24-May-07 00:45 sparseml/exporters/__init__.py
+-rw-r--r--  2.0 unx     1477 b- defN 24-May-07 00:45 sparseml/exporters/base_exporter.py
+-rw-r--r--  2.0 unx     6576 b- defN 24-May-07 00:45 sparseml/exporters/kv_cache_injector.py
+-rw-r--r--  2.0 unx     5522 b- defN 24-May-07 00:45 sparseml/exporters/onnx_to_deepsparse.py
+-rw-r--r--  2.0 unx     2350 b- defN 24-May-07 00:45 sparseml/exporters/transforms/__init__.py
+-rw-r--r--  2.0 unx     2333 b- defN 24-May-07 00:45 sparseml/exporters/transforms/base_transform.py
+-rw-r--r--  2.0 unx     1388 b- defN 24-May-07 00:45 sparseml/exporters/transforms/constants_to_initializers.py
+-rw-r--r--  2.0 unx     4630 b- defN 24-May-07 00:45 sparseml/exporters/transforms/conv_to_convinteger_add_cast_mul.py
+-rw-r--r--  2.0 unx     5973 b- defN 24-May-07 00:45 sparseml/exporters/transforms/conv_to_qlinearconv.py
+-rw-r--r--  2.0 unx     2440 b- defN 24-May-07 00:45 sparseml/exporters/transforms/delete_repeated_qdq.py
+-rw-r--r--  2.0 unx     1842 b- defN 24-May-07 00:45 sparseml/exporters/transforms/delete_trivial_onnx_adds.py
+-rw-r--r--  2.0 unx     2241 b- defN 24-May-07 00:45 sparseml/exporters/transforms/flatten_qparams.py
+-rw-r--r--  2.0 unx     3553 b- defN 24-May-07 00:45 sparseml/exporters/transforms/fold_conv_div_bn.py
+-rw-r--r--  2.0 unx     1669 b- defN 24-May-07 00:45 sparseml/exporters/transforms/fold_identity_initializers.py
+-rw-r--r--  2.0 unx     2070 b- defN 24-May-07 00:45 sparseml/exporters/transforms/fold_relu_quants.py
+-rw-r--r--  2.0 unx     4418 b- defN 24-May-07 00:45 sparseml/exporters/transforms/gemm_to_matmulinteger_add_cast_mul.py
+-rw-r--r--  2.0 unx     7629 b- defN 24-May-07 00:45 sparseml/exporters/transforms/gemm_to_qlinearmatmul.py
+-rw-r--r--  2.0 unx     1645 b- defN 24-May-07 00:45 sparseml/exporters/transforms/initializers_to_uint8.py
+-rw-r--r--  2.0 unx     6297 b- defN 24-May-07 00:45 sparseml/exporters/transforms/matmul_add_to_matmulinteger_add_cast_mul.py
+-rw-r--r--  2.0 unx     4681 b- defN 24-May-07 00:45 sparseml/exporters/transforms/matmul_to_matmulinteger_cast_mul.py
+-rw-r--r--  2.0 unx     4156 b- defN 24-May-07 00:45 sparseml/exporters/transforms/matmul_to_qlinearmatmul.py
+-rw-r--r--  2.0 unx     3770 b- defN 24-May-07 00:45 sparseml/exporters/transforms/onnx_transform.py
+-rw-r--r--  2.0 unx     3433 b- defN 24-May-07 00:45 sparseml/exporters/transforms/propagate_dequant_through_split.py
+-rw-r--r--  2.0 unx     4801 b- defN 24-May-07 00:45 sparseml/exporters/transforms/propagate_embedding_quantization.py
+-rw-r--r--  2.0 unx     4464 b- defN 24-May-07 00:45 sparseml/exporters/transforms/quantize_qat_embedding.py
+-rw-r--r--  2.0 unx     3869 b- defN 24-May-07 00:45 sparseml/exporters/transforms/quantize_residuals.py
+-rw-r--r--  2.0 unx     3331 b- defN 24-May-07 00:45 sparseml/exporters/transforms/remove_duplicate_qconv_weights.py
+-rw-r--r--  2.0 unx     2545 b- defN 24-May-07 00:45 sparseml/exporters/transforms/remove_duplicate_quantize_ops.py
+-rw-r--r--  2.0 unx     3210 b- defN 24-May-07 00:45 sparseml/exporters/transforms/skip_input_quantize.py
+-rw-r--r--  2.0 unx     1373 b- defN 24-May-07 00:45 sparseml/exporters/transforms/unwrap_batchnorms.py
+-rw-r--r--  2.0 unx      911 b- defN 24-May-07 00:45 sparseml/exporters/transforms/kv_cache/__init__.py
+-rw-r--r--  2.0 unx    30558 b- defN 24-May-07 00:45 sparseml/exporters/transforms/kv_cache/cache_keys_and_values.py
+-rw-r--r--  2.0 unx    10727 b- defN 24-May-07 00:45 sparseml/exporters/transforms/kv_cache/configs.py
+-rw-r--r--  2.0 unx     7027 b- defN 24-May-07 00:45 sparseml/exporters/transforms/kv_cache/transforms_base.py
+-rw-r--r--  2.0 unx     4974 b- defN 24-May-07 00:45 sparseml/exporters/transforms/kv_cache/transforms_codegen.py
+-rw-r--r--  2.0 unx     8801 b- defN 24-May-07 00:45 sparseml/exporters/transforms/kv_cache/transforms_llama.py
+-rw-r--r--  2.0 unx     4261 b- defN 24-May-07 00:45 sparseml/exporters/transforms/kv_cache/transforms_mpt.py
+-rw-r--r--  2.0 unx     6066 b- defN 24-May-07 00:45 sparseml/exporters/transforms/kv_cache/transforms_opt.py
+-rw-r--r--  2.0 unx      730 b- defN 24-May-07 00:45 sparseml/exporters/transforms/utils/__init__.py
+-rw-r--r--  2.0 unx    11112 b- defN 24-May-07 00:45 sparseml/exporters/transforms/utils/add_quantized_conv_matmul_add_ops.py
+-rw-r--r--  2.0 unx     6809 b- defN 24-May-07 00:45 sparseml/exporters/transforms/utils/helpers.py
+-rw-r--r--  2.0 unx    14429 b- defN 24-May-07 00:45 sparseml/exporters/transforms/utils/matching.py
+-rw-r--r--  2.0 unx      790 b- defN 24-May-07 00:45 sparseml/framework/__init__.py
+-rw-r--r--  2.0 unx     9501 b- defN 24-May-07 00:45 sparseml/framework/info.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/integrations/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/integrations/torchvision/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/integrations/torchvision/evaluator.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/integrations/torchvision/trainer.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/integrations/torchvision/data/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/integrations/torchvision/metrics/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/integrations/torchvision/model/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/integrations/torchvision/optim/__init__.py
+-rw-r--r--  2.0 unx     1144 b- defN 24-May-07 00:45 sparseml/keras/__init__.py
+-rw-r--r--  2.0 unx     8054 b- defN 24-May-07 00:45 sparseml/keras/base.py
+-rw-r--r--  2.0 unx      943 b- defN 24-May-07 00:45 sparseml/keras/datasets/__init__.py
+-rw-r--r--  2.0 unx     3297 b- defN 24-May-07 00:45 sparseml/keras/datasets/dataset.py
+-rw-r--r--  2.0 unx     2423 b- defN 24-May-07 00:45 sparseml/keras/datasets/helpers.py
+-rw-r--r--  2.0 unx     2761 b- defN 24-May-07 00:45 sparseml/keras/datasets/registry.py
+-rw-r--r--  2.0 unx      786 b- defN 24-May-07 00:45 sparseml/keras/datasets/classification/__init__.py
+-rw-r--r--  2.0 unx     8369 b- defN 24-May-07 00:45 sparseml/keras/datasets/classification/imagefolder.py
+-rw-r--r--  2.0 unx     4301 b- defN 24-May-07 00:45 sparseml/keras/datasets/classification/imagenet.py
+-rw-r--r--  2.0 unx     2727 b- defN 24-May-07 00:45 sparseml/keras/datasets/classification/imagenette.py
+-rw-r--r--  2.0 unx      793 b- defN 24-May-07 00:45 sparseml/keras/framework/__init__.py
+-rw-r--r--  2.0 unx     5906 b- defN 24-May-07 00:45 sparseml/keras/framework/info.py
+-rw-r--r--  2.0 unx      921 b- defN 24-May-07 00:45 sparseml/keras/models/__init__.py
+-rw-r--r--  2.0 unx    11814 b- defN 24-May-07 00:45 sparseml/keras/models/registry.py
+-rw-r--r--  2.0 unx      656 b- defN 24-May-07 00:45 sparseml/keras/models/classification/__init__.py
+-rw-r--r--  2.0 unx    17932 b- defN 24-May-07 00:45 sparseml/keras/models/classification/resnet.py
+-rw-r--r--  2.0 unx      768 b- defN 24-May-07 00:45 sparseml/keras/models/external/__init__.py
+-rw-r--r--  2.0 unx     4402 b- defN 24-May-07 00:45 sparseml/keras/models/external/keras_applications.py
+-rw-r--r--  2.0 unx     1166 b- defN 24-May-07 00:45 sparseml/keras/optim/__init__.py
+-rw-r--r--  2.0 unx     5677 b- defN 24-May-07 00:45 sparseml/keras/optim/manager.py
+-rw-r--r--  2.0 unx    14777 b- defN 24-May-07 00:45 sparseml/keras/optim/mask_pruning.py
+-rw-r--r--  2.0 unx    19740 b- defN 24-May-07 00:45 sparseml/keras/optim/mask_pruning_creator.py
+-rw-r--r--  2.0 unx     9183 b- defN 24-May-07 00:45 sparseml/keras/optim/modifier.py
+-rw-r--r--  2.0 unx     1676 b- defN 24-May-07 00:45 sparseml/keras/optim/modifier_epoch.py
+-rw-r--r--  2.0 unx    14736 b- defN 24-May-07 00:45 sparseml/keras/optim/modifier_lr.py
+-rw-r--r--  2.0 unx     5477 b- defN 24-May-07 00:45 sparseml/keras/optim/modifier_params.py
+-rw-r--r--  2.0 unx    24031 b- defN 24-May-07 00:45 sparseml/keras/optim/modifier_pruning.py
+-rw-r--r--  2.0 unx     1133 b- defN 24-May-07 00:45 sparseml/keras/optim/utils.py
+-rw-r--r--  2.0 unx      808 b- defN 24-May-07 00:45 sparseml/keras/sparsification/__init__.py
+-rw-r--r--  2.0 unx     1356 b- defN 24-May-07 00:45 sparseml/keras/sparsification/info.py
+-rw-r--r--  2.0 unx      962 b- defN 24-May-07 00:45 sparseml/keras/utils/__init__.py
+-rw-r--r--  2.0 unx     8202 b- defN 24-May-07 00:45 sparseml/keras/utils/callbacks.py
+-rw-r--r--  2.0 unx     1022 b- defN 24-May-07 00:45 sparseml/keras/utils/compat.py
+-rw-r--r--  2.0 unx     5737 b- defN 24-May-07 00:45 sparseml/keras/utils/exporter.py
+-rw-r--r--  2.0 unx     6087 b- defN 24-May-07 00:45 sparseml/keras/utils/logger.py
+-rw-r--r--  2.0 unx     1738 b- defN 24-May-07 00:45 sparseml/keras/utils/model.py
+-rw-r--r--  2.0 unx      800 b- defN 24-May-07 00:45 sparseml/modifiers/__init__.py
+-rw-r--r--  2.0 unx      656 b- defN 24-May-07 00:45 sparseml/modifiers/distillation/__init__.py
+-rw-r--r--  2.0 unx      654 b- defN 24-May-07 00:45 sparseml/modifiers/distillation/output/__init__.py
+-rw-r--r--  2.0 unx     1343 b- defN 24-May-07 00:45 sparseml/modifiers/distillation/output/base.py
+-rw-r--r--  2.0 unx     7612 b- defN 24-May-07 00:45 sparseml/modifiers/distillation/output/pytorch.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/modifiers/distillation/utils/__init__.py
+-rw-r--r--  2.0 unx      715 b- defN 24-May-07 00:45 sparseml/modifiers/distillation/utils/pytorch/__init__.py
+-rw-r--r--  2.0 unx    13320 b- defN 24-May-07 00:45 sparseml/modifiers/distillation/utils/pytorch/kd_factory.py
+-rw-r--r--  2.0 unx     3921 b- defN 24-May-07 00:45 sparseml/modifiers/distillation/utils/pytorch/kd_wrapper.py
+-rw-r--r--  2.0 unx     4527 b- defN 24-May-07 00:45 sparseml/modifiers/distillation/utils/pytorch/model_wrapper.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/modifiers/experimental/__init__.py
+-rw-r--r--  2.0 unx      654 b- defN 24-May-07 00:45 sparseml/modifiers/logarithmic_equalization/__init__.py
+-rw-r--r--  2.0 unx     2764 b- defN 24-May-07 00:45 sparseml/modifiers/logarithmic_equalization/base.py
+-rw-r--r--  2.0 unx     1947 b- defN 24-May-07 00:45 sparseml/modifiers/logarithmic_equalization/pytorch.py
+-rw-r--r--  2.0 unx      654 b- defN 24-May-07 00:45 sparseml/modifiers/obcq/__init__.py
+-rw-r--r--  2.0 unx     5181 b- defN 24-May-07 00:45 sparseml/modifiers/obcq/base.py
+-rw-r--r--  2.0 unx     3543 b- defN 24-May-07 00:45 sparseml/modifiers/obcq/pytorch.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/modifiers/obcq/utils/__init__.py
+-rw-r--r--  2.0 unx     6686 b- defN 24-May-07 00:45 sparseml/modifiers/obcq/utils/helpers.py
+-rw-r--r--  2.0 unx     7752 b- defN 24-May-07 00:45 sparseml/modifiers/obcq/utils/sgpt_wrapper.py
+-rw-r--r--  2.0 unx      683 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/__init__.py
+-rw-r--r--  2.0 unx     5725 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/helpers.py
+-rw-r--r--  2.0 unx      676 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/constant/__init__.py
+-rw-r--r--  2.0 unx      951 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/constant/base.py
+-rw-r--r--  2.0 unx     3103 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/constant/pytorch.py
+-rw-r--r--  2.0 unx      677 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/magnitude/__init__.py
+-rw-r--r--  2.0 unx     1168 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/magnitude/base.py
+-rw-r--r--  2.0 unx     5262 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/magnitude/pytorch.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/utils/__init__.py
+-rw-r--r--  2.0 unx      688 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/utils/pytorch/__init__.py
+-rw-r--r--  2.0 unx     5984 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/utils/pytorch/layer_mask.py
+-rw-r--r--  2.0 unx     5622 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/utils/pytorch/mask_factory.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/wanda/__init__.py
+-rw-r--r--  2.0 unx     3905 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/wanda/base.py
+-rw-r--r--  2.0 unx    10346 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/wanda/pytorch.py
+-rw-r--r--  2.0 unx      633 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/wanda/utils/__init__.py
+-rw-r--r--  2.0 unx     4469 b- defN 24-May-07 00:45 sparseml/modifiers/pruning/wanda/utils/wanda_wrapper.py
+-rw-r--r--  2.0 unx      654 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/__init__.py
+-rw-r--r--  2.0 unx     5512 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/base.py
+-rw-r--r--  2.0 unx     9236 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/pytorch.py
+-rw-r--r--  2.0 unx      671 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/modification/__init__.py
+-rw-r--r--  2.0 unx     3908 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/modification/modification_objects.py
+-rw-r--r--  2.0 unx     2573 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/modification/modify_model.py
+-rw-r--r--  2.0 unx      903 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/modification/registry.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/utils/__init__.py
+-rw-r--r--  2.0 unx     2220 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/utils/constants.py
+-rw-r--r--  2.0 unx     2906 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/utils/fake_quant_wrapper.py
+-rw-r--r--  2.0 unx    32720 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/utils/helpers.py
+-rw-r--r--  2.0 unx    13580 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/utils/quantization_scheme.py
+-rw-r--r--  2.0 unx    20463 b- defN 24-May-07 00:45 sparseml/modifiers/quantization/utils/quantize.py
+-rw-r--r--  2.0 unx      654 b- defN 24-May-07 00:45 sparseml/modifiers/quantization_vllm/__init__.py
+-rw-r--r--  2.0 unx     3353 b- defN 24-May-07 00:45 sparseml/modifiers/quantization_vllm/base.py
+-rw-r--r--  2.0 unx     5467 b- defN 24-May-07 00:45 sparseml/modifiers/quantization_vllm/pytorch.py
+-rw-r--r--  2.0 unx      654 b- defN 24-May-07 00:45 sparseml/modifiers/smoothquant/__init__.py
+-rw-r--r--  2.0 unx     7466 b- defN 24-May-07 00:45 sparseml/modifiers/smoothquant/base.py
+-rw-r--r--  2.0 unx     8167 b- defN 24-May-07 00:45 sparseml/modifiers/smoothquant/pytorch.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/modifiers/utils/__init__.py
+-rw-r--r--  2.0 unx     3944 b- defN 24-May-07 00:45 sparseml/modifiers/utils/compression_wrapper.py
+-rw-r--r--  2.0 unx     5257 b- defN 24-May-07 00:45 sparseml/modifiers/utils/layer_compressor.py
+-rw-r--r--  2.0 unx     3265 b- defN 24-May-07 00:45 sparseml/modifiers/utils/pytorch_helpers.py
+-rw-r--r--  2.0 unx     1036 b- defN 24-May-07 00:45 sparseml/onnx/__init__.py
+-rw-r--r--  2.0 unx     6202 b- defN 24-May-07 00:45 sparseml/onnx/base.py
+-rw-r--r--  2.0 unx      743 b- defN 24-May-07 00:45 sparseml/onnx/benchmark/__init__.py
+-rw-r--r--  2.0 unx    15366 b- defN 24-May-07 00:45 sparseml/onnx/benchmark/info.py
+-rw-r--r--  2.0 unx      823 b- defN 24-May-07 00:45 sparseml/onnx/framework/__init__.py
+-rw-r--r--  2.0 unx     6116 b- defN 24-May-07 00:45 sparseml/onnx/framework/info.py
+-rw-r--r--  2.0 unx      820 b- defN 24-May-07 00:45 sparseml/onnx/optim/__init__.py
+-rw-r--r--  2.0 unx    13285 b- defN 24-May-07 00:45 sparseml/onnx/optim/analyzer_model.py
+-rw-r--r--  2.0 unx    19639 b- defN 24-May-07 00:45 sparseml/onnx/optim/sensitivity_pruning.py
+-rw-r--r--  2.0 unx     6470 b- defN 24-May-07 00:45 sparseml/onnx/optim/structured_pruning.py
+-rw-r--r--  2.0 unx      815 b- defN 24-May-07 00:45 sparseml/onnx/optim/quantization/__init__.py
+-rw-r--r--  2.0 unx    14753 b- defN 24-May-07 00:45 sparseml/onnx/optim/quantization/calibration.py
+-rw-r--r--  2.0 unx    73551 b- defN 24-May-07 00:45 sparseml/onnx/optim/quantization/quantize.py
+-rw-r--r--  2.0 unx     4552 b- defN 24-May-07 00:45 sparseml/onnx/optim/quantization/quantize_model_post_training.py
+-rw-r--r--  2.0 unx      869 b- defN 24-May-07 00:45 sparseml/onnx/sparsification/__init__.py
+-rw-r--r--  2.0 unx    10209 b- defN 24-May-07 00:45 sparseml/onnx/sparsification/analyzer.py
+-rw-r--r--  2.0 unx     1363 b- defN 24-May-07 00:45 sparseml/onnx/sparsification/info.py
+-rw-r--r--  2.0 unx     8009 b- defN 24-May-07 00:45 sparseml/onnx/sparsification/model_info.py
+-rw-r--r--  2.0 unx      867 b- defN 24-May-07 00:45 sparseml/onnx/utils/__init__.py
+-rw-r--r--  2.0 unx    13013 b- defN 24-May-07 00:45 sparseml/onnx/utils/data.py
+-rw-r--r--  2.0 unx    20691 b- defN 24-May-07 00:45 sparseml/onnx/utils/graph_editor.py
+-rw-r--r--  2.0 unx     8133 b- defN 24-May-07 00:45 sparseml/onnx/utils/graph_optimizer.py
+-rw-r--r--  2.0 unx    40230 b- defN 24-May-07 00:45 sparseml/onnx/utils/helpers.py
+-rw-r--r--  2.0 unx     1958 b- defN 24-May-07 00:45 sparseml/onnx/utils/loss.py
+-rw-r--r--  2.0 unx    31591 b- defN 24-May-07 00:45 sparseml/onnx/utils/model.py
+-rw-r--r--  2.0 unx     5437 b- defN 24-May-07 00:45 sparseml/onnx/utils/sparse_tensor.py
+-rw-r--r--  2.0 unx      931 b- defN 24-May-07 00:45 sparseml/openpifpaf/__init__.py
+-rw-r--r--  2.0 unx     3713 b- defN 24-May-07 00:45 sparseml/openpifpaf/export.py
+-rw-r--r--  2.0 unx    10950 b- defN 24-May-07 00:45 sparseml/openpifpaf/train.py
+-rw-r--r--  2.0 unx     4211 b- defN 24-May-07 00:45 sparseml/openpifpaf/trainer.py
+-rw-r--r--  2.0 unx      882 b- defN 24-May-07 00:45 sparseml/optim/__init__.py
+-rw-r--r--  2.0 unx     6302 b- defN 24-May-07 00:45 sparseml/optim/analyzer.py
+-rw-r--r--  2.0 unx    25563 b- defN 24-May-07 00:45 sparseml/optim/helpers.py
+-rw-r--r--  2.0 unx    25984 b- defN 24-May-07 00:45 sparseml/optim/manager.py
+-rw-r--r--  2.0 unx    30708 b- defN 24-May-07 00:45 sparseml/optim/modifier.py
+-rw-r--r--  2.0 unx    26315 b- defN 24-May-07 00:45 sparseml/optim/sensitivity.py
+-rw-r--r--  2.0 unx     2190 b- defN 24-May-07 00:45 sparseml/pytorch/__init__.py
+-rw-r--r--  2.0 unx     6214 b- defN 24-May-07 00:45 sparseml/pytorch/base.py
+-rw-r--r--  2.0 unx      933 b- defN 24-May-07 00:45 sparseml/pytorch/opset.py
+-rw-r--r--  2.0 unx    12127 b- defN 24-May-07 00:45 sparseml/pytorch/torch_to_onnx_exporter.py
+-rw-r--r--  2.0 unx      998 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/__init__.py
+-rw-r--r--  2.0 unx     4193 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/generic.py
+-rw-r--r--  2.0 unx     3014 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/registry.py
+-rw-r--r--  2.0 unx      828 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/classification/__init__.py
+-rw-r--r--  2.0 unx     4457 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/classification/cifar.py
+-rw-r--r--  2.0 unx     3669 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/classification/imagefolder.py
+-rw-r--r--  2.0 unx     4000 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/classification/imagenet.py
+-rw-r--r--  2.0 unx     6491 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/classification/imagenette.py
+-rw-r--r--  2.0 unx     2434 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/classification/mnist.py
+-rw-r--r--  2.0 unx      767 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/detection/__init__.py
+-rw-r--r--  2.0 unx    16159 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/detection/coco.py
+-rw-r--r--  2.0 unx     5705 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/detection/helpers.py
+-rw-r--r--  2.0 unx    10759 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/detection/voc.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/image_classification/__init__.py
+-rw-r--r--  2.0 unx     9512 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/image_classification/ffcv_dataset.py
+-rw-r--r--  2.0 unx      684 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/recommendation/__init__.py
+-rw-r--r--  2.0 unx      693 b- defN 24-May-07 00:45 sparseml/pytorch/datasets/video/__init__.py
+-rw-r--r--  2.0 unx      814 b- defN 24-May-07 00:45 sparseml/pytorch/framework/__init__.py
+-rw-r--r--  2.0 unx     5580 b- defN 24-May-07 00:45 sparseml/pytorch/framework/info.py
+-rw-r--r--  2.0 unx      753 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/__init__.py
+-rw-r--r--  2.0 unx    18265 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/export.py
+-rw-r--r--  2.0 unx     4991 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/integration_helper_functions.py
+-rw-r--r--  2.0 unx    15494 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/lr_analysis.py
+-rw-r--r--  2.0 unx    14444 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/pr_sensitivity.py
+-rw-r--r--  2.0 unx    29287 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/train.py
+-rw-r--r--  2.0 unx      682 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/utils/__init__.py
+-rw-r--r--  2.0 unx     4278 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/utils/cli_helpers.py
+-rw-r--r--  2.0 unx     1257 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/utils/constants.py
+-rw-r--r--  2.0 unx    23894 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/utils/helpers.py
+-rw-r--r--  2.0 unx    12056 b- defN 24-May-07 00:45 sparseml/pytorch/image_classification/utils/trainer.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/pytorch/model_load/__init__.py
+-rw-r--r--  2.0 unx    11440 b- defN 24-May-07 00:45 sparseml/pytorch/model_load/helpers.py
+-rw-r--r--  2.0 unx      976 b- defN 24-May-07 00:45 sparseml/pytorch/models/__init__.py
+-rw-r--r--  2.0 unx    14753 b- defN 24-May-07 00:45 sparseml/pytorch/models/registry.py
+-rw-r--r--  2.0 unx      901 b- defN 24-May-07 00:45 sparseml/pytorch/models/classification/__init__.py
+-rw-r--r--  2.0 unx    11658 b- defN 24-May-07 00:45 sparseml/pytorch/models/classification/darknet.py
+-rw-r--r--  2.0 unx    40293 b- defN 24-May-07 00:45 sparseml/pytorch/models/classification/efficientnet.py
+-rw-r--r--  2.0 unx    16512 b- defN 24-May-07 00:45 sparseml/pytorch/models/classification/inception_v3.py
+-rw-r--r--  2.0 unx     4164 b- defN 24-May-07 00:45 sparseml/pytorch/models/classification/mnist.py
+-rw-r--r--  2.0 unx     9546 b- defN 24-May-07 00:45 sparseml/pytorch/models/classification/mobilenet.py
+-rw-r--r--  2.0 unx    13014 b- defN 24-May-07 00:45 sparseml/pytorch/models/classification/mobilenet_v2.py
+-rw-r--r--  2.0 unx    40800 b- defN 24-May-07 00:45 sparseml/pytorch/models/classification/resnet.py
+-rw-r--r--  2.0 unx    16649 b- defN 24-May-07 00:45 sparseml/pytorch/models/classification/vgg.py
+-rw-r--r--  2.0 unx      824 b- defN 24-May-07 00:45 sparseml/pytorch/models/detection/__init__.py
+-rw-r--r--  2.0 unx     6820 b- defN 24-May-07 00:45 sparseml/pytorch/models/detection/ssd.py
+-rw-r--r--  2.0 unx     8116 b- defN 24-May-07 00:45 sparseml/pytorch/models/detection/ssd_lite.py
+-rw-r--r--  2.0 unx     4046 b- defN 24-May-07 00:45 sparseml/pytorch/models/detection/ssd_mobilenet.py
+-rw-r--r--  2.0 unx     9069 b- defN 24-May-07 00:45 sparseml/pytorch/models/detection/ssd_resnet.py
+-rw-r--r--  2.0 unx    10188 b- defN 24-May-07 00:45 sparseml/pytorch/models/detection/yolo_v3.py
+-rw-r--r--  2.0 unx      763 b- defN 24-May-07 00:45 sparseml/pytorch/models/external/__init__.py
+-rw-r--r--  2.0 unx     6759 b- defN 24-May-07 00:45 sparseml/pytorch/models/external/torchvision.py
+-rw-r--r--  2.0 unx      676 b- defN 24-May-07 00:45 sparseml/pytorch/models/recommendation/__init__.py
+-rw-r--r--  2.0 unx      925 b- defN 24-May-07 00:45 sparseml/pytorch/nn/__init__.py
+-rw-r--r--  2.0 unx     8673 b- defN 24-May-07 00:45 sparseml/pytorch/nn/activations.py
+-rw-r--r--  2.0 unx    11854 b- defN 24-May-07 00:45 sparseml/pytorch/nn/fatrelu.py
+-rw-r--r--  2.0 unx     1690 b- defN 24-May-07 00:45 sparseml/pytorch/nn/identity.py
+-rw-r--r--  2.0 unx     2828 b- defN 24-May-07 00:45 sparseml/pytorch/nn/se.py
+-rw-r--r--  2.0 unx     1243 b- defN 24-May-07 00:45 sparseml/pytorch/optim/__init__.py
+-rw-r--r--  2.0 unx    13638 b- defN 24-May-07 00:45 sparseml/pytorch/optim/analyzer_as.py
+-rw-r--r--  2.0 unx    15582 b- defN 24-May-07 00:45 sparseml/pytorch/optim/analyzer_module.py
+-rw-r--r--  2.0 unx     3955 b- defN 24-May-07 00:45 sparseml/pytorch/optim/analyzer_pruning.py
+-rw-r--r--  2.0 unx    26838 b- defN 24-May-07 00:45 sparseml/pytorch/optim/manager.py
+-rw-r--r--  2.0 unx    36844 b- defN 24-May-07 00:45 sparseml/pytorch/optim/mask_creator_pruning.py
+-rw-r--r--  2.0 unx    23085 b- defN 24-May-07 00:45 sparseml/pytorch/optim/mask_pruning.py
+-rw-r--r--  2.0 unx    10449 b- defN 24-May-07 00:45 sparseml/pytorch/optim/mask_pruning_scorer.py
+-rw-r--r--  2.0 unx     6605 b- defN 24-May-07 00:45 sparseml/pytorch/optim/optimizer.py
+-rw-r--r--  2.0 unx    14879 b- defN 24-May-07 00:45 sparseml/pytorch/optim/sensitivity_as.py
+-rw-r--r--  2.0 unx     6101 b- defN 24-May-07 00:45 sparseml/pytorch/optim/sensitivity_lr.py
+-rw-r--r--  2.0 unx     9324 b- defN 24-May-07 00:45 sparseml/pytorch/optim/sensitivity_pruning.py
+-rw-r--r--  2.0 unx      633 b- defN 24-May-07 00:45 sparseml/pytorch/recipe_template/__init__.py
+-rw-r--r--  2.0 unx     4534 b- defN 24-May-07 00:45 sparseml/pytorch/recipe_template/cli.py
+-rw-r--r--  2.0 unx     1559 b- defN 24-May-07 00:45 sparseml/pytorch/recipe_template/description.py
+-rw-r--r--  2.0 unx    15943 b- defN 24-May-07 00:45 sparseml/pytorch/recipe_template/main.py
+-rw-r--r--  2.0 unx      992 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/__init__.py
+-rw-r--r--  2.0 unx     1366 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/info.py
+-rw-r--r--  2.0 unx    32159 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/modifier.py
+-rw-r--r--  2.0 unx    18952 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/modifier_thinning.py
+-rw-r--r--  2.0 unx      705 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/distillation/__init__.py
+-rw-r--r--  2.0 unx     4741 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/distillation/modifier_distillation.py
+-rw-r--r--  2.0 unx    14742 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/distillation/modifier_distillation_base.py
+-rw-r--r--  2.0 unx    19177 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/distillation/modifier_per_layer.py
+-rw-r--r--  2.0 unx     1276 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/__init__.py
+-rw-r--r--  2.0 unx    29250 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/mask_creator.py
+-rw-r--r--  2.0 unx    24209 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/mask_params.py
+-rw-r--r--  2.0 unx    13389 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_as.py
+-rw-r--r--  2.0 unx    12006 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_powerpropagation.py
+-rw-r--r--  2.0 unx    10455 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_acdc.py
+-rw-r--r--  2.0 unx    33219 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_base.py
+-rw-r--r--  2.0 unx     5757 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_constant.py
+-rw-r--r--  2.0 unx     8857 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_layer.py
+-rw-r--r--  2.0 unx    15595 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_magnitude.py
+-rw-r--r--  2.0 unx    63519 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_mfac.py
+-rw-r--r--  2.0 unx     8774 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_movement.py
+-rw-r--r--  2.0 unx    24121 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_obs.py
+-rw-r--r--  2.0 unx    23735 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_rigl.py
+-rw-r--r--  2.0 unx    18245 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_structured.py
+-rw-r--r--  2.0 unx    17683 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_topkast.py
+-rw-r--r--  2.0 unx     4644 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/pruning/scorer.py
+-rw-r--r--  2.0 unx      813 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/quantization/__init__.py
+-rw-r--r--  2.0 unx     2220 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/quantization/constants.py
+-rw-r--r--  2.0 unx    34914 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/quantization/helpers.py
+-rw-r--r--  2.0 unx    33626 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/quantization/legacy_modifier_quantization.py
+-rw-r--r--  2.0 unx    26778 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/quantization/modifier_quantization.py
+-rw-r--r--  2.0 unx    13511 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/quantization/quantization_scheme.py
+-rw-r--r--  2.0 unx    18047 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/quantization/quantize.py
+-rw-r--r--  2.0 unx    76796 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/quantization/quantize_qat_export.py
+-rw-r--r--  2.0 unx      790 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/training/__init__.py
+-rw-r--r--  2.0 unx     1778 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/training/modifier_epoch.py
+-rw-r--r--  2.0 unx     2883 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/training/modifier_logging.py
+-rw-r--r--  2.0 unx    24287 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/training/modifier_lr.py
+-rw-r--r--  2.0 unx    21497 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/training/modifier_params.py
+-rw-r--r--  2.0 unx     6690 b- defN 24-May-07 00:45 sparseml/pytorch/sparsification/training/modifier_regularizer.py
+-rw-r--r--  2.0 unx      943 b- defN 24-May-07 00:45 sparseml/pytorch/torchvision/__init__.py
+-rw-r--r--  2.0 unx     6490 b- defN 24-May-07 00:45 sparseml/pytorch/torchvision/export_onnx.py
+-rw-r--r--  2.0 unx     2870 b- defN 24-May-07 00:45 sparseml/pytorch/torchvision/presets.py
+-rw-r--r--  2.0 unx     2530 b- defN 24-May-07 00:45 sparseml/pytorch/torchvision/sampler.py
+-rw-r--r--  2.0 unx    43917 b- defN 24-May-07 00:45 sparseml/pytorch/torchvision/train.py
+-rw-r--r--  2.0 unx     7128 b- defN 24-May-07 00:45 sparseml/pytorch/torchvision/transforms.py
+-rw-r--r--  2.0 unx    16675 b- defN 24-May-07 00:45 sparseml/pytorch/torchvision/utils.py
+-rw-r--r--  2.0 unx     1160 b- defN 24-May-07 00:45 sparseml/pytorch/utils/__init__.py
+-rw-r--r--  2.0 unx     9706 b- defN 24-May-07 00:45 sparseml/pytorch/utils/benchmarker.py
+-rw-r--r--  2.0 unx     2846 b- defN 24-May-07 00:45 sparseml/pytorch/utils/callbacks.py
+-rw-r--r--  2.0 unx     1061 b- defN 24-May-07 00:45 sparseml/pytorch/utils/distributed.py
+-rw-r--r--  2.0 unx    30884 b- defN 24-May-07 00:45 sparseml/pytorch/utils/exporter.py
+-rw-r--r--  2.0 unx    42811 b- defN 24-May-07 00:45 sparseml/pytorch/utils/helpers.py
+-rw-r--r--  2.0 unx     1663 b- defN 24-May-07 00:45 sparseml/pytorch/utils/log_sparsification_info.py
+-rw-r--r--  2.0 unx    34449 b- defN 24-May-07 00:45 sparseml/pytorch/utils/logger.py
+-rw-r--r--  2.0 unx    27048 b- defN 24-May-07 00:45 sparseml/pytorch/utils/loss.py
+-rw-r--r--  2.0 unx    11754 b- defN 24-May-07 00:45 sparseml/pytorch/utils/model.py
+-rw-r--r--  2.0 unx    39117 b- defN 24-May-07 00:45 sparseml/pytorch/utils/module.py
+-rw-r--r--  2.0 unx     9691 b- defN 24-May-07 00:45 sparseml/pytorch/utils/sparsification.py
+-rw-r--r--  2.0 unx    30059 b- defN 24-May-07 00:45 sparseml/pytorch/utils/ssd_helpers.py
+-rw-r--r--  2.0 unx    12337 b- defN 24-May-07 00:45 sparseml/pytorch/utils/yolo_helpers.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/pytorch/utils/sparsification_info/__init__.py
+-rw-r--r--  2.0 unx    15095 b- defN 24-May-07 00:45 sparseml/pytorch/utils/sparsification_info/configs.py
+-rw-r--r--  2.0 unx     4336 b- defN 24-May-07 00:45 sparseml/pytorch/utils/sparsification_info/helpers.py
+-rw-r--r--  2.0 unx     2788 b- defN 24-May-07 00:45 sparseml/pytorch/utils/sparsification_info/module_sparsification_info.py
+-rw-r--r--  2.0 unx      655 b- defN 24-May-07 00:45 sparseml/recipe_template/__init__.py
+-rw-r--r--  2.0 unx     4788 b- defN 24-May-07 00:45 sparseml/recipe_template/utils.py
+-rw-r--r--  2.0 unx     1058 b- defN 24-May-07 00:45 sparseml/sparsification/__init__.py
+-rw-r--r--  2.0 unx     9387 b- defN 24-May-07 00:45 sparseml/sparsification/analyzer.py
+-rw-r--r--  2.0 unx     9087 b- defN 24-May-07 00:45 sparseml/sparsification/info.py
+-rw-r--r--  2.0 unx    15589 b- defN 24-May-07 00:45 sparseml/sparsification/model_info.py
+-rw-r--r--  2.0 unx     2002 b- defN 24-May-07 00:45 sparseml/sparsification/modifier_epoch.py
+-rw-r--r--  2.0 unx    10117 b- defN 24-May-07 00:45 sparseml/sparsification/modifier_lr.py
+-rw-r--r--  2.0 unx     5505 b- defN 24-May-07 00:45 sparseml/sparsification/modifier_params.py
+-rw-r--r--  2.0 unx    12845 b- defN 24-May-07 00:45 sparseml/sparsification/modifier_pruning.py
+-rw-r--r--  2.0 unx     3700 b- defN 24-May-07 00:45 sparseml/sparsification/oracle.py
+-rw-r--r--  2.0 unx    18570 b- defN 24-May-07 00:45 sparseml/sparsification/recipe_builder.py
+-rw-r--r--  2.0 unx    14413 b- defN 24-May-07 00:45 sparseml/sparsification/recipe_editor.py
+-rw-r--r--  2.0 unx     1250 b- defN 24-May-07 00:45 sparseml/sparsification/types.py
+-rw-r--r--  2.0 unx     1169 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/__init__.py
+-rw-r--r--  2.0 unx     7272 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/base.py
+-rw-r--r--  2.0 unx      925 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/datasets/__init__.py
+-rw-r--r--  2.0 unx     8121 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/datasets/dataset.py
+-rw-r--r--  2.0 unx     5600 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/datasets/helpers.py
+-rw-r--r--  2.0 unx     2768 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/datasets/registry.py
+-rw-r--r--  2.0 unx      807 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/datasets/classification/__init__.py
+-rw-r--r--  2.0 unx    12686 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/datasets/classification/cifar.py
+-rw-r--r--  2.0 unx     8690 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/datasets/classification/imagefolder.py
+-rw-r--r--  2.0 unx     2032 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/datasets/classification/imagenet.py
+-rw-r--r--  2.0 unx     4695 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/datasets/classification/imagenette.py
+-rw-r--r--  2.0 unx      805 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/framework/__init__.py
+-rw-r--r--  2.0 unx     5859 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/framework/info.py
+-rw-r--r--  2.0 unx      925 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/models/__init__.py
+-rw-r--r--  2.0 unx    19752 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/models/estimator.py
+-rw-r--r--  2.0 unx    14774 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/models/registry.py
+-rw-r--r--  2.0 unx      822 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/models/classification/__init__.py
+-rw-r--r--  2.0 unx     3540 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/models/classification/mnist.py
+-rw-r--r--  2.0 unx    11161 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/models/classification/mobilenet.py
+-rw-r--r--  2.0 unx    18359 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/models/classification/mobilenet_v2.py
+-rw-r--r--  2.0 unx    28103 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/models/classification/resnet.py
+-rw-r--r--  2.0 unx    26886 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/models/classification/vgg.py
+-rw-r--r--  2.0 unx      865 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/nn/__init__.py
+-rw-r--r--  2.0 unx    18670 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/nn/layers.py
+-rw-r--r--  2.0 unx     1238 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/__init__.py
+-rw-r--r--  2.0 unx     8607 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/analyzer_module.py
+-rw-r--r--  2.0 unx     9591 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/manager.py
+-rw-r--r--  2.0 unx    19683 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/mask_creator_pruning.py
+-rw-r--r--  2.0 unx    33919 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/mask_pruning.py
+-rw-r--r--  2.0 unx    15955 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/modifier.py
+-rw-r--r--  2.0 unx     1715 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/modifier_epoch.py
+-rw-r--r--  2.0 unx    10685 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/modifier_lr.py
+-rw-r--r--  2.0 unx     7092 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/modifier_params.py
+-rw-r--r--  2.0 unx    15702 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/modifier_pruning.py
+-rw-r--r--  2.0 unx     5682 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/schedule_lr.py
+-rw-r--r--  2.0 unx     9232 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/optim/sensitivity_pruning.py
+-rw-r--r--  2.0 unx      801 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/sparsification/__init__.py
+-rw-r--r--  2.0 unx     1385 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/sparsification/info.py
+-rw-r--r--  2.0 unx      967 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/utils/__init__.py
+-rw-r--r--  2.0 unx    10913 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/utils/exporter.py
+-rw-r--r--  2.0 unx      996 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/utils/helpers.py
+-rw-r--r--  2.0 unx     1974 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/utils/loss.py
+-rw-r--r--  2.0 unx     8119 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/utils/nets_utils.py
+-rw-r--r--  2.0 unx     1327 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/utils/summary.py
+-rw-r--r--  2.0 unx    12536 b- defN 24-May-07 00:45 sparseml/tensorflow_v1/utils/variable.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/tools/__init__.py
+-rw-r--r--  2.0 unx     1193 b- defN 24-May-07 00:45 sparseml/transformers/__init__.py
+-rw-r--r--  2.0 unx      985 b- defN 24-May-07 00:45 sparseml/transformers/base.py
+-rw-r--r--  2.0 unx    23904 b- defN 24-May-07 00:45 sparseml/transformers/export.py
+-rw-r--r--  2.0 unx     9474 b- defN 24-May-07 00:45 sparseml/transformers/integration_helper_functions.py
+-rw-r--r--  2.0 unx    30795 b- defN 24-May-07 00:45 sparseml/transformers/masked_language_modeling.py
+-rw-r--r--  2.0 unx    37002 b- defN 24-May-07 00:45 sparseml/transformers/question_answering.py
+-rw-r--r--  2.0 unx    40360 b- defN 24-May-07 00:45 sparseml/transformers/text_classification.py
+-rw-r--r--  2.0 unx      818 b- defN 24-May-07 00:45 sparseml/transformers/text_generation.py
+-rw-r--r--  2.0 unx    34389 b- defN 24-May-07 00:45 sparseml/transformers/token_classification.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/transformers/compression/__init__.py
+-rw-r--r--  2.0 unx     4305 b- defN 24-May-07 00:45 sparseml/transformers/compression/sparsity_config.py
+-rw-r--r--  2.0 unx      924 b- defN 24-May-07 00:45 sparseml/transformers/finetune/__init__.py
+-rw-r--r--  2.0 unx     4265 b- defN 24-May-07 00:45 sparseml/transformers/finetune/callbacks.py
+-rw-r--r--  2.0 unx     2519 b- defN 24-May-07 00:45 sparseml/transformers/finetune/model_args.py
+-rw-r--r--  2.0 unx    12296 b- defN 24-May-07 00:45 sparseml/transformers/finetune/runner.py
+-rw-r--r--  2.0 unx    22259 b- defN 24-May-07 00:45 sparseml/transformers/finetune/session_mixin.py
+-rw-r--r--  2.0 unx    12579 b- defN 24-May-07 00:45 sparseml/transformers/finetune/text_generation.py
+-rw-r--r--  2.0 unx      848 b- defN 24-May-07 00:45 sparseml/transformers/finetune/trainer.py
+-rw-r--r--  2.0 unx     2862 b- defN 24-May-07 00:45 sparseml/transformers/finetune/training_args.py
+-rw-r--r--  2.0 unx     1066 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/__init__.py
+-rw-r--r--  2.0 unx     8835 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/base.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/c4.py
+-rw-r--r--  2.0 unx     2537 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/cnn_dailymail.py
+-rw-r--r--  2.0 unx     4281 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/custom.py
+-rw-r--r--  2.0 unx     5601 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/data_args.py
+-rw-r--r--  2.0 unx     9217 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/data_helpers.py
+-rw-r--r--  2.0 unx     2892 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/evolcodealpaca.py
+-rw-r--r--  2.0 unx     2596 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/gsm8k.py
+-rw-r--r--  2.0 unx     3435 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/open_platypus.py
+-rw-r--r--  2.0 unx     1369 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/ptb.py
+-rw-r--r--  2.0 unx     3521 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/ultrachat_200k.py
+-rw-r--r--  2.0 unx     1237 b- defN 24-May-07 00:45 sparseml/transformers/finetune/data/wikitext.py
+-rw-r--r--  2.0 unx      950 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/__init__.py
+-rw-r--r--  2.0 unx     7889 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/compressed_tensors_utils.py
+-rw-r--r--  2.0 unx    19272 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/question_answering.py
+-rw-r--r--  2.0 unx     1874 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/sparse_config.py
+-rw-r--r--  2.0 unx    20127 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/sparse_model.py
+-rw-r--r--  2.0 unx     2327 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/sparse_tokenizer.py
+-rw-r--r--  2.0 unx    40305 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/trainer.py
+-rw-r--r--  2.0 unx     1890 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/training_args.py
+-rw-r--r--  2.0 unx     1042 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/modification/__init__.py
+-rw-r--r--  2.0 unx     2430 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/modification/base.py
+-rw-r--r--  2.0 unx     8950 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/modification/modifying_bert.py
+-rw-r--r--  2.0 unx     5713 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/modification/modifying_distilbert.py
+-rw-r--r--  2.0 unx     8385 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/modification/modifying_llama.py
+-rw-r--r--  2.0 unx     7614 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/modification/modifying_mistral.py
+-rw-r--r--  2.0 unx     2709 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/modification/modifying_mobilebert.py
+-rw-r--r--  2.0 unx     9759 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/modification/modifying_opt.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/obcq/__init__.py
+-rw-r--r--  2.0 unx    19683 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/obcq/export.py
+-rw-r--r--  2.0 unx     7695 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/obcq/obcq.py
+-rw-r--r--  2.0 unx      617 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/obcq/utils/__init__.py
+-rw-r--r--  2.0 unx     3671 b- defN 24-May-07 00:45 sparseml/transformers/sparsification/obcq/utils/helpers.py
+-rw-r--r--  2.0 unx      805 b- defN 24-May-07 00:45 sparseml/transformers/utils/__init__.py
+-rw-r--r--  2.0 unx    22843 b- defN 24-May-07 00:45 sparseml/transformers/utils/helpers.py
+-rw-r--r--  2.0 unx     6711 b- defN 24-May-07 00:45 sparseml/transformers/utils/initializers.py
+-rw-r--r--  2.0 unx     5081 b- defN 24-May-07 00:45 sparseml/transformers/utils/load_task_dataset.py
+-rw-r--r--  2.0 unx     2764 b- defN 24-May-07 00:45 sparseml/transformers/utils/load_task_model.py
+-rw-r--r--  2.0 unx     2536 b- defN 24-May-07 00:45 sparseml/transformers/utils/metrics.py
+-rw-r--r--  2.0 unx     1972 b- defN 24-May-07 00:45 sparseml/transformers/utils/optimizations.py
+-rw-r--r--  2.0 unx     1037 b- defN 24-May-07 00:45 sparseml/transformers/utils/preprocessing_functions.py
+-rw-r--r--  2.0 unx      844 b- defN 24-May-07 00:45 sparseml/utils/__init__.py
+-rw-r--r--  2.0 unx      886 b- defN 24-May-07 00:45 sparseml/utils/frameworks.py
+-rw-r--r--  2.0 unx    31604 b- defN 24-May-07 00:45 sparseml/utils/helpers.py
+-rw-r--r--  2.0 unx     3983 b- defN 24-May-07 00:45 sparseml/utils/restricted_eval.py
+-rw-r--r--  2.0 unx     1083 b- defN 24-May-07 00:45 sparseml/utils/singleton.py
+-rw-r--r--  2.0 unx     6312 b- defN 24-May-07 00:45 sparseml/utils/worker.py
+-rw-r--r--  2.0 unx     2952 b- defN 24-May-07 00:45 sparseml/utils/wrapper.py
+-rw-r--r--  2.0 unx      819 b- defN 24-May-07 00:45 sparseml/utils/datasets/__init__.py
+-rw-r--r--  2.0 unx      833 b- defN 24-May-07 00:45 sparseml/utils/datasets/cifar.py
+-rw-r--r--  2.0 unx     3750 b- defN 24-May-07 00:45 sparseml/utils/datasets/coco.py
+-rw-r--r--  2.0 unx     1217 b- defN 24-May-07 00:45 sparseml/utils/datasets/helpers.py
+-rw-r--r--  2.0 unx    23366 b- defN 24-May-07 00:45 sparseml/utils/datasets/imagenet.py
+-rw-r--r--  2.0 unx     8967 b- defN 24-May-07 00:45 sparseml/utils/datasets/imagenette.py
+-rw-r--r--  2.0 unx     1009 b- defN 24-May-07 00:45 sparseml/utils/datasets/voc.py
+-rw-r--r--  2.0 unx      633 b- defN 24-May-07 00:45 sparseml/utils/fsdp/__init__.py
+-rw-r--r--  2.0 unx     2096 b- defN 24-May-07 00:45 sparseml/utils/fsdp/context.py
+-rw-r--r--  2.0 unx     6736 b- defN 24-May-07 00:45 sparseml/utils/fsdp/helpers.py
+-rw-r--r--  2.0 unx      656 b- defN 24-May-07 00:45 sparseml/utils/pytorch/__init__.py
+-rw-r--r--  2.0 unx    11030 b- defN 24-May-07 00:45 sparseml/utils/pytorch/module.py
+-rw-r--r--  2.0 unx     1696 b- defN 24-May-07 00:45 sparseml/utils/pytorch/utils.py
+-rw-r--r--  2.0 unx      680 b- defN 24-May-07 00:45 sparseml/utils/pytorch/pruning/__init__.py
+-rw-r--r--  2.0 unx     1875 b- defN 24-May-07 00:45 sparseml/yolact/COCO.sh
+-rw-r--r--  2.0 unx     1418 b- defN 24-May-07 00:45 sparseml/yolact/COCO_test.sh
+-rw-r--r--  2.0 unx     4020 b- defN 24-May-07 00:45 sparseml/yolact/__init__.py
+-rw-r--r--  2.0 unx     1784 b- defN 24-May-07 00:45 sparseml/yolact/scripts.py
+-rw-r--r--  2.0 unx     1440 b- defN 24-May-07 00:45 sparseml/yolov5/__init__.py
+-rw-r--r--  2.0 unx     4505 b- defN 24-May-07 00:45 sparseml/yolov5/helpers.py
+-rw-r--r--  2.0 unx     1609 b- defN 24-May-07 00:45 sparseml/yolov5/scripts.py
+-rw-r--r--  2.0 unx     1220 b- defN 24-May-07 00:45 sparseml/yolov5/yolov5.status.yaml
+-rw-r--r--  2.0 unx     1117 b- defN 24-May-07 00:45 sparseml/yolov8/__init__.py
+-rw-r--r--  2.0 unx     6061 b- defN 24-May-07 00:45 sparseml/yolov8/default.yaml
+-rw-r--r--  2.0 unx     2815 b- defN 24-May-07 00:45 sparseml/yolov8/export.py
+-rw-r--r--  2.0 unx     2259 b- defN 24-May-07 00:45 sparseml/yolov8/modules.py
+-rw-r--r--  2.0 unx     7394 b- defN 24-May-07 00:45 sparseml/yolov8/train.py
+-rw-r--r--  2.0 unx    37860 b- defN 24-May-07 00:45 sparseml/yolov8/trainers.py
+-rw-r--r--  2.0 unx     2748 b- defN 24-May-07 00:45 sparseml/yolov8/val.py
+-rw-r--r--  2.0 unx     8459 b- defN 24-May-07 00:45 sparseml/yolov8/validators.py
+-rw-r--r--  2.0 unx      685 b- defN 24-May-07 00:45 sparseml/yolov8/utils/__init__.py
+-rw-r--r--  2.0 unx     6683 b- defN 24-May-07 00:45 sparseml/yolov8/utils/export_samples.py
+-rw-r--r--  2.0 unx     4041 b- defN 24-May-07 00:45 sparseml/yolov8/utils/helpers.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-May-07 00:46 sparseml_nightly-1.8.0.20240507.dist-info/LICENSE
+-rw-r--r--  2.0 unx    13747 b- defN 24-May-07 00:46 sparseml_nightly-1.8.0.20240507.dist-info/LICENSE-ULTRALYTICS
+-rw-r--r--  2.0 unx    23695 b- defN 24-May-07 00:46 sparseml_nightly-1.8.0.20240507.dist-info/METADATA
+-rw-r--r--  2.0 unx     2085 b- defN 24-May-07 00:46 sparseml_nightly-1.8.0.20240507.dist-info/NOTICE
+-rw-r--r--  2.0 unx       92 b- defN 24-May-07 00:46 sparseml_nightly-1.8.0.20240507.dist-info/WHEEL
+-rw-r--r--  2.0 unx     3121 b- defN 24-May-07 00:46 sparseml_nightly-1.8.0.20240507.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        9 b- defN 24-May-07 00:46 sparseml_nightly-1.8.0.20240507.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    56072 b- defN 24-May-07 00:46 sparseml_nightly-1.8.0.20240507.dist-info/RECORD
+561 files, 4545435 bytes uncompressed, 1222555 bytes compressed:  73.1%
```

## zipnote {}

```diff
@@ -579,14 +579,26 @@
 
 Filename: sparseml/modifiers/quantization/base.py
 Comment: 
 
 Filename: sparseml/modifiers/quantization/pytorch.py
 Comment: 
 
+Filename: sparseml/modifiers/quantization/modification/__init__.py
+Comment: 
+
+Filename: sparseml/modifiers/quantization/modification/modification_objects.py
+Comment: 
+
+Filename: sparseml/modifiers/quantization/modification/modify_model.py
+Comment: 
+
+Filename: sparseml/modifiers/quantization/modification/registry.py
+Comment: 
+
 Filename: sparseml/modifiers/quantization/utils/__init__.py
 Comment: 
 
 Filename: sparseml/modifiers/quantization/utils/constants.py
 Comment: 
 
 Filename: sparseml/modifiers/quantization/utils/fake_quant_wrapper.py
@@ -597,14 +609,23 @@
 
 Filename: sparseml/modifiers/quantization/utils/quantization_scheme.py
 Comment: 
 
 Filename: sparseml/modifiers/quantization/utils/quantize.py
 Comment: 
 
+Filename: sparseml/modifiers/quantization_vllm/__init__.py
+Comment: 
+
+Filename: sparseml/modifiers/quantization_vllm/base.py
+Comment: 
+
+Filename: sparseml/modifiers/quantization_vllm/pytorch.py
+Comment: 
+
 Filename: sparseml/modifiers/smoothquant/__init__.py
 Comment: 
 
 Filename: sparseml/modifiers/smoothquant/base.py
 Comment: 
 
 Filename: sparseml/modifiers/smoothquant/pytorch.py
@@ -1359,48 +1380,15 @@
 
 Filename: sparseml/transformers/token_classification.py
 Comment: 
 
 Filename: sparseml/transformers/compression/__init__.py
 Comment: 
 
-Filename: sparseml/transformers/compression/compressors/__init__.py
-Comment: 
-
-Filename: sparseml/transformers/compression/compressors/base.py
-Comment: 
-
-Filename: sparseml/transformers/compression/compressors/dense.py
-Comment: 
-
-Filename: sparseml/transformers/compression/compressors/sparse_bitmask.py
-Comment: 
-
-Filename: sparseml/transformers/compression/config/__init__.py
-Comment: 
-
-Filename: sparseml/transformers/compression/config/base.py
-Comment: 
-
-Filename: sparseml/transformers/compression/config/dense.py
-Comment: 
-
-Filename: sparseml/transformers/compression/config/sparse_bitmask.py
-Comment: 
-
-Filename: sparseml/transformers/compression/utils/__init__.py
-Comment: 
-
-Filename: sparseml/transformers/compression/utils/compress_save.py
-Comment: 
-
-Filename: sparseml/transformers/compression/utils/helpers.py
-Comment: 
-
-Filename: sparseml/transformers/compression/utils/safetensors_load.py
+Filename: sparseml/transformers/compression/sparsity_config.py
 Comment: 
 
 Filename: sparseml/transformers/finetune/__init__.py
 Comment: 
 
 Filename: sparseml/transformers/finetune/callbacks.py
 Comment: 
@@ -1461,14 +1449,17 @@
 
 Filename: sparseml/transformers/finetune/data/wikitext.py
 Comment: 
 
 Filename: sparseml/transformers/sparsification/__init__.py
 Comment: 
 
+Filename: sparseml/transformers/sparsification/compressed_tensors_utils.py
+Comment: 
+
 Filename: sparseml/transformers/sparsification/question_answering.py
 Comment: 
 
 Filename: sparseml/transformers/sparsification/sparse_config.py
 Comment: 
 
 Filename: sparseml/transformers/sparsification/sparse_model.py
@@ -1485,20 +1476,14 @@
 
 Filename: sparseml/transformers/sparsification/modification/__init__.py
 Comment: 
 
 Filename: sparseml/transformers/sparsification/modification/base.py
 Comment: 
 
-Filename: sparseml/transformers/sparsification/modification/modification_objects.py
-Comment: 
-
-Filename: sparseml/transformers/sparsification/modification/modify_model.py
-Comment: 
-
 Filename: sparseml/transformers/sparsification/modification/modifying_bert.py
 Comment: 
 
 Filename: sparseml/transformers/sparsification/modification/modifying_distilbert.py
 Comment: 
 
 Filename: sparseml/transformers/sparsification/modification/modifying_llama.py
@@ -1509,17 +1494,14 @@
 
 Filename: sparseml/transformers/sparsification/modification/modifying_mobilebert.py
 Comment: 
 
 Filename: sparseml/transformers/sparsification/modification/modifying_opt.py
 Comment: 
 
-Filename: sparseml/transformers/sparsification/modification/registry.py
-Comment: 
-
 Filename: sparseml/transformers/sparsification/obcq/__init__.py
 Comment: 
 
 Filename: sparseml/transformers/sparsification/obcq/export.py
 Comment: 
 
 Filename: sparseml/transformers/sparsification/obcq/obcq.py
@@ -1671,32 +1653,32 @@
 
 Filename: sparseml/yolov8/utils/export_samples.py
 Comment: 
 
 Filename: sparseml/yolov8/utils/helpers.py
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240404.dist-info/LICENSE
+Filename: sparseml_nightly-1.8.0.20240507.dist-info/LICENSE
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240404.dist-info/LICENSE-ULTRALYTICS
+Filename: sparseml_nightly-1.8.0.20240507.dist-info/LICENSE-ULTRALYTICS
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240404.dist-info/METADATA
+Filename: sparseml_nightly-1.8.0.20240507.dist-info/METADATA
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240404.dist-info/NOTICE
+Filename: sparseml_nightly-1.8.0.20240507.dist-info/NOTICE
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240404.dist-info/WHEEL
+Filename: sparseml_nightly-1.8.0.20240507.dist-info/WHEEL
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240404.dist-info/entry_points.txt
+Filename: sparseml_nightly-1.8.0.20240507.dist-info/entry_points.txt
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240404.dist-info/top_level.txt
+Filename: sparseml_nightly-1.8.0.20240507.dist-info/top_level.txt
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240404.dist-info/RECORD
+Filename: sparseml_nightly-1.8.0.20240507.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sparseml/integration_helper_functions.py

```diff
@@ -33,31 +33,37 @@
 
     image_classification = "image-classification"
     transformers = "transformers"
 
 
 def resolve_integration(
     source_path: Union[Path, str, None] = None,
+    source_model: Optional["PreTrainedModel"] = None,  # noqa F401
     integration: Optional[str] = None,
 ) -> str:
     """
     Resolve the integration to use.
 
-    If integration is not provided, attempt to infer it from the source_path.
+    If integration is not provided, attempt to infer it from the source_path or model.
     Once the integration is resolved, perform the hot import to register
     the integration helper functions.
 
     :param source_path: The path to the PyTorch model to export.
+    :param source_model: An instantiated model to export
     :param integration: Optional name of the integration to use. If not provided,
         will attempt to infer it from the source_path.
     :return: The name of the integration to use for exporting the model.
     """
 
     integration = integration or _infer_integration_from_source_path(source_path)
 
+    # attempt to infer transformers based on model attribute
+    if source_model is not None and hasattr(source_model, "config_class"):
+        integration = Integrations.transformers.value
+
     if integration == Integrations.image_classification.value:
         import sparseml.pytorch.image_classification.integration_helper_functions  # noqa F401
 
         return Integrations.image_classification.value
 
     elif integration == Integrations.transformers.value:
         import sparseml.transformers.integration_helper_functions  # noqa F401
@@ -68,14 +74,30 @@
             f"Could not infer integration from source_path:\n{source_path}\n"
             "Please specify an argument `integration` from one of "
             "the available integrations: "
             f"{[integration.value for integration in Integrations]}."
         )
 
 
+def remove_past_key_value_support_from_config(config):
+    """
+    Modify config of the causal language model so that it turns off the
+    past key value support. This means that the model initialized from
+    this config will not take past key values as input and will not output
+    past key values.
+    """
+    # not take past_key_values as input
+    config.is_decoder = True
+    # whether to use past key values an input
+    config.use_past = False
+    # whether to output past key values
+    config.use_cache = False
+    return config
+
+
 def _infer_integration_from_source_path(
     source_path: Union[Path, str, None] = None
 ) -> Optional[str]:
     """
     Infer the integration to use from the source_path.
 
     :param source_path: The path to the PyTorch model to export.
@@ -162,14 +184,15 @@
         " - the deployment target to export to "
         " - the opset to use for the export "
         " - (optionally) a dictionary of additional arguments"
         "and returns the path to the exported model",
         default=export_model,
     )
     apply_optimizations: Optional[Callable[[Any], None]] = Field(
+        None,
         description="A function that takes:"
         " - path to the exported model"
         " - names of the optimizations to apply"
         " and applies the optimizations to the model",
     )
 
     create_data_samples: Callable[
@@ -197,10 +220,11 @@
     deployment_directory_files_mandatory: List[str] = Field(
         description="A list that describes the "
         "mandatory expected files of the deployment directory",
         default=["model.onnx"],
     )
 
     deployment_directory_files_optional: Optional[List[str]] = Field(
+        None,
         description="A list that describes the "
         "optional expected files of the deployment directory",
     )
```

## sparseml/core/modifier/modifier.py

```diff
@@ -36,17 +36,17 @@
         for the model
     :param group: The group name for the modifier
     :param start: The start step for the modifier
     :param end: The end step for the modifier
     :param update: The update step for the modifier
     """
 
-    index: int = None
-    group: str = None
-    start: float = None
+    index: Optional[int] = None
+    group: Optional[str] = None
+    start: Optional[float] = None
     end: Optional[float] = None
     update: Optional[float] = None
 
     initialized_structure_: bool = False
     initialized_: bool = False
     finalized_: bool = False
     started_: bool = False
```

## sparseml/core/recipe/base.py

```diff
@@ -9,17 +9,17 @@
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from abc import ABC, abstractmethod
-from typing import Any
+from typing import Any, Optional
 
-from pydantic import BaseModel
+from pydantic import BaseModel, ConfigDict
 
 from sparseml.core.framework import Framework
 from sparseml.core.recipe.args import RecipeArgs
 
 
 __all__ = ["RecipeBase"]
 
@@ -32,22 +32,24 @@
     All inheritors of this class must implement the following methods:
         - calculate_start
         - calculate_end
         - evaluate
         - create_modifier
     """
 
+    model_config = ConfigDict(arbitrary_types_allowed=True)
+
     @abstractmethod
     def calculate_start(self) -> int:
         raise NotImplementedError()
 
     @abstractmethod
     def calculate_end(self) -> int:
         raise NotImplementedError()
 
     @abstractmethod
-    def evaluate(self, args: RecipeArgs = None, shift: int = None):
+    def evaluate(self, args: Optional[RecipeArgs] = None, shift: Optional[int] = None):
         raise NotImplementedError()
 
     @abstractmethod
     def create_modifier(self, framework: Framework) -> Any:
         raise NotImplementedError()
```

## sparseml/core/recipe/modifier.py

```diff
@@ -10,15 +10,15 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Any, Dict, Optional
 
-from pydantic import root_validator
+from pydantic import model_validator
 
 from sparseml.core.factory import ModifierFactory
 from sparseml.core.framework import Framework
 from sparseml.core.modifier import Modifier
 from sparseml.core.recipe.args import RecipeArgs
 from sparseml.core.recipe.base import RecipeBase
 
@@ -95,15 +95,16 @@
             self.type,
             framework=framework,
             allow_registered=True,
             allow_experimental=True,
             **self.args_evaluated,
         )
 
-    @root_validator(pre=True)
+    @model_validator(mode="before")
+    @classmethod
     def extract_modifier_type(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         modifier = {"group": values.pop("group")}
         assert len(values) == 1, "multiple key pairs found for modifier"
         modifier_type, args = list(values.items())[0]
 
         modifier["type"] = modifier_type
         modifier["args"] = args
```

## sparseml/core/recipe/recipe.py

```diff
@@ -16,15 +16,15 @@
 import logging
 import os
 import re
 from dataclasses import dataclass
 from typing import Any, Dict, List, Optional, Union
 
 import yaml
-from pydantic import Field, root_validator
+from pydantic import Field, model_validator
 
 from sparseml.core.framework import Framework
 from sparseml.core.modifier import StageModifiers
 from sparseml.core.modifier.modifier import Modifier
 from sparseml.core.recipe.args import RecipeArgs
 from sparseml.core.recipe.base import RecipeBase
 from sparseml.core.recipe.metadata import RecipeMetaData
@@ -148,15 +148,15 @@
                 # assume it's a string
                 _LOGGER.warning(
                     "Could not process input as a file path or zoo stub, "
                     "attempting to process it as a string."
                 )
                 _LOGGER.debug(f"Input string: {path_or_modifiers}")
                 obj = _load_json_or_yaml_string(path_or_modifiers)
-                return Recipe.parse_obj(obj)
+                return Recipe.model_validate(obj)
         else:
             _LOGGER.info(f"Loading recipe from file {path_or_modifiers}")
 
         with open(path_or_modifiers, "r") as file:
             content = file.read().strip()
             if path_or_modifiers.lower().endswith(".md"):
                 content = _parse_recipe_from_md(path_or_modifiers, content)
@@ -170,15 +170,15 @@
             else:
                 try:
                     obj = _load_json_or_yaml_string(content)
                 except ValueError:
                     raise ValueError(
                         f"Could not parse recipe from path {path_or_modifiers}"
                     )
-            return Recipe.parse_obj(obj)
+            return Recipe.model_validate(obj)
 
     @staticmethod
     def simplify_recipe(
         recipe: Union["Recipe", "RecipeTuple"], shift: Optional[int] = None
     ) -> "Recipe":
         """
         Simplify a RecipeTuple by removing stages that are not in the target_stages
@@ -387,15 +387,16 @@
             stage_modifiers = stage.create_modifier(framework)
             stage_modifiers.index = index
             stage_modifiers.group = stage.group
             modifiers.append(stage_modifiers)
 
         return modifiers
 
-    @root_validator(pre=True)
+    @model_validator(mode="before")
+    @classmethod
     def remap_stages(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         stages = []
 
         modifiers = RecipeStage.extract_dict_modifiers(values)
         if modifiers:
             default_stage = {"modifiers": modifiers, "group": "default"}
             stages.append(default_stage)
@@ -511,33 +512,17 @@
         if self.metadata is None:
             self.metadata = metadata
         else:
             self.metadata.update_missing_metadata(metadata)
 
     def dict(self, *args, **kwargs) -> Dict[str, Any]:
         """
-        >>> recipe_str = '''
-        ... test_stage:
-        ...     pruning_modifiers:
-        ...         ConstantPruningModifier:
-        ...             start: 0.0
-        ...             end: 2.0
-        ...             targets: ['re:.*weight']
-        ... '''
-        >>> recipe = Recipe.create_instance(recipe_str)
-        >>> recipe_dict = recipe.dict()
-        >>> stage = recipe_dict["stages"]["test"]
-        >>> pruning_mods = stage[0]['modifiers']['pruning']
-        >>> modifier_args = pruning_mods[0]['ConstantPruningModifier']
-        >>> modifier_args == {'start': 0.0, 'end': 2.0, 'targets': ['re:.*weight']}
-        True
-
         :return: A dictionary representation of the recipe
         """
-        dict_ = super().dict(*args, **kwargs)
+        dict_ = super().model_dump(*args, **kwargs)
         stages = {}
 
         for stage in dict_["stages"]:
             name = f"{stage['group']}_stage"
             del stage["group"]
 
             if name not in stages:
@@ -573,44 +558,50 @@
 
         return ret
 
     def _get_yaml_dict(self) -> Dict[str, Any]:
         """
         Get a dictionary representation of the recipe for yaml serialization
         The returned dict will only contain information necessary for yaml
-        serialization (ignores metadata, version, etc), and must not be used
-        in place of the dict method
+        serialization and must not be used in place of the dict method
 
         :return: A dictionary representation of the recipe for yaml serialization
         """
 
-        def _modifier_group_to_dict(modifier_group: List[Dict[str, Any]]):
-            # convert a list of modifiers to a dict of modifiers
-            return {
-                key: value
-                for modifier in modifier_group
-                for key, value in modifier.items()
-            }
+        original_recipe_dict = self.dict()
+        yaml_recipe_dict = {}
 
-        def _stage_to_dict(stage: Dict[str, Any]):
-            # convert a stage to a dict of modifiers
-            return {
-                modifier_group_name: _modifier_group_to_dict(modifier_group)
-                for modifier_group_name, modifier_group in stage["modifiers"].items()
-            }
+        # populate recipe level attributes
+        recipe_level_attributes = ["version", "args", "metadata"]
 
-        final_dict = {}
-        for stage_name, stages in self.dict()["stages"].items():
-            if len(stages) == 1:
-                final_dict[stage_name] = _stage_to_dict(stages[0])
-            else:
-                for idx, stage in enumerate(stages):
-                    final_dict[stage_name + "_" + str(idx)] = _stage_to_dict(stage)
+        for attribute in recipe_level_attributes:
+            if attribute_value := original_recipe_dict.get(attribute):
+                yaml_recipe_dict[attribute] = attribute_value
+
+        # populate stages
+        stages = original_recipe_dict["stages"]
+        for stage_name, stage_list in stages.items():
+            for idx, stage in enumerate(stage_list):
+                if len(stage_list) > 1:
+                    # resolve name clashes caused by combining recipes with
+                    # duplicate stage names
+                    final_stage_name = f"{stage_name}_{idx}"
+                else:
+                    final_stage_name = stage_name
+                stage_dict = get_yaml_serializable_stage_dict(
+                    modifiers=stage["modifiers"]
+                )
+
+                # infer run_type from stage
+                if run_type := stage.get("run_type"):
+                    stage_dict["run_type"] = run_type
 
-        return final_dict
+                yaml_recipe_dict[final_stage_name] = stage_dict
+
+        return yaml_recipe_dict
 
 
 @dataclass
 class RecipeTuple:
     """
     A simple dataclass to hold a recipe, it's target_stages, and override_args
 
@@ -700,13 +691,62 @@
     # Create a recipe string from the modifiers
     default_group_name: str = "DEFAULT"
     modifier_group_name: str = modifier_group_name or default_group_name
 
     recipe_dict = {
         f"{modifier_group_name}_stage": {
             f"{default_group_name}_modifiers": {
-                modifier.__class__.__name__: modifier.dict() for modifier in modifiers
+                modifier.__class__.__name__: modifier.model_dump()
+                for modifier in modifiers
             }
         }
     }
     recipe_str: str = yaml.dump(recipe_dict)
     return recipe_str
+
+
+def get_modifiers_dict(modifiers: List[Dict[str, Any]]) -> Dict[str, Any]:
+
+    group_dict = {}
+
+    for modifier in modifiers:
+        modifier_type = modifier["type"]
+        modifier_group = modifier["group"]
+
+        if modifier_group not in group_dict:
+            group_dict[modifier_group] = []
+
+        modifier_dict = {modifier_type: modifier["args"]}
+        group_dict[modifier_group].append(modifier_dict)
+
+    return group_dict
+
+
+def get_yaml_serializable_stage_dict(modifiers: List[Dict[str, Any]]) -> Dict[str, Any]:
+    """
+    This function is used to convert a list of modifiers into a dictionary
+    where the keys are the group names and the values are the modifiers
+    which in turn are dictionaries with the modifier type as the key and
+    the modifier args as the value.
+
+    This is needed to conform to our recipe structure during yaml serialization
+    where each stage, modifier_groups, and modifiers are represented as
+    valid yaml dictionaries.
+
+    Note: This function assumes that modifier groups do not contain the same
+    modifier type more than once in a group. This assumption is also held by
+    Recipe.create_instance(...) method.
+
+    :param modifiers: A list of dictionaries where each dictionary
+        holds all information about a modifier
+    :return: A dictionary where the keys are the group names and the values
+        are the modifiers which in turn are dictionaries with the modifier
+        type as the key and the modifier args as the value.
+    """
+    stage_dict = {}
+    for modifier in modifiers:
+        group_name = f"{modifier['group']}_modifiers"
+        modifier_type = modifier["type"]
+        if group_name not in stage_dict:
+            stage_dict[group_name] = {}
+        stage_dict[group_name][modifier_type] = modifier["args"]
+    return stage_dict
```

## sparseml/core/recipe/stage.py

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from enum import Enum
 from typing import Any, Dict, List, Optional
 
-from pydantic import Field, root_validator
+from pydantic import ConfigDict, Field, model_validator
 
 from sparseml.core.framework import Framework
 from sparseml.core.modifier import StageModifiers
 from sparseml.core.recipe.args import RecipeArgs
 from sparseml.core.recipe.base import RecipeBase
 from sparseml.core.recipe.modifier import RecipeModifier
 
@@ -42,14 +42,16 @@
     :param enabled: True to enable the stage, False otherwise
     :param modifiers: list of RecipeModifiers that are a part of this stage
     :param exclude_default: True to exclude the default modifiers from the stage,
         False otherwise
     :param args_evaluated: the evaluated RecipeArgs for the stage
     """
 
+    model_config = ConfigDict(arbitrary_types_allowed=True)
+
     group: Optional[str] = None
     run_type: Optional[StageRunType] = None
     args: Optional[RecipeArgs] = None
     enabled: bool = True
     modifiers: List[RecipeModifier] = Field(default_factory=list)
     exclude_default: bool = False
     args_evaluated: Optional[RecipeArgs] = None
@@ -135,15 +137,16 @@
             modifier = modifier.create_modifier(framework)
             modifier.group = self.group
             modifier.index = index
             stage_modifiers.modifiers.append(modifier)
 
         return stage_modifiers
 
-    @root_validator(pre=True)
+    @model_validator(mode="before")
+    @classmethod
     def remap_modifiers(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         modifiers = RecipeStage.extract_dict_modifiers(values)
         values["modifiers"] = modifiers
 
         return values
 
     @staticmethod
```

## sparseml/export/export.py

```diff
@@ -86,14 +86,15 @@
 _LOGGER = logging.getLogger(__name__)
 
 
 def export(
     source_path: Union[Path, str] = None,
     target_path: Union[Path, str, None] = None,
     model: Optional["torch.nn.Module"] = None,  # noqa F401
+    tokenizer: Optional["PreTrainedTokenizer"] = None,  # noqa F401
     onnx_model_name: str = ONNX_MODEL_NAME,
     deployment_target: str = "deepsparse",
     opset: Optional[int] = None,
     save_with_external_data: bool = False,
     external_data_chunk_size_mb: Optional[int] = None,
     num_export_samples: int = 0,
     recipe: Optional[Union[Path, str]] = None,
@@ -130,19 +131,17 @@
 
     :param source_path: The path to the PyTorch model to export. Will be
         omitted if model is provided
     :param target_path: The path to save the exported model to. If not provided
         will default to source_path
     :param model: The PyTorch model to export. If provided, the source_path
         should be set to None to avoid potential confusion and entaglement
-        of sources. This means that, the full
-        export logic will not be enforced (e.g. the final deployment directory
-        will not be complete, it will not be possible to run validate_structure
-        method or apply some optimizations that require complete deployment
-        directory structure)
+        of sources
+    :param tokenizer: An optional tokenizer to export if passing in a source through
+    the model argument. This argument takes no effect if a source_path is provided
     :param onnx_model_name: The name of the exported model.
         Defaults to ONNX_MODEL_NAME.
     :param deployment_target: The deployment target to export
         the model to. Defaults to 'deepsparse'.
     :param opset: The ONNX opset to use for exporting the model.
         Defaults to the latest supported opset.
     :param recipe: The path to the recipe to use for exporting the model.
@@ -180,14 +179,15 @@
         Defaults to None.
     """
     from sparseml.export.export_data import export_data_samples
     from sparseml.export.validators import validate_correctness as validate_correctness_
     from sparseml.export.validators import validate_structure as validate_structure_
     from sparseml.integration_helper_functions import (
         IntegrationHelperFunctions,
+        remove_past_key_value_support_from_config,
         resolve_integration,
     )
     from sparseml.pytorch.opset import TORCH_DEFAULT_ONNX_OPSET
     from sparseml.pytorch.utils.helpers import default_device
 
     opset = opset or TORCH_DEFAULT_ONNX_OPSET
 
@@ -202,16 +202,26 @@
             "Specify either source_path or model, not both"
         )
 
     if source_path is not None:
         source_path = process_source_path(source_path)
         if target_path is None:
             target_path = source_path
+        if tokenizer is not None:
+            _LOGGER.warning(
+                "Passed a tokenizer is not supported when exporting from ",
+                "a source path. The tokenizer will be ignored. ",
+            )
 
-    integration = resolve_integration(source_path, integration)
+    if model is not None and hasattr(model, "config"):
+        model.config = remove_past_key_value_support_from_config(model.config)
+
+    integration = resolve_integration(
+        source_path=source_path, source_model=model, integration=integration
+    )
     _LOGGER.info(f"Starting export for {integration} model...")
 
     if target_path is None:
         raise ValueError("targe_path is None. Provide the target_path argument.")
 
     # create the target path if it doesn't exist
     if not Path(target_path).exists():
@@ -258,14 +268,16 @@
             loaded_model_kwargs[arg_name] = arg_val
 
     # once model is loaded we can clear the SparseSession, it was only needed for
     # adding structural changes (ie quantization) to the model
     session_manager.active_session().reset()
 
     _LOGGER.info("Creating data loader for the export...")
+    if tokenizer is not None:
+        loaded_model_kwargs["tokenizer"] = tokenizer
     data_loader, loaded_data_loader_kwargs = helper_functions.create_data_loader(
         model=model,
         task=task,
         device=device,
         **loaded_model_kwargs,
     )
     # join kwargs that are created during the initialization of the model
@@ -319,14 +331,16 @@
     _LOGGER.info(
         f"Creating deployment folder {deployment_directory_name} "
         f"at directory: {target_path}..."
     )
 
     deployment_folder_dir = create_deployment_folder(
         source_path=source_path,
+        source_config=getattr(model, "config", None),
+        source_tokenizer=tokenizer,
         target_path=target_path,
         deployment_directory_name=deployment_directory_name,
         deployment_directory_files_mandatory=helper_functions.deployment_directory_files_mandatory,  # noqa: E501
         deployment_directory_files_optional=helper_functions.deployment_directory_files_optional,  # noqa: E501
         onnx_model_name=onnx_model_name,
     )
```

## sparseml/export/helpers.py

```diff
@@ -111,14 +111,16 @@
     return export_kwargs
 
 
 def create_deployment_folder(
     target_path: Union[Path, str],
     deployment_directory_files_mandatory: List[str],
     source_path: Union[Path, str, None] = None,
+    source_config: Optional["PreTrainedConfig"] = None,  # noqa F401
+    source_tokenizer: Optional["PreTrainedTokenizer"] = None,  # noqa F401
     deployment_directory_files_optional: Optional[List[str]] = None,
     deployment_directory_name: str = "deployment",
     onnx_model_name: Optional[str] = None,
 ) -> str:
     """
     Copy the relevant files to the deployment folder.
 
@@ -131,14 +133,16 @@
     :param source_path: The path to the source folder. This is where the ONNX model
         and (optionally) ONNX data file are located.
     :param target_path: The path to the target folder.
     :param deployment_directory_name: The name of the deployment directory.
         The files will be copied to target_path/deployment_directory_name.
     :param source_path: The path to the source folder (where the original model
         files are stored)
+    :param source_config: Optional Hugging Face config to copy to deployment dir
+    :param source_tokenizer: Optional Hugging Face tokenizer to copy to deployment dir
     :param deployment_directory_files_mandatory: The mandatory list of files
         to copy to the deployment directory. If the file is an ONNX model
         (or ONNX data file), the file will be copied from target_path.
         Else, the file will be copied from source_path.
     :param deployment_directory_files_optional: The optional list of files
         to copy to the deployment directory.
     :param onnx_model_name: The name of the ONNX model file. If not specified,
@@ -157,18 +161,24 @@
 
     # move the model and (if required) the data files
     move_onnx_files(
         target_path=target_path,
         deployment_folder_dir=deployment_folder_dir,
         onnx_model_name=onnx_model_name,
     )
+
     if source_path is None:
+        # exporting an instantiated model
+        if source_config is not None:
+            source_config.save_pretrained(deployment_folder_dir)
+        if source_tokenizer is not None:
+            source_tokenizer.save_pretrained(deployment_folder_dir)
         return deployment_folder_dir
 
-    # copy the relevant files from source_path
+    # exporting from a source path, copy the relevant files to deployment directory
     for file_name in deployment_directory_files_mandatory:
         copy_mandatory_deployment_files(
             file_name, source_path, target_path, onnx_model_name, deployment_folder_dir
         )
 
     for file_name in deployment_directory_files_optional:
         copy_optional_deployment_files(file_name, source_path, deployment_folder_dir)
```

## sparseml/exporters/transforms/kv_cache/configs.py

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 
 import json
 import logging
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple, Type, Union
 
-from pydantic import BaseModel, Field
+from pydantic import BaseModel, ConfigDict, Field
 
 from sparseml.exporters.transforms import OnnxTransform
 from sparseml.exporters.transforms.kv_cache.transforms_codegen import (
     AdditionalTransformsCodeGen,
 )
 from sparseml.exporters.transforms.kv_cache.transforms_llama import (
     AdditionalTransformsLLAMA,
@@ -43,30 +43,31 @@
     model_name: str = Field(
         description="The name of the model type. This should correspond to "
         "the `model_type` field in the transformer's `config.json` file."
     )
     additional_transforms: Union[
         List[Type[OnnxTransform]], Type[OnnxTransform], None
     ] = Field(
+        None,
         description="A transform class (or list thereof) to use for additional "
-        "transforms to the model required for finalizing the kv cache injection."
+        "transforms to the model required for finalizing the kv cache injection.",
     )
     key_num_attention_heads: str = Field(
         description="The key to use to get the number of attention heads from the "
         "transformer's `config.json` file."
     )
     key_num_embedding_hidden_size: str = Field(
         description="The key to use to get the hidden size "
         "from the transformer's `config.json` file."
     )
     num_attention_heads: Optional[int] = Field(
-        description="The number of attention heads."
+        None, description="The number of attention heads."
     )
     hidden_size_kv_cache: Optional[int] = Field(
-        description="The hidden size of the key/value cache. "
+        None, description="The hidden size of the key/value cache. "
     )
     multiply_batch_by_num_att_heads: bool = Field(
         default=False,
         description="Whether or not to internally multiply "
         "the batch size by the number of attention heads. "
         "This is used to reduce the number of dimensions in "
         "the key/value cache.",
@@ -79,17 +80,15 @@
     )
     transpose_key_input: Optional[Tuple[int, int, int, int]] = Field(
         default=None,
         description="The transpose indices to apply to the key of "
         "the kv cache. If this is not provided, no transpose will "
         "be applied.",
     )
-
-    class Config:
-        arbitrary_types_allowed = True
+    model_config = ConfigDict(arbitrary_types_allowed=True)
 
 
 OPT_CONFIG = KeyValueCacheConfig(
     model_name="opt",
     additional_transforms=AdditionalTransformsOPT,
     key_num_attention_heads="num_attention_heads",
     key_num_embedding_hidden_size="hidden_size",
```

## sparseml/framework/info.py

```diff
@@ -227,21 +227,21 @@
     )
 
     if path:
         path = clean_path(path)
         create_parent_dirs(path)
 
         with open(path, "w") as file:
-            file.write(info.json())
+            file.write(info.model_dump_json())
 
         _LOGGER.info(
             "saved framework info for framework %s in file at %s", framework, path
         ),
     else:
-        print(info.json(indent=4))
+        print(info.model_dump_json(indent=4))
         _LOGGER.info("printed out framework info for framework %s", framework)
 
 
 def load_framework_info(load: str) -> FrameworkInfo:
     """
     Load the framework info from a file or raw json.
     If load exists as a path, will read from the file and use that.
```

## sparseml/modifiers/obcq/utils/sgpt_wrapper.py

```diff
@@ -167,14 +167,37 @@
                     dtype = self.layer.weight_fake_quant.dtype
                     qscheme = self.layer.weight_fake_quant.qscheme
                     if qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:
                         q = torch.quantize_per_tensor(q, scale, zero_point, dtype)
                     else:
                         q = torch.quantize_per_channel(q, scale, zero_point, 0, dtype)
                     q = torch.dequantize(q)
+                elif hasattr(self.layer, "quantization_scheme"):
+                    if self.layer.quantization_scheme.weights is not None:
+                        scale = self.layer.weight_scale
+                        zero_point = self.layer.weight_zero_point
+                        from compressed_tensors.quantization.lifecycle.forward import (
+                            fake_quantize,
+                        )
+
+                        while scale.ndim < 2:
+                            scale = scale.unsqueeze(1)
+                            zero_point = zero_point.unsqueeze(1)
+
+                        while q.ndim < 2:
+                            q = q.unsqueeze(1)
+                        q = fake_quantize(
+                            q,
+                            scale[:, i],
+                            zero_point[:, i],
+                            self.layer.quantization_scheme.weights,
+                        )
+
+                while q.ndim != 1:
+                    q.squeeze()
 
                 Q1[:, i] = q
                 Losses1[:, i] = (w - q) ** 2 / d**2
 
                 err1 = (w - q) / d
                 W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))
                 Err1[:, i] = err1
```

## sparseml/modifiers/quantization/pytorch.py

```diff
@@ -16,14 +16,15 @@
 from typing import Any, Dict, Optional
 
 import torch
 from torch.nn import Module
 
 from sparseml.core import Event, EventType, State
 from sparseml.modifiers.quantization.base import QuantizationModifier
+from sparseml.modifiers.quantization.modification import modify_model
 from sparseml.modifiers.quantization.utils.helpers import (
     configure_module_bn_wrappers,
     freeze_bn_stats,
     fuse_module_conv_bn_relus,
 )
 from sparseml.modifiers.quantization.utils.quantization_scheme import (
     QuantizationScheme,
@@ -69,19 +70,24 @@
         self.scheme = QuantizationScheme.load(self.scheme)
         self.scheme_overrides = _load_quantization_schemes_dict(
             self.scheme_overrides, self.scheme
         )
 
     def on_initialize_structure(self, state: State, **kwargs):
         module = state.model.model
+        # before the structure is modified to support quantization,
+        # we need to potentially modify the model architecture
+        module = modify_model(module)
         self._enable_module_qat(module)
         state.model.model.apply(torch.quantization.disable_observer)
 
     def on_initialize(self, state: State, **kwargs) -> bool:
         raise_if_torch_quantization_not_available()
+        module = state.model.model
+        module = modify_model(module)
         if self.end and self.end != -1:
             raise ValueError(
                 "end_epoch is disabled for QuantizationModifier and can only be set to"
                 " -1 or None. Given {}".format(self.end)
             )
 
         self.calibration_dataloader_ = state.data.calib
```

## sparseml/modifiers/quantization/utils/quantization_scheme.py

```diff
@@ -17,15 +17,15 @@
 """
 from copy import deepcopy
 from functools import partial
 from typing import Any, Dict, Optional, Union
 
 import torch
 from packaging import version
-from pydantic import BaseModel, Field, validator
+from pydantic import BaseModel, Field, field_validator
 from torch.nn import Identity
 
 
 try:
     from torch import quantization as torch_quantization
 except Exception:
     torch_quantization = None
@@ -117,15 +117,16 @@
             strategy=self.strategy,
             dtype=torch.qint8,
             bits=self.num_bits,
             reduce_range=self.kwargs.get("reduce_range", False),
             qconfig_kwargs=self.kwargs,
         )
 
-    @validator("strategy")
+    @field_validator("strategy")
+    @classmethod
     def validate_strategy(cls, value):
         valid_scopes = ["tensor", "channel"]
         if value not in valid_scopes:
             raise ValueError(f"`strategy` must be one of {valid_scopes}, got {value}")
         return value
 
 
@@ -259,15 +260,15 @@
         qconfig.quantization_scheme = self
         return qconfig
 
     def __str__(self) -> str:
         """
         :return: YAML friendly string serialization
         """
-        dict_repr = self.dict()
+        dict_repr = self.model_dump()
         dict_repr = {
             key: val if val is not None else "null" for key, val in dict_repr.items()
         }
         return str(dict_repr)
 
 
 def compute_range(dtype: torch.dtype, bits: int):
```

## sparseml/modifiers/smoothquant/base.py

```diff
@@ -12,16 +12,14 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 from dataclasses import dataclass
 from typing import Dict, Generic, List, Optional, Tuple, TypeVar
 
-from pydantic import Field
-
 from sparseml.core import Modifier
 from sparseml.core.model import ModifiableModel
 from sparseml.core.model.base import LT
 from sparseml.core.state import Event, State
 
 
 VT = TypeVar("VT")  # represents a generic vector
@@ -94,15 +92,15 @@
      :param ignore: list of layers to ignore, even if they match a regex in mappings.
         It should match the name of layers whose outputs are scaled to achieve
         smoothing (the second entry of the mappings list).
      :param num_calibration_steps: number of samples to use for calibration, or None to
      use the whole dataset
     """
 
-    smoothing_strength: float = Field(validation_alias="alpha", default=0.5)
+    smoothing_strength: float = 0.5
     mappings: List[Tuple]
     ignore: Optional[List[str]] = None
     num_calibration_steps: Optional[int] = None
 
     resolved_mappings_: Optional[List] = None
     scales_: Optional[Dict] = None
```

## sparseml/pytorch/base.py

```diff
@@ -45,15 +45,16 @@
     "check_torchvision_install",
     "require_torch",
     "require_torchvision",
 ]
 
 
 _TORCH_MIN_VERSION = "1.0.0"
-_TORCH_MAX_VERSION = os.environ.get("MAX_TORCH", "2.1.10")
+# set max to 2.2.99 to account for bugfix versions with 2.2
+_TORCH_MAX_VERSION = os.environ.get("MAX_TORCH", "2.2.99")
 
 
 def check_torch_install(
     min_version: Optional[str] = _TORCH_MIN_VERSION,
     max_version: Optional[str] = _TORCH_MAX_VERSION,
     raise_on_error: bool = True,
 ) -> bool:
```

## sparseml/pytorch/torch_to_onnx_exporter.py

```diff
@@ -125,15 +125,16 @@
         The files are being stored by the _TorchOnnxExport transform.
         """
         torch_onnx_export_transform = self.transforms[0]
         assert isinstance(
             torch_onnx_export_transform, _TorchOnnxExport
         ), "Expected the first transform from self.transform to be _TorchOnnxExport"
         for file in torch_onnx_export_transform.leftover_files:
-            os.remove(file)
+            if os.path.exists(file):
+                os.remove(file)
 
 
 class _TorchOnnxExport(BaseTransform):
     def __init__(
         self,
         sample_batch: Any,
         opset: int = TORCH_DEFAULT_ONNX_OPSET,
```

## sparseml/pytorch/sparsification/quantization/quantization_scheme.py

```diff
@@ -17,15 +17,15 @@
 """
 from copy import deepcopy
 from functools import partial
 from typing import Any, Dict, Optional, Union
 
 import torch
 from packaging import version
-from pydantic import BaseModel, Field, validator
+from pydantic import BaseModel, Field, field_validator
 from torch.nn import Identity
 
 
 try:
     from torch import quantization as torch_quantization
 except Exception:
     torch_quantization = None
@@ -115,15 +115,16 @@
             strategy=self.strategy,
             dtype=torch.qint8,
             bits=self.num_bits,
             reduce_range=self.kwargs.get("reduce_range", False),
             qconfig_kwargs=self.kwargs,
         )
 
-    @validator("strategy")
+    @field_validator("strategy")
+    @classmethod
     def validate_strategy(cls, value):
         valid_scopes = ["tensor", "channel"]
         if value not in valid_scopes:
             raise ValueError(f"`strategy` must be one of {valid_scopes}, got {value}")
         return value
```

## sparseml/pytorch/utils/logger.py

```diff
@@ -41,19 +41,29 @@
     import wandb
 
     wandb_err = None
 except Exception as err:
     wandb = None
     wandb_err = err
 
+
+try:
+    from clearml import Task
+
+    clearml_err = None
+except Exception as err:
+    clearml = None
+    clearml_err = err
+
 from sparseml.utils import ALL_TOKEN, create_dirs
 
 
 __all__ = [
     "BaseLogger",
+    "ClearMLLogger",
     "LambdaLogger",
     "PythonLogger",
     "TensorBoardLogger",
     "WANDBLogger",
     "SparsificationGroupLogger",
     "LoggerManager",
     "LOGGING_LEVELS",
@@ -624,14 +634,109 @@
         """
         :param file_path: path to a file to be saved
         """
         wandb.save(file_path)
         return True
 
 
+class ClearMLLogger(LambdaLogger):
+    @staticmethod
+    def available() -> bool:
+        """
+        :return: True if wandb is available and installed, False, otherwise
+        """
+        return not clearml_err
+
+    def __init__(
+        self,
+        name: str = "clearml",
+        enabled: bool = True,
+        project_name: str = "sparseml",
+        task_name: str = "",
+    ):
+        if task_name == "":
+            now = datetime.now()
+            task_name = now.strftime("%d-%m-%Y_%H.%M.%S")
+
+        self.task = Task.init(project_name=project_name, task_name=task_name)
+
+        super().__init__(
+            lambda_func=self.log_scalar,
+            name=name,
+            enabled=enabled,
+        )
+
+    def log_hyperparams(
+        self,
+        params: Dict,
+        level: Optional[int] = None,
+    ) -> bool:
+        """
+        :param params: Each key-value pair in the dictionary is the name of the
+            hyper parameter and it's corresponding value.
+        :return: True if logged, False otherwise.
+        """
+        if not self.enabled:
+            return False
+
+        self.task.connect(params)
+        return True
+
+    def log_scalar(
+        self,
+        tag: str,
+        value: float,
+        step: Optional[int] = None,
+        wall_time: Optional[float] = None,
+        level: Optional[int] = None,
+    ) -> bool:
+        """
+        :param tag: identifying tag to log the value with
+        :param value: value to save
+        :param step: global step for when the value was taken
+        :param wall_time: global wall time for when the value was taken,
+            defaults to time.time()
+        :param kwargs: additional logging arguments to support Python and custom loggers
+        :return: True if logged, False otherwise.
+        """
+        logger = self.task.get_logger()
+        # each series is superimposed on the same plot on title
+        logger.report_scalar(
+            title=tag, series=str(level) or tag, value=value, iteration=step
+        )
+        return True
+
+    def log_scalars(
+        self,
+        tag: str,
+        values: Dict[str, float],
+        step: Optional[int] = None,
+        wall_time: Optional[float] = None,
+        level: Optional[int] = None,
+    ) -> bool:
+        """
+        :param tag: identifying tag to log the values with
+        :param values: values to save
+        :param step: global step for when the values were taken
+        :param wall_time: global wall time for when the values were taken,
+            defaults to time.time()
+        :param kwargs: additional logging arguments to support Python and custom loggers
+        :return: True if logged, False otherwise.
+        """
+        for k, v in values.items():
+            self.log_scalar(
+                tag=f"{tag}.{k}",
+                value=v,
+                step=step,
+                wall_time=wall_time,
+                level=level,
+            )
+        return True
+
+
 class SparsificationGroupLogger(BaseLogger):
     """
     Modifier logger that handles outputting values to other supported systems.
     Supported ones include:
       - Python logging
       - Tensorboard
       - Weights and Biases
```

## sparseml/pytorch/utils/sparsification.py

```diff
@@ -22,14 +22,15 @@
     Any,
     Callable,
     Dict,
     Generator,
     Iterable,
     Iterator,
     List,
+    Optional,
     Tuple,
     Union,
 )
 
 import torch
 from torch.nn import Module
 from tqdm import tqdm
@@ -53,21 +54,30 @@
 class ModuleSparsificationInfo:
     """
     Helper class for providing information related to torch Module parameters
     and the amount of sparsification applied. Includes information for pruning
     and quantization
 
     :param module: torch Module to analyze
+    :param state_dict: optional state_dict to analyze in place of the torch model. This
+    is used when analyzing an FSDP model, where the full weights may not be accessible
     """
 
-    def __init__(self, module: Module):
+    def __init__(
+        self, module: Module, state_dict: Optional[Dict[str, torch.Tensor]] = None
+    ):
         self.module = module
-        self.trainable_params = list(
-            filter(lambda param: param.requires_grad, self.module.parameters())
-        )
+        self.state_dict = state_dict
+
+        if self.state_dict is not None:
+            self.trainable_params = [param for _, param in state_dict.items()]
+        else:
+            self.trainable_params = list(
+                filter(lambda param: param.requires_grad, self.module.parameters())
+            )
 
     def __str__(self):
         return json.dumps(
             {
                 "params_summary": {
                     "total": self.params_total,
                     "sparse": self.params_sparse,
@@ -120,15 +130,17 @@
     @property
     def params_prunable_sparse(self) -> int:
         """
         :return: total number of sparse (0) parameters across prunable lauyers
         """
         return sum(
             round(tensor_sparsity(layer.weight).item() * torch.numel(layer.weight))
-            for (name, layer) in get_prunable_layers(self.module)
+            for (name, layer) in tqdm(
+                get_prunable_layers(self.module), desc="Calculating model sparsity"
+            )
         )
 
     @property
     def params_prunable_sparse_percent(self) -> float:
         """
         :return: percent of prunable parameters that have been pruned
         """
```

## sparseml/pytorch/utils/sparsification_info/configs.py

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 
 from abc import ABC, abstractmethod
 from collections import Counter, defaultdict
 from typing import Any, Dict, Generator, Tuple, Union
 
 import torch.nn
-from pydantic import BaseModel, Field
+from pydantic import BaseModel, ConfigDict, Field
 
 from sparseml.pytorch.utils.sparsification_info.helpers import (
     get_leaf_operations,
     get_precision_information,
     is_quantized,
 )
 
@@ -322,17 +322,15 @@
         "operation to a boolean flag that indicates whether "
         "the operation is quantized or not."
     )
     precision: Dict[str, Union[BaseModel, None, int]] = Field(
         description="A dictionary that maps the name of a layer"
         "to the precision of that layer."
     )
-
-    class Config:
-        arbitrary_types_allowed = True
+    model_config = ConfigDict(arbitrary_types_allowed=True)
 
     @classmethod
     def from_module(
         cls,
         module: torch.nn.Module,
     ) -> "SparsificationQuantization":
         """
```

## sparseml/sparsification/info.py

```diff
@@ -231,21 +231,21 @@
     )
 
     if path:
         path = clean_path(path)
         create_parent_dirs(path)
 
         with open(path, "w") as file:
-            file.write(info.json())
+            file.write(info.model_dump_json())
 
         _LOGGER.info(
             "saved sparsification info for framework %s in file at %s", framework, path
         ),
     else:
-        print(info.json(indent=4))
+        print(info.model_dump_json(indent=4))
         _LOGGER.info("printed out sparsification info for framework %s", framework)
 
 
 def load_sparsification_info(load: str) -> SparsificationInfo:
     """
     Load the sparsification info from a file or raw json.
     If load exists as a path, will read from the file and use that.
```

## sparseml/sparsification/model_info.py

```diff
@@ -21,15 +21,15 @@
 from abc import ABC, abstractmethod
 from collections import OrderedDict
 from copy import deepcopy
 from enum import Enum
 from typing import Any, Dict, List, Optional, Set, Union
 
 import numpy
-from pydantic import BaseModel, Field, root_validator
+from pydantic import BaseModel, Field, model_validator
 
 from sparseml.utils import clean_path, create_parent_dirs
 
 
 __all__ = [
     "LayerInfo",
     "Result",
@@ -83,15 +83,16 @@
     )
     attributes: Optional[Dict[str, Any]] = Field(
         title="attributes",
         default=None,
         description="dictionary of string attribute names to their values",
     )
 
-    @root_validator(pre=True)
+    @model_validator(mode="before")
+    @classmethod
     def check_params_if_prunable(_, values):
         prunable = values.get("prunable")
         params = values.get("params")
         if prunable and not params:
             raise ValueError(
                 f"Prunable layers must have non 0 number of params given {params} "
                 f"for layer {values.get('name')} with prunable set to {prunable}"
```

## sparseml/transformers/integration_helper_functions.py

```diff
@@ -20,25 +20,25 @@
 from pydantic import Field
 
 from sparseml.export.export_data import create_data_samples as create_data_samples_
 from sparseml.export.helpers import apply_optimizations as apply_optimizations_onnx
 from sparseml.integration_helper_functions import (
     IntegrationHelperFunctions,
     Integrations,
+    remove_past_key_value_support_from_config,
 )
 from sparseml.transformers.finetune.data.data_helpers import format_calibration_data
 from sparseml.transformers.utils.helpers import (
     ALL_TASK_NAMES,
     MANDATORY_DEPLOYMENT_FILES,
     NLG_MANDATORY_DEPLOYMENT_FILES,
     NLG_OPTIONAL_DEPLOYMENT_FILES,
     OPTIONAL_DEPLOYMENT_FILES,
     TaskNames,
     create_fake_dataloader,
-    remove_past_key_value_support_from_config,
     resolve_sequence_length,
 )
 from sparseml.transformers.utils.initializers import (
     _parse_data_args,
     initialize_config,
     initialize_sparse_model,
     initialize_tokenizer,
@@ -111,15 +111,15 @@
 
 def create_data_loader(
     model: torch.nn.Module,
     task: str,
     data_args: Optional[Dict[str, Any]] = None,
     config: Optional["AutoConfig"] = None,  # noqa F821
     source_path: Optional[str] = None,
-    sequence_length: Optional[int] = None,
+    sequence_length: int = 384,
     tokenizer: Optional["AutoTokenizer"] = None,  # noqa F821
     dataset_with_labels: bool = False,
     **kwargs,
 ):
     """
     A contract to create a dataloader and optional dictionary of
     loaded_dataloader_kwargs (any relevant objects created along with the dataloader)
```

## sparseml/transformers/compression/__init__.py

```diff
@@ -7,12 +7,7 @@
 #    http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-# flake8: noqa
-
-from .compressors import *
-from .config import *
```

## sparseml/transformers/finetune/__init__.py

```diff
@@ -10,8 +10,13 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 # flake8: noqa
 
+from .data import DataTrainingArguments, TextGenerationDataset
+from .model_args import ModelArguments
+from .session_mixin import SessionManagerMixIn
 from .text_generation import apply, compress, eval, oneshot, train
+from .trainer import Trainer
+from .training_args import TrainingArguments
```

## sparseml/transformers/finetune/runner.py

```diff
@@ -15,15 +15,14 @@
 import logging
 import math
 import os
 import re
 from typing import List, Optional
 
 import torch
-from torch.nn import Module
 from torch.utils.data import Dataset
 from transformers import AutoTokenizer
 
 import sparseml.core.session as session_manager
 from sparseml.core.recipe import Recipe, StageRunType
 from sparseml.pytorch.model_load.helpers import (
     get_completed_stages,
@@ -68,15 +67,14 @@
     """
 
     def __init__(
         self,
         data_args: "DataTrainingArguments",
         model_args: "ModelArguments",
         training_args: "TrainingArguments",
-        model: Module,
     ):
         self._data_args = data_args
         self._model_args = model_args
         self._training_args = training_args
 
         self.datasets = {}
         self.trainer = None
@@ -117,17 +115,23 @@
             dataset_manager = TextGenerationDataset.load_from_registry(
                 registry_id,
                 data_args=self._data_args,
                 split=split_str,
                 tokenizer=tokenizer,
             )
 
-            raw_dataset = dataset_manager.get_raw_dataset(self._model_args.cache_dir)
-            tokenized_dataset = dataset_manager.tokenize_and_process(raw_dataset)
-            tokenized_datasets[split_name] = tokenized_dataset
+            dataset = self._data_args.dataset
+            if hasattr(dataset, "column_names") and "input_ids" in dataset.column_names:
+                # dataset is already tokenized
+                tokenized_datasets[split_name] = dataset
+            else:
+                # dataset needs to be tokenized
+                raw_dataset = dataset_manager.get_raw_dataset()
+                tokenized_dataset = dataset_manager.tokenize_and_process(raw_dataset)
+                tokenized_datasets[split_name] = tokenized_dataset
 
         self.datasets = make_dataset_splits(
             tokenized_datasets,
             do_train=self._training_args.do_train,
             do_eval=self._training_args.do_eval,
             do_predict=self._training_args.do_predict,
             do_oneshot=self._training_args.do_oneshot,
@@ -150,14 +154,15 @@
         :param stage: which stage of the recipe to run, or None to run whole recipe
         """
         _LOGGER.info("*** One Shot ***")
 
         calib_data = format_calibration_data(
             tokenized_dataset=self.get_dataset_split("calibration"),
             num_calibration_samples=self._data_args.num_calibration_samples,
+            do_shuffle=self._data_args.shuffle_calibration_samples,
             accelerator=self.trainer.accelerator,
         )
 
         # if we don't run a forward pass after initializing the FSDP model for the
         # first time, calls to summon_full_params will fail ¯\_(ツ)_/¯
         dummy_inp = dict(next(iter(calib_data)))
         model_device = next(self.trainer.model.parameters()).device
```

## sparseml/transformers/finetune/session_mixin.py

```diff
@@ -24,19 +24,15 @@
 from torch.utils.data import DataLoader, IterableDataset
 from transformers.trainer_callback import TrainerState
 from transformers.trainer_utils import get_last_checkpoint
 
 import sparseml.core.session as session_manager
 from sparseml.core.framework import Framework
 from sparseml.core.session import callbacks
-from sparseml.pytorch.model_load.helpers import (
-    RECIPE_FILE_NAME,
-    get_session_model,
-    reload_model_state,
-)
+from sparseml.pytorch.model_load.helpers import RECIPE_FILE_NAME, get_session_model
 from sparseml.pytorch.utils import LoggerManager, ModuleSparsificationInfo
 from sparseml.transformers.finetune.callbacks import (
     DisableHalfPrecisionCallback,
     TrainingLoopCallbacks,
 )
 from sparseml.utils.fsdp.context import summon_full_params_context
 from sparseml.utils.fsdp.helpers import is_fsdp_model, save_pretrained_fsdp
@@ -45,80 +41,85 @@
 
 __all__ = [
     "SessionManagerMixIn",
 ]
 
 _LOGGER = logging.getLogger(__name__)
 TRAINER_STATE_NAME = "trainer_state.json"
+METADATA_ARGS = [
+    "per_device_train_batch_size",
+    "per_device_eval_batch_size",
+    "max_seq_length",
+    "save_safetensors",
+    "fp16",
+]
 
 
 class SessionManagerMixIn:
     """
     Mix-In class to extend the Hugging Face Trainer class to support SparseML recipes
     for one-shot and finetuning flows.
 
-    :param model_state_path: path to Pytorch model checkpoint or saved model
     :param recipe: path to recipe file to apply during training
     :param recipe_args: additional kwargs to use for evaluating recipe
-    :param metadata_args: additional kwargs for configuring training
     :param data_args: kwargs for configuring dataset loading
     :param teacher: optional teacher model to use for distillation
     """
 
     def __init__(
         self,
-        model_state_path: str,
         recipe: Optional[str] = None,
         recipe_args: Optional[Union[Dict[str, Any], str]] = None,
-        metadata_args: Optional[List[str]] = None,
         data_args: Optional["DataTrainingArguments"] = None,  # noqa: F821
         teacher: Optional[Union[Module, str]] = None,
         **kwargs,
     ):
-        # instantiate necessary state, like managers, so we can override args
-        self.model_state_path = str(model_state_path)
         self.recipe = recipe
         self.recipe_args = recipe_args
         self.teacher = teacher
 
         # parse training and metadata args
         training_args = kwargs.get("args")
         self.metadata = (
             self._extract_metadata(
-                metadata_args=metadata_args,
+                metadata_args=METADATA_ARGS,
                 training_args_dict=training_args.to_dict(),
                 data_args_dict=asdict(data_args) if data_args else {},
             )
-            if training_args and metadata_args
+            if training_args and METADATA_ARGS
             else None
         )
 
         # setup logger and session
         self.logger_manager = LoggerManager(log_python=False)
         session_manager.create_session()
 
         # call Trainer initialization
         super().__init__(**kwargs)
+        self.accelerator.wait_for_everyone()
 
         # setup callbacks and loss
         self.optim_callbacks = TrainingLoopCallbacks(self)
         self.callback_handler.add_callback(self.optim_callbacks)
         self.callback_disable_fp16 = DisableHalfPrecisionCallback(self)
         self.callback_handler.add_callback(self.callback_disable_fp16)
         self.criterion = torch.nn.CrossEntropyLoss()
 
         model_signature = inspect.signature(self.model.forward)
-        self._model_signature_columns = list(model_signature.parameters.keys())
+        self._signature_columns = list(model_signature.parameters.keys())
 
         if self.teacher is not None and teacher not in ("disable", "self"):
             teacher_signature = inspect.signature(self.teacher.forward)
             self._teacher_signature_columns = list(teacher_signature.parameters.keys())
         else:
             self._teacher_signature_columns = None
 
+        if self.is_fsdp_enabled:
+            self._prepare_model_for_fsdp()
+
     def initialize_session(
         self,
         epoch: float,
         checkpoint: Optional[str] = None,
         stage: Optional[str] = None,
     ):
         """
@@ -130,15 +131,14 @@
         :param checkpoint: Optional checkpoint to initialize from to continue training
         :param stage: Optional stage of recipe to run, or None to run all stages
         """
         session = session_manager.active_session()
         if session.lifecycle.initialized_ or session.lifecycle.finalized:
             return False
 
-        orig_state_dict = self.model.state_dict()
         train_data = self.get_train_dataloader()
 
         self.accelerator.wait_for_everyone()
         with summon_full_params_context(self.model, offload_to_cpu=True):
             session_manager.initialize(
                 model=self.model,
                 teacher_model=self.teacher,  # TODO: what about for self/disable?
@@ -150,25 +150,15 @@
                 start=epoch,
                 copy_data=False,
                 fsdp_active=self.is_fsdp_enabled,
                 metadata=self.metadata,
             )
         self.accelerator.wait_for_everyone()
         model = get_session_model()
-        self.model = model
-
-        # reload the state dict for the model now that architecture matches expected
-        # TODO: what if there is a quant modifier in the original recipe and we want to
-        # continue adjusting its zero point and range?
-        load_path = checkpoint or self.model_state_path
-        if reload_model_state(model, load_path, orig_state_dict):
-            _LOGGER.info(
-                "Reloaded model state after SparseML recipe structure modifications "
-                f"from {load_path}"
-            )
+        self.model_wrapped = self.model = model
 
         if self.recipe is None:
             _LOGGER.warning(
                 "No training recipe was provided, finetuning will be run "
                 "without event callbacks to SparseML. To supply a recipe "
                 "pass a yaml file or string to the `recipe` argument."
             )
@@ -294,15 +284,15 @@
             False otherwise
         :return: the resulting loss if not return_outputs, otherwise a tuple
             containing the loss and the model's outputs
         """
         self._check_super_defined("compute_loss")
 
         # TODO: do we need these model signature columns?
-        inputs = {k: inputs[k] for k in inputs if k in self._model_signature_columns}
+        inputs = {k: inputs[k] for k in inputs if k in self._signature_columns}
         loss = super().compute_loss(model, inputs, return_outputs=return_outputs)
 
         # take the mean across multiple GPUs
         # this is done outside the compute_loss function in the parent, replicating it
         # here for SparseML logging and distillation
         loss = loss.mean()
 
@@ -367,19 +357,15 @@
         output = super().train(*args, **kwargs)
         self.accelerator.wait_for_everyone()
         self.finalize_session()
 
         self.accelerator.wait_for_everyone()
 
         # log model sparsity
-        with summon_full_params_context(self.model, offload_to_cpu=True):
-            if self.accelerator.is_main_process:
-                if not qat_active(self.model):
-                    self.log_model_sparsification()
-
+        self.maybe_log_model_sparsification()
         self.accelerator.wait_for_everyone()
 
         return output
 
     def evaluate(self, *args, **kwargs):
         """
         Run a sparsification evaluation cycle.
@@ -429,19 +415,15 @@
             calib_data=calib_data,
             start=-1,
             copy_data=False,
             accelerator=self.accelerator,
         )
 
         # log model sparsity
-        with summon_full_params_context(self.model, offload_to_cpu=True):
-            if self.accelerator.is_main_process:
-                if not qat_active(self.model):
-                    self.log_model_sparsification()
-
+        self.maybe_log_model_sparsification()
         self.accelerator.wait_for_everyone()
 
     def save_model(
         self, output_dir: Optional[str] = None, _internal_call=False, _is_oneshot=False
     ):
         """
         Override of the save_model function and expects it to exist in the parent.
@@ -468,53 +450,69 @@
                 accelerator=self.accelerator,
                 output_dir=output_dir,
                 save_compressed=self.args.save_compressed,
                 save_safetensors=self.metadata.get("save_safetensors", False),
             )
 
         self.save_state()
-        self.tokenizer.save_pretrained(output_dir)
-        if not _is_oneshot:  # optimizer/scheduler not relevant to one-shot
-            self.save_optimizer_and_scheduler(output_dir)
+        if self.tokenizer is not None:
+            self.tokenizer.save_pretrained(output_dir)
 
         if not self.recipe:
             return
 
-        # save recipe, will contain modifiers from the model's original recipe as well
-        # as those added from self.recipe
-        recipe_path = os.path.join(output_dir, RECIPE_FILE_NAME)
-        session = session_manager.active_session()
-        recipe_yaml_str = session.get_serialized_recipe()
-        with open(recipe_path, "w") as fp:
-            fp.write(recipe_yaml_str)
+        if self.accelerator.is_main_process:
+            # save recipe, will contain modifiers from the model's original recipe as
+            # well as those added from self.recipe
+            recipe_path = os.path.join(output_dir, RECIPE_FILE_NAME)
+            session = session_manager.active_session()
+            recipe_yaml_str = session.get_serialized_recipe()
+            with open(recipe_path, "w") as fp:
+                fp.write(recipe_yaml_str)
+
+            _LOGGER.info(f"Saved SparseML recipe with model state to {recipe_path}")
 
-        _LOGGER.info(f"Saved SparseML recipe with model state to {recipe_path}")
         self.accelerator.wait_for_everyone()
 
+    def maybe_log_model_sparsification(self):
+        """
+        Log info on model sparsity and quantization if possible. Only print logs on the
+        main process, and avoid logging for quantized FSDP models
+        """
+        with summon_full_params_context(self.model, offload_to_cpu=True):
+            # offload to avoid OOM errors
+            if not self.accelerator.is_main_process:
+                # only calculate stats rank0 GPU
+                return
+            if self.is_fsdp_enabled and qat_active(self.model):
+                # due to state dict changes we can't log sparsity info with quantized
+                # models in FSDP
+                return
+
+            self.log_model_sparsification()
+
     def log_model_sparsification(self):
         """
         Log the current model sparsification info including pruned and quantized states
         """
         sparsification_info = ModuleSparsificationInfo(self.model)
 
         _LOGGER.info(
-            f"Sparsification info for {self.model_state_path}: "
+            f"Sparsification info for {type(self.model).__name__}: "
             f"{sparsification_info.params_total} total params. "
-            f"Of those there are {sparsification_info.params_prunable_total} prunable "
+        )
+        _LOGGER.info(
+            f"There are {sparsification_info.params_prunable_total} prunable "
             f"params which have {sparsification_info.params_prunable_sparse_percent} "
             "avg sparsity."
         )
-        model_type = (
-            "sparse"
-            if sparsification_info.params_prunable_sparse_percent > 5
-            else "dense"
-        )
         _LOGGER.info(
-            f"{model_type} model detected, "
-            f"all sparsification info: {sparsification_info}"
+            f"There are {sparsification_info.params_quantizable} quantizable "
+            f"params, with a quantization percentage of "
+            f"{sparsification_info.params_quantized_percent}."
         )
 
     def _prepare_model_for_fsdp(self):
         """
         Sets up FSDP ahead of time so we can run one-shot in FSDP mode
         """
         self.model.to("cpu")
```

## sparseml/transformers/finetune/text_generation.py

```diff
@@ -45,22 +45,14 @@
     get_shared_tokenizer_src,
 )
 from sparseml.transformers.utils.helpers import detect_last_checkpoint
 
 
 _LOGGER: logging.Logger = logging.getLogger(__name__)
 
-metadata_args = [
-    "per_device_train_batch_size",
-    "per_device_eval_batch_size",
-    "max_seq_length",
-    "save_safetensors",
-    "fp16",
-]
-
 
 def train(**kwargs):
     """
     CLI entrypoint for running training
     """
     model_args, data_args, training_args = parse_args(**kwargs)
     training_args.do_train = True
@@ -130,18 +122,14 @@
         if not isinstance(training_args.recipe_args, dict):
             arg_dict = {}
             for recipe_arg in training_args.recipe_args:
                 key, value = recipe_arg.split("=")
                 arg_dict[key] = value
             training_args.recipe_args = arg_dict
 
-    # when set to true in FSDP mode this causes issues, the model arguments show up
-    # as *args and **kwargs so all columns get removed
-    training_args.remove_unused_columns = False
-
     return model_args, data_args, training_args
 
 
 def intialize_model_from_path(
     model_args: ModelArguments,
     training_args: TrainingArguments,
 ):
@@ -315,42 +303,35 @@
     session_manager.pre_initialize_structure(model=model, framework=Framework.pytorch)
 
     # intialize session manager
     apply_recipe_structure_to_model(model, None, model_path)
 
     # Load datasets
     stage_runner = StageRunner(
-        model_args=model_args,
-        data_args=data_args,
-        training_args=training_args,
-        model=model,
+        model_args=model_args, data_args=data_args, training_args=training_args
     )
     stage_runner.populate_datasets(tokenizer=tokenizer)
     train_dataset = stage_runner.get_dataset_split("train")
     eval_dataset = stage_runner.get_dataset_split("validation")
     calib_dataset = stage_runner.get_dataset_split("calibration")
 
     # Initialize our Trainer
     data_collator = DefaultDataCollator()
     trainer = Trainer(
         model_init=get_session_model,
         teacher=teacher,
-        model_state_path=model_path,
         recipe=training_args.recipe,
-        metadata_args=metadata_args,
         recipe_args=training_args.recipe_args,
         args=training_args,
         data_args=data_args,
         train_dataset=train_dataset or calib_dataset,
         eval_dataset=eval_dataset,
         tokenizer=tokenizer,
         data_collator=data_collator,
     )
-    if trainer.is_fsdp_enabled:
-        trainer._prepare_model_for_fsdp()
     stage_runner.trainer = trainer
 
     # alternating Training/One-shot
     if training_args.run_stages:
         checkpoint = None
         if last_checkpoint is not None:
             checkpoint = last_checkpoint
```

## sparseml/transformers/finetune/trainer.py

```diff
@@ -8,106 +8,17 @@
 #
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import os
-import warnings
-from typing import Any, Callable, Dict, Optional, Union
-
-import torch
-from torch.nn import Module
 from transformers import Trainer as HFTransformersTrainer
-from transformers.trainer_pt_utils import reissue_pt_warnings
 
 from sparseml.transformers.finetune.session_mixin import SessionManagerMixIn
 
 
 __all__ = ["Trainer"]
 
-TRAINER_STATE_NAME = "trainer_state.json"
-OPTIMIZER_NAME = "optimizer.pt"
-SCHEDULER_NAME = "scheduler.pt"
-SCALER_NAME = "scaler.pt"
-
 
 class Trainer(SessionManagerMixIn, HFTransformersTrainer):
-    """
-    Training implementation for running sparsification recipes with HF Trainer.
-
-    :param model: the model to use with the trainer and apply sparsification to
-    :param model_state_path: the state path to the model,
-        used to load config and tokenizer settings
-    :param recipe: the recipe, if any, to apply to the modle and training
-        process
-    :param recipe_args: A json string, csv key=value string, or dictionary containing
-        arguments to override the root arguments within the recipe such as
-        learning rate or num epochs
-    :param teacher: teacher model for distillation. Set to 'self' to distill
-        from the loaded model or 'disable' to turn of distillation
-    :param kwargs: key word arguments passed to the parent class
-    """
-
-    def __init__(
-        self,
-        model_state_path: str,
-        model: Optional[Module] = None,
-        model_init: Optional[Callable] = None,
-        recipe: Optional[str] = None,
-        recipe_args: Optional[Union[Dict[str, Any], str]] = None,
-        teacher: Optional[Union[Module, str]] = None,
-        **kwargs,
-    ):
-        super().__init__(
-            model=model,
-            model_init=model_init,
-            model_state_path=model_state_path,
-            recipe=recipe,
-            recipe_args=recipe_args,
-            teacher=teacher,
-            **kwargs,
-        )
-
-    def save_optimizer_and_scheduler(self, output_dir: Optional[str] = None):
-        """
-        Save optimizer, scheduler and scaler
-
-        :param output_dir: The output model directory to save the above
-        """
-        if output_dir is None:
-            output_dir = self.args.output_dir
-
-        if self.is_world_process_zero():
-            if self.optimizer is not None:
-                torch.save(
-                    self.optimizer.state_dict(),
-                    os.path.join(output_dir, "optimizer.pt"),
-                )
-            with warnings.catch_warnings(record=True) as caught_warnings:
-                if self.lr_scheduler is not None:
-                    torch.save(
-                        self.lr_scheduler.state_dict(),
-                        os.path.join(output_dir, "scheduler.pt"),
-                    )
-            reissue_pt_warnings(caught_warnings)
-
-    def _save_checkpoint(self, model, trial, metrics=None):
-        # Call into the save checkpoint by HF Transformers, which saves the
-        # best metric if required
-        super()._save_checkpoint(model, trial, metrics=metrics)
-        if (
-            self.args.metric_for_best_model is None
-            or self.args.best_model_after_epoch is None
-        ):
-            return
-
-        if self.state.epoch <= self.args.best_model_after_epoch:
-            self.state.best_metric = None
-            self.state.best_model_checkpoint = None
-
-    def _dummy_lr_scheduler(self):
-        return torch.optim.lr_scheduler.MultiplicativeLR(
-            self.optimizer,
-            lambda _: 1.0,
-        )
+    pass
```

## sparseml/transformers/finetune/training_args.py

```diff
@@ -28,18 +28,14 @@
 
     :param best_model_after_epoch (`int`, *optional*, defaults to None):
         The epoch after which best model will be saved; used in conjunction
         with `load_best_model_at_end` and `metric_for_best_model` training
         arguments
     """
 
-    best_model_after_epoch: int = field(
-        default=None,
-        metadata={"help": "Epoch after which best model will be saved."},
-    )
     recipe: Optional[str] = field(
         default=None,
         metadata={
             "help": (
                 "Path to a SparseML sparsification recipe, see "
                 "https://github.com/neuralmagic/sparseml for more information"
             ),
```

## sparseml/transformers/finetune/data/__init__.py

```diff
@@ -14,13 +14,14 @@
 
 # flake8: noqa
 
 from .base import TextGenerationDataset
 from .c4 import C4Dataset
 from .cnn_dailymail import CNNDailyMailDataset
 from .custom import CustomDataset
+from .data_args import DataTrainingArguments
 from .evolcodealpaca import EvolCodeAlpacaDataset
 from .gsm8k import GSM8KDataset
 from .open_platypus import OpenPlatypusDataset
 from .ptb import PtbDataset
 from .ultrachat_200k import UltraChatDataset
 from .wikitext import WikiTextDataset
```

## sparseml/transformers/finetune/data/base.py

```diff
@@ -107,15 +107,15 @@
             self.data_args,
             cache_dir,
             split=self.split,
             streaming=self.data_args.streaming,
             **self.raw_kwargs,
         )
 
-    def tokenize_and_process(self, raw_dataset: Dataset) -> Dataset:
+    def tokenize_and_process(self, raw_dataset: Optional[Dataset] = None) -> Dataset:
         """
         Sets up the raw dataset for finetuning, performs tokenization, concatenates
         entries to max sequence length if desired, and adds labels to each entry
 
         :param raw_dataset: dataset to process
         """
         # helper fn for tokenizing text column
@@ -164,14 +164,17 @@
 
             # mask out padding in the labels as well
             padding = len(data["attention_mask"]) - sum(data["attention_mask"])
             if padding > 0:
                 data["labels"][-padding:] = [LABELS_MASK_VALUE] * padding
             return data
 
+        if raw_dataset is None:
+            raw_dataset = self.get_raw_dataset()
+
         dataset = self.map(
             raw_dataset,
             function=tokenize_fn,
             batched=True,
             remove_columns=[self.text_column],
             num_proc=self.data_args.preprocessing_num_workers,
             load_from_cache_file=not self.data_args.overwrite_cache,
```

## sparseml/transformers/finetune/data/data_args.py

```diff
@@ -114,14 +114,20 @@
         default=None,
         metadata={"help": "Optional percentages of each split to download"},
     )
     num_calibration_samples: Optional[int] = field(
         default=512,
         metadata={"help": "Number of samples to use for one-shot calibration"},
     )
+    shuffle_calibration_samples: Optional[bool] = field(
+        default=True,
+        metadata={
+            "help": "whether to shuffle the dataset before selecting calibration data"
+        },
+    )
     streaming: Optional[bool] = field(
         default=False,
         metadata={"help": "True to stream data from a cloud dataset"},
     )
     overwrite_cache: bool = field(
         default=False,
         metadata={"help": "Overwrite the cached preprocessed datasets or not."},
```

## sparseml/transformers/finetune/data/data_helpers.py

```diff
@@ -14,15 +14,15 @@
 
 import logging
 import os
 from typing import Any, Callable, Dict, List, Optional
 
 import torch
 from datasets import Dataset, load_dataset
-from torch.utils.data import DataLoader, RandomSampler
+from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
 from transformers.data import default_data_collator
 
 
 LOGGER = logging.getLogger(__name__)
 LABELS_MASK_VALUE = -100
 
 __all__ = [
@@ -32,47 +32,53 @@
     "get_custom_datasets_from_path",
 ]
 
 
 def format_calibration_data(
     tokenized_dataset: Dataset,
     num_calibration_samples: Optional[int] = None,
+    do_shuffle: bool = True,
     collate_fn: Callable = default_data_collator,
     accelerator: Optional[Any] = None,
 ) -> List[torch.Tensor]:
     """
     Creates a dataloader out of the calibration dataset split, trimming it to
     the desired number of calibration samples
 
     :param tokenized_dataset: dataset to convert to dataloader
     :param num_calibration_samples: number of data samples to convert
+    :param do_shuffle: whether to shuffle the dataset before selecting calibration
+    samples, true by default
     :param collate_fn: optional custom collate function, or use default
     :param accelerator: optional accelerator for if preparing in FSDP mode
     :return: list of trimmed calibration data tensors
     """
     safe_calibration_samples = len(tokenized_dataset)
     if num_calibration_samples is not None:
         safe_calibration_samples = min(len(tokenized_dataset), num_calibration_samples)
         if safe_calibration_samples != num_calibration_samples:
             LOGGER.warn(
                 f"Requested {num_calibration_samples} calibration samples but "
                 f"the provided dataset only has {safe_calibration_samples}. "
             )
 
-    shuffled_calibration = tokenized_dataset.shuffle()
-    shuffled_calibration = shuffled_calibration.select(range(safe_calibration_samples))
+    if do_shuffle:
+        tokenized_dataset = tokenized_dataset.shuffle()
+    tokenized_calibration = tokenized_dataset.select(range(safe_calibration_samples))
 
     dataloader_params = {
         "batch_size": 1,
-        "sampler": RandomSampler(shuffled_calibration),
+        "sampler": RandomSampler(tokenized_calibration)
+        if do_shuffle
+        else SequentialSampler(tokenized_calibration),
         "collate_fn": collate_fn,
         "pin_memory": True,
     }
 
-    calib_dataloader = DataLoader(shuffled_calibration, **dataloader_params)
+    calib_dataloader = DataLoader(tokenized_calibration, **dataloader_params)
     if accelerator:
         calib_dataloader = accelerator.prepare(calib_dataloader)
 
     return calib_dataloader
 
 
 def get_raw_dataset(
```

## sparseml/transformers/finetune/data/gsm8k.py

```diff
@@ -24,15 +24,15 @@
     Child text generation class for the Grade School Math 8k dataset
 
     :param data_args: configuration settings for dataset loading
     :param split: split from dataset to load, for instance `test` or `train[:5%]`
     :param tokenizer: tokenizer to use on dataset
     """
 
-    GSM_TEMPLATE = "Question: {question}.\nAnswer:"
+    GSM_TEMPLATE = "Question: {question}\nAnswer:"
 
     def __init__(self, data_args, split, tokenizer):
         data_args = deepcopy(data_args)
         data_args.dataset = "gsm8k"
         super().__init__(
             text_column="text", data_args=data_args, split=split, tokenizer=tokenizer
         )
```

## sparseml/transformers/sparsification/__init__.py

```diff
@@ -15,13 +15,14 @@
 """
 Objects, classes, and methods for applying sparsification algorithms to
 Hugging Face transformers flows
 """
 
 # flake8: noqa
 
+from .modification import *
 from .question_answering import *
 from .sparse_config import *
 from .sparse_model import *
 from .sparse_tokenizer import *
 from .trainer import *
 from .training_args import *
```

## sparseml/transformers/sparsification/sparse_model.py

```diff
@@ -26,27 +26,29 @@
     AutoModelForQuestionAnswering,
     AutoModelForSequenceClassification,
     AutoModelForTokenClassification,
     PreTrainedModel,
 )
 from transformers.file_utils import WEIGHTS_NAME
 
+from compressed_tensors.compressors import ModelCompressor
+from compressed_tensors.quantization import (
+    QuantizationConfig,
+    apply_quantization_config,
+    load_pretrained_quantization,
+)
+from sparseml.modifiers.quantization.modification import modify_model
 from sparseml.pytorch.model_load.helpers import (
     apply_recipe_structure_to_model,
     log_model_load,
 )
-from sparseml.transformers.compression.utils import (
-    get_safetensors_folder,
-    infer_compressor_from_model_config,
+from sparseml.transformers.sparsification.compressed_tensors_utils import (
     modify_save_pretrained,
 )
-from sparseml.transformers.sparsification.modification import modify_model
-from sparseml.transformers.utils.helpers import resolve_recipe
-from sparseml.utils import download_zoo_training_dir
-from sparseml.utils.fsdp.context import main_process_first_context
+from sparseml.transformers.utils.helpers import download_model_directory, resolve_recipe
 
 
 __all__ = ["SparseAutoModel", "SparseAutoModelForCausalLM", "get_shared_tokenizer_src"]
 
 
 _LOGGER = logging.getLogger(__name__)
 
@@ -57,19 +59,17 @@
     Its lifecycle is defined as follows:
     1. If pretrained_model_name_or_path is a SparseZoo stub
        the appropriate SparseZoo model will be downloaded
        (if required) and the path to the deployment directory
        of the model will be retrieved
     2. The original model definition will be loaded, without
         the model weights
-    3. The model will be potentially modifier by `modify_model`
-        function, so that is compatible with SparseML
-    4. The appropriate recipy will be applied to the model
+    3. The appropriate recipy will be applied to the model
        if requested or required
-    5. The appropriate set of weights will be loaded into the model
+    4. The appropriate set of weights will be loaded into the model
     """
 
     @classmethod
     def from_pretrained(
         cls,
         pretrained_model_name_or_path,
         recipe: Optional[Union[str, Path]] = None,
@@ -97,57 +97,58 @@
 
         pretrained_model_name_or_path = (
             pretrained_model_name_or_path.as_posix()
             if isinstance(pretrained_model_name_or_path, Path)
             else pretrained_model_name_or_path
         )
 
-        if pretrained_model_name_or_path.startswith("zoo:"):
-            _LOGGER.debug(
-                "Passed zoo stub to SparseAutoModelForCausalLM object. "
-                "Loading model from SparseZoo training files..."
-            )
-            with main_process_first_context():
-                pretrained_model_name_or_path = download_zoo_training_dir(
-                    zoo_stub=pretrained_model_name_or_path
-                )
+        pretrained_model_name_or_path = download_model_directory(
+            pretrained_model_name_or_path, **kwargs
+        )
 
         # determine compression format, if any, from the model config
-        compressor = infer_compressor_from_model_config(pretrained_model_name_or_path)
+        compressor = ModelCompressor.from_pretrained(pretrained_model_name_or_path)
+        quantization_config = QuantizationConfig.from_model_config(
+            pretrained_model_name_or_path
+        )
 
         # temporarily set the log level to error, to ignore printing out long missing
         # and unexpected key error messages (these are EXPECTED for quantized models)
         logger = logging.getLogger("transformers.modeling_utils")
         restore_log_level = logger.getEffectiveLevel()
         logger.setLevel(level=logging.ERROR)
         model = super(AutoModelForCausalLM, cls).from_pretrained(
             pretrained_model_name_or_path, *model_args, **kwargs
         )
         logger.setLevel(level=restore_log_level)
-        model = modify_model(model)
         # override the PreTrainedModel instance with compression save function
         modify_save_pretrained(model)
 
         # If model is compressed on disk, decompress and load the weights
         if compressor is not None:
-            # if we loaded from a HF stub, find the cached model
-            model_path = get_safetensors_folder(
-                pretrained_model_name_or_path, cache_dir=kwargs.get("cache_dir", None)
-            )
-
             # decompress weights
-            compressor.overwrite_weights(model_path=model_path, model=model)
+            compressor.overwrite_weights(
+                model_path=pretrained_model_name_or_path, model=model
+            )
 
-        recipe = resolve_recipe(recipe=recipe, model_path=pretrained_model_name_or_path)
-        if recipe:
-            apply_recipe_structure_to_model(
-                model=model,
-                model_path=pretrained_model_name_or_path,
-                recipe_path=recipe,
+        if quantization_config is not None:
+            # if we loaded from a HF stub, find the cached model
+            apply_quantization_config(model, quantization_config)
+            load_pretrained_quantization(model, pretrained_model_name_or_path)
+        else:
+            recipe = resolve_recipe(
+                recipe=recipe, model_path=pretrained_model_name_or_path
             )
+            if recipe:
+                apply_recipe_structure_to_model(
+                    model=model,
+                    model_path=pretrained_model_name_or_path,
+                    recipe_path=recipe,
+                )
+
         return model
 
 
 class SparseAutoModel:
     """
     Factory class for creating sparse models using transformers AutoModel classes
     """
```

## sparseml/transformers/sparsification/modification/__init__.py

```diff
@@ -7,15 +7,21 @@
 #    http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 # flake8: noqa
-from .modify_model import modify_model
-from .modifying_bert import *
-from .modifying_distilbert import *
-from .modifying_llama import *
-from .modifying_mistral import *
-from .modifying_mobilebert import *
-from .modifying_opt import *
+# isort:skip_file
+
+# the modification module that adds modifications
+# for transformers models to enable quantization
+
+# import all the modification functions for the different models
+from .modifying_bert import modify
+from .modifying_llama import modify
+from .modifying_mistral import modify
+from .modifying_distilbert import modify
+from .modifying_mobilebert import modify
+from .modifying_opt import modify
```

## sparseml/transformers/sparsification/modification/base.py

```diff
@@ -20,15 +20,15 @@
 
 
 _LOGGER = logging.getLogger(__name__)
 
 __all__ = ["check_transformers_version"]
 
 _TRANSFORMERS_MIN_VERSION = "4.39.0"
-_TRANSFORMERS_MAX_VERSION = "4.39.2"
+_TRANSFORMERS_MAX_VERSION = "4.39.3"
 
 
 def check_transformers_version(
     module_version_max: str = _TRANSFORMERS_MAX_VERSION,
     model_version_min: str = _TRANSFORMERS_MIN_VERSION,
 ) -> bool:
     """
@@ -52,14 +52,14 @@
     max_version = version.parse(module_version_max)
     min_version = version.parse(model_version_min)
 
     if not (min_version <= current_version <= max_version):
         _LOGGER.warning(
             "Attempting to modify the transformers model to support "
             "the SparseML-specific functionalities. However, the detected "
-            f"transformers version ({current_version}) does not fall within the"
+            f"transformers version ({current_version}) does not fall within the "
             f"supported version range ({min_version} - {max_version}). "
             "This may lead to unexpected behavior. Please ensure that the "
             "correct transformers version is installed."
         )
         return False
     return True
```

## sparseml/transformers/sparsification/modification/modifying_bert.py

```diff
@@ -10,76 +10,67 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """
 Modification to the original Bert model required in the
-context of SparseML
+context of SparseML quantization
 """
 
 
-import logging
 import math
 from typing import Optional, Tuple
 
 import torch
 from torch import nn
-from transformers.models.bert.modeling_bert import BertAttention, BertSelfAttention
+from transformers.models.bert.modeling_bert import BertSelfAttention
 
+from sparseml.modifiers.quantization.modification.modification_objects import QATMatMul
+from sparseml.modifiers.quantization.modification.registry import ModificationRegistry
 from sparseml.pytorch.utils.helpers import swap_modules
-from sparseml.transformers.sparsification.modification.modification_objects import (
-    QATMatMul,
+from sparseml.transformers.sparsification.modification.base import (
+    check_transformers_version,
 )
-from sparseml.transformers.sparsification.modification.registry import (
-    ModificationRegistry,
-)
-
-
-_LOGGER = logging.getLogger(__name__)
 
 
 @ModificationRegistry.register(name="BertModel", alias=["BertForQuestionAnswering"])
 def modify(model: nn.Module) -> nn.Module:
     """
     Modify the Bert model to be compatible with SparseML
+    quantization
 
-    1. Replaces the MultiHeadSelfAttention modules with
-        MultiHeadSelfAttentionWithQuantizableMatmuls modules
-
-    Note: This function will not alter any of the alternatives
-    to the MultiHeadSelfAttention module such as BertAttention
-
+    Replaces the attention modules with
+    MultiHeadSelfAttentionWithQuantizableMatmuls modules
     :param model: the original Bert model
     :return: the modified Bert model
     """
+    check_transformers_version()
     for name, submodule in model.named_modules():
-        if isinstance(submodule, BertSelfAttention):
+        if isinstance(submodule, BertSelfAttention) and not isinstance(
+            submodule, BertSelfAttentionWithQuantizableMatmuls
+        ):
             swap_modules(
                 model, name, BertSelfAttentionWithQuantizableMatmuls(submodule)
             )
-        elif isinstance(submodule, BertAttention):
-            _LOGGER.debug(
-                f"The model contains {submodule.__class__.__name__} "
-                "module, which will not be modified"
-            )
     return model
 
 
 class BertSelfAttentionWithQuantizableMatmuls(BertSelfAttention):
     """
-    Wrapper around the original BertSelfAttention module to replace the
+    Wrapper around the original attention module to replace the
     matmul operations with quantizable matmul operations
 
-    :param bert_self_attention: the original BertSelfAttention module
+    :param bert_self_attention: the original attention module to be
+        wrapped and modified
     """
 
     def __init__(self, bert_self_attention: BertSelfAttention):
         self.__class__ = type(
-            bert_self_attention.__class__.__name__,
+            self.__class__.__name__,
             (self.__class__, bert_self_attention.__class__),
             {},
         )
         self.__dict__ = bert_self_attention.__dict__
 
         self.attention_scores_matmul = QATMatMul()
         self.context_layer_matmul = QATMatMul()
```

## sparseml/transformers/sparsification/modification/modifying_distilbert.py

```diff
@@ -10,78 +10,70 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """
 Modification to the original DistilBert model required in the
-context of SparseML
+context of SparseML quantization
 """
 
-import logging
 import math
 from typing import Optional, Tuple
 
 import torch
 from torch import nn
 from transformers.models.distilbert.modeling_distilbert import (
     DistilBertFlashAttention2,
     MultiHeadSelfAttention,
 )
 
+from sparseml.modifiers.quantization.modification.modification_objects import QATMatMul
+from sparseml.modifiers.quantization.modification.registry import ModificationRegistry
 from sparseml.pytorch.utils.helpers import swap_modules
-from sparseml.transformers.sparsification.modification.modification_objects import (
-    QATMatMul,
+from sparseml.transformers.sparsification.modification.base import (
+    check_transformers_version,
 )
-from sparseml.transformers.sparsification.modification.registry import (
-    ModificationRegistry,
-)
-
-
-_LOGGER = logging.getLogger(__name__)
 
 
 @ModificationRegistry.register(name="DistilBertModel")
 def modify(model: nn.Module) -> nn.Module:
     """
     Modify the DistilBert model to be compatible with SparseML
+    quantization
 
-    1. Replaces the MultiHeadSelfAttention modules with
-        MultiHeadSelfAttentionWithQuantizableMatmuls modules
-
-    Note: This function will not alter any of the alternatives
-    to the MultiHeadSelfAttention module such as DistilBertFlashAttention2
+    Replaces the attention modules with
+    MultiHeadSelfAttentionWithQuantizableMatmuls modules
 
     :param model: the original DistilBert model
     :return: the modified DistilBert model
     """
+    check_transformers_version()
     for name, submodule in model.named_modules():
-        if isinstance(submodule, MultiHeadSelfAttention):
+        if isinstance(
+            submodule, (MultiHeadSelfAttention, DistilBertFlashAttention2)
+        ) and not isinstance(submodule, MultiHeadSelfAttentionWithQuantizableMatmuls):
             swap_modules(
                 model, name, MultiHeadSelfAttentionWithQuantizableMatmuls(submodule)
             )
-        if isinstance(submodule, DistilBertFlashAttention2):
-            _LOGGER.debug(
-                f"The model contains {submodule.__class__.__name__} "
-                "module, which will not be modified"
-            )
     return model
 
 
 class MultiHeadSelfAttentionWithQuantizableMatmuls(MultiHeadSelfAttention):
     """
-    Wrapper around the original MultiHeadSelfAttention module to replace the
-    matmul operations with quantizable matmul operations
+    Wrapper around the original attention module to introduce
+    MultiHeadSelfAttention  with quantizable matmul operations
 
-    :param mhs_attention: the original MultiHeadSelfAttention module
+    :param mhs_attention: the original attention module to be
+        wrapped and modified
     """
 
     def __init__(self, mhs_attention: MultiHeadSelfAttention):
         self.__class__ = type(
-            mhs_attention.__class__.__name__,
+            self.__class__.__name__,
             (self.__class__, mhs_attention.__class__),
             {},
         )
         self.__dict__ = mhs_attention.__dict__
 
         self.attention_scores_matmul = QATMatMul
         self.context_layer_matmul = QATMatMul
```

## sparseml/transformers/sparsification/modification/modifying_llama.py

```diff
@@ -10,69 +10,61 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """
 Modification to the original LLaMa model required in the
-context of SparseML
+context of SparseML quantization
 """
 
-import logging
 import math
-from typing import Optional, Tuple
+from typing import Optional, Tuple, Union
 
 import torch
 import torch.nn.functional as F
 from torch import nn
 from transformers.models.llama.modeling_llama import (
     Cache,
     LlamaAttention,
     LlamaFlashAttention2,
     LlamaSdpaAttention,
     apply_rotary_pos_emb,
     repeat_kv,
 )
 
-from sparseml.pytorch.utils.helpers import swap_modules
-from sparseml.transformers.sparsification.modification.modification_objects import (
+from sparseml.modifiers.quantization.modification.modification_objects import (
     QuantizableIdentity,
     QuantizableMatMul,
 )
-from sparseml.transformers.sparsification.modification.registry import (
-    ModificationRegistry,
+from sparseml.modifiers.quantization.modification.registry import ModificationRegistry
+from sparseml.pytorch.utils.helpers import swap_modules
+from sparseml.transformers.sparsification.modification.base import (
+    check_transformers_version,
 )
 
 
-_LOGGER = logging.getLogger(__name__)
-
-
 @ModificationRegistry.register(name="LlamaModel", alias=["LlamaForCausalLM"])
 def modify(model: nn.Module) -> nn.Module:
     """
     Modify the LLaMa model to be compatible with SparseML
+    quantization
 
-    1. Replaces the LlamaAttention modules with
-        LlamaAttentionWithQuantizableMatmuls modules
-
-    Note: This function will not alter any of the alternatives
-    to the LlamaAttention module such as LlamaFlashAttention2
-    or LlamaSdpaAttention
+    Replaces the attention modules with
+    LlamaAttentionWithQuantizableMatmuls modules
 
     :param model: the original LLaMa model
     :return: the modified LLaMa model
     """
+    check_transformers_version()
     for name, submodule in model.named_modules():
-        if isinstance(submodule, LlamaAttention):
+        if isinstance(
+            submodule, (LlamaAttention, LlamaFlashAttention2, LlamaSdpaAttention)
+        ) and not isinstance(submodule, LlamaAttentionWithQuantizableMatmuls):
             swap_modules(model, name, LlamaAttentionWithQuantizableMatmuls(submodule))
-        elif isinstance(submodule, (LlamaSdpaAttention, LlamaFlashAttention2)):
-            _LOGGER.debug(
-                f"The model contains {submodule.__class__.__name__} "
-                "module, which will not be modified"
-            )
     return model
 
 
 class MatMulLeftInput_QK(QuantizableIdentity):
     ...
 
 
@@ -94,23 +86,29 @@
 
 class MatMulOutput_PV(QuantizableIdentity):
     ...
 
 
 class LlamaAttentionWithQuantizableMatmuls(LlamaAttention):
     """
-    Wrapper around the original LlamaAttention module to replace the
-    matmul operations with quantizable matmul operations
+    Wrapper around the original attention module to introduce
+    LlamaAttention with quantizable matmul operations
 
-    :param llama_attention: the original LlamaAttention module
+    :param llama_attention: the original attention module to be
+        wrapped and modified
     """
 
-    def __init__(self, llama_attention: LlamaAttention):
+    def __init__(
+        self,
+        llama_attention: Union[
+            LlamaAttention, LlamaFlashAttention2, LlamaSdpaAttention
+        ],
+    ):
         self.__class__ = type(
-            llama_attention.__class__.__name__,
+            self.__class__.__name__,
             (self.__class__, llama_attention.__class__),
             {},
         )
         self.__dict__ = llama_attention.__dict__
 
         self.attn_weights_matmul = QuantizableMatMul(
             MatMulLeftInput_QK, MatMulRightInput_QK, MatMulOutput_QK
```

## sparseml/transformers/sparsification/modification/modifying_mistral.py

```diff
@@ -10,68 +10,61 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """
 Modification to the original Mistral model required in the
-context of SparseML
+context of SparseML quantization
 """
-import logging
+
 import math
 import warnings
-from typing import Optional, Tuple
+from typing import Optional, Tuple, Union
 
 import torch
 from torch import nn
 from transformers.models.mistral.modeling_mistral import (
     Cache,
     MistralAttention,
     MistralFlashAttention2,
     MistralSdpaAttention,
     apply_rotary_pos_emb,
     repeat_kv,
 )
 
-from sparseml.pytorch.utils.helpers import swap_modules
-from sparseml.transformers.sparsification.modification.modification_objects import (
+from sparseml.modifiers.quantization.modification.modification_objects import (
     QuantizableIdentity,
     QuantizableMatMul,
 )
-from sparseml.transformers.sparsification.modification.registry import (
-    ModificationRegistry,
+from sparseml.modifiers.quantization.modification.registry import ModificationRegistry
+from sparseml.pytorch.utils.helpers import swap_modules
+from sparseml.transformers.sparsification.modification.base import (
+    check_transformers_version,
 )
 
 
-_LOGGER = logging.getLogger(__name__)
-
-
 @ModificationRegistry.register(name="MistralModel", alias=["MistralForCausalLM"])
 def modify(model: torch.nn.Module) -> torch.nn.Module:
     """
     Modify the Mistral model to be compatible with SparseML
+    quantization
 
-    1. Replaces the MistralAttention modules with
-        MistralAttentionWithQuantizableMatmuls modules
-
-    Note: This function will not alter any of the alternatives
-    to the MistralAttention module such as MistralFlashAttention2
-    or MistralSdpaAttention
+    Replaces the attention modules with
+    MistralAttentionWithQuantizableMatmuls modules
 
     :param model: the original Mistral model
     :return: the modified Mistral model
     """
+    check_transformers_version()
     for name, submodule in model.named_modules():
-        if isinstance(submodule, MistralAttention):
+        if isinstance(
+            submodule, (MistralAttention, MistralFlashAttention2, MistralSdpaAttention)
+        ) and not isinstance(submodule, MistralAttentionWithQuantizableMatmuls):
             swap_modules(model, name, MistralAttentionWithQuantizableMatmuls(submodule))
-        if isinstance(submodule, (MistralSdpaAttention, MistralFlashAttention2)):
-            _LOGGER.debug(
-                f"The model contains {submodule.__class__.__name__} "
-                "module, which will not be modified"
-            )
     return model
 
 
 class MatMulLeftInput_QK(QuantizableIdentity):
     ...
 
 
@@ -85,24 +78,30 @@
 
 class MatMulRightInput_PV(QuantizableIdentity):
     ...
 
 
 class MistralAttentionWithQuantizableMatmuls(MistralAttention):
     """
-    Wrapper around the original MistralAttention module to replace the
-    matmul operations with quantizable matmul operations
+    Wrapper around the original attention module to introduce
+    MistralAttention with quantizable matmul operations
 
-    :param mistral_attention: the original MistralAttention module
+    :param mistral_attention: the original attention module to be
+        wrapped and modified
 
     """
 
-    def __init__(self, mistral_attention: MistralAttention):
+    def __init__(
+        self,
+        mistral_attention: Union[
+            MistralAttention, MistralFlashAttention2, MistralSdpaAttention
+        ],
+    ):
         self.__class__ = type(
-            mistral_attention.__class__.__name__,
+            self.__class__.__name__,
             (self.__class__, mistral_attention.__class__),
             {},
         )
         self.__dict__ = mistral_attention.__dict__
 
         self.attn_weights_matmul = QuantizableMatMul(
             MatMulLeftInput_QK, MatMulRightInput_QK
```

## sparseml/transformers/sparsification/modification/modifying_mobilebert.py

```diff
@@ -10,47 +10,45 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """
 Modification to the original MobileBert model required in the
-context of SparseML
+context of SparseML quantization
 """
 
-import logging
-
 from torch import nn
 from transformers.models.mobilebert.modeling_mobilebert import MobileBertEmbeddings
 
+from sparseml.modifiers.quantization.modification.modification_objects import QATLinear
+from sparseml.modifiers.quantization.modification.registry import ModificationRegistry
 from sparseml.pytorch.utils.helpers import swap_modules
-from sparseml.transformers.sparsification.modification.modification_objects import (
-    QATLinear,
-)
-from sparseml.transformers.sparsification.modification.registry import (
-    ModificationRegistry,
+from sparseml.transformers.sparsification.modification.base import (
+    check_transformers_version,
 )
 
 
-_LOGGER = logging.getLogger(__name__)
-
-
 @ModificationRegistry.register(name="MobileBertModel")
 def modify(model: nn.Module) -> nn.Module:
     """
     Modify the MobileBert model to be compatible with SparseML
+    quantization
 
-    1. Replaces the MobileBertEmbeddings modules with
-        MobileBertEmbeddingsWithQuantizableMatmuls modules
+    Replaces the MobileBertEmbeddings modules with
+    MobileBertEmbeddingsWithQuantizableMatmuls modules
 
     :param model: the original MobileBert model
     :return: the modified MobileBert model
     """
+    check_transformers_version()
     for name, submodule in model.named_modules():
-        if isinstance(submodule, MobileBertEmbeddings):
+        if isinstance(submodule, MobileBertEmbeddings) and not isinstance(
+            submodule, MobileBertEmbeddingsWithQuantizableLinear
+        ):
             swap_modules(
                 model, name, MobileBertEmbeddingsWithQuantizableLinear(submodule)
             )
     return model
 
 
 class MobileBertEmbeddingsWithQuantizableLinear(MobileBertEmbeddings):
@@ -59,15 +57,15 @@
     linear projection with a quantizable linear projection
 
     :param mobilebert_emb: the original MobileBertEmbeddings module
     """
 
     def __init__(self, mobilebert_emb: MobileBertEmbeddings):
         self.__class__ = type(
-            mobilebert_emb.__class__.__name__,
+            self.__class__.__name__,
             (self.__class__, mobilebert_emb.__class__),
             {},
         )
         self.__dict__ = mobilebert_emb.__dict__
 
         embed_dim_multiplier = 3 if self.trigram_input else 1
         embedded_input_size = self.embedding_size * embed_dim_multiplier
```

## sparseml/transformers/sparsification/modification/modifying_opt.py

```diff
@@ -10,59 +10,52 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """
 Modification to the original OPT model required in the
-context of SparseML
+context of SparseML quantization
 """
 
-import logging
-from typing import Optional, Tuple
+from typing import Optional, Tuple, Union
 
 import torch
 from torch import nn
 from transformers.models.opt.modeling_opt import OPTAttention, OptFlashAttention2
 
-from sparseml.pytorch.utils.helpers import swap_modules
-from sparseml.transformers.sparsification.modification.modification_objects import (
+from sparseml.modifiers.quantization.modification.modification_objects import (
     QuantizableBatchMatmul,
     QuantizableIdentity,
 )
-from sparseml.transformers.sparsification.modification.registry import (
-    ModificationRegistry,
+from sparseml.modifiers.quantization.modification.registry import ModificationRegistry
+from sparseml.pytorch.utils.helpers import swap_modules
+from sparseml.transformers.sparsification.modification.base import (
+    check_transformers_version,
 )
 
 
-_LOGGER = logging.getLogger(__name__)
-
-
 @ModificationRegistry.register(name="OPTModel", alias=["OPTForCausalLM"])
 def modify(model: nn.Module) -> nn.Module:
     """
     Modify the OPT model to be compatible with SparseML
+    quantization
 
-    1. Replaces the OPTAttention modules with
-        OPTAttentionWithQuantizableMatmuls modules
-
-    Note: This function will not alter any of the alternatives
-    to the OPTAttention module such as OptFlashAttention2
+    Replaces the OPT attention modules with
+    OPTAttentionWithQuantizableMatmuls modules
 
-    :param model: the original LLaMa model
-    :return: the modified LLaMa model
+    :param model: the original OPT model
+    :return: the modified OPT model
     """
+    check_transformers_version()
     for name, submodule in model.named_modules():
-        if isinstance(submodule, OPTAttention):
+        if isinstance(submodule, (OPTAttention, OptFlashAttention2)) and not isinstance(
+            submodule, OPTAttentionWithQuantizableMatmuls
+        ):
             swap_modules(model, name, OPTAttentionWithQuantizableMatmuls(submodule))
-        elif isinstance(submodule, OptFlashAttention2):
-            _LOGGER.debug(
-                f"The model contains {submodule.__class__.__name__} "
-                "module, which will not be modified"
-            )
     return model
 
 
 class BMMLeftInput_QK(QuantizableIdentity):
     ...
 
 
@@ -84,23 +77,24 @@
 
 class BMMOutput_PV(QuantizableIdentity):
     ...
 
 
 class OPTAttentionWithQuantizableMatmuls(OPTAttention):
     """
-    Wrapper around the original OPTAttention module to replace the
-    matmul operations with quantizable matmul operations
+    Wrapper around the original attention module to introduce
+    OPTAttention with quantizable matmul operations
 
-    :param opt_attention: the original OPTAttention module
+    :param opt_attention: the original attention module to be
+        wrapped and modified
     """
 
-    def __init__(self, opt_attention: OPTAttention):
+    def __init__(self, opt_attention: Union[OptFlashAttention2, OPTAttention]):
         self.__class__ = type(
-            opt_attention.__class__.__name__,
+            self.__class__.__name__,
             (self.__class__, opt_attention.__class__),
             {},
         )
         self.__dict__ = opt_attention.__dict__
 
         self.attn_weights_bmm = QuantizableBatchMatmul(
             BMMLeftInput_QK, BMMRightInput_QK, BMMOutput_QK
```

## sparseml/transformers/utils/helpers.py

```diff
@@ -30,16 +30,18 @@
 import requests
 import torch
 import transformers
 from transformers import AutoConfig
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import PaddingStrategy
 
-from huggingface_hub import HUGGINGFACE_CO_URL_HOME, hf_hub_download
+from huggingface_hub import HUGGINGFACE_CO_URL_HOME, HfFileSystem, hf_hub_download
 from sparseml.export.helpers import ONNX_MODEL_NAME
+from sparseml.utils import download_zoo_training_dir
+from sparseml.utils.fsdp.context import main_process_first_context
 from sparsezoo import Model, setup_model
 
 
 _LOGGER = logging.getLogger(__name__)
 
 
 __all__ = [
@@ -48,14 +50,16 @@
     "detect_last_checkpoint",
     "TaskNames",
     "is_transformer_model",
     "resolve_sequence_length",
     "ALL_TASK_NAMES",
     "create_fake_dataloader",
     "POSSIBLE_TOKENIZER_FILES",
+    "download_repo_from_huggingface_hub",
+    "download_model_directory",
 ]
 
 
 class TaskNames(Enum):
     mlm = {"masked-language-modeling", "mlm"}
     qa = {"question-answering", "qa"}
     token_classification = {"token-classification", "ner"}
@@ -67,15 +71,15 @@
     }
     text_generation = {"text-generation"}
 
 
 ALL_TASK_NAMES = list(set.union(*[task_names.value for task_names in TaskNames]))
 ONNX_MODEL_NAME_INTERMEDIATE = "model-orig.onnx"
 RECIPE_NAME = "recipe.yaml"
-SPARSITY_CONFIG_NAME = "sparsity_config"
+
 MANDATORY_DEPLOYMENT_FILES = {
     ONNX_MODEL_NAME,
     "tokenizer_config.json",
     "config.json",
 }
 OPTIONAL_DEPLOYMENT_FILES = {"tokenizer.json", "tokenizer.model"}
 NLG_MANDATORY_DEPLOYMENT_FILES = {"special_tokens_map.json"}
@@ -88,30 +92,15 @@
     "vocab.json",
     "merges.txt",
     "tokenizer.json",
     "tokenizer.model",
     "special_tokens_map.json",
     "tokenizer_config.json",
 }
-
-
-def remove_past_key_value_support_from_config(config: AutoConfig) -> AutoConfig:
-    """
-    Modify config of the causal language model so that it turns off the
-    past key value support. This means that the model initialized from
-    this config will not take past key values as input and will not output
-    past key values.
-    """
-    # not take past_key_values as input
-    config.is_decoder = True
-    # whether to use past key values an input
-    config.use_past = False
-    # whether to output past key values
-    config.use_cache = False
-    return config
+RELEVANT_HF_SUFFIXES = ["json", "md", "bin", "safetensors", "yaml", "yml", "py"]
 
 
 def is_transformer_model(source_path: Union[Path, str]) -> bool:
     """
     :param source_path: The path to the model
     :return: Whether the model is a transformers model or not
     """
@@ -358,19 +347,16 @@
 
     _LOGGER.info(
         "model_path is a huggingface model id. "
         "Attempting to download recipe from "
         f"{HUGGINGFACE_CO_URL_HOME}"
     )
     try:
-        _LOGGER.info(
-            f"Found recipe: {recipe_name} for model id: "
-            f"{model_path}. Downloading..."
-        )
         recipe = hf_hub_download(repo_id=model_path, filename=recipe_name)
+        _LOGGER.info(f"Found recipe: {recipe_name} for model id: {model_path}.")
     except Exception as e:
         _LOGGER.info(
             f"Unable to to find recipe {recipe_name} "
             f"for model id: {model_path}: {e}. "
             "Skipping recipe resolution."
         )
         recipe = None
@@ -552,7 +538,98 @@
 
     # target is a HuggingFace stub
     with suppress(Exception):
         # suppress any errors if the recipe is not found on HuggingFace
         recipe_path = hf_hub_download(repo_id=target, filename=DEFAULT_RECIPE_NAME)
 
     return recipe_path
+
+
+def download_repo_from_huggingface_hub(repo_id, **kwargs):
+    """
+    Download relevant model files from the Hugging Face Hub
+    using the huggingface_hub.hf_hub_download function
+
+    Note(s):
+    - Does not download the entire repo, only the relevant files
+    for the model, such as the model weights, tokenizer files, etc.
+    - Does not re-download files that already exist locally, unless
+    the force_download flag is set to True
+
+    :pre-condition: the repo_id must be a valid Hugging Face Hub repo id
+    :param repo_id: the repo id to download
+    :param kwargs: additional keyword arguments to pass to hf_hub_download
+    """
+    hf_filesystem = HfFileSystem()
+    files = hf_filesystem.ls(repo_id)
+
+    if not files:
+        raise ValueError(f"Could not find any files in HF repo {repo_id}")
+
+    # All file(s) from hf_filesystem have "name" key
+    # Extract the file names from the files
+    relevant_file_names = (
+        Path(file["name"]).name
+        for file in files
+        if any(file["name"].endswith(suffix) for suffix in RELEVANT_HF_SUFFIXES)
+    )
+
+    hub_kwargs_names = (
+        "subfolder",
+        "repo_type",
+        "revision",
+        "library_name",
+        "library_version",
+        "cache_dir",
+        "local_dir",
+        "local_dir_use_symlinks",
+        "user_agent",
+        "force_download",
+        "force_filename",
+        "proxies",
+        "etag_timeout",
+        "resume_download",
+        "token",
+        "local_files_only",
+        "headers",
+        "legacy_cache_layout",
+        "endpoint",
+    )
+    hub_kwargs = {name: kwargs[name] for name in hub_kwargs_names if name in kwargs}
+
+    for file_name in relevant_file_names:
+        last_file = hf_hub_download(repo_id=repo_id, filename=file_name, **hub_kwargs)
+
+    # parent directory of the last file is the model directory
+    return str(Path(last_file).parent.resolve().absolute())
+
+
+def download_model_directory(pretrained_model_name_or_path: str, **kwargs):
+    """
+    Download the model directory from the HF hub or SparseZoo if the model
+    is not found locally
+
+    :param pretrained_model_name_or_path: the name of or path to the model to load
+        can be a SparseZoo/HuggingFace model stub
+    :param kwargs: additional keyword arguments to pass to the download function
+    :return: the path to the downloaded model directory
+    """
+    pretrained_model_path: Path = Path(pretrained_model_name_or_path)
+
+    if pretrained_model_path.exists():
+        _LOGGER.debug(
+            "Model directory already exists locally.",
+        )
+        return pretrained_model_name_or_path
+
+    with main_process_first_context():
+        if pretrained_model_name_or_path.startswith("zoo:"):
+            _LOGGER.debug(
+                "Passed zoo stub to SparseAutoModelForCausalLM object. "
+                "Loading model from SparseZoo training files..."
+            )
+            return download_zoo_training_dir(zoo_stub=pretrained_model_name_or_path)
+
+        _LOGGER.debug("Downloading model from HuggingFace Hub.")
+        return download_repo_from_huggingface_hub(
+            repo_id=pretrained_model_name_or_path, **kwargs
+        )
```

## sparseml/utils/fsdp/helpers.py

```diff
@@ -159,22 +159,25 @@
     :param save_compressed: whether to compress sparse weights on disk
     """
     with FullyShardedDataParallel.state_dict_type(
         model, StateDictType.FULL_STATE_DICT, full_state_dict_config
     ):
         state_dict = accelerator.get_state_dict(model, unwrap=False)
 
-    accelerator.unwrap_model(model).save_pretrained(
-        output_dir,
-        is_main_process=accelerator.is_main_process,
-        save_function=accelerator.save,
-        state_dict=state_dict,
-        save_compressed=save_compressed,
-        safe_serialization=save_safetensors,
-    )
+    if accelerator.is_main_process:
+        accelerator.unwrap_model(model).save_pretrained(
+            output_dir,
+            is_main_process=accelerator.is_main_process,
+            save_function=accelerator.save,
+            state_dict=state_dict,
+            save_compressed=save_compressed,
+            safe_serialization=save_safetensors,
+        )
+
+    accelerator.wait_for_everyone()
 
 
 def get_fsdp_parent(layer_name: str, model: Module) -> Optional[Module]:
     """
     Gets the closest parent of layer_name that is wrapped by FSDP. If no FSDP wrapper
     is found just return None
```

## sparseml/utils/pytorch/module.py

```diff
@@ -21,14 +21,15 @@
 from typing import Dict, List, Optional, Tuple, Union
 
 import torch
 from packaging import version
 from torch.nn import Linear, Module, Parameter
 from torch.nn.modules.conv import _ConvNd
 
+from compressed_tensors.quantization.utils import is_module_quantized
 from sparseml.core.model.base import ModelParameterizedLayer
 from sparseml.utils.fsdp.context import fix_fsdp_module_name, summon_full_params_context
 
 
 try:
     quant_err = None
     from torch.nn.qat import Conv2d as QATConv2d
@@ -279,14 +280,16 @@
 
     :param module: PyTorch model to check for quantization
     :return: True if quantization is active anywhere in the model, False otherwise
     """
     for _, layer in module.named_modules():
         if isinstance(layer, torch.quantization.FakeQuantize):
             return True
+        if is_module_quantized(layer):
+            return True
 
     return False
 
 
 def get_layers_params(
     targets: Union[str, List[str]], module: Module
 ) -> Dict[str, ModelParameterizedLayer[Parameter, Module]]:
```

## Comparing `sparseml/transformers/compression/compressors/__init__.py` & `sparseml/modifiers/quantization_vllm/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -10,10 +10,8 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 # flake8: noqa
 
-from .base import ModelCompressor
-from .dense import DenseCompressor
-from .sparse_bitmask import BitmaskCompressor
+from .base import *
```

## Comparing `sparseml/transformers/compression/config/__init__.py` & `sparseml/modifiers/quantization/modification/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -7,13 +7,9 @@
 #    http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
 # flake8: noqa
-
-from .base import CompressionConfig
-from .dense import DenseSparsityConfig
-from .sparse_bitmask import BitmaskConfig
+from .modify_model import modify_model
```

## Comparing `sparseml/transformers/compression/config/sparse_bitmask.py` & `sparseml/modifiers/quantization/modification/registry.py`

 * *Files 25% similar despite different names*

```diff
@@ -7,30 +7,16 @@
 #    http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+from sparsezoo.utils.registry import RegistryMixin
 
-from typing import Optional
 
-from sparseml.transformers.compression.config import CompressionConfig
-
-
-__all__ = ["BitmaskConfig"]
-
-
-@CompressionConfig.register(name="sparse_bitmask")
-class BitmaskConfig(CompressionConfig):
+class ModificationRegistry(RegistryMixin):
     """
-    Configuration for storing a sparse model using
-    bitmask compression
-
-    :param global_sparsity: average sparsity of the entire model
-    :param sparsity_structure: structure of the sparsity, such as
-    "unstructured", "2:4", "8:16" etc
+    A registry for modification functions that can be applied to models
+    so that they can be compatible with the quantization format required by the
+    SparseML library.
     """
-
-    format: str = "sparse_bitmask"
-    global_sparsity: Optional[float] = 0.0
-    sparsity_structure: Optional[str] = "unstructured"
```

## Comparing `sparseml/transformers/compression/utils/compress_save.py` & `sparseml/transformers/sparsification/compressed_tensors_utils.py`

 * *Files 19% similar despite different names*

```diff
@@ -18,17 +18,23 @@
 import weakref
 from functools import wraps
 from typing import Optional
 
 from transformers import PreTrainedModel
 from transformers.file_utils import CONFIG_NAME
 
-from sparseml.transformers.compression.compressors import ModelCompressor
-from sparseml.transformers.compression.config import CompressionConfig
-from sparseml.transformers.utils.helpers import SPARSITY_CONFIG_NAME
+from compressed_tensors import (
+    QUANTIZATION_CONFIG_NAME,
+    SPARSITY_CONFIG_NAME,
+    CompressionConfig,
+    ModelCompressor,
+    QuantizationConfig,
+)
+from compressed_tensors.quantization.utils import is_model_quantized
+from sparseml.transformers.compression.sparsity_config import SparsityConfigMetadata
 from sparseml.utils.pytorch import qat_active
 
 
 _LOGGER = logging.getLogger(__name__)
 
 __all__ = ["modify_save_pretrained"]
 
@@ -70,53 +76,90 @@
             :param save_compresed: whether or not to compress the model on disk
             :param skip_compression_stats: whether to skip the calculation of
             compression statistics (such as global sparsity and sparsity structure) when
             saving a model in dense format
             :param kwargs: additional kwargs to pass on to model.save_pretrained
             """
             model = model_ref()
+            # state_dict gets passed in as a kwarg for FSDP models
+            state_dict = kwargs.get("state_dict", None)
 
-            if qat_active(model):
+            # check if we are in the old quantization framework
+            if qat_active(model) and not is_model_quantized(model):
                 _LOGGER.info(
-                    "Compression for quantized models is not yet supported. Save will "
-                    "be run without compression and no sparsity statistics will be "
-                    "calculated."
+                    "Compression for models quantized with QuantizationModifer is not "
+                    "supported. Save will be run without compression and no sparsity "
+                    "statistics will be calculated. To save a quantized model in a "
+                    "compressed state please use vLLMQuantizationModifier instead."
                 )
-                return original_save_pretrained.__get__(model, model_class)(
+
+                original_save_pretrained.__get__(model, model_class)(
+                    save_directory, **kwargs
+                )
+
+                return
+
+            elif qat_active(model):  # quantized in new framework
+                _LOGGER.info(
+                    "Sparsity compression for quantized models is not yet supported. "
+                    "No sparsity statistics will be calculated and no sparsity config "
+                    "will be saved."
+                )
+
+                original_save_pretrained.__get__(model, model_class)(
                     save_directory, **kwargs
                 )
 
+                quant_config = QuantizationConfig.from_pretrained(model)
+                quant_config_data = quant_config.model_dump(exclude_unset=True)
+                config_file_path = os.path.join(save_directory, CONFIG_NAME)
+
+                # add the sparsity config to the model's config file
+                with open(config_file_path, "r") as config_file:
+                    config_data = json.load(config_file)
+                config_data[QUANTIZATION_CONFIG_NAME] = quant_config_data
+                with open(config_file_path, "w") as config_file:
+                    json.dump(config_data, config_file, indent=2, sort_keys=True)
+
+                return
+
             if sparsity_config is not None:
-                sparsity_config.fill_config_details(model)
+                sparsity_config.global_sparsity = (
+                    SparsityConfigMetadata.infer_global_sparsity(
+                        model, state_dict=state_dict
+                    )
+                )
+                sparsity_config.sparsity_structure = (
+                    SparsityConfigMetadata.infer_sparsity_structure()
+                )
+
             elif not skip_compression_stats:
                 # try to infer a sparsity config from the model if none is provided
                 _LOGGER.info(
                     "Inferring a sparsity configuration requires a global sparsity "
                     "calculation. This can be costly for large models. To skip the "
                     "calculation of compression statistics set "
                     "skip_compression_stats=True"
                 )
-                sparsity_config = CompressionConfig.infer_config_from_model(
-                    model, compress=save_compressed
+                sparsity_config = SparsityConfigMetadata.from_pretrained(
+                    model, state_dict=state_dict, compress=save_compressed
                 )
 
             if sparsity_config is None:
                 # model is not sparse, save as dense
                 return original_save_pretrained.__get__(model, model_class)(
                     save_directory, **kwargs
                 )
 
             # if we've gotten to this point we have a config so we can run compression
             kwargs["safe_serialization"] = True
             compressor = ModelCompressor.load_from_registry(
                 sparsity_config.format, config=sparsity_config
             )
 
-            # state_dict gets passed in as a kwarg for FSDP models
-            state_dict = kwargs.get("state_dict", None)
             if state_dict is None:
                 state_dict = model.state_dict()
 
             # make sure we're on the main process when saving
             if state_dict is not None and len(state_dict) > 0:
                 compressed_state_dict = compressor.compress(state_dict)
                 kwargs["state_dict"] = compressed_state_dict
```

## Comparing `sparseml/transformers/sparsification/modification/modification_objects.py` & `sparseml/modifiers/quantization/modification/modification_objects.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """
 Set of helper objects that are used to modify
-the HuggingFace transformer models
+the quantized models
 """
 
 import torch
 
 
 __all__ = [
     "QuantizableIdentity",
```

## Comparing `sparseml/transformers/sparsification/modification/modify_model.py` & `sparseml/modifiers/quantization/modification/modify_model.py`

 * *Files 20% similar despite different names*

```diff
@@ -11,60 +11,63 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import logging
 import os
 
-import torch
-
-from sparseml.transformers.sparsification.modification.registry import (
-    ModificationRegistry,
-)
+from sparseml.modifiers.quantization.modification.registry import ModificationRegistry
 
 
 _LOGGER = logging.getLogger(__name__)
 
 
-def modify_model(model: torch.nn.Module, disable: int = False) -> torch.nn.Module:
+def modify_model(
+    model: "torch.nn.Module", disable: bool = False  # noqa: F821
+) -> "torch.nn.Module":  # noqa: F821
     """
-    Modify the original transformers model so that it is
-    compatible with the SparseML library.
+    Modify the original model so that it is
+    compatible with the quantization format required by the
+    SparseML library.
+
     The model will be modified, if there exist a modification
     function for the model in the registry of modifications.
     Otherwise, the original model will be returned.
 
-    :param model: The original HuggingFace transformers model
-    :return: The potentially modified model
+    :param model: The original model to be modified
+    :param disable: If True, the modification will be disabled
+    :return: The potentially modified model to support
+        SparseML quantization
     """
     model_name = model.__class__.__name__
-    NM_DISABLE_TRANSFORMERS_MODIFICATION = os.environ.get(
-        "NM_DISABLE_TRANSFORMERS_MODIFICATION", "False"
+    NM_DISABLE_QUANTIZATION_MODIFICATION = os.environ.get(
+        "NM_DISABLE_QUANTIZATION_MODIFICATION", "False"
     ).lower() in ["true", "1"]
+
     try:
         modification_func = ModificationRegistry.get_value_from_registry(model_name)
     except KeyError:
         _LOGGER.debug(
             f"No modification function found for the model {model_name}. "
             "Returning the original model. Available modification functions"
             f"are available for models: {ModificationRegistry.registered_names()}"
         )
         return model
 
-    if NM_DISABLE_TRANSFORMERS_MODIFICATION:
+    if NM_DISABLE_QUANTIZATION_MODIFICATION:
         _LOGGER.debug(
             "Application of the modification function to model "
             "disabled through the environment variable."
         )
         return model
 
     if disable:
         _LOGGER.debug(
             "Application of the modification function for to model "
             "disabled through the `disable` argument."
         )
         return model
 
     _LOGGER.info(
-        f"Modifying the model {model_name} to be compatible with SparseML library"
+        f"Modifying the model {model_name} to be compatible with SparseML quantization"
     )
     return modification_func(model)
```

## Comparing `sparseml_nightly-1.8.0.20240404.dist-info/LICENSE` & `sparseml_nightly-1.8.0.20240507.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `sparseml_nightly-1.8.0.20240404.dist-info/LICENSE-ULTRALYTICS` & `sparseml_nightly-1.8.0.20240507.dist-info/LICENSE-ULTRALYTICS`

 * *Files identical despite different names*

## Comparing `sparseml_nightly-1.8.0.20240404.dist-info/METADATA` & `sparseml_nightly-1.8.0.20240507.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,16 @@
 Metadata-Version: 2.1
 Name: sparseml-nightly
-Version: 1.8.0.20240404
+Version: 1.8.0.20240507
 Summary: Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models
 Home-page: https://github.com/neuralmagic/sparseml
 Author: Neuralmagic, Inc.
 Author-email: support@neuralmagic.com
 License: Apache
 Keywords: inference,machine learning,neural network,computer vision,nlp,cv,deep learning,torch,pytorch,tensorflow,keras,sparsity,pruning,deep learning libraries,onnx,quantization,automl
-Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Education
 Classifier: Intended Audience :: Information Technology
 Classifier: Intended Audience :: Science/Research
@@ -24,24 +23,23 @@
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Requires-Python: >=3.8.0,<3.12
 Description-Content-Type: text/markdown
 License-File: LICENSE
 License-File: LICENSE-ULTRALYTICS
 License-File: NOTICE
 Requires-Dist: sparsezoo-nightly ~=1.8.0
-Requires-Dist: setuptools <=59.5.0
 Requires-Dist: pyyaml >=5.0.0
 Requires-Dist: numpy >=1.17.0
 Requires-Dist: matplotlib >=3.0.0
 Requires-Dist: merge-args >=0.1.0
 Requires-Dist: onnx <1.15.0,>=1.5.0
 Requires-Dist: pandas >=0.25.0
 Requires-Dist: packaging >=20.0
 Requires-Dist: psutil >=5.0.0
-Requires-Dist: pydantic <2.0.0,>=1.8.2
+Requires-Dist: pydantic <2.8.0,>=2.0.0
 Requires-Dist: requests >=2.0.0
 Requires-Dist: scikit-learn >=0.24.2
 Requires-Dist: tqdm >=4.0.0
 Requires-Dist: toposort >=1.0
 Requires-Dist: GPUtil >=1.4.0
 Requires-Dist: protobuf <=3.20.3,>=3.12.2
 Requires-Dist: click !=8.0.0,>=7.1.2
@@ -62,38 +60,40 @@
 Requires-Dist: pytest >=6.0.0 ; extra == 'dev'
 Requires-Dist: pytest-mock >=3.6.0 ; extra == 'dev'
 Requires-Dist: pytest-rerunfailures >=13.0 ; extra == 'dev'
 Requires-Dist: tensorboard <2.9,>=1.0 ; extra == 'dev'
 Requires-Dist: tensorboardX >=1.0 ; extra == 'dev'
 Requires-Dist: evaluate >=0.4.1 ; extra == 'dev'
 Requires-Dist: parameterized ; extra == 'dev'
+Requires-Dist: clearml ==1.14.4 ; extra == 'dev'
 Provides-Extra: docs
 Requires-Dist: m2r2 >=0.2.7 ; extra == 'docs'
 Requires-Dist: mistune <3,>=2.0.3 ; extra == 'docs'
 Requires-Dist: myst-parser >=0.14.0 ; extra == 'docs'
 Requires-Dist: rinohtype ~=0.4.2 ; extra == 'docs'
 Requires-Dist: sphinx ~=3.5.0 ; extra == 'docs'
 Requires-Dist: sphinx-copybutton ~=0.3.0 ; extra == 'docs'
 Requires-Dist: sphinx-markdown-tables ~=0.0.15 ; extra == 'docs'
 Requires-Dist: sphinx-multiversion ~=0.2.4 ; extra == 'docs'
 Requires-Dist: sphinx-pydantic ~=0.1.0 ; extra == 'docs'
 Requires-Dist: sphinx-rtd-theme ~=0.5.0 ; extra == 'docs'
 Requires-Dist: docutils <0.17 ; extra == 'docs'
 Provides-Extra: llm
-Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'llm'
+Requires-Dist: torch <2.3,>=1.7.0 ; extra == 'llm'
 Requires-Dist: gputils ; extra == 'llm'
 Requires-Dist: transformers <4.40 ; extra == 'llm'
 Requires-Dist: datasets <2.19 ; extra == 'llm'
 Requires-Dist: dvc ; extra == 'llm'
 Requires-Dist: scikit-learn ; extra == 'llm'
 Requires-Dist: seqeval ; extra == 'llm'
 Requires-Dist: einops ; extra == 'llm'
 Requires-Dist: evaluate >=0.4.1 ; extra == 'llm'
 Requires-Dist: accelerate >=0.20.3 ; extra == 'llm'
 Requires-Dist: safetensors >=0.4.1 ; extra == 'llm'
+Requires-Dist: compressed-tensors ; extra == 'llm'
 Requires-Dist: sentencepiece ; extra == 'llm'
 Provides-Extra: notebook
 Requires-Dist: jupyter >=1.0.0 ; extra == 'notebook'
 Requires-Dist: ipywidgets >=7.0.0 ; extra == 'notebook'
 Provides-Extra: onnxruntime
 Requires-Dist: onnxruntime >=1.0.0 ; extra == 'onnxruntime'
 Provides-Extra: openpifpaf
@@ -106,43 +106,44 @@
 Requires-Dist: tensorboard <2.0.0 ; extra == 'tf_v1'
 Requires-Dist: tf2onnx <1.6,>=1.0.0 ; extra == 'tf_v1'
 Provides-Extra: tf_v1_gpu
 Requires-Dist: tensorflow-gpu <2.0.0 ; extra == 'tf_v1_gpu'
 Requires-Dist: tensorboard <2.0.0 ; extra == 'tf_v1_gpu'
 Requires-Dist: tf2onnx <1.6,>=1.0.0 ; extra == 'tf_v1_gpu'
 Provides-Extra: torch
-Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'torch'
+Requires-Dist: torch <2.3,>=1.7.0 ; extra == 'torch'
 Requires-Dist: gputils ; extra == 'torch'
 Provides-Extra: torch_all
-Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'torch_all'
+Requires-Dist: torch <2.3,>=1.7.0 ; extra == 'torch_all'
 Requires-Dist: gputils ; extra == 'torch_all'
 Requires-Dist: torchvision <0.17,>=0.3.0 ; extra == 'torch_all'
 Requires-Dist: torchaudio <=2.0.1 ; extra == 'torch_all'
 Provides-Extra: torchvision
-Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'torchvision'
+Requires-Dist: torch <2.3,>=1.7.0 ; extra == 'torchvision'
 Requires-Dist: gputils ; extra == 'torchvision'
 Requires-Dist: torchvision <0.17,>=0.3.0 ; extra == 'torchvision'
 Requires-Dist: opencv-python <=4.6.0.66 ; extra == 'torchvision'
 Provides-Extra: transformers
-Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'transformers'
+Requires-Dist: torch <2.3,>=1.7.0 ; extra == 'transformers'
 Requires-Dist: gputils ; extra == 'transformers'
 Requires-Dist: transformers <4.40 ; extra == 'transformers'
 Requires-Dist: datasets <2.19 ; extra == 'transformers'
 Requires-Dist: dvc ; extra == 'transformers'
 Requires-Dist: scikit-learn ; extra == 'transformers'
 Requires-Dist: seqeval ; extra == 'transformers'
 Requires-Dist: einops ; extra == 'transformers'
 Requires-Dist: evaluate >=0.4.1 ; extra == 'transformers'
 Requires-Dist: accelerate >=0.20.3 ; extra == 'transformers'
 Requires-Dist: safetensors >=0.4.1 ; extra == 'transformers'
+Requires-Dist: compressed-tensors ; extra == 'transformers'
 Provides-Extra: ultralytics
 Requires-Dist: ultralytics ==8.0.124 ; extra == 'ultralytics'
-Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'ultralytics'
+Requires-Dist: torch <2.3,>=1.7.0 ; extra == 'ultralytics'
 Provides-Extra: yolov5
-Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'yolov5'
+Requires-Dist: torch <2.3,>=1.7.0 ; extra == 'yolov5'
 Requires-Dist: gputils ; extra == 'yolov5'
 Requires-Dist: torchvision <0.17,>=0.3.0 ; extra == 'yolov5'
 Requires-Dist: opencv-python <=4.6.0.66 ; extra == 'yolov5'
 Requires-Dist: nm-yolov5-nightly <=1.8.0 ; extra == 'yolov5'
 
 <!--
 Copyright (c) 2021 - present / Neuralmagic, Inc. All Rights Reserved.
@@ -156,23 +157,26 @@
 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-<h1><img alt="tool icon" src="https://raw.githubusercontent.com/neuralmagic/sparseml/main/docs/source/icon-sparseml.png" />&nbsp;&nbsp;SparseML</h1>
+<h1 style="display: flex; align-items: center;" >
+     <img width="100" height="100" alt="tool icon" src="https://neuralmagic.com/wp-content/uploads/2024/03/icon_SparseML-002.svg" />
+      <span>&nbsp;&nbsp;SparseML</span>
+  </h1>
 
 <h3>Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models</h3>
 
 <p>
     <a href="https://docs.neuralmagic.com/sparseml/">
         <img alt="Documentation" src="https://img.shields.io/badge/documentation-darkred?&style=for-the-badge&logo=read-the-docs" height=25>
     </a>
-    <a href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ/">
+    <a href="https://neuralmagic.com/community/">
         <img src="https://img.shields.io/badge/slack-purple?style=for-the-badge&logo=slack" height=25>
     </a>
     <a href="https://github.com/neuralmagic/sparseml/issues">
         <img src="https://img.shields.io/badge/support%20forums-navy?style=for-the-badge&logo=github" height=25>
     </a>
     <a href="https://github.com/neuralmagic/sparseml/actions/workflows/test-check.yaml">
         <img alt="Main" src="https://img.shields.io/github/workflow/status/neuralmagic/sparseml/Test%20Checks/main?label=build&style=for-the-badge" height=25>
@@ -368,15 +372,15 @@
 
 ### Contribute
 
 We appreciate contributions to the code, examples, integrations, and documentation as well as bug reports and feature requests! [Learn how here.](https://github.com/neuralmagic/sparseml/blob/main/CONTRIBUTING.md)
 
 ### Join
 
-For user help or questions about SparseML, sign up or log in to our [**Neural Magic Community Slack**](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ). We are growing the community member by member and happy to see you there. Bugs, feature requests, or additional questions can also be posted to our [GitHub Issue Queue.](https://github.com/neuralmagic/sparseml/issues)
+For user help or questions about SparseML, sign up or log in to our [**Neural Magic Community Slack**](https://neuralmagic.com/community/). We are growing the community member by member and happy to see you there. Bugs, feature requests, or additional questions can also be posted to our [GitHub Issue Queue.](https://github.com/neuralmagic/sparseml/issues)
 
 You can get the latest news, webinar and event invites, research papers, and other ML Performance tidbits by [subscribing](https://neuralmagic.com/subscribe/) to the Neural Magic community.
 
 For more general questions about Neural Magic, please fill out this [form.](http://neuralmagic.com/contact/)
 
 ### Cite
 
@@ -409,9 +413,7 @@
     author={Sidak Pal Singh and Dan Alistarh},
     year={2020},
     eprint={2004.14340},
     archivePrefix={arXiv},
     primaryClass={cs.LG}
 }
 ```
-
-
```

## Comparing `sparseml_nightly-1.8.0.20240404.dist-info/NOTICE` & `sparseml_nightly-1.8.0.20240507.dist-info/NOTICE`

 * *Files identical despite different names*

## Comparing `sparseml_nightly-1.8.0.20240404.dist-info/entry_points.txt` & `sparseml_nightly-1.8.0.20240507.dist-info/entry_points.txt`

 * *Files 0% similar despite different names*

```diff
@@ -34,8 +34,7 @@
 sparseml.yolact.download = sparseml.yolact.scripts:download
 sparseml.yolact.export_onnx = sparseml.yolact.scripts:export
 sparseml.yolact.train = sparseml.yolact.scripts:train
 sparseml.yolact.validation = sparseml.yolact.scripts:val
 sparseml.yolov5.export_onnx = sparseml.yolov5.scripts:export
 sparseml.yolov5.train = sparseml.yolov5.scripts:train
 sparseml.yolov5.validation = sparseml.yolov5.scripts:val
-
```

## Comparing `sparseml_nightly-1.8.0.20240404.dist-info/RECORD` & `sparseml_nightly-1.8.0.20240507.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 sparseml/__init__.py,sha256=T_bs6LwcOiYZveynfxxfZFRqE8qsaHtcSq6s9zbhpgo,1489
 sparseml/analytics.py,sha256=WhXdKgK1-ll9sRzn2n8z-FAikQ02J8rUeWmceiX5S7E,898
 sparseml/base.py,sha256=oOPiaU2JhI0beFwp8Obe7hbt3kUuNvI5XD1wTBCtLnc,10284
-sparseml/integration_helper_functions.py,sha256=SU3dLigJSAPd4oo1A_-yOKiJBoz6w_o-1foXmuUyYc4,7288
+sparseml/integration_helper_functions.py,sha256=y05wU7rd3rbICNegVGUDxR8mg71uiziNRbfVpiocJrw,8182
 sparseml/log.py,sha256=K5E3goPRIPY7Uc14JcX-4oLXVWu7ecOPf1NYBGHc1r0,2483
 sparseml/version.py,sha256=hZ3UsXlWWuL16aEv7XuvhtK5QBxEVEuaTkojPRlSvxU,1607
 sparseml/benchmark/__init__.py,sha256=WynUhMzXq3-iQAhPwcmcMOj2_xCa9brLubs7gxq9T3U,758
 sparseml/benchmark/info.py,sha256=VjZ7cUD66h1u7IJeRXKjiSfvS8aSr-cbAJdnsTmLmoI,17763
 sparseml/benchmark/serialization.py,sha256=FPpcC93x5H86Fqil9YbERUpltu7qhhIQjsyYaSlW_20,10778
 sparseml/core/__init__.py,sha256=UPGGUU562tk80yMF9HvstVoQIUcGO8SD6WGnFVO0FK4,915
 sparseml/core/event.py,sha256=p8tVMOeVgQMGZF0n4L2kkeI-CqEHZv3F_6Vgb2RwPk0,6842
@@ -26,27 +26,27 @@
 sparseml/core/logger/utils/__init__.py,sha256=gpKo4866OtqxB1-pP7pE80fvJYQHkhmL3y6KWa4ZX6s,667
 sparseml/core/logger/utils/frequency_manager.py,sha256=yjPe67GtmRqZKrUjiipAoaCIoAJkNVkXWDNx1MzLq7o,12725
 sparseml/core/model/__init__.py,sha256=l15jfDdshL6wY07mU2MIXE4M32bVWxQVJ-oMs3BcCR8,693
 sparseml/core/model/base.py,sha256=eBTyDegoLzLfAUcsNnbxsjN50u9gTMHB8FZGDN8IPx0,5895
 sparseml/core/model/pytorch.py,sha256=bEvw5sYeGmNJzc_G4D8hHhUYJfev9L7mUPLJSgWFAYQ,5641
 sparseml/core/modifier/__init__.py,sha256=sb7lldB_FbIIuxVFAm4PT8Lm_VWFuQV8heF7_pnAOyY,699
 sparseml/core/modifier/base.py,sha256=d-3OIr5jc3OxqdM2msiDEjscO6KEWkihAF1RS6vzWUg,3326
-sparseml/core/modifier/modifier.py,sha256=IyiVMIHrMf3vBbdT8g5SlUunBinghfnPixevyoGSSXs,10810
+sparseml/core/modifier/modifier.py,sha256=d-PyFLmo15jAQ8UJ1bGKsKLGhYudwZFT1AyM82S2jnE,10840
 sparseml/core/modifier/stage.py,sha256=EOnM-KCga3t9sWQTqpDvDHz0QXuEPreIx_8d1RGS7Yc,6093
 sparseml/core/optimizer/__init__.py,sha256=6ezDyN2xcdvs_Yjwdjm1sEZuHEbcLa2-x6hDsReZBS8,672
 sparseml/core/optimizer/base.py,sha256=WK2Y3xLrO4jxRIyLhUf6hCfKU_XX1mhgfEsydGBxmOs,3409
 sparseml/core/optimizer/pytorch.py,sha256=y3lJvrh8sBzU13fgOq6DEkv0Kp0DtL7QAihSkbidVcM,4584
 sparseml/core/recipe/__init__.py,sha256=brwArG2SqTYBSJnInO3XzLwE56xyJv5ZaC93FN9S1pI,790
 sparseml/core/recipe/args.py,sha256=71JWFOpanUGjj0u9_ndJKtFmbn8LoP4lA6CDNiBplIg,6872
-sparseml/core/recipe/base.py,sha256=SB3ZV6HNPzf7mzRDdIoGpqwfXpRRx5qhL9tI0HapcsY,1599
+sparseml/core/recipe/base.py,sha256=KHor_VETkvMWuQk1c845-olaWvg5MwGaIqDnGe78-Uk,1702
 sparseml/core/recipe/container.py,sha256=BOzgtac0xIemEC6k9bfcUGfZfEGZ1dWdANL8KKlD2M8,6313
 sparseml/core/recipe/metadata.py,sha256=7ZZqQ0F3aV2Fgq7IzCQV-6i04ySBw_oS9-NKnR_K0wk,2932
-sparseml/core/recipe/modifier.py,sha256=8_R_Pi6JGvXVwmkegN9P0DYHf5tUPoqbPicS_bT9xHY,4224
-sparseml/core/recipe/recipe.py,sha256=sedLkXQfhq1Sv44O_oGK_d0GTvi0dtcV0mD-KaXWGIw,24978
-sparseml/core/recipe/stage.py,sha256=R9iXDwMv_leN4ICTrsintQzEy9PG_IiyCRbC6pKVZhQ,7629
+sparseml/core/recipe/modifier.py,sha256=mhSV7skrrsMvbptRACh1LU4T4iS-RuH1aiXiIXaqvCc,4248
+sparseml/core/recipe/recipe.py,sha256=hbqLYURwrv7oqL8bvEb1FcB0hK0jrQ8boNtr_AMZnH8,26428
+sparseml/core/recipe/stage.py,sha256=TMuJmUuLEUZVBvRhFC9tOHzwex1kl6Bwm84TjrerkV8,7726
 sparseml/core/utils/__init__.py,sha256=2HPzh0XB3stE92JJPMvl-Cu7cwa0Qu1Gg1ktlBethDA,663
 sparseml/core/utils/session_helpers.py,sha256=GwxgJCatxeJMmLu_atCcdm-1HoKIYIDmQS8i-DCPzHg,1025
 sparseml/deepsparse/__init__.py,sha256=o5ITzTEOlH--61_SXMISrmPcRk4ci91FGlR-aQxwOn4,863
 sparseml/deepsparse/base.py,sha256=nRjU6TSao0J2iWXcdHwj62X6dVT9vl6TQ2y4Bdn3KN0,3516
 sparseml/deepsparse/framework/__init__.py,sha256=ZHrXXM3AMH78zKcF8Y40IBuy38UFy4fLrvytozw_bNY,801
 sparseml/deepsparse/framework/info.py,sha256=HVDPHFXggFeDqbpM20PkByf1nBccwKnjpTWh0EfrVas,6032
 sparseml/deepsparse/sparsification/__init__.py,sha256=re_2FtHWvO-iQQwRRJZDKz3l_pFZuwe266-4ZFxQqbY,813
@@ -56,18 +56,18 @@
 sparseml/evaluation/evaluator.py,sha256=RUY6L3621CtuKlwRjuu8hSuBzoeU9ERypZmfBs9rX0g,2140
 sparseml/evaluation/integrations_config.yaml,sha256=PCSqljI_6wesSy45O-mNeSN9CtRHWtdnd9fiK1pTXXU,940
 sparseml/evaluation/registry.py,sha256=VHS7blDGfq4WzFB5PxAkfOIYuHTWki-UGuGukCP9XzQ,5571
 sparseml/evaluation/integrations/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/evaluation/integrations/lm_evaluation_harness.py,sha256=2adnmKtGMzowJUveeWbaaZUjmf6YO2a8sHPVFiGcwNQ,6000
 sparseml/evaluation/integrations/perplexity.py,sha256=wBdnkBYv8bhFqt2PA3wVr3nN0GATRkn3qWgdgHKXH88,10987
 sparseml/export/__init__.py,sha256=5CrKZql6PYJq3BpNLBG397pVG7rVxU4-NIgpQeKY188,661
-sparseml/export/export.py,sha256=Z-v8Lyz_RsB74gmU9s79m33vAv3nPoT_uIRexvJRaIA,21172
+sparseml/export/export.py,sha256=iKWiwk0_fdyaVB63aT1iKs3h6esr5GjJBwGquJBnYa4,21753
 sparseml/export/export_data.py,sha256=5t9jEycKHOdcBVdHkA6ktRZxQ5luyqNRALPlSvmkdKE,8049
 sparseml/export/export_torch_model.py,sha256=SiA69b7p0KWhfM2XtYbKaYW6YHIvIdFHL_tWJdGBX58,2681
-sparseml/export/helpers.py,sha256=maoxhetc4Fu94XevWFyZ6-BQErjSZOHZie_rwKyWaFE,13052
+sparseml/export/helpers.py,sha256=7qxHSworXI7uQ1gIu1sTdc_IEOA_L2yARNaZVkWTOjY,13656
 sparseml/export/validators.py,sha256=wBxARY9fXsszUGApMY9Fxf5ZjYUGh_Tg1I-C_G_l488,8868
 sparseml/exporters/__init__.py,sha256=VPcyVcyPnl7Kp05h3ws-UC2jgKR6W-cr11d-m665w9A,786
 sparseml/exporters/base_exporter.py,sha256=M1HEe9GNf51uYx1z-qxWjIYo5iGZ_Ish8uZ3mj6OZR8,1477
 sparseml/exporters/kv_cache_injector.py,sha256=2fKoRPBE7yrR6h3KXxeU7EbsU5ufyZUIbSnRP2FTjwc,6576
 sparseml/exporters/onnx_to_deepsparse.py,sha256=e3RNDtyKAbmYTF-AHV00FAg-ilQDidUPTnUU9oQm4Rg,5522
 sparseml/exporters/transforms/__init__.py,sha256=0SMdBqAFpcoDTWDklE2nSk9G4sZOAUyUfeWUJSlRAD0,2350
 sparseml/exporters/transforms/base_transform.py,sha256=IpvdUdEADl2SfqHawcp6-lcNgz-R3L0zpS05WaWF5h8,2333
@@ -93,26 +93,26 @@
 sparseml/exporters/transforms/quantize_residuals.py,sha256=f3kNF930ymHI5CmCA0OmYW7vDKvEm9sYhN0UX0UC1EQ,3869
 sparseml/exporters/transforms/remove_duplicate_qconv_weights.py,sha256=mod-7bnlkg20LjerZ7tKKmp_7XyO3FKo13BtIZkDsmM,3331
 sparseml/exporters/transforms/remove_duplicate_quantize_ops.py,sha256=2FSXzaY3jdxOgj3xBaUOUqtk-hzg4kwhheELhoOKpS8,2545
 sparseml/exporters/transforms/skip_input_quantize.py,sha256=ANiYDwSos9gPW4960fMwmGubTDgpAcqFVSIHKf_ffYw,3210
 sparseml/exporters/transforms/unwrap_batchnorms.py,sha256=ly7GHCXN_2r8Gnk8PABWCqcoS-NL3GMfquEK0hRlFCs,1373
 sparseml/exporters/transforms/kv_cache/__init__.py,sha256=0AaAgdoxgzUJ7DHzwwTooYoaqf1EGNR9XCtryChM6Wg,911
 sparseml/exporters/transforms/kv_cache/cache_keys_and_values.py,sha256=BcJ0s1n3fc28g7IBtPpAB_Az0cJn-3AaIRw9VbxQ3NA,30558
-sparseml/exporters/transforms/kv_cache/configs.py,sha256=U9YLMJFjdoKkearGBu3rFXTh5uzIj1NmgZe2o7kQ2N4,10686
+sparseml/exporters/transforms/kv_cache/configs.py,sha256=9kAMkaijArZH8VrC8Nna2WO01p8pbqHOOUD0vc516OQ,10727
 sparseml/exporters/transforms/kv_cache/transforms_base.py,sha256=AVqMSwWhCD2F5Hjysqn6CJcNL4khrw8CJmRr9Sa4Ymc,7027
 sparseml/exporters/transforms/kv_cache/transforms_codegen.py,sha256=6uFGzuLUoCXCsplN2QKDcfua3dl8-OtczDh3KGAeEeQ,4974
 sparseml/exporters/transforms/kv_cache/transforms_llama.py,sha256=WK66JhF6l1jlftZUdelwFYcfQIWFmbyPtKCqbuwZ3c4,8801
 sparseml/exporters/transforms/kv_cache/transforms_mpt.py,sha256=TfHedCOirrPUyT5BRMZChwFI6MZnhCXRiiWeRSVbbQg,4261
 sparseml/exporters/transforms/kv_cache/transforms_opt.py,sha256=JHgNKL6e4YSIXuBEEEADMcc_C0xqyd57y7ZT8zRHqsQ,6066
 sparseml/exporters/transforms/utils/__init__.py,sha256=tXSSUnzdyIc_H73xQ5dYhHb1Gj4Nb7GLLDVwJds8c_s,730
 sparseml/exporters/transforms/utils/add_quantized_conv_matmul_add_ops.py,sha256=u7pD8zO2qtcCQkxN5SDYiujYvnhjfNAVn_3bcHqacWs,11112
 sparseml/exporters/transforms/utils/helpers.py,sha256=0KP47hMugJBKAhgRRj97LjOml8L7rRV8OYOTne9z58Y,6809
 sparseml/exporters/transforms/utils/matching.py,sha256=CnVJLbJARBJwvFjoKh9vwYt6dLP4WXG3p7nCY5VUFaE,14429
 sparseml/framework/__init__.py,sha256=EcIX435yx552d7BkbxmAbQ_X3KAISInHVbD9X-AT1Uk,790
-sparseml/framework/info.py,sha256=TaZAMYdhBHcjl9Xyk3PWjJkDB6g3_f4vD3B6KmhgOl8,9479
+sparseml/framework/info.py,sha256=6uQnasFeMP9_H9sAwMvLzAaJE-biIhu0Q__aAMxdvU0,9501
 sparseml/integrations/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/integrations/torchvision/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/integrations/torchvision/evaluator.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/integrations/torchvision/trainer.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/integrations/torchvision/data/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/integrations/torchvision/metrics/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/integrations/torchvision/model/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
@@ -168,15 +168,15 @@
 sparseml/modifiers/logarithmic_equalization/base.py,sha256=4Omr8jRxMIAImM-ws6iGVMSXxt1awoj-g6CCVTFyoJk,2764
 sparseml/modifiers/logarithmic_equalization/pytorch.py,sha256=KeHI-gDVJbjEgWh0mEWKCdp1dXwXeFOLjWE_JVgBnP8,1947
 sparseml/modifiers/obcq/__init__.py,sha256=mRxLhbapfZ3Q_3wV_fkiovbQlxdXGbk9yOorkzifdxU,654
 sparseml/modifiers/obcq/base.py,sha256=wK2wIPY28TfqU68iio6ivaPQ6ranOxTvJIQr7Vc0mYI,5181
 sparseml/modifiers/obcq/pytorch.py,sha256=waCQ6AymBlflz0kHBBj6Q1L5d7dNngwcbyFeUVks6SU,3543
 sparseml/modifiers/obcq/utils/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/modifiers/obcq/utils/helpers.py,sha256=eb8XeB7V1brhTMLjIEdzmSQonYnKDZKKilasL8FC24A,6686
-sparseml/modifiers/obcq/utils/sgpt_wrapper.py,sha256=nJliltPq_VDICpsoGYeFPO6EAJEe33CHilm0e1XP8P0,6753
+sparseml/modifiers/obcq/utils/sgpt_wrapper.py,sha256=-lAZLIaIO3xYEoJTjyjfoT6ol8zOau-__nyCEGDD9-g,7752
 sparseml/modifiers/pruning/__init__.py,sha256=3qxXBFnaK2JrDJy13FFckyqF9tpLNdRCAtETUVy2Fso,683
 sparseml/modifiers/pruning/helpers.py,sha256=mgqnvArk40p9_9K-dcN1QzkQICmllgk5YNsYv1luXE0,5725
 sparseml/modifiers/pruning/constant/__init__.py,sha256=ZX5IYPTigTjstA21M3sbHbu9ZfamE0tTeA9tPPctczs,676
 sparseml/modifiers/pruning/constant/base.py,sha256=08lKBIpaMUWhEYoBx317EzZXMF6xVjAOGH9brsD0EuA,951
 sparseml/modifiers/pruning/constant/pytorch.py,sha256=ph_C27HPWu_TIWBPqAs_eqpTkAJ-cpBVbE0T7rMPgU4,3103
 sparseml/modifiers/pruning/magnitude/__init__.py,sha256=E59xmevUYsLvjYqhjxzCLTAWMpb2ltTxNf6XDo-1ejw,677
 sparseml/modifiers/pruning/magnitude/base.py,sha256=3cATdYRcasOxgNZzaOXAnGyh4kbKRrsJ4W6dQnMyTbo,1168
@@ -188,23 +188,30 @@
 sparseml/modifiers/pruning/wanda/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/modifiers/pruning/wanda/base.py,sha256=bsXiTsATTrjdieYq8VmiJAODkxu6EbMwnBOU984fBsc,3905
 sparseml/modifiers/pruning/wanda/pytorch.py,sha256=k3fWJRaUCfssCjjC-AQcDfJN0Fvu6DuWbf2s43Hx7DQ,10346
 sparseml/modifiers/pruning/wanda/utils/__init__.py,sha256=TF5uEKrpc2qYdbgmXh9xVp2GQ3p--by-LoCegY19EeI,633
 sparseml/modifiers/pruning/wanda/utils/wanda_wrapper.py,sha256=mOwf1k0A6NZOLCP2QlTgfpdm1lGJYunrJQW6763sNjs,4469
 sparseml/modifiers/quantization/__init__.py,sha256=mRxLhbapfZ3Q_3wV_fkiovbQlxdXGbk9yOorkzifdxU,654
 sparseml/modifiers/quantization/base.py,sha256=zsxVrP_mLh895z1g56o1xtNgUAMVgqMGKmUk0GBnmYg,5512
-sparseml/modifiers/quantization/pytorch.py,sha256=Gzy_AemrYNkbr29RLo4MuqVcLc_heF8bdQlrkvCRxDA,8924
+sparseml/modifiers/quantization/pytorch.py,sha256=49nTlrrWrzzddVP8y4HslqMtuwbWyaNjAaS2PQHE1ec,9236
+sparseml/modifiers/quantization/modification/__init__.py,sha256=Me4cLLNRvO3rILs-4-mhXnOTEHxJ25HdGeUXs4QrH44,671
+sparseml/modifiers/quantization/modification/modification_objects.py,sha256=oexoX7Mypb5IH0V2Z38RKd4h3oDFyepyYeMU993lCNI,3908
+sparseml/modifiers/quantization/modification/modify_model.py,sha256=uKo4GB17-iiyoeXvYdHfkHj1XPWq-s4n9w599I0RFKU,2573
+sparseml/modifiers/quantization/modification/registry.py,sha256=p1dwxGVlL5tEiqSa7fE8iwJHd0IIpP3dPZyG6CnqeX8,903
 sparseml/modifiers/quantization/utils/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/modifiers/quantization/utils/constants.py,sha256=sla-j0QwLFq92AbDZkHkNLxugpGrNEQJIuLWoWqV3Ks,2220
 sparseml/modifiers/quantization/utils/fake_quant_wrapper.py,sha256=EZC7DFVz_Z4CVtVfeSpkaE9-g3OIN8WDZAaW6V7foVU,2906
 sparseml/modifiers/quantization/utils/helpers.py,sha256=7U5uorqKSWLExvkVOyGU8b3noKlcG6tGqhjsomYihrg,32720
-sparseml/modifiers/quantization/utils/quantization_scheme.py,sha256=VvDo8KBtc6uTWnKDM9izgEcWtTdXGxkR_E-zKQ2aTO0,13545
+sparseml/modifiers/quantization/utils/quantization_scheme.py,sha256=_e_lBGKV0FYJ43YyZVcSzwhpHL4U5DGDcCLq6OBhKXc,13580
 sparseml/modifiers/quantization/utils/quantize.py,sha256=AeGY6YBREVkyRT4QDJ4KQswcavPlemYNcQmC6mhQPGs,20463
+sparseml/modifiers/quantization_vllm/__init__.py,sha256=mRxLhbapfZ3Q_3wV_fkiovbQlxdXGbk9yOorkzifdxU,654
+sparseml/modifiers/quantization_vllm/base.py,sha256=eVRluDTEOv68VIorYBvMsJuyIVy95oQVylICOGEqjqg,3353
+sparseml/modifiers/quantization_vllm/pytorch.py,sha256=0PcLWlATGlmIv3UCc1f5wR5VL5RiiSYC5EaDz46In0g,5467
 sparseml/modifiers/smoothquant/__init__.py,sha256=QQWiZMR2IxBVSYmsAkop1F_xCGhHzMlukySMVz4lv2I,654
-sparseml/modifiers/smoothquant/base.py,sha256=_KxGDJh54i_0Ltrw6NnN0FaRPWWbxjiK-3yzZHkH5-E,7535
+sparseml/modifiers/smoothquant/base.py,sha256=dbQCSYpUZh09R5uNrPMmmvBhYzz5vFAojBXuk57V6zs,7466
 sparseml/modifiers/smoothquant/pytorch.py,sha256=bv9FmqGfiafK1TOh-MM66eBUXVHgdc5ECDye5PUwtQk,8167
 sparseml/modifiers/utils/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/modifiers/utils/compression_wrapper.py,sha256=qjq6mycdiMvWUktylSr4gWplzHxQEctkwnP1XORUSvE,3944
 sparseml/modifiers/utils/layer_compressor.py,sha256=PgICAZVJ3qFixoA-TD6eykBByF_d4O3LF0WukdvHVyk,5257
 sparseml/modifiers/utils/pytorch_helpers.py,sha256=HjpIZmD8KnyLh8opb6YxpCbZ8JZTDlkONHloYxsKC_4,3265
 sparseml/onnx/__init__.py,sha256=KeWF6EQ7cpX_b-fMhiG5n6pVPUvRxiWMpzpiI9EagFI,1036
 sparseml/onnx/base.py,sha256=Kv3YC0Xh1xlxpIuwh9916SHuz96phaf36KbL71DSZWk,6202
@@ -239,17 +246,17 @@
 sparseml/optim/__init__.py,sha256=3oGQ4LY0MSrbTAwuyPHEOEyYiZ7JI7OX2BgWup5NPuw,882
 sparseml/optim/analyzer.py,sha256=LUYBYyvfVwaw_V1aAiOmaybUGJQk4vRK5RG3k3nD7pA,6302
 sparseml/optim/helpers.py,sha256=Zb7YPYe07vuVyYUhHJ-BlcMrZa-sxt1TUZ-Ec9Z6o_U,25563
 sparseml/optim/manager.py,sha256=KeDVHj9ea9EUaV908qfvq8ONwS8nUmAvQb1yXflOBWo,25984
 sparseml/optim/modifier.py,sha256=qYfVUL9thw94p4oEsNZgitUJ3y9izWYb8nUI3enyzOU,30708
 sparseml/optim/sensitivity.py,sha256=neMP_hTRqzDUV0MrATevC2-JQYnOIvNW_AlIf_y_ydw,26315
 sparseml/pytorch/__init__.py,sha256=n1a6y27bzfG73uLbifhIx5DY9hDg1RM1pTIN1DgBpDw,2190
-sparseml/pytorch/base.py,sha256=BcTCMVYqQcFL4LzkR7rLu-mOLSOk3AFkMYkBnRaIM50,6154
+sparseml/pytorch/base.py,sha256=LknLOIbzcvsMagZYDcMjtvG85LACQ5W-6zuTWjBfGuo,6214
 sparseml/pytorch/opset.py,sha256=ByKnQMoN6U9oQ3HWh270oqBJcJAHe7ucJM0gWokMabs,933
-sparseml/pytorch/torch_to_onnx_exporter.py,sha256=YgkHmFYTHbxhRRe3NNy0zx2FPI-XxNQGNvcpSR1IFgs,12086
+sparseml/pytorch/torch_to_onnx_exporter.py,sha256=eXfUGNQgncZCD27CVdKtRIHEMs9TZX4-zqQQcVKfVI0,12127
 sparseml/pytorch/datasets/__init__.py,sha256=2xoH_fCMojldFY1RCWSkt9pGfa8SzHAxU5em-_ZiwV8,998
 sparseml/pytorch/datasets/generic.py,sha256=nl-fFlpd7u5sNswe1AORZvQUne3sqrrPWnNgOUp6_Fc,4193
 sparseml/pytorch/datasets/registry.py,sha256=Ju8hAl3qkaplvkjZgt6WvpKmujgv0qJ690Buo1ekipg,3014
 sparseml/pytorch/datasets/classification/__init__.py,sha256=GMKhziJkRwSC7E_1Nox8FxnxhPFozZ6MxQWUWi1BN_Y,828
 sparseml/pytorch/datasets/classification/cifar.py,sha256=o6KRltIr6tSjaxy0BXsc0QosjMdCmx0sxfxocBEMEhw,4457
 sparseml/pytorch/datasets/classification/imagefolder.py,sha256=rOvwqy2Zp89VkT7zat2hQxRmgPyvCpSAhZ4mWEsL9JI,3669
 sparseml/pytorch/datasets/classification/imagenet.py,sha256=CHzsckzsSsUCLR-oxaxJ8NH7j4WBqg_Wg5ge27bPk5Q,4000
@@ -345,15 +352,15 @@
 sparseml/pytorch/sparsification/pruning/modifier_pruning_topkast.py,sha256=6ukbXWtRe_4wZ-awRfD7uZPQiIVPaTOm1Jf-z3qNO9Y,17683
 sparseml/pytorch/sparsification/pruning/scorer.py,sha256=8gyYSv611GOJwmpQeD7FsWVrLxF8jeM_f59zhQlC6f8,4644
 sparseml/pytorch/sparsification/quantization/__init__.py,sha256=OjbeCzzhoeFJEJQvLmXZc89pghO83sLRhmvNF7_q_oQ,813
 sparseml/pytorch/sparsification/quantization/constants.py,sha256=sla-j0QwLFq92AbDZkHkNLxugpGrNEQJIuLWoWqV3Ks,2220
 sparseml/pytorch/sparsification/quantization/helpers.py,sha256=Q1kXCoQ0t4gEFFUEmf_skQvlyjJhhAuAJwIkBBWT3l8,34914
 sparseml/pytorch/sparsification/quantization/legacy_modifier_quantization.py,sha256=TToI4dGYZZEzB1q61m1YVg0THlvH7Ew1CZ2y-ajpTts,33626
 sparseml/pytorch/sparsification/quantization/modifier_quantization.py,sha256=LkHNB1IsiFlZugUecB4-LZMjWuoboFYf_PXPx6uPl1g,26778
-sparseml/pytorch/sparsification/quantization/quantization_scheme.py,sha256=-Ceik2aGOCuxxe0lijUJvCLR25HPEbRUT6ZPzUa0RsQ,13482
+sparseml/pytorch/sparsification/quantization/quantization_scheme.py,sha256=wyzrw4Aul1YjVFtpr_CNNF0yxMgfKzibzTbmOVHKejY,13511
 sparseml/pytorch/sparsification/quantization/quantize.py,sha256=3egXmm43szb7MapnJbA_jZCMXMSjBHs3KLw8Pp4gPqE,18047
 sparseml/pytorch/sparsification/quantization/quantize_qat_export.py,sha256=6JLbd3NUy9J0_pygSypJZsPtfxLlY8ZFJfK7kJSRrSY,76796
 sparseml/pytorch/sparsification/training/__init__.py,sha256=4GGhQf6KVSOYwtr_WL9Xcvx4mR-k8yKMVI_zuTBHGO4,790
 sparseml/pytorch/sparsification/training/modifier_epoch.py,sha256=_5p1EZk2qbwTnWsMXbVan3vRj1IRyIM4wYYZdLGA5GE,1778
 sparseml/pytorch/sparsification/training/modifier_logging.py,sha256=K1NMqhea5EBPko6yim8CThShbiSJKN5P_ca_TCyZXto,2883
 sparseml/pytorch/sparsification/training/modifier_lr.py,sha256=m_tCydo_BHiXCHMUKZu6iqKLAP24acAT8-NLjH9RIlE,24287
 sparseml/pytorch/sparsification/training/modifier_params.py,sha256=G_0eVqu0Cup8WNLFhkfPmYnKtJlNrxcJt0FcOOdq9tA,21497
@@ -368,31 +375,31 @@
 sparseml/pytorch/utils/__init__.py,sha256=WWPrEPVj8YsCZN0JemnAF5z7hOf0nL203uixlqWoAeo,1160
 sparseml/pytorch/utils/benchmarker.py,sha256=Zw1pEj7wDe4ZU_-G0JOCymdLXydN19K1QZKzPJvBp9E,9706
 sparseml/pytorch/utils/callbacks.py,sha256=1z10T8xE0IY5mcL2ARSsHDt6sDWRVF-Nq6zVf9OvD0s,2846
 sparseml/pytorch/utils/distributed.py,sha256=lJtEvgMg6LvH9iUwhveCaWNrx_t7pjn-sOW9rfYViwI,1061
 sparseml/pytorch/utils/exporter.py,sha256=GvKBjd2WVLKJCBjfXXokF8c3vt_tq-KY_mmOgJFhcs4,30884
 sparseml/pytorch/utils/helpers.py,sha256=viLrRpSYGLDcJAAj2rRkRcoQuwJfKMbrUTG7-mifl_I,42811
 sparseml/pytorch/utils/log_sparsification_info.py,sha256=o9WdrrDU8W1J09NHLnW2cja5PXNq442UFzgpXf8po8M,1663
-sparseml/pytorch/utils/logger.py,sha256=KOhmFXNj46gmy5XovwQOZigBuHidVmNqDnVWMUXD7PE,31374
+sparseml/pytorch/utils/logger.py,sha256=X2MptgS4GYLZ0winBt33nweBXQ2Azxe-AXZLzOFaJsU,34449
 sparseml/pytorch/utils/loss.py,sha256=RWFyThMBaB-7jAVuS3_CU5bmsWOVFG1jqzPciSpVsj8,27048
 sparseml/pytorch/utils/model.py,sha256=KCEZ6cNkIzLKd6N8KqJe7GX3GD-PoWErZk05nEbQuJc,11754
 sparseml/pytorch/utils/module.py,sha256=gWKNLqn8bI_q97u_QC95WkxBPAclMCwz3HmrZEHEYtU,39117
-sparseml/pytorch/utils/sparsification.py,sha256=LmG2WfSnp2rZo372dRirNsBFTd_qw1-ptZLkDZti0t0,9180
+sparseml/pytorch/utils/sparsification.py,sha256=U7H4AE-dOR5LQeQ2nSO2larkP1Qcl74FOdmynxWF4cU,9691
 sparseml/pytorch/utils/ssd_helpers.py,sha256=05fySE5KNwrt7UF213niHp2STRSeprlU1NbLAjpYRaU,30059
 sparseml/pytorch/utils/yolo_helpers.py,sha256=nFlDPOEWfooq8GrUzKRpr7vS36m9NQMPGZSqCYmGMks,12337
 sparseml/pytorch/utils/sparsification_info/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
-sparseml/pytorch/utils/sparsification_info/configs.py,sha256=t0GTpRnT4LnmxLT9XUcuh0FZna1EzCUw-VNSkHluTi8,15081
+sparseml/pytorch/utils/sparsification_info/configs.py,sha256=q76_pbiuTu7vzS9MJ69AiCzhPUMbHOJh9pWU26gCPg0,15095
 sparseml/pytorch/utils/sparsification_info/helpers.py,sha256=8bGeZ3_WO9HmxQwf-zeKI0ax9DG6jRoMeXJWeeorI3o,4336
 sparseml/pytorch/utils/sparsification_info/module_sparsification_info.py,sha256=S-WyGuIZgQ8lZX4pj1WmXjyHEuEupMLu5ZGWvHbVIG8,2788
 sparseml/recipe_template/__init__.py,sha256=0HHlF01-zO71wTzxQ9AE8UvqmXbgoyEEykAVGcyVjWA,655
 sparseml/recipe_template/utils.py,sha256=_ASxYYYJPIkK-Ffsicn9hezlHt6dXBLObCPqh9Uz6zE,4788
 sparseml/sparsification/__init__.py,sha256=YmxZ5Gbbn3zfInFZDA2aH9gM-gK-JRgYTnOgXSNskAQ,1058
 sparseml/sparsification/analyzer.py,sha256=ky-GJNpDUssyN8HBxxqBC_LmUxnaL9eL9Rl2aVgbNgU,9387
-sparseml/sparsification/info.py,sha256=kH1RZJGpz0yHgFVvJaj0WEzID8WCozMjiJdQ5M08PTQ,9065
-sparseml/sparsification/model_info.py,sha256=fNOh_pVc9lGaBLcEf3YIemIbJfZxFNR4JuILvRN-pks,15565
+sparseml/sparsification/info.py,sha256=EliK_RtWfbIuKx-EQvEN47UtQT9h1aRCG2uQesCQn_Q,9087
+sparseml/sparsification/model_info.py,sha256=TJbT7PYhqu6qeX1GMJXszz2L4DPetUsb-OlVrXcOuMc,15589
 sparseml/sparsification/modifier_epoch.py,sha256=JKUM2cVQPYGx4vKbpF5IGwQFWM0NevFHc8dq8InQdbQ,2002
 sparseml/sparsification/modifier_lr.py,sha256=-3t0ozub6vfyh9NAgvgRgbyXIP1yu9_URuArr5TdTXg,10117
 sparseml/sparsification/modifier_params.py,sha256=n6O1tD4nsOTeeGJ5ym4hB7PQwSEWNduymW8_ObjAO7g,5505
 sparseml/sparsification/modifier_pruning.py,sha256=7_i4dzdkfOFcD7A_d8cublgX5KB-hyOGOJZtU41JUwU,12845
 sparseml/sparsification/oracle.py,sha256=9J8iIdr_aztwc06tRgw-2ZdGN_1RgJBYWZDY1ZfHhAY,3700
 sparseml/sparsification/recipe_builder.py,sha256=i8cFAkDemk8UXiPff5s9necHXECk6WO2AySnqiqr8MU,18570
 sparseml/sparsification/recipe_editor.py,sha256=6PqExzex1nmMj2a0jkE7JUeeuMTGA27GCH32C9B8V40,14413
@@ -442,79 +449,66 @@
 sparseml/tensorflow_v1/utils/nets_utils.py,sha256=cOTKgw7PVPEjRVU-bvcsNLdBJGTi-A7djJaUaZQwsT4,8119
 sparseml/tensorflow_v1/utils/summary.py,sha256=A2ub7KDwzsaindIuCHa7N3cRpCU6d7QbD5uNHV-R6H8,1327
 sparseml/tensorflow_v1/utils/variable.py,sha256=6ziLaNOLnxQrSPWJ2RaINqg4w-4NK9reVJWH-weK3-k,12536
 sparseml/tools/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/transformers/__init__.py,sha256=IG8RRDXHDzWCpN1GPagT3HBeNJu96Iiaf6LTCHNoVuc,1193
 sparseml/transformers/base.py,sha256=-d4OhazJ6EDau7Ug3OjwISsP8NQKjU5ptnsvazBiJZs,985
 sparseml/transformers/export.py,sha256=h1fumr-V_TGUTT3VITtPhT7P5qhXd4MKdHJgRXL8AgE,23904
-sparseml/transformers/integration_helper_functions.py,sha256=4swIpJyutt9WX7375JRX9Vo8lWKwDAvFaY7US7RNZOM,9485
+sparseml/transformers/integration_helper_functions.py,sha256=YizR5nvLJjCo8l1UPCc4gYur0aeg5lHuLO7O24qrU1o,9474
 sparseml/transformers/masked_language_modeling.py,sha256=VxuHhEPXUu2nuv73eTUYbtVSnK101NrtYpwOQaS8QCA,30795
 sparseml/transformers/question_answering.py,sha256=b5fMA_dMKsxl-BEI2yIkn1hjJ2kz8VfYwZBHInnD8pQ,37002
 sparseml/transformers/text_classification.py,sha256=QfpUTL7nfPNcMYVxNhySFjLd4BKBhGqSR-YS83oQEOk,40360
 sparseml/transformers/text_generation.py,sha256=RDCzUC4tjfgkyzAOdkYvie563fez1dYivD_U4cmpWu8,818
 sparseml/transformers/token_classification.py,sha256=Opb0gL8Das6sDsxdDd9DirFUWVg7IYHpMGImL_yfU-Q,34389
-sparseml/transformers/compression/__init__.py,sha256=e-naxCnurIkngVTQ3eIU13CqTg0N0jQ_Xm0_3wZYtgo,683
-sparseml/transformers/compression/compressors/__init__.py,sha256=fXlgLEeFgLXdc7ofYWIJ07VOUUoD5pcPiSAKndHsAqg,749
-sparseml/transformers/compression/compressors/base.py,sha256=KWuHhgpEX4FzvensMYAK5DnAf3hNaoJejdiIn4tPyik,3052
-sparseml/transformers/compression/compressors/dense.py,sha256=akD31OMTJfee2QkGS4ZlJn1LUuKn5XnJSuyyPmRbDMQ,1160
-sparseml/transformers/compression/compressors/sparse_bitmask.py,sha256=soSzKXVc_GujyAOKWi-rS24-hPFXnMB3dwrNEEPBYUI,8382
-sparseml/transformers/compression/config/__init__.py,sha256=fV4XY44QQ5krlLZhf11OIg7Q91vDcTW5blRITa7LL4c,751
-sparseml/transformers/compression/config/base.py,sha256=Ova_943QUCtB7RcCjPRIQvcdCmSWd5TS7X9PLMRkeJQ,3846
-sparseml/transformers/compression/config/dense.py,sha256=j5Tp-xGtqMK6TtModLIA4h5nc1BkKW6VWM2TXKtkDRQ,1263
-sparseml/transformers/compression/config/sparse_bitmask.py,sha256=Q6lcE-7xlc-KhaldDWfG7foHwlCcW3gZqQ4KnK2Gx4s,1236
-sparseml/transformers/compression/utils/__init__.py,sha256=VJzYDHjYGHcqVRrO06Pgstju2Df4rhVhQt3W-LIdIbk,718
-sparseml/transformers/compression/utils/compress_save.py,sha256=J5CbPJH3LZdQAW2_1xm_tr4yQqQwIxVs78WhV10GXfY,6092
-sparseml/transformers/compression/utils/helpers.py,sha256=VngJebXKN0tyxLlzl5--DhwOLUJ8BxI55MwXHdaBPos,1778
-sparseml/transformers/compression/utils/safetensors_load.py,sha256=k4BhpnRE_3ANo3GQgycLFu7ernYSZMM1sF8H3irncaI,6966
-sparseml/transformers/finetune/__init__.py,sha256=0bOoxD9cLg2VVtreGrcrakhzdMeUmFeTWFhm3oZkRp8,701
+sparseml/transformers/compression/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
+sparseml/transformers/compression/sparsity_config.py,sha256=5en5eBF0qjM_wQ9grbLh7x36kGsJTT4BvgzKH-ya894,4305
+sparseml/transformers/finetune/__init__.py,sha256=Vk0iJWfdAnWVZD58-mpadaedYIVwHvYfCEnSVG0Ipqg,924
 sparseml/transformers/finetune/callbacks.py,sha256=14k2PnjA85cRwzzuIqF1yfxcKrQ4t9RUKckFdeK3-IU,4265
 sparseml/transformers/finetune/model_args.py,sha256=RkppN_0YiHjK1PUCAuKZsGvIs52FHLAc8pJNqymbHPE,2519
-sparseml/transformers/finetune/runner.py,sha256=aQxCzxEBRAe8Z_YAIY_Tz491hvQdn2r7VN-z2F3ZVBA,11988
-sparseml/transformers/finetune/session_mixin.py,sha256=aMV5pL-mnoDaaqWIQvBWT0ZqCKHgmcDWAqww48Zkmnw,22620
-sparseml/transformers/finetune/text_generation.py,sha256=lvCIcuV1N9GVfYYjXe3BSjw3JE6LAKejpKrWy0zNho0,13101
-sparseml/transformers/finetune/trainer.py,sha256=jRbQ-Je-V2Ym2z30_K6pgsuznZB_W79blXF7R3x5m60,4126
-sparseml/transformers/finetune/training_args.py,sha256=BLe9Wjcf-HVpds_1XS8NPHZzm10WQ3_Ii-EjS0ODc2U,3005
-sparseml/transformers/finetune/data/__init__.py,sha256=1suS2CfhJGBW08wniPgbFuOLG4g71rBu7f15OrI7ZrQ,1021
-sparseml/transformers/finetune/data/base.py,sha256=GwABRyBbLgooASRO-XpNvrj-_omS_oVAvBdI_59iSFg,8736
+sparseml/transformers/finetune/runner.py,sha256=Fz1qSF2Ma2oFau3gBWbyDdRtqaBL-qeZvvlai8xVfzY,12296
+sparseml/transformers/finetune/session_mixin.py,sha256=c0oPo4S5Bz00SYjC6ZoKU7obhfN-OFEcVfHltZzGSgI,22259
+sparseml/transformers/finetune/text_generation.py,sha256=-LAvPZN6uX3uPwk3E6tIwJNAgTWDpiTtvCXjuFnNPH8,12579
+sparseml/transformers/finetune/trainer.py,sha256=QdJL2QGaiys1qIbWpXPbZFGpBdB8RlEPyrwMBWBk7cI,848
+sparseml/transformers/finetune/training_args.py,sha256=Rm9lBlwAUyXejwFOg7nt4lQ3LAoH-KAtMnVbkBBzHZw,2862
+sparseml/transformers/finetune/data/__init__.py,sha256=doDqaoV2wuT3dvkTikkPgXp1m-a_EF4xNpe1N6friGI,1066
+sparseml/transformers/finetune/data/base.py,sha256=CvtceP79JbTtEza7zz2Kqcrg_NjvSxVRj7U-JbMD6qo,8835
 sparseml/transformers/finetune/data/c4.py,sha256=tJW0Dnf5rs6uFmgoRD0TGuf9Sl0qLDkiVdMAtwhceSo,1322
 sparseml/transformers/finetune/data/cnn_dailymail.py,sha256=EpASl2Rd2L2IURBm8ALVkUbE-RAxzoW0LFmOsRxJs10,2537
 sparseml/transformers/finetune/data/custom.py,sha256=WqKecNf45cS1pGe8T9M_AccvSFAFO5YNJEUoyHA9tbY,4281
-sparseml/transformers/finetune/data/data_args.py,sha256=MEwc-aPL8TxZsEUtRuZVDN1OEg8YD4RhZ1nqtX-Th58,5399
-sparseml/transformers/finetune/data/data_helpers.py,sha256=XPcTLw3EFHlYXUazPOyx0EJsoZOIg0ORwBcTkjL9ML8,8961
+sparseml/transformers/finetune/data/data_args.py,sha256=JiJuTb8Sxak7nSaBmXJFC1NjWo2x-0nDD6a_TUpywOQ,5601
+sparseml/transformers/finetune/data/data_helpers.py,sha256=1C_l-PDZKGfQbGITHFD5lvPBnO5dld1EITr2EGcKgWk,9217
 sparseml/transformers/finetune/data/evolcodealpaca.py,sha256=-P8hAodeEQ3QDGw17UXf7igRHlCU2e74ntjzExXtZ4I,2892
-sparseml/transformers/finetune/data/gsm8k.py,sha256=jHAwAUScU9a4jsnl0edtmvhSUVLlxFy07fczMjN90gI,2597
+sparseml/transformers/finetune/data/gsm8k.py,sha256=OqqhdOiWUWN1duUVd3q99IeatWJZHnRVWLoyu7Nb2Fs,2596
 sparseml/transformers/finetune/data/open_platypus.py,sha256=ZJEPEiiJtKpxKm5rOTUHdANnTk0C24mGL3lpghSpj8w,3435
 sparseml/transformers/finetune/data/ptb.py,sha256=mr572aTmdSK7AyY3btPE0eQh27Xcx-dnWY76jcdtMFg,1369
 sparseml/transformers/finetune/data/ultrachat_200k.py,sha256=lRNNO7JV0SIxp1SMp2RG25R4jBpD2_LopsjqCc1KhB8,3521
 sparseml/transformers/finetune/data/wikitext.py,sha256=jHCYaAqMx2vgInMV2-Hg7wXh-TU4TZ691rAkVkP9UnM,1237
-sparseml/transformers/sparsification/__init__.py,sha256=f1YW4ztJxM1OlRPimYAW4nIabqDUSaUArM-VuFbJtSA,922
+sparseml/transformers/sparsification/__init__.py,sha256=3fU5HHECbruEOuyDo-KhGxyzqai_koFF94WvmsMzGj4,950
+sparseml/transformers/sparsification/compressed_tensors_utils.py,sha256=KhoZV8yEZ-EcO_vgSBN9KqGGUCuby6dz1yFv9NV1fxA,7889
 sparseml/transformers/sparsification/question_answering.py,sha256=YENvQh5ZVine9K7cEts2N_N7oUYTdYeF0Gp9LFzBRII,19272
 sparseml/transformers/sparsification/sparse_config.py,sha256=1zAwQcO2yQB-b0kecaAXECJMjlwIQPEKnui90Wvj7Dw,1874
-sparseml/transformers/sparsification/sparse_model.py,sha256=z17-AfeLo1fkzCvePtYjd3LVK_EFtBIoKY5TqjlHPFk,20264
+sparseml/transformers/sparsification/sparse_model.py,sha256=f48cdV-Ptkyo_IcwbYuD1M_UlGoWdNdN2bAGARJuneU,20127
 sparseml/transformers/sparsification/sparse_tokenizer.py,sha256=kW_7pQ4awjsGMx3pzPvOS89n6T3ehBes1geop02XWKY,2327
 sparseml/transformers/sparsification/trainer.py,sha256=5MdTsqPTE-ZmqYr9e4Z0wucQLK8u7g-ahMlilZQX-tk,40305
 sparseml/transformers/sparsification/training_args.py,sha256=gGivIDchLi__PZMNQRmzDOaQcQLkUVFystq3LaVko3Y,1890
-sparseml/transformers/sparsification/modification/__init__.py,sha256=LpHfgSjpeYu_-E54ddk0bOd3X_uuUMcQQvKBi4GfXtU,866
-sparseml/transformers/sparsification/modification/base.py,sha256=tSMEYZzwsjLiuU5gi4egMSJVmwwtwIf8OjapiTNoejw,2429
-sparseml/transformers/sparsification/modification/modification_objects.py,sha256=hZzGEv1pg7z8CKpdQpBpnB-0cBVuwJM2c8SxYODAL5E,3922
-sparseml/transformers/sparsification/modification/modify_model.py,sha256=RI2DrXwX0MfCQmWQkUUwJV0beemUa7hZGq7wNp5Q6MQ,2434
-sparseml/transformers/sparsification/modification/modifying_bert.py,sha256=EpgzqGReQFmV1K02yPRLw8vqIll41QbqROJkTjffZ5E,9148
-sparseml/transformers/sparsification/modification/modifying_distilbert.py,sha256=L6XEQu_d4NFw5RD4s05GrBJc3tcjSnVuUur8Z4kgWP0,5883
-sparseml/transformers/sparsification/modification/modifying_llama.py,sha256=Qu-GDKn5TgGAJPcgVUwVQw1kS3kImn6pBEfR19fU2R4,8457
-sparseml/transformers/sparsification/modification/modifying_mistral.py,sha256=lfNzP0SA6a2MvoHIenjUb9RMckkkyIMm6bMsLAk0dlA,7689
-sparseml/transformers/sparsification/modification/modifying_mobilebert.py,sha256=ZCUhkLeX36sQ75yRuxXETUlwG4VoChnoEljxiznCQHs,2549
-sparseml/transformers/sparsification/modification/modifying_opt.py,sha256=VPe9yCPySaVZZHuMjGyHxzCqasB6GeAZg391NIkYtrk,9862
-sparseml/transformers/sparsification/modification/registry.py,sha256=U4AeTD4xJHt3pnn6iXskq3zP1GY1d26h9owV4r3OdzE,1486
+sparseml/transformers/sparsification/modification/__init__.py,sha256=5TbTkuMCuK6RXztyDiWn1_B48M__n6SvbGyCJKhtUTw,1042
+sparseml/transformers/sparsification/modification/base.py,sha256=nJ-lCfnmkW1p5p7wQqMFb3jP7UeXMDWqffMJ1S50_N4,2430
+sparseml/transformers/sparsification/modification/modifying_bert.py,sha256=5paspFWIdMYHYby0yt4epd4u_sj3hhdHKE2wGDPFZbU,8950
+sparseml/transformers/sparsification/modification/modifying_distilbert.py,sha256=L49vYnnl-JQ0zAsWSpMYAHR04reLA2_-Q0xpTaiXz20,5713
+sparseml/transformers/sparsification/modification/modifying_llama.py,sha256=28OF3crGRbl-xXp4McX0PMYknR70QCeEOT2NzzW9Q8U,8385
+sparseml/transformers/sparsification/modification/modifying_mistral.py,sha256=ebSMvATYTT55WvkQzsxwF3Ws50EunSpN7vc5ZQ5vPSc,7614
+sparseml/transformers/sparsification/modification/modifying_mobilebert.py,sha256=XXVHLelJJqG8JH9GN5F5NAaKgxTK2gNiVxPEdHiTbNE,2709
+sparseml/transformers/sparsification/modification/modifying_opt.py,sha256=4lVxFXk4bQMMH_XDENvmpFTblGqOqA0CUlvQC927_lY,9759
 sparseml/transformers/sparsification/obcq/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/transformers/sparsification/obcq/export.py,sha256=6gJHqz0J4tNanmEC0fi1xn6jQ4Y0WEqzJg1nHfH4PXM,19683
 sparseml/transformers/sparsification/obcq/obcq.py,sha256=QLN1ywZCCMF5GH-Z0-ETf4EpsdfR589h8ws-_-EJL2A,7695
 sparseml/transformers/sparsification/obcq/utils/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/transformers/sparsification/obcq/utils/helpers.py,sha256=btmRVzMnGC4rxqeFRaZxVfewVhoerib6xA62cBuyFfA,3671
 sparseml/transformers/utils/__init__.py,sha256=NVWYxKuoVwMglNalk_zs0PiDi6ec4JHG-DsuCNEDkBY,805
-sparseml/transformers/utils/helpers.py,sha256=few6gxw9b04X2xVCbNJC26vyoB_1NIlXB7SlGZ8WCzM,20055
+sparseml/transformers/utils/helpers.py,sha256=QeBiWOc7XxFFDZOC3znyfWTZvVOZOgly3HnByu7ISAg,22843
 sparseml/transformers/utils/initializers.py,sha256=UrFAqZhQemFOGu8ljmpqpm2mDV4rpP7NfNMzscbtz58,6711
 sparseml/transformers/utils/load_task_dataset.py,sha256=zwOhd7deFZmgerr1ohhICHulOrMYnxx2XEs9bQbXFG0,5081
 sparseml/transformers/utils/load_task_model.py,sha256=9YQhIMpFeZB48uaJjgOD1Vy_D8NDp2s2JzUPsqpbOrE,2764
 sparseml/transformers/utils/metrics.py,sha256=ciZsxgdrzlSMnyzbgKOFE3spWsKql0z82BiYFRXiVOU,2536
 sparseml/transformers/utils/optimizations.py,sha256=InpU3m6pHp_2X9PEq_ueYL4iHI512LprVJAaY5Io_6A,1972
 sparseml/transformers/utils/preprocessing_functions.py,sha256=fPmmo1StTu4845lqZvyRyrXZOro-BrmWTMX3hJPUBZM,1037
 sparseml/utils/__init__.py,sha256=-WKBN4DERhw7rTJkarilkyAM9pw4rW8qkXGRPSEJ8SM,844
@@ -529,17 +523,17 @@
 sparseml/utils/datasets/coco.py,sha256=5w9Fm0O2kUtrC1upa6DCC4WZ13df-ZlORPUHEwGFDLc,3750
 sparseml/utils/datasets/helpers.py,sha256=1i1jiOoj1q-_Nef5dJBWHoO9v7ZjogggihDjBMNbf_A,1217
 sparseml/utils/datasets/imagenet.py,sha256=qAL7K6tmlfwBY7FI14wcVb-7g3Rzs4wtZIOV1kvoV0w,23366
 sparseml/utils/datasets/imagenette.py,sha256=AIxAI0wfUqu08XPJ1cG8SNUVocjoeFHVepQZDfsztFI,8967
 sparseml/utils/datasets/voc.py,sha256=CzSEnTe-KqWyOLwleame66PtYqF8vuq9EXaNgmms6u0,1009
 sparseml/utils/fsdp/__init__.py,sha256=TF5uEKrpc2qYdbgmXh9xVp2GQ3p--by-LoCegY19EeI,633
 sparseml/utils/fsdp/context.py,sha256=z3d6oH1eebYnOTVd8-fGRXo6pMV2NLhTiXMNYbQ8v0g,2096
-sparseml/utils/fsdp/helpers.py,sha256=TJN6AASbh6H7kLvNXh8d1R81BRO3HiXcOdXNVTaE5pk,6631
+sparseml/utils/fsdp/helpers.py,sha256=QjaA8Ruq24ZeueQaVOifJ6EPiBg_-fOOBsHlICJkK0s,6736
 sparseml/utils/pytorch/__init__.py,sha256=zgv8znrlT0w5SeQUkCnbZGVhFyBS_bD4Ojx7x2xz7tI,656
-sparseml/utils/pytorch/module.py,sha256=37S5evDCaNt1ZKWDrc8WGpQngu66wWcKn9v9M48UsyM,10897
+sparseml/utils/pytorch/module.py,sha256=UpK6qBv_v0dhHHVDH4v2zDWiWn9DWntHojz2pPZGnHw,11030
 sparseml/utils/pytorch/utils.py,sha256=x5G0_FNF12xX-WFd5Al2GcRfSzRprx-1rjxWp4AHBII,1696
 sparseml/utils/pytorch/pruning/__init__.py,sha256=N03cP9aDRA_oYadFQPEHg7vkR8hzQrh8CXeDN4AZPb8,680
 sparseml/yolact/COCO.sh,sha256=Q_1I3VZwS4N1e3AwOoimQH2plK2K0athUDWooWR4ZVQ,1875
 sparseml/yolact/COCO_test.sh,sha256=ZoA_h_68zM8EWF510o24_5xBaQtbJLMHenNjSzldYYc,1418
 sparseml/yolact/__init__.py,sha256=5t9V0aePHnzJBik6z1r4W_3DsSn0lKpYjl3pIuZQft0,4020
 sparseml/yolact/scripts.py,sha256=7GE3xp_B8JJpa2H-Vum6rmiOJHcChyffuno4-yteUUw,1784
 sparseml/yolov5/__init__.py,sha256=LiVvIlHR1FF63Ixm2MabdDH1Ul8sFEUEIb3Sbs4awLI,1440
@@ -553,15 +547,15 @@
 sparseml/yolov8/train.py,sha256=eetdfCbRQJdBWc7XHb9GYPqMQxZzbKG7FG-4xQeTutk,7394
 sparseml/yolov8/trainers.py,sha256=uQsIX5Zyv6mkK8NLJqmM5kukHojZTZmVEKTKm1pkgxU,37860
 sparseml/yolov8/val.py,sha256=hlFImvknSpV1nONOxA3ivYgvzm64EmK0_Lh-JHLbOsw,2748
 sparseml/yolov8/validators.py,sha256=fkEnjRp1day-KY0n7DZ_VI-zmNqKbalh-g6U-lwHmfQ,8459
 sparseml/yolov8/utils/__init__.py,sha256=6JekgnibQP-8p8Dm1dGiIEGCGdBOAkSOnEds0BMSYhQ,685
 sparseml/yolov8/utils/export_samples.py,sha256=HFmQsXpmXZlxLTNBvHwp-lcM5mVcZN1ouBfAXTdYFeM,6683
 sparseml/yolov8/utils/helpers.py,sha256=8JZNaTT1zPKiZaOccmMtQu5mXCSMGkq8BDEgUqaPVIs,4041
-sparseml_nightly-1.8.0.20240404.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-sparseml_nightly-1.8.0.20240404.dist-info/LICENSE-ULTRALYTICS,sha256=JtH3RHvJDRVTEhj6JJZJH5MQFk5IkSg_TxYAqtPEOOs,13747
-sparseml_nightly-1.8.0.20240404.dist-info/METADATA,sha256=Cz0r_itIm479UISFLO3GHvjvZDd2Si3p0iwaoB_joDM,23627
-sparseml_nightly-1.8.0.20240404.dist-info/NOTICE,sha256=jkdOJGVbNDogFmOaXgc_Qr6U35t67RqzUJgf_Yc8C5w,2085
-sparseml_nightly-1.8.0.20240404.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-sparseml_nightly-1.8.0.20240404.dist-info/entry_points.txt,sha256=AOCpCae2aLRUTce3NNDHYPZevTpJ-GTOQuzy8YPafmc,3122
-sparseml_nightly-1.8.0.20240404.dist-info/top_level.txt,sha256=JOOlWKgkyuJBScnty7pC1SQ58fOo1ONbslvMdxB6L2M,9
-sparseml_nightly-1.8.0.20240404.dist-info/RECORD,,
+sparseml_nightly-1.8.0.20240507.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+sparseml_nightly-1.8.0.20240507.dist-info/LICENSE-ULTRALYTICS,sha256=JtH3RHvJDRVTEhj6JJZJH5MQFk5IkSg_TxYAqtPEOOs,13747
+sparseml_nightly-1.8.0.20240507.dist-info/METADATA,sha256=b5z6PEyo90HxrgQcNSoDTxXb02_Fkyjy38QM1mWqeGQ,23695
+sparseml_nightly-1.8.0.20240507.dist-info/NOTICE,sha256=jkdOJGVbNDogFmOaXgc_Qr6U35t67RqzUJgf_Yc8C5w,2085
+sparseml_nightly-1.8.0.20240507.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+sparseml_nightly-1.8.0.20240507.dist-info/entry_points.txt,sha256=u1Yng_f353ZXz7vzzh64RRfgz68Dv155G2dtG2uMXkg,3121
+sparseml_nightly-1.8.0.20240507.dist-info/top_level.txt,sha256=JOOlWKgkyuJBScnty7pC1SQ58fOo1ONbslvMdxB6L2M,9
+sparseml_nightly-1.8.0.20240507.dist-info/RECORD,,
```

