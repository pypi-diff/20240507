# Comparing `tmp/spark_rapids_user_tools-24.2.4-248_e093c7c-py3-none-any.whl.zip` & `tmp/spark_rapids_user_tools-24.4.0-250_bc0a0ae-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,130 +1,131 @@
-Zip file size: 866245 bytes, number of entries: 128
--rw-r--r--  2.0 unx      755 b- defN 24-Apr-30 11:13 spark_rapids_pytools/__init__.py
--rw-r--r--  2.0 unx      992 b- defN 24-Apr-30 11:13 spark_rapids_pytools/build.py
--rw-r--r--  2.0 unx     1450 b- defN 24-Apr-30 11:13 spark_rapids_pytools/wrapper.py
--rw-r--r--  2.0 unx      646 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/__init__.py
--rw-r--r--  2.0 unx     8507 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/azurestorage.py
--rw-r--r--  2.0 unx    16540 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/databricks_aws.py
--rw-r--r--  2.0 unx     1268 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/databricks_aws_job.py
--rw-r--r--  2.0 unx    19913 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/databricks_azure.py
--rw-r--r--  2.0 unx     2428 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/databricks_azure_job.py
--rw-r--r--  2.0 unx    27376 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/dataproc.py
--rw-r--r--  2.0 unx     8455 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/dataproc_gke.py
--rw-r--r--  2.0 unx      929 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/dataproc_gke_job.py
--rw-r--r--  2.0 unx      922 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/dataproc_job.py
--rw-r--r--  2.0 unx    23288 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/emr.py
--rw-r--r--  2.0 unx     1252 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/emr_job.py
--rw-r--r--  2.0 unx     4939 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/gstorage.py
--rw-r--r--  2.0 unx    13941 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/onprem.py
--rw-r--r--  2.0 unx     4055 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/s3storage.py
--rw-r--r--  2.0 unx    53339 b- defN 24-Apr-30 11:13 spark_rapids_pytools/cloud_api/sp_types.py
--rw-r--r--  2.0 unx      658 b- defN 24-Apr-30 11:13 spark_rapids_pytools/common/__init__.py
--rw-r--r--  2.0 unx     3927 b- defN 24-Apr-30 11:13 spark_rapids_pytools/common/cluster_inference.py
--rw-r--r--  2.0 unx      978 b- defN 24-Apr-30 11:13 spark_rapids_pytools/common/exceptions.py
--rw-r--r--  2.0 unx     5119 b- defN 24-Apr-30 11:13 spark_rapids_pytools/common/prop_manager.py
--rw-r--r--  2.0 unx    20141 b- defN 24-Apr-30 11:13 spark_rapids_pytools/common/sys_storage.py
--rw-r--r--  2.0 unx    14532 b- defN 24-Apr-30 11:13 spark_rapids_pytools/common/utilities.py
--rw-r--r--  2.0 unx      659 b- defN 24-Apr-30 11:13 spark_rapids_pytools/pricing/__init__.py
--rw-r--r--  2.0 unx     3429 b- defN 24-Apr-30 11:13 spark_rapids_pytools/pricing/databricks_azure_pricing.py
--rw-r--r--  2.0 unx     3522 b- defN 24-Apr-30 11:13 spark_rapids_pytools/pricing/databricks_pricing.py
--rw-r--r--  2.0 unx     1234 b- defN 24-Apr-30 11:13 spark_rapids_pytools/pricing/dataproc_gke_pricing.py
--rw-r--r--  2.0 unx     4247 b- defN 24-Apr-30 11:13 spark_rapids_pytools/pricing/dataproc_pricing.py
--rw-r--r--  2.0 unx     4747 b- defN 24-Apr-30 11:13 spark_rapids_pytools/pricing/emr_pricing.py
--rw-r--r--  2.0 unx     6698 b- defN 24-Apr-30 11:13 spark_rapids_pytools/pricing/price_provider.py
--rw-r--r--  2.0 unx      666 b- defN 24-Apr-30 11:13 spark_rapids_pytools/rapids/__init__.py
--rw-r--r--  2.0 unx     6026 b- defN 24-Apr-30 11:13 spark_rapids_pytools/rapids/bootstrap.py
--rw-r--r--  2.0 unx     6780 b- defN 24-Apr-30 11:13 spark_rapids_pytools/rapids/diagnostic.py
--rw-r--r--  2.0 unx    10472 b- defN 24-Apr-30 11:13 spark_rapids_pytools/rapids/profiling.py
--rw-r--r--  2.0 unx    59208 b- defN 24-Apr-30 11:13 spark_rapids_pytools/rapids/qualification.py
--rw-r--r--  2.0 unx     5739 b- defN 24-Apr-30 11:13 spark_rapids_pytools/rapids/rapids_job.py
--rw-r--r--  2.0 unx    43836 b- defN 24-Apr-30 11:13 spark_rapids_pytools/rapids/rapids_tool.py
--rw-r--r--  2.0 unx     8938 b- defN 24-Apr-30 11:13 spark_rapids_pytools/rapids/tool_ctxt.py
--rw-r--r--  2.0 unx      273 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/bootstrap-conf.yaml
--rw-r--r--  2.0 unx     1073 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/cluster-configs.yaml
--rwxr-xr-x  2.0 unx     5816 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/collect.sh
--rw-r--r--  2.0 unx    30566 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/databricks-premium-catalog.json
--rw-r--r--  2.0 unx    13549 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/databricks_aws-configs.json
--rw-r--r--  2.0 unx    11272 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/databricks_azure-configs.json
--rw-r--r--  2.0 unx     9280 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/dataproc-configs.json
--rw-r--r--  2.0 unx     9069 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/dataproc_gke-configs.json
--rw-r--r--  2.0 unx       30 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/diagnostic-conf.yaml
--rw-r--r--  2.0 unx    12779 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/emr-configs.json
--rw-r--r--  2.0 unx     5159 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/onprem-configs.json
--rw-r--r--  2.0 unx    38903 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/premium-databricks-azure-catalog.json
--rw-r--r--  2.0 unx     1517 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/profiling-conf.yaml
--rw-r--r--  2.0 unx     8164 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/qualification-conf.yaml
--rw-r--r--  2.0 unx     7779 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/dev/prepackage_mgr.py
--rw-r--r--  2.0 unx     2195 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/dev/process_databricks_azure_pricing.py
--rw-r--r--  2.0 unx   405769 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/qualx/models/xgboost/databricks-aws.json
--rw-r--r--  2.0 unx   160200 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/qualx/models/xgboost/databricks-azure.json
--rw-r--r--  2.0 unx   795042 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/qualx/models/xgboost/dataproc.json
--rw-r--r--  2.0 unx   361292 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/qualx/models/xgboost/dataproc_2.1-orc.json
--rw-r--r--  2.0 unx   530621 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/qualx/models/xgboost/onprem.json
--rw-r--r--  2.0 unx      669 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms
--rw-r--r--  2.0 unx      518 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms
--rw-r--r--  2.0 unx      855 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms
--rw-r--r--  2.0 unx      537 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms
--rw-r--r--  2.0 unx      241 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/cluster_template/databricks_aws.ms
--rw-r--r--  2.0 unx      291 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/cluster_template/databricks_azure.ms
--rw-r--r--  2.0 unx      579 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/cluster_template/dataproc.ms
--rw-r--r--  2.0 unx      759 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/cluster_template/emr.ms
--rw-r--r--  2.0 unx       40 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/node_template/databricks_aws.ms
--rw-r--r--  2.0 unx       40 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/node_template/databricks_azure.ms
--rw-r--r--  2.0 unx      293 b- defN 24-Apr-30 11:13 spark_rapids_pytools/resources/templates/node_template/emr.ms
--rw-r--r--  2.0 unx      630 b- defN 24-Apr-30 11:13 spark_rapids_pytools/wrappers/__init__.py
--rw-r--r--  2.0 unx    19338 b- defN 24-Apr-30 11:13 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py
--rw-r--r--  2.0 unx    18795 b- defN 24-Apr-30 11:13 spark_rapids_pytools/wrappers/databricks_azure_wrapper.py
--rw-r--r--  2.0 unx     9834 b- defN 24-Apr-30 11:13 spark_rapids_pytools/wrappers/dataproc_gke_wrapper.py
--rw-r--r--  2.0 unx    18774 b- defN 24-Apr-30 11:13 spark_rapids_pytools/wrappers/dataproc_wrapper.py
--rw-r--r--  2.0 unx    19300 b- defN 24-Apr-30 11:13 spark_rapids_pytools/wrappers/emr_wrapper.py
--rw-r--r--  2.0 unx    12518 b- defN 24-Apr-30 11:13 spark_rapids_pytools/wrappers/onprem_wrapper.py
--rw-r--r--  2.0 unx     1008 b- defN 24-Apr-30 11:13 spark_rapids_tools/__init__.py
--rw-r--r--  2.0 unx     5286 b- defN 24-Apr-30 11:13 spark_rapids_tools/enums.py
--rw-r--r--  2.0 unx     2099 b- defN 24-Apr-30 11:13 spark_rapids_tools/exceptions.py
--rw-r--r--  2.0 unx     1167 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/__init__.py
--rw-r--r--  2.0 unx     4759 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/cluster.py
--rw-r--r--  2.0 unx      649 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/databricks/__init__.py
--rw-r--r--  2.0 unx     1808 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/databricks/dbcluster.py
--rw-r--r--  2.0 unx      647 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/dataproc/__init__.py
--rw-r--r--  2.0 unx     2243 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/dataproc/dataproccluster.py
--rw-r--r--  2.0 unx      642 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/emr/__init__.py
--rw-r--r--  2.0 unx     1243 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/emr/emrcluster.py
--rw-r--r--  2.0 unx      645 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/onprem/__init__.py
--rw-r--r--  2.0 unx     1768 b- defN 24-Apr-30 11:13 spark_rapids_tools/cloud/onprem/onpremcluster.py
--rw-r--r--  2.0 unx      706 b- defN 24-Apr-30 11:13 spark_rapids_tools/cmdli/__init__.py
--rw-r--r--  2.0 unx    27691 b- defN 24-Apr-30 11:13 spark_rapids_tools/cmdli/argprocessor.py
--rw-r--r--  2.0 unx    16404 b- defN 24-Apr-30 11:13 spark_rapids_tools/cmdli/tools_cli.py
--rw-r--r--  2.0 unx     1433 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/__init__.py
--rw-r--r--  2.0 unx     5549 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/cspfs.py
--rw-r--r--  2.0 unx    11165 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/csppath.py
--rw-r--r--  2.0 unx      774 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/adls/__init__.py
--rw-r--r--  2.0 unx     1658 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/adls/adlsfs.py
--rw-r--r--  2.0 unx      785 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/adls/adlspath.py
--rw-r--r--  2.0 unx      751 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/gcs/__init__.py
--rw-r--r--  2.0 unx     1528 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/gcs/gcsfs.py
--rw-r--r--  2.0 unx      784 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/gcs/gcspath.py
--rw-r--r--  2.0 unx      755 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/hdfs/__init__.py
--rw-r--r--  2.0 unx     1761 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/hdfs/hdfsfs.py
--rw-r--r--  2.0 unx      784 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/hdfs/hdfspath.py
--rw-r--r--  2.0 unx      760 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/local/__init__.py
--rw-r--r--  2.0 unx      865 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/local/localfs.py
--rw-r--r--  2.0 unx      780 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/local/localpath.py
--rw-r--r--  2.0 unx      726 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/s3/__init__.py
--rw-r--r--  2.0 unx     1732 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/s3/s3fs.py
--rw-r--r--  2.0 unx      776 b- defN 24-Apr-30 11:13 spark_rapids_tools/storagelib/s3/s3path.py
--rw-r--r--  2.0 unx      653 b- defN 24-Apr-30 11:13 spark_rapids_tools/tools/__init__.py
--rw-r--r--  2.0 unx     1096 b- defN 24-Apr-30 11:13 spark_rapids_tools/tools/autotuner.py
--rw-r--r--  2.0 unx    54414 b- defN 24-Apr-30 11:13 spark_rapids_tools/tools/model_xgboost.py
--rw-r--r--  2.0 unx     4764 b- defN 24-Apr-30 11:13 spark_rapids_tools/tools/speedup_category.py
--rw-r--r--  2.0 unx     1714 b- defN 24-Apr-30 11:13 spark_rapids_tools/tools/top_candidates.py
--rw-r--r--  2.0 unx     4081 b- defN 24-Apr-30 11:13 spark_rapids_tools/tools/unsupported_ops_stage_duration.py
--rw-r--r--  2.0 unx      987 b- defN 24-Apr-30 11:13 spark_rapids_tools/utils/__init__.py
--rw-r--r--  2.0 unx     4844 b- defN 24-Apr-30 11:13 spark_rapids_tools/utils/propmanager.py
--rw-r--r--  2.0 unx    12742 b- defN 24-Apr-30 11:13 spark_rapids_tools/utils/util.py
--rw-r--r--  2.0 unx    21086 b- defN 24-Apr-30 11:13 spark_rapids_user_tools-24.2.4.dist-info/LICENSE
--rw-r--r--  2.0 unx     4126 b- defN 24-Apr-30 11:13 spark_rapids_user_tools-24.2.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-30 11:13 spark_rapids_user_tools-24.2.4.dist-info/WHEEL
--rw-r--r--  2.0 unx      133 b- defN 24-Apr-30 11:13 spark_rapids_user_tools-24.2.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       40 b- defN 24-Apr-30 11:13 spark_rapids_user_tools-24.2.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    13384 b- defN 24-Apr-30 11:13 spark_rapids_user_tools-24.2.4.dist-info/RECORD
-128 files, 3143197 bytes uncompressed, 844187 bytes compressed:  73.1%
+Zip file size: 868787 bytes, number of entries: 129
+-rw-r--r--  2.0 unx      755 b- defN 24-May-07 18:00 spark_rapids_pytools/__init__.py
+-rw-r--r--  2.0 unx      992 b- defN 24-May-07 18:00 spark_rapids_pytools/build.py
+-rw-r--r--  2.0 unx     1450 b- defN 24-May-07 18:00 spark_rapids_pytools/wrapper.py
+-rw-r--r--  2.0 unx      646 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/__init__.py
+-rw-r--r--  2.0 unx     8507 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/azurestorage.py
+-rw-r--r--  2.0 unx    16540 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/databricks_aws.py
+-rw-r--r--  2.0 unx     1268 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/databricks_aws_job.py
+-rw-r--r--  2.0 unx    19913 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/databricks_azure.py
+-rw-r--r--  2.0 unx     2428 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/databricks_azure_job.py
+-rw-r--r--  2.0 unx    27376 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/dataproc.py
+-rw-r--r--  2.0 unx     8455 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/dataproc_gke.py
+-rw-r--r--  2.0 unx      929 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/dataproc_gke_job.py
+-rw-r--r--  2.0 unx      922 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/dataproc_job.py
+-rw-r--r--  2.0 unx    23288 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/emr.py
+-rw-r--r--  2.0 unx     1252 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/emr_job.py
+-rw-r--r--  2.0 unx     4939 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/gstorage.py
+-rw-r--r--  2.0 unx    13941 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/onprem.py
+-rw-r--r--  2.0 unx     4055 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/s3storage.py
+-rw-r--r--  2.0 unx    53339 b- defN 24-May-07 18:00 spark_rapids_pytools/cloud_api/sp_types.py
+-rw-r--r--  2.0 unx      658 b- defN 24-May-07 18:00 spark_rapids_pytools/common/__init__.py
+-rw-r--r--  2.0 unx     3927 b- defN 24-May-07 18:00 spark_rapids_pytools/common/cluster_inference.py
+-rw-r--r--  2.0 unx      978 b- defN 24-May-07 18:00 spark_rapids_pytools/common/exceptions.py
+-rw-r--r--  2.0 unx     5119 b- defN 24-May-07 18:00 spark_rapids_pytools/common/prop_manager.py
+-rw-r--r--  2.0 unx    20141 b- defN 24-May-07 18:00 spark_rapids_pytools/common/sys_storage.py
+-rw-r--r--  2.0 unx    14532 b- defN 24-May-07 18:00 spark_rapids_pytools/common/utilities.py
+-rw-r--r--  2.0 unx      659 b- defN 24-May-07 18:00 spark_rapids_pytools/pricing/__init__.py
+-rw-r--r--  2.0 unx     3429 b- defN 24-May-07 18:00 spark_rapids_pytools/pricing/databricks_azure_pricing.py
+-rw-r--r--  2.0 unx     3522 b- defN 24-May-07 18:00 spark_rapids_pytools/pricing/databricks_pricing.py
+-rw-r--r--  2.0 unx     1234 b- defN 24-May-07 18:00 spark_rapids_pytools/pricing/dataproc_gke_pricing.py
+-rw-r--r--  2.0 unx     4247 b- defN 24-May-07 18:00 spark_rapids_pytools/pricing/dataproc_pricing.py
+-rw-r--r--  2.0 unx     4747 b- defN 24-May-07 18:00 spark_rapids_pytools/pricing/emr_pricing.py
+-rw-r--r--  2.0 unx     6698 b- defN 24-May-07 18:00 spark_rapids_pytools/pricing/price_provider.py
+-rw-r--r--  2.0 unx      666 b- defN 24-May-07 18:00 spark_rapids_pytools/rapids/__init__.py
+-rw-r--r--  2.0 unx     6026 b- defN 24-May-07 18:00 spark_rapids_pytools/rapids/bootstrap.py
+-rw-r--r--  2.0 unx     6780 b- defN 24-May-07 18:00 spark_rapids_pytools/rapids/diagnostic.py
+-rw-r--r--  2.0 unx     3673 b- defN 24-May-07 18:00 spark_rapids_pytools/rapids/prediction.py
+-rw-r--r--  2.0 unx    10472 b- defN 24-May-07 18:00 spark_rapids_pytools/rapids/profiling.py
+-rw-r--r--  2.0 unx    59208 b- defN 24-May-07 18:00 spark_rapids_pytools/rapids/qualification.py
+-rw-r--r--  2.0 unx     5739 b- defN 24-May-07 18:00 spark_rapids_pytools/rapids/rapids_job.py
+-rw-r--r--  2.0 unx    43836 b- defN 24-May-07 18:00 spark_rapids_pytools/rapids/rapids_tool.py
+-rw-r--r--  2.0 unx     8938 b- defN 24-May-07 18:00 spark_rapids_pytools/rapids/tool_ctxt.py
+-rw-r--r--  2.0 unx      273 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/bootstrap-conf.yaml
+-rw-r--r--  2.0 unx     1073 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/cluster-configs.yaml
+-rwxr-xr-x  2.0 unx     5816 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/collect.sh
+-rw-r--r--  2.0 unx    30566 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/databricks-premium-catalog.json
+-rw-r--r--  2.0 unx    13549 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/databricks_aws-configs.json
+-rw-r--r--  2.0 unx    11272 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/databricks_azure-configs.json
+-rw-r--r--  2.0 unx     9280 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/dataproc-configs.json
+-rw-r--r--  2.0 unx     9069 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/dataproc_gke-configs.json
+-rw-r--r--  2.0 unx       30 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/diagnostic-conf.yaml
+-rw-r--r--  2.0 unx    12779 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/emr-configs.json
+-rw-r--r--  2.0 unx     5159 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/onprem-configs.json
+-rw-r--r--  2.0 unx    38903 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/premium-databricks-azure-catalog.json
+-rw-r--r--  2.0 unx     1476 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/profiling-conf.yaml
+-rw-r--r--  2.0 unx     8195 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/qualification-conf.yaml
+-rw-r--r--  2.0 unx     7779 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/dev/prepackage_mgr.py
+-rw-r--r--  2.0 unx     2195 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/dev/process_databricks_azure_pricing.py
+-rw-r--r--  2.0 unx   405769 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/qualx/models/xgboost/databricks-aws.json
+-rw-r--r--  2.0 unx   160200 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/qualx/models/xgboost/databricks-azure.json
+-rw-r--r--  2.0 unx   795042 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/qualx/models/xgboost/dataproc.json
+-rw-r--r--  2.0 unx   361292 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/qualx/models/xgboost/dataproc_2.1-orc.json
+-rw-r--r--  2.0 unx   530621 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/qualx/models/xgboost/onprem.json
+-rw-r--r--  2.0 unx      669 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms
+-rw-r--r--  2.0 unx      518 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms
+-rw-r--r--  2.0 unx      855 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms
+-rw-r--r--  2.0 unx      537 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms
+-rw-r--r--  2.0 unx      241 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/cluster_template/databricks_aws.ms
+-rw-r--r--  2.0 unx      291 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/cluster_template/databricks_azure.ms
+-rw-r--r--  2.0 unx      579 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/cluster_template/dataproc.ms
+-rw-r--r--  2.0 unx      759 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/cluster_template/emr.ms
+-rw-r--r--  2.0 unx       40 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/node_template/databricks_aws.ms
+-rw-r--r--  2.0 unx       40 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/node_template/databricks_azure.ms
+-rw-r--r--  2.0 unx      293 b- defN 24-May-07 18:00 spark_rapids_pytools/resources/templates/node_template/emr.ms
+-rw-r--r--  2.0 unx      630 b- defN 24-May-07 18:00 spark_rapids_pytools/wrappers/__init__.py
+-rw-r--r--  2.0 unx    19372 b- defN 24-May-07 18:00 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py
+-rw-r--r--  2.0 unx    18829 b- defN 24-May-07 18:00 spark_rapids_pytools/wrappers/databricks_azure_wrapper.py
+-rw-r--r--  2.0 unx     9868 b- defN 24-May-07 18:00 spark_rapids_pytools/wrappers/dataproc_gke_wrapper.py
+-rw-r--r--  2.0 unx    18808 b- defN 24-May-07 18:00 spark_rapids_pytools/wrappers/dataproc_wrapper.py
+-rw-r--r--  2.0 unx    19334 b- defN 24-May-07 18:00 spark_rapids_pytools/wrappers/emr_wrapper.py
+-rw-r--r--  2.0 unx    12552 b- defN 24-May-07 18:00 spark_rapids_pytools/wrappers/onprem_wrapper.py
+-rw-r--r--  2.0 unx     1008 b- defN 24-May-07 18:00 spark_rapids_tools/__init__.py
+-rw-r--r--  2.0 unx     5286 b- defN 24-May-07 18:00 spark_rapids_tools/enums.py
+-rw-r--r--  2.0 unx     2099 b- defN 24-May-07 18:00 spark_rapids_tools/exceptions.py
+-rw-r--r--  2.0 unx     1167 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/__init__.py
+-rw-r--r--  2.0 unx     4759 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/cluster.py
+-rw-r--r--  2.0 unx      649 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/databricks/__init__.py
+-rw-r--r--  2.0 unx     1808 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/databricks/dbcluster.py
+-rw-r--r--  2.0 unx      647 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/dataproc/__init__.py
+-rw-r--r--  2.0 unx     2243 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/dataproc/dataproccluster.py
+-rw-r--r--  2.0 unx      642 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/emr/__init__.py
+-rw-r--r--  2.0 unx     1243 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/emr/emrcluster.py
+-rw-r--r--  2.0 unx      645 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/onprem/__init__.py
+-rw-r--r--  2.0 unx     1768 b- defN 24-May-07 18:00 spark_rapids_tools/cloud/onprem/onpremcluster.py
+-rw-r--r--  2.0 unx      706 b- defN 24-May-07 18:00 spark_rapids_tools/cmdli/__init__.py
+-rw-r--r--  2.0 unx    28314 b- defN 24-May-07 18:00 spark_rapids_tools/cmdli/argprocessor.py
+-rw-r--r--  2.0 unx    18580 b- defN 24-May-07 18:00 spark_rapids_tools/cmdli/tools_cli.py
+-rw-r--r--  2.0 unx     1433 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/__init__.py
+-rw-r--r--  2.0 unx     5549 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/cspfs.py
+-rw-r--r--  2.0 unx    11165 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/csppath.py
+-rw-r--r--  2.0 unx      774 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/adls/__init__.py
+-rw-r--r--  2.0 unx     1658 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/adls/adlsfs.py
+-rw-r--r--  2.0 unx      785 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/adls/adlspath.py
+-rw-r--r--  2.0 unx      751 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/gcs/__init__.py
+-rw-r--r--  2.0 unx     1528 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/gcs/gcsfs.py
+-rw-r--r--  2.0 unx      784 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/gcs/gcspath.py
+-rw-r--r--  2.0 unx      755 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/hdfs/__init__.py
+-rw-r--r--  2.0 unx     1761 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/hdfs/hdfsfs.py
+-rw-r--r--  2.0 unx      784 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/hdfs/hdfspath.py
+-rw-r--r--  2.0 unx      760 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/local/__init__.py
+-rw-r--r--  2.0 unx      865 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/local/localfs.py
+-rw-r--r--  2.0 unx      780 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/local/localpath.py
+-rw-r--r--  2.0 unx      726 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/s3/__init__.py
+-rw-r--r--  2.0 unx     1732 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/s3/s3fs.py
+-rw-r--r--  2.0 unx      776 b- defN 24-May-07 18:00 spark_rapids_tools/storagelib/s3/s3path.py
+-rw-r--r--  2.0 unx      653 b- defN 24-May-07 18:00 spark_rapids_tools/tools/__init__.py
+-rw-r--r--  2.0 unx     1096 b- defN 24-May-07 18:00 spark_rapids_tools/tools/autotuner.py
+-rw-r--r--  2.0 unx    55186 b- defN 24-May-07 18:00 spark_rapids_tools/tools/model_xgboost.py
+-rw-r--r--  2.0 unx     4764 b- defN 24-May-07 18:00 spark_rapids_tools/tools/speedup_category.py
+-rw-r--r--  2.0 unx     1714 b- defN 24-May-07 18:00 spark_rapids_tools/tools/top_candidates.py
+-rw-r--r--  2.0 unx     4081 b- defN 24-May-07 18:00 spark_rapids_tools/tools/unsupported_ops_stage_duration.py
+-rw-r--r--  2.0 unx      987 b- defN 24-May-07 18:00 spark_rapids_tools/utils/__init__.py
+-rw-r--r--  2.0 unx     4844 b- defN 24-May-07 18:00 spark_rapids_tools/utils/propmanager.py
+-rw-r--r--  2.0 unx    12742 b- defN 24-May-07 18:00 spark_rapids_tools/utils/util.py
+-rw-r--r--  2.0 unx    21086 b- defN 24-May-07 18:00 spark_rapids_user_tools-24.4.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     4488 b- defN 24-May-07 18:00 spark_rapids_user_tools-24.4.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-07 18:00 spark_rapids_user_tools-24.4.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx      133 b- defN 24-May-07 18:00 spark_rapids_user_tools-24.4.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       40 b- defN 24-May-07 18:00 spark_rapids_user_tools-24.4.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    13482 b- defN 24-May-07 18:00 spark_rapids_user_tools-24.4.0.dist-info/RECORD
+129 files, 3151095 bytes uncompressed, 846571 bytes compressed:  73.1%
```

## zipnote {}

```diff
@@ -99,14 +99,17 @@
 
 Filename: spark_rapids_pytools/rapids/bootstrap.py
 Comment: 
 
 Filename: spark_rapids_pytools/rapids/diagnostic.py
 Comment: 
 
+Filename: spark_rapids_pytools/rapids/prediction.py
+Comment: 
+
 Filename: spark_rapids_pytools/rapids/profiling.py
 Comment: 
 
 Filename: spark_rapids_pytools/rapids/qualification.py
 Comment: 
 
 Filename: spark_rapids_pytools/rapids/rapids_job.py
@@ -360,26 +363,26 @@
 
 Filename: spark_rapids_tools/utils/propmanager.py
 Comment: 
 
 Filename: spark_rapids_tools/utils/util.py
 Comment: 
 
-Filename: spark_rapids_user_tools-24.2.4.dist-info/LICENSE
+Filename: spark_rapids_user_tools-24.4.0.dist-info/LICENSE
 Comment: 
 
-Filename: spark_rapids_user_tools-24.2.4.dist-info/METADATA
+Filename: spark_rapids_user_tools-24.4.0.dist-info/METADATA
 Comment: 
 
-Filename: spark_rapids_user_tools-24.2.4.dist-info/WHEEL
+Filename: spark_rapids_user_tools-24.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: spark_rapids_user_tools-24.2.4.dist-info/entry_points.txt
+Filename: spark_rapids_user_tools-24.4.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: spark_rapids_user_tools-24.2.4.dist-info/top_level.txt
+Filename: spark_rapids_user_tools-24.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: spark_rapids_user_tools-24.2.4.dist-info/RECORD
+Filename: spark_rapids_user_tools-24.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## spark_rapids_pytools/__init__.py

```diff
@@ -12,9 +12,9 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """init file of the spark_rapids_pytools package."""
 
 from spark_rapids_pytools.build import get_version
 
-VERSION = '24.02.4'
+VERSION = '24.04.0'
 __version__ = get_version(VERSION)
```

## spark_rapids_pytools/resources/profiling-conf.yaml

```diff
@@ -6,15 +6,15 @@
       section: '### D. Recommended Configuration ###'
       sparkProperties: 'Spark Properties:'
       comments: 'Comments:'
 sparkRapids:
   mvnUrl: 'https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark-tools_2.12'
   repoUrl: '{}/{}/rapids-4-spark-tools_2.12-{}.jar'
   mainClass: 'com.nvidia.spark.rapids.tool.profiling.ProfileMain'
-  outputDocURL: 'https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-profiling-tool.html#understanding-profiling-tool-detailed-output-and-examples'
+  outputDocURL: 'https://docs.nvidia.com/spark-rapids/user-guide/latest/profiling/quickstart.html#profiling-output'
   enableAutoTuner: true
   requireEventLogs: true
   cli:
     toolOptions:
       - any
       - a
       - application-name
```

## spark_rapids_pytools/resources/qualification-conf.yaml

```diff
@@ -61,15 +61,15 @@
       compactWidth: true
       timeUnits: 's'
       columnWidth: 14
 sparkRapids:
   mvnUrl: 'https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark-tools_2.12'
   repoUrl: '{}/{}/rapids-4-spark-tools_2.12-{}.jar'
   mainClass: 'com.nvidia.spark.rapids.tool.qualification.QualificationMain'
-  outputDocURL: 'https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-qualification-tool.html#understanding-the-qualification-tool-output'
+  outputDocURL: 'https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/quickstart.html#qualification-output'
   enableAutoTuner: true
   requireEventLogs: true
   gpu:
     device: 't4'
     workersPerNode: 2
     cudaVersion: '11.5'
   cli:
@@ -256,14 +256,16 @@
     predictionModel:
       outputDirectory: 'xgboost_predictions'
       files:
         perSql:
           name: 'per_sql.csv'
         perApp:
           name: 'per_app.csv'
+        shapValues:
+          name: 'shap_values.csv'
       updateResult:
         subsetColumns:
           - 'appId'
           - 'speedup'
           - 'appDuration_pred'
         estimationModelColumn: 'Speed Up Estimation Model'
         remapColumns:
```

## spark_rapids_pytools/wrappers/databricks_aws_wrapper.py

```diff
@@ -106,15 +106,15 @@
                 (e.g. 30 for 30% discount).
         :param global_discount: A percent discount for both the cpu and gpu cluster costs in the form of an
                 integer value (e.g. 30 for 30% discount).
         :param rapids_options: A list of valid Qualification tool options.
                 Note that the wrapper ignores ["output-directory", "platform"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Qualification tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-qualification-tool.html#qualification-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/jar-usage.html#running-the-qualification-tool-standalone-on-spark-event-logs
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         profile = Utils.get_value_or_pop(profile, rapids_options, 'p')
         aws_profile = Utils.get_value_or_pop(aws_profile,  rapids_options, 'a')
         remote_folder = Utils.get_value_or_pop(remote_folder, rapids_options, 'r')
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j', Utilities.get_system_memory_in_gb())
         eventlogs = Utils.get_value_or_pop(eventlogs, rapids_options, 'e')
@@ -203,15 +203,15 @@
                "~/.databrickscfg" on Unix, Linux, or macOS.
         :param verbose: True or False to enable verbosity to the wrapper script.
         :param jvm_heap_size: The maximum heap size of the JVM in gigabytes.
         :param rapids_options: A list of valid Profiling tool options.
                 Note that the wrapper ignores ["output-directory", "worker-info"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Profiling tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-profiling-tool.html#profiling-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/profiling/jar-usage.html#prof-tool-title-options
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         profile = Utils.get_value_or_pop(profile, rapids_options, 'p')
         aws_profile = Utils.get_value_or_pop(aws_profile,  rapids_options, 'a')
         credentials_file = Utils.get_value_or_pop(credentials_file, rapids_options, 'c')
         gpu_cluster = Utils.get_value_or_pop(gpu_cluster, rapids_options, 'g')
         remote_folder = Utils.get_value_or_pop(remote_folder, rapids_options, 'r')
```

## spark_rapids_pytools/wrappers/databricks_azure_wrapper.py

```diff
@@ -104,15 +104,15 @@
                 (e.g. 30 for 30% discount).
         :param global_discount: A percent discount for both the cpu and gpu cluster costs in the form of an
                 integer value (e.g. 30 for 30% discount).
         :param rapids_options: A list of valid Qualification tool options.
                 Note that the wrapper ignores ["output-directory", "platform"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Qualification tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-qualification-tool.html#qualification-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/jar-usage.html#running-the-qualification-tool-standalone-on-spark-event-logs
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         profile = Utils.get_value_or_pop(profile, rapids_options, 'p')
         remote_folder = Utils.get_value_or_pop(remote_folder, rapids_options, 'r')
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j',
                                                Utilities.get_system_memory_in_gb())
         eventlogs = Utils.get_value_or_pop(eventlogs, rapids_options, 'e')
@@ -197,15 +197,15 @@
                "~/.databrickscfg" on Unix, Linux, or macOS.
         :param verbose: True or False to enable verbosity to the wrapper script.
         :param jvm_heap_size: The maximum heap size of the JVM in gigabytes.
         :param rapids_options: A list of valid Profiling tool options.
                 Note that the wrapper ignores ["output-directory", "worker-info"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Profiling tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-profiling-tool.html#profiling-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/profiling/jar-usage.html#prof-tool-title-options
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         profile = Utils.get_value_or_pop(profile, rapids_options, 'p')
         credentials_file = Utils.get_value_or_pop(credentials_file, rapids_options, 'c')
         gpu_cluster = Utils.get_value_or_pop(gpu_cluster, rapids_options, 'g')
         remote_folder = Utils.get_value_or_pop(remote_folder, rapids_options, 'r')
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j', Utilities.get_system_memory_in_gb())
```

## spark_rapids_pytools/wrappers/dataproc_gke_wrapper.py

```diff
@@ -101,15 +101,15 @@
                 (e.g. 30 for 30% discount).
         :param global_discount: A percent discount for both the cpu and gpu cluster costs in the form of an
                 integer value (e.g. 30 for 30% discount).
         :param rapids_options: A list of valid Qualification tool options.
                 Note that the wrapper ignores ["output-directory", "platform"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Qualification tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-qualification-tool.html#qualification-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/jar-usage.html#running-the-qualification-tool-standalone-on-spark-event-logs
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         remote_folder = Utils.get_value_or_pop(remote_folder, rapids_options, 'r')
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j',
                                                Utilities.get_system_memory_in_gb())
         eventlogs = Utils.get_value_or_pop(eventlogs, rapids_options, 'e')
         filter_apps = Utils.get_value_or_pop(filter_apps, rapids_options, 'f')
```

## spark_rapids_pytools/wrappers/dataproc_wrapper.py

```diff
@@ -103,15 +103,15 @@
                 (e.g. 30 for 30% discount).
         :param global_discount: A percent discount for both the cpu and gpu cluster costs in the form of an
                 integer value (e.g. 30 for 30% discount).
         :param rapids_options: A list of valid Qualification tool options.
                 Note that the wrapper ignores ["output-directory", "platform"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Qualification tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-qualification-tool.html#qualification-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/jar-usage.html#running-the-qualification-tool-standalone-on-spark-event-logs
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         remote_folder = Utils.get_value_or_pop(remote_folder, rapids_options, 'r')
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j',
                                                Utilities.get_system_memory_in_gb())
         eventlogs = Utils.get_value_or_pop(eventlogs, rapids_options, 'e')
         filter_apps = Utils.get_value_or_pop(filter_apps, rapids_options, 'f')
@@ -193,15 +193,15 @@
                "$HOME/.config/gcloud/application_default_credentials.json"
         :param verbose: True or False to enable verbosity to the wrapper script
         :param jvm_heap_size: The maximum heap size of the JVM in gigabytes
         :param rapids_options: A list of valid Profiling tool options.
                 Note that the wrapper ignores ["output-directory", "worker-info"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Profiling tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-profiling-tool.html#profiling-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/profiling/jar-usage.html#prof-tool-title-options
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         credentials_file = Utils.get_value_or_pop(credentials_file, rapids_options, 'c')
         gpu_cluster = Utils.get_value_or_pop(gpu_cluster, rapids_options, 'g')
         remote_folder = Utils.get_value_or_pop(remote_folder, rapids_options, 'r')
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j',
                                                Utilities.get_system_memory_in_gb())
```

## spark_rapids_pytools/wrappers/emr_wrapper.py

```diff
@@ -100,15 +100,15 @@
                 (e.g. 30 for 30% discount).
         :param global_discount: A percent discount for both the cpu and gpu cluster costs in the form of an
                 integer value (e.g. 30 for 30% discount).
         :param rapids_options: A list of valid Qualification tool options.
                 Note that the wrapper ignores ["output-directory", "platform"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Qualification tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-qualification-tool.html#qualification-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/jar-usage.html#running-the-qualification-tool-standalone-on-spark-event-logs
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         profile = Utils.get_value_or_pop(profile, rapids_options, 'p')
         remote_folder = Utils.get_value_or_pop(remote_folder, rapids_options, 'r')
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j',
                                                Utilities.get_system_memory_in_gb())
         eventlogs = Utils.get_value_or_pop(eventlogs, rapids_options, 'e')
@@ -187,15 +187,15 @@
                 from maven repo.
         :param verbose: True or False to enable verbosity to the wrapper script.
         :param jvm_heap_size: The maximum heap size of the JVM in gigabytes.
         :param rapids_options: A list of valid Profiling tool options.
                 Note that the wrapper ignores ["output-directory", "worker-info"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Profiling tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-profiling-tool.html#profiling-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/profiling/jar-usage.html#prof-tool-title-options
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         profile = Utils.get_value_or_pop(profile, rapids_options, 'p')
         gpu_cluster = Utils.get_value_or_pop(gpu_cluster, rapids_options, 'g')
         remote_folder = Utils.get_value_or_pop(remote_folder, rapids_options, 'r')
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j',
                                                Utilities.get_system_memory_in_gb())
```

## spark_rapids_pytools/wrappers/onprem_wrapper.py

```diff
@@ -85,15 +85,15 @@
                 (e.g. 30 for 30% discount).
         :param global_discount: A percent discount for both the cpu and gpu cluster costs in the form of an
                 integer value (e.g. 30 for 30% discount).
         :param rapids_options: A list of valid Qualification tool options.
                 Note that the wrapper ignores ["output-directory", "platform"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Qualification tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-qualification-tool.html#qualification-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/jar-usage.html#running-the-qualification-tool-standalone-on-spark-event-logs
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j',
                                                Utilities.get_system_memory_in_gb())
         eventlogs = Utils.get_value_or_pop(eventlogs, rapids_options, 'e')
         filter_apps = Utils.get_value_or_pop(filter_apps, rapids_options, 'f')
         local_folder = Utils.get_value_or_pop(local_folder, rapids_options, 'l')
@@ -170,15 +170,15 @@
         If missing, the wrapper downloads the latest rapids-4-spark-tools_*.jar from maven repo
         :param verbose: True or False to enable verbosity to the wrapper script
         :param jvm_heap_size: The maximum heap size of the JVM in gigabytes
         :param rapids_options: A list of valid Profiling tool options.
         Note that the wrapper ignores ["output-directory", "worker-info"] flags, and it does not support
         multiple "spark-property" arguments.
         For more details on Profiling tool options, please visit
-        https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-profiling-tool.html#profiling-tool-options
+        https://docs.nvidia.com/spark-rapids/user-guide/latest/profiling/jar-usage.html#prof-tool-title-options
         """
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         jvm_heap_size = Utils.get_value_or_pop(jvm_heap_size, rapids_options, 'j',
                                                Utilities.get_system_memory_in_gb())
         eventlogs = Utils.get_value_or_pop(eventlogs, rapids_options, 'e')
         tools_jar = Utils.get_value_or_pop(tools_jar, rapids_options, 't')
         worker_info = Utils.get_value_or_pop(worker_info, rapids_options, 'w')
```

## spark_rapids_tools/cmdli/argprocessor.py

```diff
@@ -648,7 +648,27 @@
         if self.platform is None:
             error_flag |= 2
             components.append('platform')
         if error_flag > 0:
             missing = str.join(' and ', components)
             raise ValueError(f'Cmd requires [{missing}] to be specified')
         return self
+
+
+@dataclass
+@register_tool_arg_validator('prediction')
+class PredictUserArgModel(AbsToolUserArgModel):
+    """
+    Represents the arguments collected by the user to run the prediction tool.
+    This is used as doing preliminary validation against some of the common pattern
+    """
+    qual_output: str = None
+    prof_output: str = None
+
+    def build_tools_args(self) -> dict:
+        return {
+            'runtimePlatform': self.platform,
+            'qual_output': self.qual_output,
+            'prof_output': self.prof_output,
+            'output_folder': self.output_folder,
+            'platformOpts': {}
+        }
```

## spark_rapids_tools/cmdli/tools_cli.py

```diff
@@ -18,14 +18,15 @@
 import fire
 
 from spark_rapids_tools.cmdli.argprocessor import AbsToolUserArgModel
 from spark_rapids_tools.enums import QualGpuClusterReshapeType
 from spark_rapids_tools.utils.util import gen_app_banner, init_environment
 from spark_rapids_pytools.common.utilities import Utils, ToolLogging
 from spark_rapids_pytools.rapids.bootstrap import Bootstrap
+from spark_rapids_pytools.rapids.prediction import Prediction
 from spark_rapids_pytools.rapids.profiling import ProfilingAsLocal
 from spark_rapids_pytools.rapids.qualification import QualificationAsLocal
 
 
 class ToolsCLI(object):  # pylint: disable=too-few-public-methods
     """CLI that provides a runtime environment that simplifies running cost and performance analysis
     using the RAPIDS Accelerator for Apache Spark.
@@ -115,15 +116,15 @@
         :param jvm_threads: Number of thread to use for parallel processing on the eventlogs batch.
                 Default is calculated as a function of the total number of cores and the heap size on the host.
         :param verbose: True or False to enable verbosity of the script.
         :param rapids_options: A list of valid Qualification tool options.
                 Note that the wrapper ignores ["output-directory", "platform"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Qualification tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-qualification-tool.html#qualification-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/jar-usage.html#running-the-qualification-tool-standalone-on-spark-event-logs
         """
         platform = Utils.get_value_or_pop(platform, rapids_options, 'p')
         target_platform = Utils.get_value_or_pop(target_platform, rapids_options, 't')
         output_folder = Utils.get_value_or_pop(output_folder, rapids_options, 'o')
         filter_apps = Utils.get_value_or_pop(filter_apps, rapids_options, 'f')
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         if verbose:
@@ -190,15 +191,15 @@
         :param jvm_threads: Number of thread to use for parallel processing on the eventlogs batch.
                 Default is calculated as a function of the total number of cores and the heap size on the host.
         :param verbose: True or False to enable verbosity of the script.
         :param rapids_options: A list of valid Profiling tool options.
                 Note that the wrapper ignores ["output-directory", "worker-info"] flags, and it does not support
                 multiple "spark-property" arguments.
                 For more details on Profiling tool options, please visit
-                https://docs.nvidia.com/spark-rapids/user-guide/latest/spark-profiling-tool.html#profiling-tool-options
+                https://docs.nvidia.com/spark-rapids/user-guide/latest/profiling/jar-usage.html#prof-tool-title-options
         """
         eventlogs = Utils.get_value_or_pop(eventlogs, rapids_options, 'e')
         cluster = Utils.get_value_or_pop(cluster, rapids_options, 'c')
         platform = Utils.get_value_or_pop(platform, rapids_options, 'p')
         output_folder = Utils.get_value_or_pop(output_folder, rapids_options, 'o')
         verbose = Utils.get_value_or_pop(verbose, rapids_options, 'v', False)
         if verbose:
@@ -253,14 +254,51 @@
         if boot_args:
             tool_obj = Bootstrap(platform_type=boot_args['runtimePlatform'],
                                  cluster=cluster,
                                  output_folder=boot_args['outputFolder'],
                                  wrapper_options=boot_args)
             tool_obj.launch()
 
+    def prediction(self,
+                   qual_output: str = None,
+                   prof_output: str = None,
+                   output_folder: str = None,
+                   platform: str = 'onprem',
+                   verbose: bool = False):
+        """The prediction cmd takes existing qualification and profiling tool output and runs the
+        estimation model in the qualification tools for GPU speedups.
+
+        :param qual_output: path to the directory which contains the qualification tool output. E.g. user should
+                            specify the parent directory $WORK_DIR where $WORK_DIR/rapids_4_spark_qualification_output
+                            exists.
+        :param prof_output: path to the directory that contains the profiling tool output. E.g. user should
+                            specify the parent directory $WORK_DIR where $WORK_DIR/rapids_4_spark_profile exists.
+        :param output_folder: path to store the output.
+        :param platform: defines one of the following "onprem", "dataproc", "databricks-aws",
+                         and "databricks-azure", default to "onprem".
+        """
+        if verbose:
+            ToolLogging.enable_debug_mode()
+
+        init_environment('pred')
+
+        predict_args = AbsToolUserArgModel.create_tool_args('prediction',
+                                                            platform=platform,
+                                                            qual_output=qual_output,
+                                                            prof_output=prof_output,
+                                                            output_folder=output_folder)
+
+        if predict_args:
+            tool_obj = Prediction(platform_type=predict_args['runtimePlatform'],
+                                  qual_output=predict_args['qual_output'],
+                                  prof_output=predict_args['prof_output'],
+                                  output_folder=predict_args['output_folder'],
+                                  wrapper_options=predict_args)
+            tool_obj.launch()
+
 
 def main():
     # Make Python Fire not use a pager when it prints a help text
     fire.core.Display = lambda lines, out: out.write('\n'.join(lines) + '\n')
     print(gen_app_banner())
     fire.Fire(ToolsCLI())
```

## spark_rapids_tools/tools/model_xgboost.py

```diff
@@ -22,14 +22,15 @@
 import traceback
 from pathlib import Path
 from typing import Optional, Mapping, List, Dict, Callable, Tuple
 
 import numpy as np
 import pandas as pd
 import xgboost as xgb
+import shap
 from tabulate import tabulate
 from xgboost.core import XGBoostError
 
 from spark_rapids_pytools.common.utilities import Utils
 
 logger = logging.getLogger(__name__)
 FILTER_SPILLS = False  # remove queries with any disk/mem spills
@@ -1184,14 +1185,15 @@
 
 
 def predict_model(
         xgb_model: xgb.Booster,
         cpu_aug_tbl: pd.DataFrame,
         feature_cols: List[str],
         label_col: str,
+        output_info: Optional[dict] = None,
 ) -> pd.DataFrame:
     """Use model to predict on feature data."""
     model_features = xgb_model.feature_names
 
     missing = set(model_features) - set(feature_cols)
     extra = set(feature_cols) - set(model_features)
     if missing:
@@ -1201,14 +1203,28 @@
 
     x_dim = cpu_aug_tbl[model_features]
     y_dim = cpu_aug_tbl[label_col] if label_col else None
 
     dmat = xgb.DMatrix(x_dim, y_dim)
     y_pred = xgb_model.predict(dmat)
 
+    # shapley explainer for prediction
+    pd.set_option('display.max_rows', None)
+    explainer = shap.TreeExplainer(xgb_model)
+    shap_values = explainer.shap_values(x_dim)
+    shap_vals = np.abs(shap_values).mean(axis=0)
+    feature_importance = pd.DataFrame(
+            list(zip(feature_cols, shap_vals)), columns=['feature', 'shap_value']
+    )
+    feature_importance.sort_values(by=['shap_value'], ascending=False, inplace=True)
+    shap_values_path = output_info['shapValues']['path']
+    logger.info('Writing SHAPley values to: %s', shap_values_path)
+    feature_importance.to_csv(shap_values_path, index=False)
+    logger.info('Feature importance (SHAPley values)\n %s', feature_importance)
+
     if y_dim is not None:
         # evaluation
         if LOG_LABEL:
             y_pred = np.exp(y_pred)
 
         preds = {'y': y_dim, 'y_pred': y_pred}
         preds_df = pd.DataFrame(preds)
@@ -1426,15 +1442,15 @@
                 if node_level_supp is not None and any(input_df['fraction_supported'] != 1.0)
                 else 'raw'
             )
             logger.info('Predicting dataset (%s): %s', filter_str, dataset)
             features, feature_cols, label_col = extract_model_features(input_df)
             # note: dataset name is already stored in the 'appName' field
             try:
-                results = predict_model(xgb_model, features, feature_cols, label_col)
+                results = predict_model(xgb_model, features, feature_cols, label_col, output_info)
 
                 # compute per-app speedups
                 summary = _compute_summary(results)
                 dataset_summaries.append(summary)
                 if INTERMEDIATE_DATA_ENABLED:
                     _print_summary(summary)
```

## Comparing `spark_rapids_user_tools-24.2.4.dist-info/LICENSE` & `spark_rapids_user_tools-24.4.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `spark_rapids_user_tools-24.2.4.dist-info/METADATA` & `spark_rapids_user_tools-24.4.0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: spark-rapids-user-tools
-Version: 24.2.4
+Version: 24.4.0
 Summary: A simple wrapper process around cloud service providers to run tools for the RAPIDS Accelerator for Apache Spark.
 Author-email: NVIDIA Corporation <spark-rapids-support@nvidia.com>
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
@@ -27,14 +27,15 @@
 Requires-Dist: pydantic ==2.1.1
 Requires-Dist: pylint-pydantic ==0.3.0
 Requires-Dist: pyarrow ==14.0.1
 Requires-Dist: azure-storage-blob ==12.17.0
 Requires-Dist: adlfs ==2023.4.0
 Requires-Dist: progress ==1.6
 Requires-Dist: xgboost ==2.0.3
+Requires-Dist: shap ==0.44.1
 Requires-Dist: psutil ==5.9.8
 Provides-Extra: test
 Requires-Dist: tox ; extra == 'test'
 Requires-Dist: pytest ; extra == 'test'
 Requires-Dist: cli-test-helpers ; extra == 'test'
 
 # spark-rapids-user-tools
@@ -53,15 +54,16 @@
    make sure the cluster is healthy and ready for Spark jobs.
 
 
 ## Getting started
 
 Set up a Python environment with a version between 3.8 and 3.10
 
-1. Run the project in a virtual environment.
+1. Run the project in a virtual environment. Note, .venv is the directory created to put
+   the virtual env in, so modify if you want a different location.
     ```sh
     $ python -m venv .venv
     $ source .venv/bin/activate
     ```
 2. Install spark-rapids-user-tools 
     - Using released package.
       
@@ -85,21 +87,28 @@
 
 3. Make sure to install CSP SDK if you plan to run the tool wrapper.
 
 ## Building from source
 
 Set up a Python environment similar to the steps above.
 
-1. Run the provided build script to compile the project.
+1. Create a virtual environment. Note, .venv is the directory created to put
+   the virtual env in, so modify if you want a different location.
+    ```sh
+    $ python -m venv .venv
+    $ source .venv/bin/activate
+    ```
+
+2. Run the provided build script to compile the project.
 
    ```sh
    $> ./build.sh
    ```
  
-2. **Fat Mode:** Similar to `fat jar` in Java, this mode solves the problem when web access is not
+3. **Fat Mode:** Similar to `fat jar` in Java, this mode solves the problem when web access is not
    available to download resources having Url-paths (http/https).  
    The command builds the tools jar file and downloads the necessary dependencies and packages them
    with the source code into a single 'wheel' file.
 
    ```sh
    $> ./build.sh fat
    ```
```

## Comparing `spark_rapids_user_tools-24.2.4.dist-info/RECORD` & `spark_rapids_user_tools-24.4.0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-spark_rapids_pytools/__init__.py,sha256=qoTSJhmjwxxcpcOdHC7OHvvKIVCVRntQHvJIIi92myI,755
+spark_rapids_pytools/__init__.py,sha256=gLjUsy6i6OcZRfESz8jNHMmB7-S0V7zbk3gPg7XGwWs,755
 spark_rapids_pytools/build.py,sha256=Ej4Pc2jPIyeVUUOyC67ZMc6Tj90NKj0DrXyniJc0FnY,992
 spark_rapids_pytools/wrapper.py,sha256=0MurKLBYvNb3UVHy6BUdAAVjEZMRkinN6Us0I0hNn7k,1450
 spark_rapids_pytools/cloud_api/__init__.py,sha256=NQSbmxhLzvnZrf4ltpgCLMjCEERPv5QoYo62LEdDBs8,646
 spark_rapids_pytools/cloud_api/azurestorage.py,sha256=yuFk7H5Y8aY0H5dOmeJBS03uhgS2nah5v0Kw2GJzDnE,8507
 spark_rapids_pytools/cloud_api/databricks_aws.py,sha256=bS7mULmMYC2l8xfAsE4WBZ2KOYDEqoO_lQyVIke_VuQ,16540
 spark_rapids_pytools/cloud_api/databricks_aws_job.py,sha256=ZNYM2pa8fObRhc9c9VcqCT6HxGJNfHeFhdoVYNek51E,1268
 spark_rapids_pytools/cloud_api/databricks_azure.py,sha256=_JJpeSsVhjrc2kyAEklqcQmUtxXTcAkEd_r4-HMyu3M,19913
@@ -29,14 +29,15 @@
 spark_rapids_pytools/pricing/dataproc_gke_pricing.py,sha256=pXU5hYvWV4fz1PzGqLScURCmZL1DatwT3iv4btgxfC8,1234
 spark_rapids_pytools/pricing/dataproc_pricing.py,sha256=4-RuYxW9V0K0mqj2mqCJsKEUbvxPXHCwF4AsK1z6UXE,4247
 spark_rapids_pytools/pricing/emr_pricing.py,sha256=9KdPjKSC4RFAg40ZpMHbrVEUXaFWDd9unNNaOcdpROw,4747
 spark_rapids_pytools/pricing/price_provider.py,sha256=fUOt34BZeAvsC3WDTnRRCBwchVV0sfkRJT5aGNQ_sxg,6698
 spark_rapids_pytools/rapids/__init__.py,sha256=xiYk9b76AV_v3fEELcYzXCUPvXQhCD687eJ5RCYQW7M,666
 spark_rapids_pytools/rapids/bootstrap.py,sha256=V2-inHdECJYpPXQMuO1KhWvYIGF8M2fYq7fcXVnwAWo,6026
 spark_rapids_pytools/rapids/diagnostic.py,sha256=481_MjrbCtrpS8RJLyM0MgMFRxVkXqToMFHhriQS26k,6780
+spark_rapids_pytools/rapids/prediction.py,sha256=ifLYyxV37q49CKeaTzJfyco4pF8hjxsz8Fia4BPfjww,3673
 spark_rapids_pytools/rapids/profiling.py,sha256=fnDo7I1kv22iaUChompMMNW6ywlCdTtKEYxTVTZabMg,10472
 spark_rapids_pytools/rapids/qualification.py,sha256=Ij2uIW75wnDiMtPQX_Vp9nTW8fNh2PBTb4LfuBFh3iE,59208
 spark_rapids_pytools/rapids/rapids_job.py,sha256=HId6wvfXwsPVb26meKGn_C1PcS68NGT41uNgZZx6vTY,5739
 spark_rapids_pytools/rapids/rapids_tool.py,sha256=4MrN3xLQP4XMOADPXKA2zEynzlQLWTE8kDnGpuQ3Mxg,43836
 spark_rapids_pytools/rapids/tool_ctxt.py,sha256=UFHyD_XnOG80Jj1GQX-MFHAhF5mP9Hth3Fe9wudQqks,8938
 spark_rapids_pytools/resources/bootstrap-conf.yaml,sha256=rqjuQ2gwyOmavD3c8q_mWM5qXCxThczg2ldyjdGwgw4,273
 spark_rapids_pytools/resources/cluster-configs.yaml,sha256=5gklmC6mR1EeKvpeL-m0zrFJGaQnDiBGeXRFGdCoXqM,1073
@@ -46,16 +47,16 @@
 spark_rapids_pytools/resources/databricks_azure-configs.json,sha256=kya7pJ59A-ZDyWAnVlozt9WHrcH4E40A0zFkGwCSkz8,11272
 spark_rapids_pytools/resources/dataproc-configs.json,sha256=GxhdwkyyDseM1jiqs25gEZqJzImJcwPCSd5xl3Mqt6U,9280
 spark_rapids_pytools/resources/dataproc_gke-configs.json,sha256=qp5sd3rPC8PUTZAr1gOEM4DV_ZnZ13bnkQLj9PFkx8A,9069
 spark_rapids_pytools/resources/diagnostic-conf.yaml,sha256=vS32UyRhj5ox2MZ8-6EgMjXEWlGeaw9izYiAdAoWpfE,30
 spark_rapids_pytools/resources/emr-configs.json,sha256=2YFA1VCj6oetMNTTFjsAsqxxDV9U2IkYk8RI_EI0i0A,12779
 spark_rapids_pytools/resources/onprem-configs.json,sha256=LQ2uszvZ0nDpK2HhoAUzsHj-5S-tFeXM4kinJLbkiM0,5159
 spark_rapids_pytools/resources/premium-databricks-azure-catalog.json,sha256=KGxUoiNPVTVzQaR3SN_FV9PH80fEnp6JnMhnv2YFOBY,38903
-spark_rapids_pytools/resources/profiling-conf.yaml,sha256=BqY12DJ1tkXDJZLITCaODyGx2KWEWNFmC3COFhIS21o,1517
-spark_rapids_pytools/resources/qualification-conf.yaml,sha256=Nu-EC9TjaahWm4eVcCsY5RQmyO2I28k-73bN5bQv2NM,8164
+spark_rapids_pytools/resources/profiling-conf.yaml,sha256=MFaTz9mZNlsyRRdjz_WPUxpZNbaz0TP12TDPMfvoEwA,1476
+spark_rapids_pytools/resources/qualification-conf.yaml,sha256=x4u2VGwycWiNKVrjR7wQEH38QFvvL6ACwt66KG5j8pA,8195
 spark_rapids_pytools/resources/dev/prepackage_mgr.py,sha256=pHORWIxdDWTWvSi4poTDbPdtd-zY-_46wteqNQULlZo,7779
 spark_rapids_pytools/resources/dev/process_databricks_azure_pricing.py,sha256=lmXGoTdrwdrhh8UO7DSL-kgbLOJP1X0kgJPq6cJfF3U,2195
 spark_rapids_pytools/resources/qualx/models/xgboost/databricks-aws.json,sha256=ZCFDXaUBzW-0RMjzVbUg5RQARR2w21MfeRmMIJfA3oU,405769
 spark_rapids_pytools/resources/qualx/models/xgboost/databricks-azure.json,sha256=4GLZ4J6cV77PnmjQpSlFAJWYXkhqCteNG4yF444smIs,160200
 spark_rapids_pytools/resources/qualx/models/xgboost/dataproc.json,sha256=JzTuXAidAbOUmK0xoJpsMKm1kIU2QCGOp-wyatTAyGQ,795042
 spark_rapids_pytools/resources/qualx/models/xgboost/dataproc_2.1-orc.json,sha256=ZSd34Yy1NiDYjpjPlBubZFSq-Ov9uH4UCGaAcn7Mwq8,361292
 spark_rapids_pytools/resources/qualx/models/xgboost/onprem.json,sha256=Iql4KaLjmk_VZgm9dbmO-qFsqFq-1fV4abaoGcd2S3U,530621
@@ -67,36 +68,36 @@
 spark_rapids_pytools/resources/templates/cluster_template/databricks_azure.ms,sha256=0sVKMfyTRLAPhPlq6EZXHQhRQ3NgQiNkhN8Lxld_v5o,291
 spark_rapids_pytools/resources/templates/cluster_template/dataproc.ms,sha256=ZP44POz4-FuHy7oPBGVHFtIgb7b_HCbkBiE6bupGB2w,579
 spark_rapids_pytools/resources/templates/cluster_template/emr.ms,sha256=vsF6bl07eE-tPRe4z9uknNkCgAIEZTHEXkMQQyt9dJs,759
 spark_rapids_pytools/resources/templates/node_template/databricks_aws.ms,sha256=mCFoGf_Ebacb3iFSvvlpY36AuqkDfektDYI1Avh4_ws,40
 spark_rapids_pytools/resources/templates/node_template/databricks_azure.ms,sha256=mCFoGf_Ebacb3iFSvvlpY36AuqkDfektDYI1Avh4_ws,40
 spark_rapids_pytools/resources/templates/node_template/emr.ms,sha256=wkvwpv0eNLTdVvhTD49CCNmhRpQDo6373i-d-dr0uOk,293
 spark_rapids_pytools/wrappers/__init__.py,sha256=CQ7Mf-YyBFNl6xmQGsPARv1w5GU3jGliByn5IHCcLkE,630
-spark_rapids_pytools/wrappers/databricks_aws_wrapper.py,sha256=H_vlUCwbjREQRY31p5Hw8lFtvBefuz_EhKJ1FeElY0A,19338
-spark_rapids_pytools/wrappers/databricks_azure_wrapper.py,sha256=lRajc_YFt92vJ0Rocek018wL74CltKKCBV0ewuQVxMA,18795
-spark_rapids_pytools/wrappers/dataproc_gke_wrapper.py,sha256=wjk8HJksf4RtE0si9W_V0Gzy0-IUsZ9epvXwMmAcYDo,9834
-spark_rapids_pytools/wrappers/dataproc_wrapper.py,sha256=f-I5f9E2cAemVXSW31V37MmqSYKUd-VZVUnkf89rxn8,18774
-spark_rapids_pytools/wrappers/emr_wrapper.py,sha256=OBgvCus5WefPd6B_3IM4U_X316iIJHfU2erAPevRG0Q,19300
-spark_rapids_pytools/wrappers/onprem_wrapper.py,sha256=haFjePrP0RquDvDMsbeY3pu0NwYE8w6H7ikCKtAzUbk,12518
+spark_rapids_pytools/wrappers/databricks_aws_wrapper.py,sha256=rwHG0Hsm_KcYayHVt9GuuLUfciWh_dl809cjrzWLvlk,19372
+spark_rapids_pytools/wrappers/databricks_azure_wrapper.py,sha256=5Dr8T9vqpz_H9kPjQHpVUVthaJjTviUOzMWsxht0PEw,18829
+spark_rapids_pytools/wrappers/dataproc_gke_wrapper.py,sha256=AUzxS3rsGLfOopPuO-2m4_sJ77OJOqOOZvHP998yQzI,9868
+spark_rapids_pytools/wrappers/dataproc_wrapper.py,sha256=6thPzITnW5P0Q_7NHFgkq4H9IgerllpPYYcKlWyxivs,18808
+spark_rapids_pytools/wrappers/emr_wrapper.py,sha256=wJzDpUUkX7aMH6TM9-tH5LV1RZtjz1fYf8azWS-4JLY,19334
+spark_rapids_pytools/wrappers/onprem_wrapper.py,sha256=_tla_gko4-LC4LcDaFwJJ9Zie4yGCG_Cy1DLslO-3Qs,12552
 spark_rapids_tools/__init__.py,sha256=-0oB-FN5JKvEAAUxnGso-Jh2UWNd9i3OPFjvwDPbfU4,1008
 spark_rapids_tools/enums.py,sha256=EuVrROjeUYE5fcfXBFrxmU9D_Bj1ucHBtvHhwjcLjto,5286
 spark_rapids_tools/exceptions.py,sha256=6xrZtHNsjyo7gfdADbUNzOA20uwcNEoQxD6MVEsRZjc,2099
 spark_rapids_tools/cloud/__init__.py,sha256=OrIS_DhBbuf7qGRQKJkhaea2Vnrgm-PJupTQxD15j3I,1167
 spark_rapids_tools/cloud/cluster.py,sha256=Jl1B6rc3fX9GAK3GTaswgmK2C3wW8mSErPwrNO2PDrM,4759
 spark_rapids_tools/cloud/databricks/__init__.py,sha256=6mPK0BNMsebQ_dlPe5FYKHgLY_Yf1ZSh-Scg2ASomME,649
 spark_rapids_tools/cloud/databricks/dbcluster.py,sha256=QKl0GhR06zh2eJE5FcHyCHfvJ5a_xj6-CyoGVScyBTo,1808
 spark_rapids_tools/cloud/dataproc/__init__.py,sha256=QWLmBGs6bzhb4whnN_TvU7GsUihgbyKvUv9qKvx3FpM,647
 spark_rapids_tools/cloud/dataproc/dataproccluster.py,sha256=t5oklFfZ6MklRLBgv2Dq5qYf31eA1uYJRnP8feX1peo,2243
 spark_rapids_tools/cloud/emr/__init__.py,sha256=LJPak4Ick6bn_3K6K21jmKnIkf4wzR_rV1QJo9RWqYo,642
 spark_rapids_tools/cloud/emr/emrcluster.py,sha256=ArpLpkmBqEbOXpG95pTvEH7L0URcydso7rcSx617kXo,1243
 spark_rapids_tools/cloud/onprem/__init__.py,sha256=7E7vQE_IQU6lyC-JxhFF4qQi-9TtBLGwCcTeoBmF8Kg,645
 spark_rapids_tools/cloud/onprem/onpremcluster.py,sha256=IFl10vifXM-GiAut6ypKPODw6z1X_iua-fe2SGeDf8g,1768
 spark_rapids_tools/cmdli/__init__.py,sha256=wE1eSGQcQL7HYzcV1uASzcaStzToaPhnYA3WVfiVQU4,706
-spark_rapids_tools/cmdli/argprocessor.py,sha256=R14i1FCU2Q0IELUDCUPunf_r978OjqkLYbas5v3xdp0,27691
-spark_rapids_tools/cmdli/tools_cli.py,sha256=wyiMnMM294OqOZx_edWNDdbXl_0Ytu4bgDZPrjmU5PE,16404
+spark_rapids_tools/cmdli/argprocessor.py,sha256=o-RuoKh92dXGbHyI7J9kz37W3hRjE5dQmCPhLEBHhOE,28314
+spark_rapids_tools/cmdli/tools_cli.py,sha256=sxP2UrF2Br3JSW44qQWvJeZ6goRVndpzy-VTH1PQiQ4,18580
 spark_rapids_tools/storagelib/__init__.py,sha256=waHQ2-xmRCwyGQKJFpt8eiu4EENK__F-9WomBesjAE8,1433
 spark_rapids_tools/storagelib/cspfs.py,sha256=8r0Xzd4t4WtytdTgyiS534xHle0NzH2IH2yHHB5q2_0,5549
 spark_rapids_tools/storagelib/csppath.py,sha256=8jtAtnN0CVrXoxdvuj0c9HgipIGj1jGUOALdxDQvvqo,11165
 spark_rapids_tools/storagelib/adls/__init__.py,sha256=Q3DU1690fE2xZSuZ3eE1noPmuFPEbNMN_ToU6nJl3-E,774
 spark_rapids_tools/storagelib/adls/adlsfs.py,sha256=lxorVz2ZEEHC6Wa1vJrblsPWdFVcBzVt-NbHk2f-JCY,1658
 spark_rapids_tools/storagelib/adls/adlspath.py,sha256=VRCEQC25jcq7pkX23tfJTrRWCXjCc43DbPwJ9ZiYPqc,785
 spark_rapids_tools/storagelib/gcs/__init__.py,sha256=nT2ev0R9w22C_fgPYRZwO4_QK9A-P0ylfqYoJSZiDl4,751
@@ -109,20 +110,20 @@
 spark_rapids_tools/storagelib/local/localfs.py,sha256=dpw6TEPNltnv0KBzVFdpAfoBpZfZqbrQd6e6m538feg,865
 spark_rapids_tools/storagelib/local/localpath.py,sha256=gJ25FAZ2D_m5y_d5zii-fK73ZFCpiXnBUFimYLq7_X0,780
 spark_rapids_tools/storagelib/s3/__init__.py,sha256=z8CUs721lyZHAG3_Vcg-6gIP9VOPE3V5lEudb2lBnc8,726
 spark_rapids_tools/storagelib/s3/s3fs.py,sha256=vvEFZkLpOIwpW58DNi8atD-GYVv3wJjZVaOhd4g46WI,1732
 spark_rapids_tools/storagelib/s3/s3path.py,sha256=7JtLB5Nl6z1M6TMBGIuyumNKC3wsgkTY06n3J0BceTg,776
 spark_rapids_tools/tools/__init__.py,sha256=xOL1EJyVKvW3o4dCrtqghVhb7kf4Dgjb06x5nli2fXA,653
 spark_rapids_tools/tools/autotuner.py,sha256=nnP-ZN4fnONmKgGIyZV8DY-G0oLD6b4JGuEpLCuPBrc,1096
-spark_rapids_tools/tools/model_xgboost.py,sha256=gwz--J3dilRkzEzVaK7YY8yI5NOdVSndhua7gCprznc,54414
+spark_rapids_tools/tools/model_xgboost.py,sha256=sSNLsQZLge5pU_o96hxOG5uYJz-G0xeUMtlvuf15eeY,55186
 spark_rapids_tools/tools/speedup_category.py,sha256=7pxG5wG4JnugCUD0hpHVrQrRIN423tJd_rcuP4d1DIM,4764
 spark_rapids_tools/tools/top_candidates.py,sha256=NH8d2miKr6v_CfeXdOReb25fc6znO91T6Z4hPYpFE3M,1714
 spark_rapids_tools/tools/unsupported_ops_stage_duration.py,sha256=QeyovcFJsOlPUK-B3pO_cD68xjqVCyHV_HTNYisVfG0,4081
 spark_rapids_tools/utils/__init__.py,sha256=eYOmNcGR_4p8EvM41Jqd2VlnTwck1742DNa_Q_np5VQ,987
 spark_rapids_tools/utils/propmanager.py,sha256=7Cu8Qxi1owWO1cPjHYPyASxos-_iH5J86uYZEyLkrBs,4844
 spark_rapids_tools/utils/util.py,sha256=F6GtFq_PXkotITKalzhROnRJR-WHVCm54eW3OR9iOsk,12742
-spark_rapids_user_tools-24.2.4.dist-info/LICENSE,sha256=RnI8IUCDrXfGVHFfYTsT8OKEuaOtkMGDQOn8foh7D00,21086
-spark_rapids_user_tools-24.2.4.dist-info/METADATA,sha256=47DWDquc_6R7Ry95LDb-npDyp2U1quK6WiJY9ppZNfs,4126
-spark_rapids_user_tools-24.2.4.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-spark_rapids_user_tools-24.2.4.dist-info/entry_points.txt,sha256=MvX--wvYR0O5R5ArlTJeaCT3R7auC--PsQAD5haufDM,133
-spark_rapids_user_tools-24.2.4.dist-info/top_level.txt,sha256=OhA3L9VLtMKwE1-Dc8K7lmhLZ7aya3lxG_ifOnYWip8,40
-spark_rapids_user_tools-24.2.4.dist-info/RECORD,,
+spark_rapids_user_tools-24.4.0.dist-info/LICENSE,sha256=RnI8IUCDrXfGVHFfYTsT8OKEuaOtkMGDQOn8foh7D00,21086
+spark_rapids_user_tools-24.4.0.dist-info/METADATA,sha256=c695qNohQRVB0Z4llMXtjdUV1J5Q_3wdCZxo_fGCDrk,4488
+spark_rapids_user_tools-24.4.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+spark_rapids_user_tools-24.4.0.dist-info/entry_points.txt,sha256=MvX--wvYR0O5R5ArlTJeaCT3R7auC--PsQAD5haufDM,133
+spark_rapids_user_tools-24.4.0.dist-info/top_level.txt,sha256=OhA3L9VLtMKwE1-Dc8K7lmhLZ7aya3lxG_ifOnYWip8,40
+spark_rapids_user_tools-24.4.0.dist-info/RECORD,,
```

