# Comparing `tmp/google-ai-generativelanguage-0.6.2.tar.gz` & `tmp/google-ai-generativelanguage-0.6.3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "google-ai-generativelanguage-0.6.2.tar", last modified: Mon Apr 15 13:58:11 2024, max compression
+gzip compressed data, was "google-ai-generativelanguage-0.6.3.tar", last modified: Tue May  7 20:43:16 2024, max compression
```

## Comparing `google-ai-generativelanguage-0.6.2.tar` & `google-ai-generativelanguage-0.6.3.tar`

### file list

```diff
@@ -1,290 +1,290 @@
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.537348 google-ai-generativelanguage-0.6.2/
--rw-rw-r--   0 root         (0)     1003    11358 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/LICENSE
--rw-rw-r--   0 root         (0)     1003      860 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/MANIFEST.in
--rw-r--r--   0 root         (0)     1003     5604 2024-04-15 13:58:11.537348 google-ai-generativelanguage-0.6.2/PKG-INFO
--rw-rw-r--   0 root         (0)     1003     4216 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/README.rst
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.297332 google-ai-generativelanguage-0.6.2/google/
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.301332 google-ai-generativelanguage-0.6.2/google/ai/
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.309333 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage/
--rw-rw-r--   0 root         (0)     1003     9811 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage/__init__.py
--rw-rw-r--   0 root         (0)     1003      652 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage/gapic_version.py
--rw-rw-r--   0 root         (0)     1003       89 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage/py.typed
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.313333 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/
--rw-rw-r--   0 root         (0)     1003     2229 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/__init__.py
--rw-rw-r--   0 root         (0)     1003     3669 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/gapic_metadata.json
--rw-rw-r--   0 root         (0)     1003      652 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/gapic_version.py
--rw-rw-r--   0 root         (0)     1003       89 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/py.typed
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.313333 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/__init__.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.317333 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/
--rw-rw-r--   0 root         (0)     1003      781 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    45932 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    61250 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/client.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.329334 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/
--rw-rw-r--   0 root         (0)     1003     1442 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003    10725 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    19633 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    19938 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    46280 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.333334 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/
--rw-rw-r--   0 root         (0)     1003      761 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    27020 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    43332 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/client.py
--rw-rw-r--   0 root         (0)     1003     5747 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/pagers.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.337334 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/
--rw-rw-r--   0 root         (0)     1003     1372 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     7270 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    15410 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    15664 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    26746 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.341335 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/
--rw-rw-r--   0 root         (0)     1003     1759 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/__init__.py
--rw-rw-r--   0 root         (0)     1003     2895 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/citation.py
--rw-rw-r--   0 root         (0)     1003     3720 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/content.py
--rw-rw-r--   0 root         (0)     1003    20990 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/generative_service.py
--rw-rw-r--   0 root         (0)     1003     4802 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/model.py
--rw-rw-r--   0 root         (0)     1003     3143 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/model_service.py
--rw-rw-r--   0 root         (0)     1003     6161 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/safety.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.345335 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/
--rw-rw-r--   0 root         (0)     1003     8304 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/__init__.py
--rw-rw-r--   0 root         (0)     1003    21107 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/gapic_metadata.json
--rw-rw-r--   0 root         (0)     1003      652 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/gapic_version.py
--rw-rw-r--   0 root         (0)     1003       89 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/py.typed
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.345335 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/__init__.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.349335 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/
--rw-rw-r--   0 root         (0)     1003      769 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    24748 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    40660 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/client.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.353335 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/
--rw-rw-r--   0 root         (0)     1003     1400 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     7451 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    13414 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    13641 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    18401 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.357336 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/
--rw-rw-r--   0 root         (0)     1003      757 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    24788 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    41498 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/client.py
--rw-rw-r--   0 root         (0)     1003     5737 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/pagers.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.361336 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/
--rw-rw-r--   0 root         (0)     1003     1358 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     7453 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    15040 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    15336 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    24426 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.365336 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/
--rw-rw-r--   0 root         (0)     1003      781 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    47141 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    62455 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/client.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.369336 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/
--rw-rw-r--   0 root         (0)     1003     1442 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003    10886 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    18398 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    18725 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    41119 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.373337 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/
--rw-rw-r--   0 root         (0)     1003      761 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    47264 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    62596 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/client.py
--rw-rw-r--   0 root         (0)     1003    10950 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/pagers.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.377337 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/
--rw-rw-r--   0 root         (0)     1003     1372 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003    11616 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    19686 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    20146 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    41543 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.381337 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/
--rw-rw-r--   0 root         (0)     1003      781 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    41727 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    57530 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/client.py
--rw-rw-r--   0 root         (0)     1003     6013 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/pagers.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.385338 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/
--rw-rw-r--   0 root         (0)     1003     1442 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003    10464 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    18156 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    18520 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    41155 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.393338 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/
--rw-rw-r--   0 root         (0)     1003      777 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    97421 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/async_client.py
--rw-rw-r--   0 root         (0)     1003   112654 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/client.py
--rw-rw-r--   0 root         (0)     1003    16011 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/pagers.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.397338 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/
--rw-rw-r--   0 root         (0)     1003     1428 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003    20205 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    33226 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    33948 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003   105077 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.401339 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/
--rw-rw-r--   0 root         (0)     1003      757 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    35868 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    51394 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/client.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.405339 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/
--rw-rw-r--   0 root         (0)     1003     1358 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     9013 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    15599 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    15899 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    28406 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.421340 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/
--rw-rw-r--   0 root         (0)     1003     7020 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/__init__.py
--rw-rw-r--   0 root         (0)     1003     2903 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/citation.py
--rw-rw-r--   0 root         (0)     1003    17360 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/content.py
--rw-rw-r--   0 root         (0)     1003    11601 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/discuss_service.py
--rw-rw-r--   0 root         (0)     1003     3393 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/file.py
--rw-rw-r--   0 root         (0)     1003     3555 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/file_service.py
--rw-rw-r--   0 root         (0)     1003    38481 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/generative_service.py
--rw-rw-r--   0 root         (0)     1003     4806 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/model.py
--rw-rw-r--   0 root         (0)     1003     9676 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/model_service.py
--rw-rw-r--   0 root         (0)     1003     4530 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/permission.py
--rw-rw-r--   0 root         (0)     1003     6362 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/permission_service.py
--rw-rw-r--   0 root         (0)     1003    13703 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/retriever.py
--rw-rw-r--   0 root         (0)     1003    24469 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/retriever_service.py
--rw-rw-r--   0 root         (0)     1003     8594 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/safety.py
--rw-rw-r--   0 root         (0)     1003    14378 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/text_service.py
--rw-rw-r--   0 root         (0)     1003    13772 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/tuned_model.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.421340 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/
--rw-rw-r--   0 root         (0)     1003     2426 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/__init__.py
--rw-rw-r--   0 root         (0)     1003     3627 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/gapic_metadata.json
--rw-rw-r--   0 root         (0)     1003      652 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/gapic_version.py
--rw-rw-r--   0 root         (0)     1003       89 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/py.typed
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.425340 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/__init__.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.425340 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/
--rw-rw-r--   0 root         (0)     1003      769 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    24701 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    40613 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/client.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.429340 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/
--rw-rw-r--   0 root         (0)     1003     1400 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     7391 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    13355 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    13582 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    18341 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.433341 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/
--rw-rw-r--   0 root         (0)     1003      761 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    21080 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    36905 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/client.py
--rw-rw-r--   0 root         (0)     1003     5792 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/pagers.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.437341 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/
--rw-rw-r--   0 root         (0)     1003     1372 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     7203 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    12847 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    13101 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    16216 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.441341 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/
--rw-rw-r--   0 root         (0)     1003      757 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    25037 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    40877 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/client.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.445341 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/
--rw-rw-r--   0 root         (0)     1003     1358 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     7273 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    13103 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    13341 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    17420 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.449342 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/
--rw-rw-r--   0 root         (0)     1003     1847 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/__init__.py
--rw-rw-r--   0 root         (0)     1003     2905 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/citation.py
--rw-rw-r--   0 root         (0)     1003    11613 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/discuss_service.py
--rw-rw-r--   0 root         (0)     1003     4670 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/model.py
--rw-rw-r--   0 root         (0)     1003     3158 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/model_service.py
--rw-rw-r--   0 root         (0)     1003     7878 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/safety.py
--rw-rw-r--   0 root         (0)     1003    10981 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/text_service.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.453342 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/
--rw-rw-r--   0 root         (0)     1003     4188 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/__init__.py
--rw-rw-r--   0 root         (0)     1003     8993 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/gapic_metadata.json
--rw-rw-r--   0 root         (0)     1003      652 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/gapic_version.py
--rw-rw-r--   0 root         (0)     1003       89 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/py.typed
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.453342 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/__init__.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.453342 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/
--rw-rw-r--   0 root         (0)     1003      769 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    24142 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    40676 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/client.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.457342 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/
--rw-rw-r--   0 root         (0)     1003     1400 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     6769 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    13417 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    13644 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    18404 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.465343 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/
--rw-rw-r--   0 root         (0)     1003      761 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    45130 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    62639 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/client.py
--rw-rw-r--   0 root         (0)     1003    10967 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/pagers.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.469343 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/
--rw-rw-r--   0 root         (0)     1003     1372 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     9226 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    19696 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    20156 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    41554 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.473343 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/
--rw-rw-r--   0 root         (0)     1003      781 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    40149 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    57825 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/client.py
--rw-rw-r--   0 root         (0)     1003     6022 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/pagers.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.477344 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/
--rw-rw-r--   0 root         (0)     1003     1442 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     8758 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    18165 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    18529 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    40318 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.481344 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/
--rw-rw-r--   0 root         (0)     1003      757 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/__init__.py
--rw-rw-r--   0 root         (0)     1003    34750 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/async_client.py
--rw-rw-r--   0 root         (0)     1003    51520 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/client.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.485344 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/
--rw-rw-r--   0 root         (0)     1003     1358 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/__init__.py
--rw-rw-r--   0 root         (0)     1003     7647 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/base.py
--rw-rw-r--   0 root         (0)     1003    15604 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/grpc.py
--rw-rw-r--   0 root         (0)     1003    15904 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/grpc_asyncio.py
--rw-rw-r--   0 root         (0)     1003    28412 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/rest.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.493345 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/
--rw-rw-r--   0 root         (0)     1003     3416 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/__init__.py
--rw-rw-r--   0 root         (0)     1003     2905 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/citation.py
--rw-rw-r--   0 root         (0)     1003    11613 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/discuss_service.py
--rw-rw-r--   0 root         (0)     1003     4670 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/model.py
--rw-rw-r--   0 root         (0)     1003     8924 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/model_service.py
--rw-rw-r--   0 root         (0)     1003     4460 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/permission.py
--rw-rw-r--   0 root         (0)     1003     6197 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/permission_service.py
--rw-rw-r--   0 root         (0)     1003     7974 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/safety.py
--rw-rw-r--   0 root         (0)     1003    13777 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/text_service.py
--rw-rw-r--   0 root         (0)     1003    12841 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/tuned_model.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.497345 google-ai-generativelanguage-0.6.2/google_ai_generativelanguage.egg-info/
--rw-r--r--   0 root         (0)     1003     5604 2024-04-15 13:58:11.000000 google-ai-generativelanguage-0.6.2/google_ai_generativelanguage.egg-info/PKG-INFO
--rw-r--r--   0 root         (0)     1003    15729 2024-04-15 13:58:11.000000 google-ai-generativelanguage-0.6.2/google_ai_generativelanguage.egg-info/SOURCES.txt
--rw-r--r--   0 root         (0)     1003        1 2024-04-15 13:58:11.000000 google-ai-generativelanguage-0.6.2/google_ai_generativelanguage.egg-info/dependency_links.txt
--rw-r--r--   0 root         (0)     1003        1 2024-04-15 13:58:11.000000 google-ai-generativelanguage-0.6.2/google_ai_generativelanguage.egg-info/not-zip-safe
--rw-r--r--   0 root         (0)     1003      305 2024-04-15 13:58:11.000000 google-ai-generativelanguage-0.6.2/google_ai_generativelanguage.egg-info/requires.txt
--rw-r--r--   0 root         (0)     1003        7 2024-04-15 13:58:11.000000 google-ai-generativelanguage-0.6.2/google_ai_generativelanguage.egg-info/top_level.txt
--rw-r--r--   0 root         (0)     1003       38 2024-04-15 13:58:11.537348 google-ai-generativelanguage-0.6.2/setup.cfg
--rw-rw-r--   0 root         (0)     1003     3201 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/setup.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.497345 google-ai-generativelanguage-0.6.2/tests/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/__init__.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.497345 google-ai-generativelanguage-0.6.2/tests/unit/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/__init__.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.497345 google-ai-generativelanguage-0.6.2/tests/unit/gapic/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/__init__.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.501345 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1/__init__.py
--rw-rw-r--   0 root         (0)     1003   199891 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1/test_generative_service.py
--rw-rw-r--   0 root         (0)     1003   136179 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1/test_model_service.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.517346 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/__init__.py
--rw-rw-r--   0 root         (0)     1003   113238 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_discuss_service.py
--rw-rw-r--   0 root         (0)     1003   133274 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_file_service.py
--rw-rw-r--   0 root         (0)     1003   201546 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_generative_service.py
--rw-rw-r--   0 root         (0)     1003   233901 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_model_service.py
--rw-rw-r--   0 root         (0)     1003   208818 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_permission_service.py
--rw-rw-r--   0 root         (0)     1003   481948 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_retriever_service.py
--rw-rw-r--   0 root         (0)     1003   151028 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_text_service.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.529347 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta2/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta2/__init__.py
--rw-rw-r--   0 root         (0)     1003   113184 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta2/test_discuss_service.py
--rw-rw-r--   0 root         (0)     1003   114125 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta2/test_model_service.py
--rw-rw-r--   0 root         (0)     1003   110150 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta2/test_text_service.py
-drwxr-sr-x   0 root         (0)     1003        0 2024-04-15 13:58:11.537348 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/
--rw-rw-r--   0 root         (0)     1003      600 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/__init__.py
--rw-rw-r--   0 root         (0)     1003   113246 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/test_discuss_service.py
--rw-rw-r--   0 root         (0)     1003   233746 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/test_model_service.py
--rw-rw-r--   0 root         (0)     1003   209388 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/test_permission_service.py
--rw-rw-r--   0 root         (0)     1003   151575 2024-04-15 13:54:43.000000 google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/test_text_service.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.958746 google-ai-generativelanguage-0.6.3/
+-rw-rw-r--   0 root         (0)     1003    11358 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/LICENSE
+-rw-rw-r--   0 root         (0)     1003      860 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/MANIFEST.in
+-rw-r--r--   0 root         (0)     1003     5604 2024-05-07 20:43:16.958746 google-ai-generativelanguage-0.6.3/PKG-INFO
+-rw-rw-r--   0 root         (0)     1003     4216 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/README.rst
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.878729 google-ai-generativelanguage-0.6.3/google/
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.886731 google-ai-generativelanguage-0.6.3/google/ai/
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.890731 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage/
+-rw-rw-r--   0 root         (0)     1003     9811 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage/__init__.py
+-rw-rw-r--   0 root         (0)     1003      652 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage/gapic_version.py
+-rw-rw-r--   0 root         (0)     1003       89 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage/py.typed
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.894732 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/
+-rw-rw-r--   0 root         (0)     1003     2229 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/__init__.py
+-rw-rw-r--   0 root         (0)     1003     3669 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/gapic_metadata.json
+-rw-rw-r--   0 root         (0)     1003      652 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/gapic_version.py
+-rw-rw-r--   0 root         (0)     1003       89 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/py.typed
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.894732 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/__init__.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.894732 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/
+-rw-rw-r--   0 root         (0)     1003      781 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    45630 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    61605 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/client.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.894732 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1442 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003    10725 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    20180 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    23467 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    46280 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.898733 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/
+-rw-rw-r--   0 root         (0)     1003      761 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    27688 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    43871 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/client.py
+-rw-rw-r--   0 root         (0)     1003     5747 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/pagers.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.898733 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1372 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     7270 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    15957 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    16817 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    26746 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.902734 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/
+-rw-rw-r--   0 root         (0)     1003     1759 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/__init__.py
+-rw-rw-r--   0 root         (0)     1003     2895 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/citation.py
+-rw-rw-r--   0 root         (0)     1003     3720 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/content.py
+-rw-rw-r--   0 root         (0)     1003    20990 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/generative_service.py
+-rw-rw-r--   0 root         (0)     1003     4802 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/model.py
+-rw-rw-r--   0 root         (0)     1003     3143 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/model_service.py
+-rw-rw-r--   0 root         (0)     1003     6161 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/safety.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.902734 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/
+-rw-rw-r--   0 root         (0)     1003     8304 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/__init__.py
+-rw-rw-r--   0 root         (0)     1003    21107 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/gapic_metadata.json
+-rw-rw-r--   0 root         (0)     1003      652 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/gapic_version.py
+-rw-rw-r--   0 root         (0)     1003       89 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/py.typed
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.902734 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/__init__.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.902734 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/
+-rw-rw-r--   0 root         (0)     1003      769 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    24819 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    41192 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/client.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.902734 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1400 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     7451 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    13961 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    15520 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    18401 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.906735 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/
+-rw-rw-r--   0 root         (0)     1003      757 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    25723 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    41865 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/client.py
+-rw-rw-r--   0 root         (0)     1003     5737 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/pagers.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.906735 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1358 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     7453 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    15587 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    16871 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    24426 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.906735 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/
+-rw-rw-r--   0 root         (0)     1003      781 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    46944 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    62973 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/client.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.910735 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1442 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003    10886 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    19199 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    23056 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    41119 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.910735 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/
+-rw-rw-r--   0 root         (0)     1003      761 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    46546 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    62699 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/client.py
+-rw-rw-r--   0 root         (0)     1003    10950 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/pagers.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.910735 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1372 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003    11616 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    20233 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    24755 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    41543 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.914736 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/
+-rw-rw-r--   0 root         (0)     1003      781 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    41591 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    57782 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/client.py
+-rw-rw-r--   0 root         (0)     1003     6013 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/pagers.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.914736 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1442 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003    10464 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    18703 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    22250 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    41155 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.914736 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/
+-rw-rw-r--   0 root         (0)     1003      777 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    96015 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003   111634 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/client.py
+-rw-rw-r--   0 root         (0)     1003    16011 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/pagers.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.918737 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1428 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003    20205 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    33773 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    44233 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003   105077 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.918737 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/
+-rw-rw-r--   0 root         (0)     1003      757 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    35603 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    51755 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/client.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.918737 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1358 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     9013 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    16146 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    18854 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    28406 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.922738 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/
+-rw-rw-r--   0 root         (0)     1003     7020 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/__init__.py
+-rw-rw-r--   0 root         (0)     1003     2903 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/citation.py
+-rw-rw-r--   0 root         (0)     1003    17562 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/content.py
+-rw-rw-r--   0 root         (0)     1003    11601 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/discuss_service.py
+-rw-rw-r--   0 root         (0)     1003     4234 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/file.py
+-rw-rw-r--   0 root         (0)     1003     3555 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/file_service.py
+-rw-rw-r--   0 root         (0)     1003    40746 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/generative_service.py
+-rw-rw-r--   0 root         (0)     1003     4806 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/model.py
+-rw-rw-r--   0 root         (0)     1003     9676 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/model_service.py
+-rw-rw-r--   0 root         (0)     1003     4530 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/permission.py
+-rw-rw-r--   0 root         (0)     1003     6362 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/permission_service.py
+-rw-rw-r--   0 root         (0)     1003    13703 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/retriever.py
+-rw-rw-r--   0 root         (0)     1003    24469 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/retriever_service.py
+-rw-rw-r--   0 root         (0)     1003     8594 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/safety.py
+-rw-rw-r--   0 root         (0)     1003    14378 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/text_service.py
+-rw-rw-r--   0 root         (0)     1003    13772 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/tuned_model.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.926739 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/
+-rw-rw-r--   0 root         (0)     1003     2426 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/__init__.py
+-rw-rw-r--   0 root         (0)     1003     3627 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/gapic_metadata.json
+-rw-rw-r--   0 root         (0)     1003      652 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/gapic_version.py
+-rw-rw-r--   0 root         (0)     1003       89 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/py.typed
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.926739 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/__init__.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.926739 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/
+-rw-rw-r--   0 root         (0)     1003      769 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    24772 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    41145 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/client.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.926739 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1400 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     7391 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    13902 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    15461 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    18341 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.926739 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/
+-rw-rw-r--   0 root         (0)     1003      761 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    21126 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    37444 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/client.py
+-rw-rw-r--   0 root         (0)     1003     5792 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/pagers.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.930740 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1372 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     7203 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    13394 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    14948 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    16216 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.930740 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/
+-rw-rw-r--   0 root         (0)     1003      757 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    25081 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    41409 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/client.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.930740 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1358 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     7273 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    13650 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    15194 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    17420 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.934740 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/
+-rw-rw-r--   0 root         (0)     1003     1847 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/__init__.py
+-rw-rw-r--   0 root         (0)     1003     2905 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/citation.py
+-rw-rw-r--   0 root         (0)     1003    11613 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/discuss_service.py
+-rw-rw-r--   0 root         (0)     1003     4670 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/model.py
+-rw-rw-r--   0 root         (0)     1003     3158 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/model_service.py
+-rw-rw-r--   0 root         (0)     1003     7878 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/safety.py
+-rw-rw-r--   0 root         (0)     1003    10981 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/text_service.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.934740 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/
+-rw-rw-r--   0 root         (0)     1003     4188 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/__init__.py
+-rw-rw-r--   0 root         (0)     1003     8993 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/gapic_metadata.json
+-rw-rw-r--   0 root         (0)     1003      652 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/gapic_version.py
+-rw-rw-r--   0 root         (0)     1003       89 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/py.typed
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.934740 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/__init__.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.934740 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/
+-rw-rw-r--   0 root         (0)     1003      769 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    24835 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    41208 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/client.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.938741 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1400 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     6769 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    13964 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    14829 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    18404 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.938741 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/
+-rw-rw-r--   0 root         (0)     1003      761 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    46589 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    62742 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/client.py
+-rw-rw-r--   0 root         (0)     1003    10967 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/pagers.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.938741 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1372 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     9226 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    20243 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    22336 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    41554 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.942742 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/
+-rw-rw-r--   0 root         (0)     1003      781 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    41568 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    58077 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/client.py
+-rw-rw-r--   0 root         (0)     1003     6022 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/pagers.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.942742 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1442 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     8758 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    18712 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    20524 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    40318 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.942742 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/
+-rw-rw-r--   0 root         (0)     1003      757 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/__init__.py
+-rw-rw-r--   0 root         (0)     1003    35729 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/async_client.py
+-rw-rw-r--   0 root         (0)     1003    51881 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/client.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.946743 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/
+-rw-rw-r--   0 root         (0)     1003     1358 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/__init__.py
+-rw-rw-r--   0 root         (0)     1003     7647 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/base.py
+-rw-rw-r--   0 root         (0)     1003    16151 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/grpc.py
+-rw-rw-r--   0 root         (0)     1003    17471 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/grpc_asyncio.py
+-rw-rw-r--   0 root         (0)     1003    28412 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/rest.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.946743 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/
+-rw-rw-r--   0 root         (0)     1003     3416 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/__init__.py
+-rw-rw-r--   0 root         (0)     1003     2905 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/citation.py
+-rw-rw-r--   0 root         (0)     1003    11613 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/discuss_service.py
+-rw-rw-r--   0 root         (0)     1003     4670 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/model.py
+-rw-rw-r--   0 root         (0)     1003     8924 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/model_service.py
+-rw-rw-r--   0 root         (0)     1003     4460 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/permission.py
+-rw-rw-r--   0 root         (0)     1003     6197 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/permission_service.py
+-rw-rw-r--   0 root         (0)     1003     7974 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/safety.py
+-rw-rw-r--   0 root         (0)     1003    13777 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/text_service.py
+-rw-rw-r--   0 root         (0)     1003    12841 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/tuned_model.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.950744 google-ai-generativelanguage-0.6.3/google_ai_generativelanguage.egg-info/
+-rw-r--r--   0 root         (0)     1003     5604 2024-05-07 20:43:16.000000 google-ai-generativelanguage-0.6.3/google_ai_generativelanguage.egg-info/PKG-INFO
+-rw-r--r--   0 root         (0)     1003    15729 2024-05-07 20:43:16.000000 google-ai-generativelanguage-0.6.3/google_ai_generativelanguage.egg-info/SOURCES.txt
+-rw-r--r--   0 root         (0)     1003        1 2024-05-07 20:43:16.000000 google-ai-generativelanguage-0.6.3/google_ai_generativelanguage.egg-info/dependency_links.txt
+-rw-r--r--   0 root         (0)     1003        1 2024-05-07 20:43:16.000000 google-ai-generativelanguage-0.6.3/google_ai_generativelanguage.egg-info/not-zip-safe
+-rw-r--r--   0 root         (0)     1003      305 2024-05-07 20:43:16.000000 google-ai-generativelanguage-0.6.3/google_ai_generativelanguage.egg-info/requires.txt
+-rw-r--r--   0 root         (0)     1003        7 2024-05-07 20:43:16.000000 google-ai-generativelanguage-0.6.3/google_ai_generativelanguage.egg-info/top_level.txt
+-rw-r--r--   0 root         (0)     1003       38 2024-05-07 20:43:16.958746 google-ai-generativelanguage-0.6.3/setup.cfg
+-rw-rw-r--   0 root         (0)     1003     3201 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/setup.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.950744 google-ai-generativelanguage-0.6.3/tests/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/__init__.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.950744 google-ai-generativelanguage-0.6.3/tests/unit/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/__init__.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.950744 google-ai-generativelanguage-0.6.3/tests/unit/gapic/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/__init__.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.950744 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1/__init__.py
+-rw-rw-r--   0 root         (0)     1003   222708 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1/test_generative_service.py
+-rw-rw-r--   0 root         (0)     1003   144964 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1/test_model_service.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.954745 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/__init__.py
+-rw-rw-r--   0 root         (0)     1003   122418 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_discuss_service.py
+-rw-rw-r--   0 root         (0)     1003   151192 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_file_service.py
+-rw-rw-r--   0 root         (0)     1003   228747 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_generative_service.py
+-rw-rw-r--   0 root         (0)     1003   266003 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_model_service.py
+-rw-rw-r--   0 root         (0)     1003   236137 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_permission_service.py
+-rw-rw-r--   0 root         (0)     1003   571435 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_retriever_service.py
+-rw-rw-r--   0 root         (0)     1003   168921 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_text_service.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.958746 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta2/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta2/__init__.py
+-rw-rw-r--   0 root         (0)     1003   122364 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta2/test_discuss_service.py
+-rw-rw-r--   0 root         (0)     1003   122910 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta2/test_model_service.py
+-rw-rw-r--   0 root         (0)     1003   118974 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta2/test_text_service.py
+drwxr-sr-x   0 root         (0)     1003        0 2024-05-07 20:43:16.958746 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/
+-rw-rw-r--   0 root         (0)     1003      600 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/__init__.py
+-rw-rw-r--   0 root         (0)     1003   122426 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/test_discuss_service.py
+-rw-rw-r--   0 root         (0)     1003   265848 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/test_model_service.py
+-rw-rw-r--   0 root         (0)     1003   236707 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/test_permission_service.py
+-rw-rw-r--   0 root         (0)     1003   169468 2024-05-07 20:40:10.000000 google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/test_text_service.py
```

### Comparing `google-ai-generativelanguage-0.6.2/LICENSE` & `google-ai-generativelanguage-0.6.3/LICENSE`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/MANIFEST.in` & `google-ai-generativelanguage-0.6.3/MANIFEST.in`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/PKG-INFO` & `google-ai-generativelanguage-0.6.3/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: google-ai-generativelanguage
-Version: 0.6.2
+Version: 0.6.3
 Summary: Google Ai Generativelanguage API client library
 Home-page: https://github.com/googleapis/google-cloud-python/tree/main/packages/google-ai-generativelanguage
 Author: Google LLC
 Author-email: googleapis-packages@google.com
 License: Apache 2.0
 Platform: Posix; MacOS X; Windows
 Classifier: Development Status :: 4 - Beta
```

### Comparing `google-ai-generativelanguage-0.6.2/README.rst` & `google-ai-generativelanguage-0.6.3/README.rst`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage/gapic_version.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage/gapic_version.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,8 +9,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-__version__ = "0.6.2"  # {x-release-please-version}
+__version__ = "0.6.3"  # {x-release-please-version}
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/gapic_metadata.json` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/gapic_metadata.json`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/gapic_version.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/gapic_version.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,8 +9,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-__version__ = "0.6.2"  # {x-release-please-version}
+__version__ = "0.6.3"  # {x-release-please-version}
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/async_client.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,14 +15,15 @@
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
     AsyncIterable,
     Awaitable,
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -197,29 +198,37 @@
         type(GenerativeServiceClient).get_transport_class, type(GenerativeServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, GenerativeServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[
+                str,
+                GenerativeServiceTransport,
+                Callable[..., GenerativeServiceTransport],
+            ]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the generative service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.GenerativeServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,GenerativeServiceTransport,Callable[..., GenerativeServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the GenerativeServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -343,48 +352,40 @@
                    candidates are returned only if there was something
                    wrong with the prompt (see prompt_feedback) -
                    feedback on each candidate is reported on
                    finish_reason and safety_ratings.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.GenerateContentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.GenerateContentRequest):
+            request = generative_service.GenerateContentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if contents:
             request.contents.extend(contents)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.generate_content,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.generate_content
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -487,48 +488,40 @@
                    candidates are returned only if there was something
                    wrong with the prompt (see prompt_feedback) -
                    feedback on each candidate is reported on
                    finish_reason and safety_ratings.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.GenerateContentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.GenerateContentRequest):
+            request = generative_service.GenerateContentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if contents:
             request.contents.extend(contents)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.stream_generate_content,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.stream_generate_content
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -615,48 +608,40 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1.types.EmbedContentResponse:
                 The response to an EmbedContentRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, content])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.EmbedContentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.EmbedContentRequest):
+            request = generative_service.EmbedContentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if content is not None:
             request.content = content
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.embed_content,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.embed_content
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -752,48 +737,40 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1.types.BatchEmbedContentsResponse:
                 The response to a BatchEmbedContentsRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, requests])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.BatchEmbedContentsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.BatchEmbedContentsRequest):
+            request = generative_service.BatchEmbedContentsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if requests:
             request.requests.extend(requests)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.batch_embed_contents,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.batch_embed_contents
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -886,48 +863,40 @@
             google.ai.generativelanguage_v1.types.CountTokensResponse:
                 A response from CountTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.CountTokensRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.CountTokensRequest):
+            request = generative_service.CountTokensRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if contents:
             request.contents.extend(contents)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.count_tokens,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.count_tokens
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/client.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Iterable,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
@@ -524,29 +525,37 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, GenerativeServiceTransport]] = None,
+        transport: Optional[
+            Union[
+                str,
+                GenerativeServiceTransport,
+                Callable[..., GenerativeServiceTransport],
+            ]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the generative service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, GenerativeServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,GenerativeServiceTransport,Callable[..., GenerativeServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the GenerativeServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -647,16 +656,24 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[GenerativeServiceTransport],
+                Callable[..., GenerativeServiceTransport],
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., GenerativeServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -748,27 +765,25 @@
                    candidates are returned only if there was something
                    wrong with the prompt (see prompt_feedback) -
                    feedback on each candidate is reported on
                    finish_reason and safety_ratings.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.GenerateContentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.GenerateContentRequest):
             request = generative_service.GenerateContentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if contents is not None:
@@ -883,27 +898,25 @@
                    candidates are returned only if there was something
                    wrong with the prompt (see prompt_feedback) -
                    feedback on each candidate is reported on
                    finish_reason and safety_ratings.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.GenerateContentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.GenerateContentRequest):
             request = generative_service.GenerateContentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if contents is not None:
@@ -1002,27 +1015,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1.types.EmbedContentResponse:
                 The response to an EmbedContentRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, content])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.EmbedContentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.EmbedContentRequest):
             request = generative_service.EmbedContentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if content is not None:
@@ -1130,27 +1141,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1.types.BatchEmbedContentsResponse:
                 The response to a BatchEmbedContentsRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, requests])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.BatchEmbedContentsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.BatchEmbedContentsRequest):
             request = generative_service.BatchEmbedContentsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if requests is not None:
@@ -1255,27 +1264,25 @@
             google.ai.generativelanguage_v1.types.CountTokensResponse:
                 A response from CountTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.CountTokensRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.CountTokensRequest):
             request = generative_service.CountTokensRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if contents is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/grpc.py`

 * *Files 2% similar despite different names*

```diff
@@ -47,15 +47,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -67,36 +67,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -114,15 +117,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -155,15 +158,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,98 +9,53 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import gapic_v1, grpc_helpers
+import google.auth  # type: ignore
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
-from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1.types import generative_service
+from google.ai.generativelanguage_v1beta.types import generative_service
 
 from .base import DEFAULT_CLIENT_INFO, GenerativeServiceTransport
-from .grpc import GenerativeServiceGrpcTransport
 
 
-class GenerativeServiceGrpcAsyncIOTransport(GenerativeServiceTransport):
-    """gRPC AsyncIO backend transport for GenerativeService.
+class GenerativeServiceGrpcTransport(GenerativeServiceTransport):
+    """gRPC backend transport for GenerativeService.
 
     API for using Large Models that generate multimodal content
     and have additional capabilities beyond text generation.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _grpc_channel: aio.Channel
-    _stubs: Dict[str, Callable] = {}
-
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> aio.Channel:
-        """Create and return a gRPC AsyncIO channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            aio.Channel: A gRPC AsyncIO channel object.
-        """
-
-        return grpc_helpers_async.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
+    _stubs: Dict[str, Callable]
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -112,68 +67,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional(Sequence[str])): A list of scopes. This argument is
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
+          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
+
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -200,15 +158,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -219,225 +179,253 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @property
-    def grpc_channel(self) -> aio.Channel:
-        """Create the channel designed to connect to this service.
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> grpc.Channel:
+        """Create and return a gRPC channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+                This argument is mutually exclusive with credentials.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            grpc.Channel: A gRPC channel object.
 
-        This property caches on the instance; repeated calls return
-        the same channel.
+        Raises:
+            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
+              and ``credentials_file`` are passed.
         """
-        # Return the channel from cache.
+
+        return grpc_helpers.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
+
+    @property
+    def grpc_channel(self) -> grpc.Channel:
+        """Return the channel designed to connect to this service."""
         return self._grpc_channel
 
     @property
     def generate_content(
         self,
     ) -> Callable[
         [generative_service.GenerateContentRequest],
-        Awaitable[generative_service.GenerateContentResponse],
+        generative_service.GenerateContentResponse,
     ]:
         r"""Return a callable for the generate content method over gRPC.
 
         Generates a response from the model given an input
         ``GenerateContentRequest``.
 
+        Input capabilities differ between models, including tuned
+        models. See the `model
+        guide <https://ai.google.dev/models/gemini>`__ and `tuning
+        guide <https://ai.google.dev/docs/model_tuning_guidance>`__ for
+        details.
+
         Returns:
             Callable[[~.GenerateContentRequest],
-                    Awaitable[~.GenerateContentResponse]]:
+                    ~.GenerateContentResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "generate_content" not in self._stubs:
             self._stubs["generate_content"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1.GenerativeService/GenerateContent",
+                "/google.ai.generativelanguage.v1beta.GenerativeService/GenerateContent",
                 request_serializer=generative_service.GenerateContentRequest.serialize,
                 response_deserializer=generative_service.GenerateContentResponse.deserialize,
             )
         return self._stubs["generate_content"]
 
     @property
+    def generate_answer(
+        self,
+    ) -> Callable[
+        [generative_service.GenerateAnswerRequest],
+        generative_service.GenerateAnswerResponse,
+    ]:
+        r"""Return a callable for the generate answer method over gRPC.
+
+        Generates a grounded answer from the model given an input
+        ``GenerateAnswerRequest``.
+
+        Returns:
+            Callable[[~.GenerateAnswerRequest],
+                    ~.GenerateAnswerResponse]:
+                A function that, when called, will call the underlying RPC
+                on the server.
+        """
+        # Generate a "stub function" on-the-fly which will actually make
+        # the request.
+        # gRPC handles serialization and deserialization, so we just need
+        # to pass in the functions for each.
+        if "generate_answer" not in self._stubs:
+            self._stubs["generate_answer"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.GenerativeService/GenerateAnswer",
+                request_serializer=generative_service.GenerateAnswerRequest.serialize,
+                response_deserializer=generative_service.GenerateAnswerResponse.deserialize,
+            )
+        return self._stubs["generate_answer"]
+
+    @property
     def stream_generate_content(
         self,
     ) -> Callable[
         [generative_service.GenerateContentRequest],
-        Awaitable[generative_service.GenerateContentResponse],
+        generative_service.GenerateContentResponse,
     ]:
         r"""Return a callable for the stream generate content method over gRPC.
 
         Generates a streamed response from the model given an input
         ``GenerateContentRequest``.
 
         Returns:
             Callable[[~.GenerateContentRequest],
-                    Awaitable[~.GenerateContentResponse]]:
+                    ~.GenerateContentResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "stream_generate_content" not in self._stubs:
             self._stubs["stream_generate_content"] = self.grpc_channel.unary_stream(
-                "/google.ai.generativelanguage.v1.GenerativeService/StreamGenerateContent",
+                "/google.ai.generativelanguage.v1beta.GenerativeService/StreamGenerateContent",
                 request_serializer=generative_service.GenerateContentRequest.serialize,
                 response_deserializer=generative_service.GenerateContentResponse.deserialize,
             )
         return self._stubs["stream_generate_content"]
 
     @property
     def embed_content(
         self,
     ) -> Callable[
         [generative_service.EmbedContentRequest],
-        Awaitable[generative_service.EmbedContentResponse],
+        generative_service.EmbedContentResponse,
     ]:
         r"""Return a callable for the embed content method over gRPC.
 
         Generates an embedding from the model given an input
         ``Content``.
 
         Returns:
             Callable[[~.EmbedContentRequest],
-                    Awaitable[~.EmbedContentResponse]]:
+                    ~.EmbedContentResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "embed_content" not in self._stubs:
             self._stubs["embed_content"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1.GenerativeService/EmbedContent",
+                "/google.ai.generativelanguage.v1beta.GenerativeService/EmbedContent",
                 request_serializer=generative_service.EmbedContentRequest.serialize,
                 response_deserializer=generative_service.EmbedContentResponse.deserialize,
             )
         return self._stubs["embed_content"]
 
     @property
     def batch_embed_contents(
         self,
     ) -> Callable[
         [generative_service.BatchEmbedContentsRequest],
-        Awaitable[generative_service.BatchEmbedContentsResponse],
+        generative_service.BatchEmbedContentsResponse,
     ]:
         r"""Return a callable for the batch embed contents method over gRPC.
 
         Generates multiple embeddings from the model given
         input text in a synchronous call.
 
         Returns:
             Callable[[~.BatchEmbedContentsRequest],
-                    Awaitable[~.BatchEmbedContentsResponse]]:
+                    ~.BatchEmbedContentsResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "batch_embed_contents" not in self._stubs:
             self._stubs["batch_embed_contents"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1.GenerativeService/BatchEmbedContents",
+                "/google.ai.generativelanguage.v1beta.GenerativeService/BatchEmbedContents",
                 request_serializer=generative_service.BatchEmbedContentsRequest.serialize,
                 response_deserializer=generative_service.BatchEmbedContentsResponse.deserialize,
             )
         return self._stubs["batch_embed_contents"]
 
     @property
     def count_tokens(
         self,
     ) -> Callable[
-        [generative_service.CountTokensRequest],
-        Awaitable[generative_service.CountTokensResponse],
+        [generative_service.CountTokensRequest], generative_service.CountTokensResponse
     ]:
         r"""Return a callable for the count tokens method over gRPC.
 
         Runs a model's tokenizer on input content and returns
         the token count.
 
         Returns:
             Callable[[~.CountTokensRequest],
-                    Awaitable[~.CountTokensResponse]]:
+                    ~.CountTokensResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "count_tokens" not in self._stubs:
             self._stubs["count_tokens"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1.GenerativeService/CountTokens",
+                "/google.ai.generativelanguage.v1beta.GenerativeService/CountTokens",
                 request_serializer=generative_service.CountTokensRequest.serialize,
                 response_deserializer=generative_service.CountTokensResponse.deserialize,
             )
         return self._stubs["count_tokens"]
 
     def close(self):
-        return self.grpc_channel.close()
-
-    @property
-    def cancel_operation(
-        self,
-    ) -> Callable[[operations_pb2.CancelOperationRequest], None]:
-        r"""Return a callable for the cancel_operation method over gRPC."""
-        # Generate a "stub function" on-the-fly which will actually make
-        # the request.
-        # gRPC handles serialization and deserialization, so we just need
-        # to pass in the functions for each.
-        if "cancel_operation" not in self._stubs:
-            self._stubs["cancel_operation"] = self.grpc_channel.unary_unary(
-                "/google.longrunning.Operations/CancelOperation",
-                request_serializer=operations_pb2.CancelOperationRequest.SerializeToString,
-                response_deserializer=None,
-            )
-        return self._stubs["cancel_operation"]
+        self.grpc_channel.close()
 
     @property
-    def get_operation(
-        self,
-    ) -> Callable[[operations_pb2.GetOperationRequest], operations_pb2.Operation]:
-        r"""Return a callable for the get_operation method over gRPC."""
-        # Generate a "stub function" on-the-fly which will actually make
-        # the request.
-        # gRPC handles serialization and deserialization, so we just need
-        # to pass in the functions for each.
-        if "get_operation" not in self._stubs:
-            self._stubs["get_operation"] = self.grpc_channel.unary_unary(
-                "/google.longrunning.Operations/GetOperation",
-                request_serializer=operations_pb2.GetOperationRequest.SerializeToString,
-                response_deserializer=operations_pb2.Operation.FromString,
-            )
-        return self._stubs["get_operation"]
-
-    @property
-    def list_operations(
-        self,
-    ) -> Callable[
-        [operations_pb2.ListOperationsRequest], operations_pb2.ListOperationsResponse
-    ]:
-        r"""Return a callable for the list_operations method over gRPC."""
-        # Generate a "stub function" on-the-fly which will actually make
-        # the request.
-        # gRPC handles serialization and deserialization, so we just need
-        # to pass in the functions for each.
-        if "list_operations" not in self._stubs:
-            self._stubs["list_operations"] = self.grpc_channel.unary_unary(
-                "/google.longrunning.Operations/ListOperations",
-                request_serializer=operations_pb2.ListOperationsRequest.SerializeToString,
-                response_deserializer=operations_pb2.ListOperationsResponse.FromString,
-            )
-        return self._stubs["list_operations"]
+    def kind(self) -> str:
+        return "grpc"
 
 
-__all__ = ("GenerativeServiceGrpcAsyncIOTransport",)
+__all__ = ("GenerativeServiceGrpcTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/generative_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/async_client.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -190,29 +191,33 @@
         type(ModelServiceClient).get_transport_class, type(ModelServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, ModelServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, ModelServiceTransport, Callable[..., ModelServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the model service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.ModelServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,ModelServiceTransport,Callable[..., ModelServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the ModelServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -313,37 +318,38 @@
         Returns:
             google.ai.generativelanguage_v1.types.Model:
                 Information about a Generative
                 Language Model.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.GetModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.GetModelRequest):
+            request = model_service.GetModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_model,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -439,39 +445,40 @@
                 Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.ListModelsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.ListModelsRequest):
+            request = model_service.ListModelsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if page_size is not None:
             request.page_size = page_size
         if page_token is not None:
             request.page_token = page_token
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_models,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_models
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/client.py`

 * *Files 0% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -520,29 +521,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, ModelServiceTransport]] = None,
+        transport: Optional[
+            Union[str, ModelServiceTransport, Callable[..., ModelServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the model service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ModelServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,ModelServiceTransport,Callable[..., ModelServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the ModelServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -640,16 +645,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[ModelServiceTransport], Callable[..., ModelServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., ModelServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -718,27 +730,25 @@
         Returns:
             google.ai.generativelanguage_v1.types.Model:
                 Information about a Generative
                 Language Model.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.GetModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.GetModelRequest):
             request = model_service.GetModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -844,27 +854,25 @@
                 Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.ListModelsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.ListModelsRequest):
             request = model_service.ListModelsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if page_size is not None:
                 request.page_size = page_size
             if page_token is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/pagers.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/pagers.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/grpc_asyncio.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,53 +9,99 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers
-import google.auth  # type: ignore
+from google.api_core import exceptions as core_exceptions
+from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
+from grpc.experimental import aio  # type: ignore
 
 from google.ai.generativelanguage_v1.types import model, model_service
 
 from .base import DEFAULT_CLIENT_INFO, ModelServiceTransport
+from .grpc import ModelServiceGrpcTransport
 
 
-class ModelServiceGrpcTransport(ModelServiceTransport):
-    """gRPC backend transport for ModelService.
+class ModelServiceGrpcAsyncIOTransport(ModelServiceTransport):
+    """gRPC AsyncIO backend transport for ModelService.
 
     Provides methods for getting metadata information about
     Generative Models.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _stubs: Dict[str, Callable]
+    _grpc_channel: aio.Channel
+    _stubs: Dict[str, Callable] = {}
+
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> aio.Channel:
+        """Create and return a gRPC AsyncIO channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            aio.Channel: A gRPC AsyncIO channel object.
+        """
+
+        return grpc_helpers_async.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -67,68 +113,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
+            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
-
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -155,15 +204,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -174,75 +225,35 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> grpc.Channel:
-        """Create and return a gRPC channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is mutually exclusive with credentials.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            grpc.Channel: A gRPC channel object.
+    @property
+    def grpc_channel(self) -> aio.Channel:
+        """Create the channel designed to connect to this service.
 
-        Raises:
-            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
-              and ``credentials_file`` are passed.
+        This property caches on the instance; repeated calls return
+        the same channel.
         """
-
-        return grpc_helpers.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
-
-    @property
-    def grpc_channel(self) -> grpc.Channel:
-        """Return the channel designed to connect to this service."""
+        # Return the channel from cache.
         return self._grpc_channel
 
     @property
-    def get_model(self) -> Callable[[model_service.GetModelRequest], model.Model]:
+    def get_model(
+        self,
+    ) -> Callable[[model_service.GetModelRequest], Awaitable[model.Model]]:
         r"""Return a callable for the get model method over gRPC.
 
         Gets information about a specific Model.
 
         Returns:
             Callable[[~.GetModelRequest],
-                    ~.Model]:
+                    Awaitable[~.Model]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -253,22 +264,24 @@
                 response_deserializer=model.Model.deserialize,
             )
         return self._stubs["get_model"]
 
     @property
     def list_models(
         self,
-    ) -> Callable[[model_service.ListModelsRequest], model_service.ListModelsResponse]:
+    ) -> Callable[
+        [model_service.ListModelsRequest], Awaitable[model_service.ListModelsResponse]
+    ]:
         r"""Return a callable for the list models method over gRPC.
 
         Lists models available through the API.
 
         Returns:
             Callable[[~.ListModelsRequest],
-                    ~.ListModelsResponse]:
+                    Awaitable[~.ListModelsResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -276,16 +289,31 @@
             self._stubs["list_models"] = self.grpc_channel.unary_unary(
                 "/google.ai.generativelanguage.v1.ModelService/ListModels",
                 request_serializer=model_service.ListModelsRequest.serialize,
                 response_deserializer=model_service.ListModelsResponse.deserialize,
             )
         return self._stubs["list_models"]
 
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.get_model: gapic_v1.method_async.wrap_method(
+                self.get_model,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.list_models: gapic_v1.method_async.wrap_method(
+                self.list_models,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+        }
+
     def close(self):
-        self.grpc_channel.close()
+        return self.grpc_channel.close()
 
     @property
     def cancel_operation(
         self,
     ) -> Callable[[operations_pb2.CancelOperationRequest], None]:
         r"""Return a callable for the cancel_operation method over gRPC."""
         # Generate a "stub function" on-the-fly which will actually make
@@ -332,13 +360,9 @@
             self._stubs["list_operations"] = self.grpc_channel.unary_unary(
                 "/google.longrunning.Operations/ListOperations",
                 request_serializer=operations_pb2.ListOperationsRequest.SerializeToString,
                 response_deserializer=operations_pb2.ListOperationsResponse.FromString,
             )
         return self._stubs["list_operations"]
 
-    @property
-    def kind(self) -> str:
-        return "grpc"
-
 
-__all__ = ("ModelServiceGrpcTransport",)
+__all__ = ("ModelServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/grpc.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,98 +9,53 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import gapic_v1, grpc_helpers
+import google.auth  # type: ignore
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
-from grpc.experimental import aio  # type: ignore
 
 from google.ai.generativelanguage_v1.types import model, model_service
 
 from .base import DEFAULT_CLIENT_INFO, ModelServiceTransport
-from .grpc import ModelServiceGrpcTransport
 
 
-class ModelServiceGrpcAsyncIOTransport(ModelServiceTransport):
-    """gRPC AsyncIO backend transport for ModelService.
+class ModelServiceGrpcTransport(ModelServiceTransport):
+    """gRPC backend transport for ModelService.
 
     Provides methods for getting metadata information about
     Generative Models.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _grpc_channel: aio.Channel
-    _stubs: Dict[str, Callable] = {}
-
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> aio.Channel:
-        """Create and return a gRPC AsyncIO channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            aio.Channel: A gRPC AsyncIO channel object.
-        """
-
-        return grpc_helpers_async.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
+    _stubs: Dict[str, Callable]
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -112,68 +67,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional(Sequence[str])): A list of scopes. This argument is
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
+          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
+
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -200,15 +158,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -219,35 +179,75 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @property
-    def grpc_channel(self) -> aio.Channel:
-        """Create the channel designed to connect to this service.
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> grpc.Channel:
+        """Create and return a gRPC channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+                This argument is mutually exclusive with credentials.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            grpc.Channel: A gRPC channel object.
 
-        This property caches on the instance; repeated calls return
-        the same channel.
+        Raises:
+            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
+              and ``credentials_file`` are passed.
         """
-        # Return the channel from cache.
+
+        return grpc_helpers.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
+
+    @property
+    def grpc_channel(self) -> grpc.Channel:
+        """Return the channel designed to connect to this service."""
         return self._grpc_channel
 
     @property
-    def get_model(
-        self,
-    ) -> Callable[[model_service.GetModelRequest], Awaitable[model.Model]]:
+    def get_model(self) -> Callable[[model_service.GetModelRequest], model.Model]:
         r"""Return a callable for the get model method over gRPC.
 
         Gets information about a specific Model.
 
         Returns:
             Callable[[~.GetModelRequest],
-                    Awaitable[~.Model]]:
+                    ~.Model]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -258,24 +258,22 @@
                 response_deserializer=model.Model.deserialize,
             )
         return self._stubs["get_model"]
 
     @property
     def list_models(
         self,
-    ) -> Callable[
-        [model_service.ListModelsRequest], Awaitable[model_service.ListModelsResponse]
-    ]:
+    ) -> Callable[[model_service.ListModelsRequest], model_service.ListModelsResponse]:
         r"""Return a callable for the list models method over gRPC.
 
         Lists models available through the API.
 
         Returns:
             Callable[[~.ListModelsRequest],
-                    Awaitable[~.ListModelsResponse]]:
+                    ~.ListModelsResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -284,15 +282,15 @@
                 "/google.ai.generativelanguage.v1.ModelService/ListModels",
                 request_serializer=model_service.ListModelsRequest.serialize,
                 response_deserializer=model_service.ListModelsResponse.deserialize,
             )
         return self._stubs["list_models"]
 
     def close(self):
-        return self.grpc_channel.close()
+        self.grpc_channel.close()
 
     @property
     def cancel_operation(
         self,
     ) -> Callable[[operations_pb2.CancelOperationRequest], None]:
         r"""Return a callable for the cancel_operation method over gRPC."""
         # Generate a "stub function" on-the-fly which will actually make
@@ -339,9 +337,13 @@
             self._stubs["list_operations"] = self.grpc_channel.unary_unary(
                 "/google.longrunning.Operations/ListOperations",
                 request_serializer=operations_pb2.ListOperationsRequest.SerializeToString,
                 response_deserializer=operations_pb2.ListOperationsResponse.FromString,
             )
         return self._stubs["list_operations"]
 
+    @property
+    def kind(self) -> str:
+        return "grpc"
+
 
-__all__ = ("ModelServiceGrpcAsyncIOTransport",)
+__all__ = ("ModelServiceGrpcTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/services/model_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/model_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/citation.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/citation.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/content.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/content.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/generative_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/generative_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/model.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/model.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/model_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/model_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1/types/safety.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/types/safety.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/gapic_metadata.json` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/gapic_metadata.json`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/gapic_version.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/gapic_version.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,8 +9,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-__version__ = "0.6.2"  # {x-release-please-version}
+__version__ = "0.6.3"  # {x-release-please-version}
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/async_client.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -195,29 +196,33 @@
         type(DiscussServiceClient).get_transport_class, type(DiscussServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, DiscussServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, DiscussServiceTransport, Callable[..., DiscussServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the discuss service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.DiscussServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,DiscussServiceTransport,Callable[..., DiscussServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the DiscussServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -384,26 +389,29 @@
 
                 This includes candidate messages and
                 conversation history in the form of
                 chronologically-ordered messages.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [model, prompt, temperature, candidate_count, top_p, top_k]
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = discuss_service.GenerateMessageRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, discuss_service.GenerateMessageRequest):
+            request = discuss_service.GenerateMessageRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
@@ -414,28 +422,17 @@
         if top_p is not None:
             request.top_p = top_p
         if top_k is not None:
             request.top_k = top_k
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.generate_message,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.generate_message
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -534,48 +531,40 @@
             google.ai.generativelanguage_v1beta.types.CountMessageTokensResponse:
                 A response from CountMessageTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = discuss_service.CountMessageTokensRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, discuss_service.CountMessageTokensRequest):
+            request = discuss_service.CountMessageTokensRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.count_message_tokens,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.count_message_tokens
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/client.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -523,29 +524,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, DiscussServiceTransport]] = None,
+        transport: Optional[
+            Union[str, DiscussServiceTransport, Callable[..., DiscussServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the discuss service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, DiscussServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,DiscussServiceTransport,Callable[..., DiscussServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the DiscussServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -646,16 +651,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[DiscussServiceTransport], Callable[..., DiscussServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., DiscussServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -790,29 +802,27 @@
 
                 This includes candidate messages and
                 conversation history in the form of
                 chronologically-ordered messages.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [model, prompt, temperature, candidate_count, top_p, top_k]
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a discuss_service.GenerateMessageRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, discuss_service.GenerateMessageRequest):
             request = discuss_service.GenerateMessageRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
@@ -931,27 +941,25 @@
             google.ai.generativelanguage_v1beta.types.CountMessageTokensResponse:
                 A response from CountMessageTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a discuss_service.CountMessageTokensRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, discuss_service.CountMessageTokensRequest):
             request = discuss_service.CountMessageTokensRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/grpc.py`

 * *Files 3% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 from google.api_core import gapic_v1, grpc_helpers
 import google.auth  # type: ignore
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
 
-from google.ai.generativelanguage_v1beta.types import discuss_service
+from google.ai.generativelanguage_v1beta3.types import discuss_service
 
 from .base import DEFAULT_CLIENT_INFO, DiscussServiceTransport
 
 
 class DiscussServiceGrpcTransport(DiscussServiceTransport):
     """gRPC backend transport for DiscussService.
 
@@ -49,15 +49,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -69,36 +69,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -116,15 +119,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -157,15 +160,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -252,15 +257,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "generate_message" not in self._stubs:
             self._stubs["generate_message"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.DiscussService/GenerateMessage",
+                "/google.ai.generativelanguage.v1beta3.DiscussService/GenerateMessage",
                 request_serializer=discuss_service.GenerateMessageRequest.serialize,
                 response_deserializer=discuss_service.GenerateMessageResponse.deserialize,
             )
         return self._stubs["generate_message"]
 
     @property
     def count_message_tokens(
@@ -282,15 +287,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "count_message_tokens" not in self._stubs:
             self._stubs["count_message_tokens"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.DiscussService/CountMessageTokens",
+                "/google.ai.generativelanguage.v1beta3.DiscussService/CountMessageTokens",
                 request_serializer=discuss_service.CountMessageTokensRequest.serialize,
                 response_deserializer=discuss_service.CountMessageTokensResponse.deserialize,
             )
         return self._stubs["count_message_tokens"]
 
     def close(self):
         self.grpc_channel.close()
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/grpc.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,100 +9,55 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import gapic_v1, grpc_helpers
+import google.auth  # type: ignore
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
-from grpc.experimental import aio  # type: ignore
 
 from google.ai.generativelanguage_v1beta.types import discuss_service
 
 from .base import DEFAULT_CLIENT_INFO, DiscussServiceTransport
-from .grpc import DiscussServiceGrpcTransport
 
 
-class DiscussServiceGrpcAsyncIOTransport(DiscussServiceTransport):
-    """gRPC AsyncIO backend transport for DiscussService.
+class DiscussServiceGrpcTransport(DiscussServiceTransport):
+    """gRPC backend transport for DiscussService.
 
     An API for using Generative Language Models (GLMs) in dialog
     applications.
     Also known as large language models (LLMs), this API provides
     models that are trained for multi-turn dialog.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _grpc_channel: aio.Channel
-    _stubs: Dict[str, Callable] = {}
-
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> aio.Channel:
-        """Create and return a gRPC AsyncIO channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            aio.Channel: A gRPC AsyncIO channel object.
-        """
-
-        return grpc_helpers_async.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
+    _stubs: Dict[str, Callable]
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -114,68 +69,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional(Sequence[str])): A list of scopes. This argument is
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
+          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
+
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -202,15 +160,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -221,39 +181,81 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @property
-    def grpc_channel(self) -> aio.Channel:
-        """Create the channel designed to connect to this service.
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> grpc.Channel:
+        """Create and return a gRPC channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+                This argument is mutually exclusive with credentials.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            grpc.Channel: A gRPC channel object.
 
-        This property caches on the instance; repeated calls return
-        the same channel.
+        Raises:
+            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
+              and ``credentials_file`` are passed.
         """
-        # Return the channel from cache.
+
+        return grpc_helpers.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
+
+    @property
+    def grpc_channel(self) -> grpc.Channel:
+        """Return the channel designed to connect to this service."""
         return self._grpc_channel
 
     @property
     def generate_message(
         self,
     ) -> Callable[
         [discuss_service.GenerateMessageRequest],
-        Awaitable[discuss_service.GenerateMessageResponse],
+        discuss_service.GenerateMessageResponse,
     ]:
         r"""Return a callable for the generate message method over gRPC.
 
         Generates a response from the model given an input
         ``MessagePrompt``.
 
         Returns:
             Callable[[~.GenerateMessageRequest],
-                    Awaitable[~.GenerateMessageResponse]]:
+                    ~.GenerateMessageResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -266,24 +268,24 @@
         return self._stubs["generate_message"]
 
     @property
     def count_message_tokens(
         self,
     ) -> Callable[
         [discuss_service.CountMessageTokensRequest],
-        Awaitable[discuss_service.CountMessageTokensResponse],
+        discuss_service.CountMessageTokensResponse,
     ]:
         r"""Return a callable for the count message tokens method over gRPC.
 
         Runs a model's tokenizer on a string and returns the
         token count.
 
         Returns:
             Callable[[~.CountMessageTokensRequest],
-                    Awaitable[~.CountMessageTokensResponse]]:
+                    ~.CountMessageTokensResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -292,11 +294,15 @@
                 "/google.ai.generativelanguage.v1beta.DiscussService/CountMessageTokens",
                 request_serializer=discuss_service.CountMessageTokensRequest.serialize,
                 response_deserializer=discuss_service.CountMessageTokensResponse.deserialize,
             )
         return self._stubs["count_message_tokens"]
 
     def close(self):
-        return self.grpc_channel.close()
+        self.grpc_channel.close()
+
+    @property
+    def kind(self) -> str:
+        return "grpc"
 
 
-__all__ = ("DiscussServiceGrpcAsyncIOTransport",)
+__all__ = ("DiscussServiceGrpcTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/discuss_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/async_client.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -189,29 +190,33 @@
         type(FileServiceClient).get_transport_class, type(FileServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, FileServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, FileServiceTransport, Callable[..., FileServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the file service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.FileServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,FileServiceTransport,Callable[..., FileServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the FileServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -296,23 +301,24 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.CreateFileResponse:
                 Response for CreateFile.
         """
         # Create or coerce a protobuf request object.
-        request = file_service.CreateFileRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, file_service.CreateFileRequest):
+            request = file_service.CreateFileRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.create_file,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.create_file
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -375,23 +381,24 @@
                 Response for ListFiles.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        request = file_service.ListFilesRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, file_service.ListFilesRequest):
+            request = file_service.ListFilesRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_files,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_files
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -466,37 +473,36 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.File:
                 A file uploaded to the API.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = file_service.GetFileRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, file_service.GetFileRequest):
+            request = file_service.GetFileRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_file,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[self._client._transport.get_file]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -561,37 +567,38 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = file_service.DeleteFileRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, file_service.DeleteFileRequest):
+            request = file_service.DeleteFileRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.delete_file,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.delete_file
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/client.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -519,29 +520,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, FileServiceTransport]] = None,
+        transport: Optional[
+            Union[str, FileServiceTransport, Callable[..., FileServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the file service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, FileServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,FileServiceTransport,Callable[..., FileServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the FileServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -639,16 +644,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[FileServiceTransport], Callable[..., FileServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., FileServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -701,18 +713,16 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.CreateFileResponse:
                 Response for CreateFile.
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a file_service.CreateFileRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, file_service.CreateFileRequest):
             request = file_service.CreateFileRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.create_file]
 
@@ -781,18 +791,16 @@
                 Response for ListFiles.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a file_service.ListFilesRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, file_service.ListFilesRequest):
             request = file_service.ListFilesRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.list_files]
 
@@ -873,27 +881,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.File:
                 A file uploaded to the API.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a file_service.GetFileRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, file_service.GetFileRequest):
             request = file_service.GetFileRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -968,27 +974,25 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a file_service.DeleteFileRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, file_service.DeleteFileRequest):
             request = file_service.DeleteFileRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/pagers.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/pagers.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/grpc_asyncio.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,53 +9,99 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers
-import google.auth  # type: ignore
+from google.api_core import exceptions as core_exceptions
+from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
+from grpc.experimental import aio  # type: ignore
 
 from google.ai.generativelanguage_v1beta.types import file, file_service
 
 from .base import DEFAULT_CLIENT_INFO, FileServiceTransport
+from .grpc import FileServiceGrpcTransport
 
 
-class FileServiceGrpcTransport(FileServiceTransport):
-    """gRPC backend transport for FileService.
+class FileServiceGrpcAsyncIOTransport(FileServiceTransport):
+    """gRPC AsyncIO backend transport for FileService.
 
     An API for uploading and managing files.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _stubs: Dict[str, Callable]
+    _grpc_channel: aio.Channel
+    _stubs: Dict[str, Callable] = {}
+
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> aio.Channel:
+        """Create and return a gRPC AsyncIO channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            aio.Channel: A gRPC AsyncIO channel object.
+        """
+
+        return grpc_helpers_async.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -67,68 +113,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
+            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
-
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -155,15 +204,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -174,77 +225,37 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> grpc.Channel:
-        """Create and return a gRPC channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is mutually exclusive with credentials.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            grpc.Channel: A gRPC channel object.
+    @property
+    def grpc_channel(self) -> aio.Channel:
+        """Create the channel designed to connect to this service.
 
-        Raises:
-            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
-              and ``credentials_file`` are passed.
+        This property caches on the instance; repeated calls return
+        the same channel.
         """
-
-        return grpc_helpers.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
-
-    @property
-    def grpc_channel(self) -> grpc.Channel:
-        """Return the channel designed to connect to this service."""
+        # Return the channel from cache.
         return self._grpc_channel
 
     @property
     def create_file(
         self,
-    ) -> Callable[[file_service.CreateFileRequest], file_service.CreateFileResponse]:
+    ) -> Callable[
+        [file_service.CreateFileRequest], Awaitable[file_service.CreateFileResponse]
+    ]:
         r"""Return a callable for the create file method over gRPC.
 
         Creates a ``File``.
 
         Returns:
             Callable[[~.CreateFileRequest],
-                    ~.CreateFileResponse]:
+                    Awaitable[~.CreateFileResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -255,23 +266,25 @@
                 response_deserializer=file_service.CreateFileResponse.deserialize,
             )
         return self._stubs["create_file"]
 
     @property
     def list_files(
         self,
-    ) -> Callable[[file_service.ListFilesRequest], file_service.ListFilesResponse]:
+    ) -> Callable[
+        [file_service.ListFilesRequest], Awaitable[file_service.ListFilesResponse]
+    ]:
         r"""Return a callable for the list files method over gRPC.
 
         Lists the metadata for ``File``\ s owned by the requesting
         project.
 
         Returns:
             Callable[[~.ListFilesRequest],
-                    ~.ListFilesResponse]:
+                    Awaitable[~.ListFilesResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -280,22 +293,22 @@
                 "/google.ai.generativelanguage.v1beta.FileService/ListFiles",
                 request_serializer=file_service.ListFilesRequest.serialize,
                 response_deserializer=file_service.ListFilesResponse.deserialize,
             )
         return self._stubs["list_files"]
 
     @property
-    def get_file(self) -> Callable[[file_service.GetFileRequest], file.File]:
+    def get_file(self) -> Callable[[file_service.GetFileRequest], Awaitable[file.File]]:
         r"""Return a callable for the get file method over gRPC.
 
         Gets the metadata for the given ``File``.
 
         Returns:
             Callable[[~.GetFileRequest],
-                    ~.File]:
+                    Awaitable[~.File]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -306,22 +319,22 @@
                 response_deserializer=file.File.deserialize,
             )
         return self._stubs["get_file"]
 
     @property
     def delete_file(
         self,
-    ) -> Callable[[file_service.DeleteFileRequest], empty_pb2.Empty]:
+    ) -> Callable[[file_service.DeleteFileRequest], Awaitable[empty_pb2.Empty]]:
         r"""Return a callable for the delete file method over gRPC.
 
         Deletes the ``File``.
 
         Returns:
             Callable[[~.DeleteFileRequest],
-                    ~.Empty]:
+                    Awaitable[~.Empty]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -329,16 +342,37 @@
             self._stubs["delete_file"] = self.grpc_channel.unary_unary(
                 "/google.ai.generativelanguage.v1beta.FileService/DeleteFile",
                 request_serializer=file_service.DeleteFileRequest.serialize,
                 response_deserializer=empty_pb2.Empty.FromString,
             )
         return self._stubs["delete_file"]
 
-    def close(self):
-        self.grpc_channel.close()
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.create_file: gapic_v1.method_async.wrap_method(
+                self.create_file,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.list_files: gapic_v1.method_async.wrap_method(
+                self.list_files,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.get_file: gapic_v1.method_async.wrap_method(
+                self.get_file,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.delete_file: gapic_v1.method_async.wrap_method(
+                self.delete_file,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+        }
 
-    @property
-    def kind(self) -> str:
-        return "grpc"
+    def close(self):
+        return self.grpc_channel.close()
 
 
-__all__ = ("FileServiceGrpcTransport",)
+__all__ = ("FileServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/grpc.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,98 +9,53 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import gapic_v1, grpc_helpers
+import google.auth  # type: ignore
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
-from grpc.experimental import aio  # type: ignore
 
 from google.ai.generativelanguage_v1beta.types import file, file_service
 
 from .base import DEFAULT_CLIENT_INFO, FileServiceTransport
-from .grpc import FileServiceGrpcTransport
 
 
-class FileServiceGrpcAsyncIOTransport(FileServiceTransport):
-    """gRPC AsyncIO backend transport for FileService.
+class FileServiceGrpcTransport(FileServiceTransport):
+    """gRPC backend transport for FileService.
 
     An API for uploading and managing files.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _grpc_channel: aio.Channel
-    _stubs: Dict[str, Callable] = {}
-
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> aio.Channel:
-        """Create and return a gRPC AsyncIO channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            aio.Channel: A gRPC AsyncIO channel object.
-        """
-
-        return grpc_helpers_async.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
+    _stubs: Dict[str, Callable]
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -112,68 +67,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional(Sequence[str])): A list of scopes. This argument is
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
+          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
+
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -200,15 +158,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -219,37 +179,77 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @property
-    def grpc_channel(self) -> aio.Channel:
-        """Create the channel designed to connect to this service.
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> grpc.Channel:
+        """Create and return a gRPC channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+                This argument is mutually exclusive with credentials.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            grpc.Channel: A gRPC channel object.
 
-        This property caches on the instance; repeated calls return
-        the same channel.
+        Raises:
+            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
+              and ``credentials_file`` are passed.
         """
-        # Return the channel from cache.
+
+        return grpc_helpers.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
+
+    @property
+    def grpc_channel(self) -> grpc.Channel:
+        """Return the channel designed to connect to this service."""
         return self._grpc_channel
 
     @property
     def create_file(
         self,
-    ) -> Callable[
-        [file_service.CreateFileRequest], Awaitable[file_service.CreateFileResponse]
-    ]:
+    ) -> Callable[[file_service.CreateFileRequest], file_service.CreateFileResponse]:
         r"""Return a callable for the create file method over gRPC.
 
         Creates a ``File``.
 
         Returns:
             Callable[[~.CreateFileRequest],
-                    Awaitable[~.CreateFileResponse]]:
+                    ~.CreateFileResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -260,25 +260,23 @@
                 response_deserializer=file_service.CreateFileResponse.deserialize,
             )
         return self._stubs["create_file"]
 
     @property
     def list_files(
         self,
-    ) -> Callable[
-        [file_service.ListFilesRequest], Awaitable[file_service.ListFilesResponse]
-    ]:
+    ) -> Callable[[file_service.ListFilesRequest], file_service.ListFilesResponse]:
         r"""Return a callable for the list files method over gRPC.
 
         Lists the metadata for ``File``\ s owned by the requesting
         project.
 
         Returns:
             Callable[[~.ListFilesRequest],
-                    Awaitable[~.ListFilesResponse]]:
+                    ~.ListFilesResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -287,22 +285,22 @@
                 "/google.ai.generativelanguage.v1beta.FileService/ListFiles",
                 request_serializer=file_service.ListFilesRequest.serialize,
                 response_deserializer=file_service.ListFilesResponse.deserialize,
             )
         return self._stubs["list_files"]
 
     @property
-    def get_file(self) -> Callable[[file_service.GetFileRequest], Awaitable[file.File]]:
+    def get_file(self) -> Callable[[file_service.GetFileRequest], file.File]:
         r"""Return a callable for the get file method over gRPC.
 
         Gets the metadata for the given ``File``.
 
         Returns:
             Callable[[~.GetFileRequest],
-                    Awaitable[~.File]]:
+                    ~.File]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -313,22 +311,22 @@
                 response_deserializer=file.File.deserialize,
             )
         return self._stubs["get_file"]
 
     @property
     def delete_file(
         self,
-    ) -> Callable[[file_service.DeleteFileRequest], Awaitable[empty_pb2.Empty]]:
+    ) -> Callable[[file_service.DeleteFileRequest], empty_pb2.Empty]:
         r"""Return a callable for the delete file method over gRPC.
 
         Deletes the ``File``.
 
         Returns:
             Callable[[~.DeleteFileRequest],
-                    Awaitable[~.Empty]]:
+                    ~.Empty]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -337,11 +335,15 @@
                 "/google.ai.generativelanguage.v1beta.FileService/DeleteFile",
                 request_serializer=file_service.DeleteFileRequest.serialize,
                 response_deserializer=empty_pb2.Empty.FromString,
             )
         return self._stubs["delete_file"]
 
     def close(self):
-        return self.grpc_channel.close()
+        self.grpc_channel.close()
+
+    @property
+    def kind(self) -> str:
+        return "grpc"
 
 
-__all__ = ("FileServiceGrpcAsyncIOTransport",)
+__all__ = ("FileServiceGrpcTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/file_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/file_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,14 +15,15 @@
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
     AsyncIterable,
     Awaitable,
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -197,29 +198,37 @@
         type(GenerativeServiceClient).get_transport_class, type(GenerativeServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, GenerativeServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[
+                str,
+                GenerativeServiceTransport,
+                Callable[..., GenerativeServiceTransport],
+            ]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the generative service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.GenerativeServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,GenerativeServiceTransport,Callable[..., GenerativeServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the GenerativeServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -270,14 +279,20 @@
         retry: OptionalRetry = gapic_v1.method.DEFAULT,
         timeout: Union[float, object] = gapic_v1.method.DEFAULT,
         metadata: Sequence[Tuple[str, str]] = (),
     ) -> generative_service.GenerateContentResponse:
         r"""Generates a response from the model given an input
         ``GenerateContentRequest``.
 
+        Input capabilities differ between models, including tuned
+        models. See the `model
+        guide <https://ai.google.dev/models/gemini>`__ and `tuning
+        guide <https://ai.google.dev/docs/model_tuning_guidance>`__ for
+        details.
+
         .. code-block:: python
 
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
@@ -343,48 +358,40 @@
                    candidates are returned only if there was something
                    wrong with the prompt (see prompt_feedback) -
                    feedback on each candidate is reported on
                    finish_reason and safety_ratings.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.GenerateContentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.GenerateContentRequest):
+            request = generative_service.GenerateContentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if contents:
             request.contents.extend(contents)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.generate_content,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.generate_content
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -512,52 +519,44 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.GenerateAnswerResponse:
                 Response from the model for a
                 grounded answer.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents, safety_settings, answer_style])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.GenerateAnswerRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.GenerateAnswerRequest):
+            request = generative_service.GenerateAnswerRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if answer_style is not None:
             request.answer_style = answer_style
         if contents:
             request.contents.extend(contents)
         if safety_settings:
             request.safety_settings.extend(safety_settings)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.generate_answer,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.generate_answer
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -660,48 +659,40 @@
                    candidates are returned only if there was something
                    wrong with the prompt (see prompt_feedback) -
                    feedback on each candidate is reported on
                    finish_reason and safety_ratings.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.GenerateContentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.GenerateContentRequest):
+            request = generative_service.GenerateContentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if contents:
             request.contents.extend(contents)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.stream_generate_content,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.stream_generate_content
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -788,48 +779,40 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.EmbedContentResponse:
                 The response to an EmbedContentRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, content])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.EmbedContentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.EmbedContentRequest):
+            request = generative_service.EmbedContentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if content is not None:
             request.content = content
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.embed_content,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.embed_content
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -925,48 +908,40 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.BatchEmbedContentsResponse:
                 The response to a BatchEmbedContentsRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, requests])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.BatchEmbedContentsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.BatchEmbedContentsRequest):
+            request = generative_service.BatchEmbedContentsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if requests:
             request.requests.extend(requests)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.batch_embed_contents,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.batch_embed_contents
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -1039,15 +1014,15 @@
 
                 Format: ``models/{model}``
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             contents (:class:`MutableSequence[google.ai.generativelanguage_v1beta.types.Content]`):
-                Required. The input given to the
+                Optional. The input given to the
                 model as a prompt.
 
                 This corresponds to the ``contents`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
@@ -1059,48 +1034,40 @@
             google.ai.generativelanguage_v1beta.types.CountTokensResponse:
                 A response from CountTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = generative_service.CountTokensRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, generative_service.CountTokensRequest):
+            request = generative_service.CountTokensRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if contents:
             request.contents.extend(contents)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.count_tokens,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.count_tokens
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/client.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Iterable,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
@@ -524,29 +525,37 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, GenerativeServiceTransport]] = None,
+        transport: Optional[
+            Union[
+                str,
+                GenerativeServiceTransport,
+                Callable[..., GenerativeServiceTransport],
+            ]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the generative service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, GenerativeServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,GenerativeServiceTransport,Callable[..., GenerativeServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the GenerativeServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -647,16 +656,24 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[GenerativeServiceTransport],
+                Callable[..., GenerativeServiceTransport],
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., GenerativeServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -675,14 +692,20 @@
         retry: OptionalRetry = gapic_v1.method.DEFAULT,
         timeout: Union[float, object] = gapic_v1.method.DEFAULT,
         metadata: Sequence[Tuple[str, str]] = (),
     ) -> generative_service.GenerateContentResponse:
         r"""Generates a response from the model given an input
         ``GenerateContentRequest``.
 
+        Input capabilities differ between models, including tuned
+        models. See the `model
+        guide <https://ai.google.dev/models/gemini>`__ and `tuning
+        guide <https://ai.google.dev/docs/model_tuning_guidance>`__ for
+        details.
+
         .. code-block:: python
 
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
@@ -748,27 +771,25 @@
                    candidates are returned only if there was something
                    wrong with the prompt (see prompt_feedback) -
                    feedback on each candidate is reported on
                    finish_reason and safety_ratings.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.GenerateContentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.GenerateContentRequest):
             request = generative_service.GenerateContentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if contents is not None:
@@ -908,27 +929,25 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.GenerateAnswerResponse:
                 Response from the model for a
                 grounded answer.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents, safety_settings, answer_style])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.GenerateAnswerRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.GenerateAnswerRequest):
             request = generative_service.GenerateAnswerRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if contents is not None:
@@ -1047,27 +1066,25 @@
                    candidates are returned only if there was something
                    wrong with the prompt (see prompt_feedback) -
                    feedback on each candidate is reported on
                    finish_reason and safety_ratings.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.GenerateContentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.GenerateContentRequest):
             request = generative_service.GenerateContentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if contents is not None:
@@ -1166,27 +1183,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.EmbedContentResponse:
                 The response to an EmbedContentRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, content])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.EmbedContentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.EmbedContentRequest):
             request = generative_service.EmbedContentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if content is not None:
@@ -1294,27 +1309,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.BatchEmbedContentsResponse:
                 The response to a BatchEmbedContentsRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, requests])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.BatchEmbedContentsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.BatchEmbedContentsRequest):
             request = generative_service.BatchEmbedContentsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if requests is not None:
@@ -1399,15 +1412,15 @@
 
                 Format: ``models/{model}``
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             contents (MutableSequence[google.ai.generativelanguage_v1beta.types.Content]):
-                Required. The input given to the
+                Optional. The input given to the
                 model as a prompt.
 
                 This corresponds to the ``contents`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
@@ -1419,27 +1432,25 @@
             google.ai.generativelanguage_v1beta.types.CountTokensResponse:
                 A response from CountTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, contents])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a generative_service.CountTokensRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, generative_service.CountTokensRequest):
             request = generative_service.CountTokensRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if contents is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/grpc.py`

 * *Files 16% similar despite different names*

```diff
@@ -12,31 +12,34 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers
+from google.api_core import gapic_v1, grpc_helpers, operations_v1
 import google.auth  # type: ignore
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
+from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
 
-from google.ai.generativelanguage_v1beta.types import generative_service
+from google.ai.generativelanguage_v1beta.types import tuned_model as gag_tuned_model
+from google.ai.generativelanguage_v1beta.types import model, model_service
+from google.ai.generativelanguage_v1beta.types import tuned_model
 
-from .base import DEFAULT_CLIENT_INFO, GenerativeServiceTransport
+from .base import DEFAULT_CLIENT_INFO, ModelServiceTransport
 
 
-class GenerativeServiceGrpcTransport(GenerativeServiceTransport):
-    """gRPC backend transport for GenerativeService.
+class ModelServiceGrpcTransport(ModelServiceTransport):
+    """gRPC backend transport for ModelService.
 
-    API for using Large Models that generate multimodal content
-    and have additional capabilities beyond text generation.
+    Provides methods for getting metadata information about
+    Generative Models.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
@@ -47,15 +50,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -67,36 +70,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -108,21 +114,22 @@
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
+        self._operations_client: Optional[operations_v1.OperationsClient] = None
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -155,15 +162,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -227,194 +236,216 @@
 
     @property
     def grpc_channel(self) -> grpc.Channel:
         """Return the channel designed to connect to this service."""
         return self._grpc_channel
 
     @property
-    def generate_content(
+    def operations_client(self) -> operations_v1.OperationsClient:
+        """Create the client designed to process long-running operations.
+
+        This property caches on the instance; repeated calls return the same
+        client.
+        """
+        # Quick check: Only create a new client if we do not already have one.
+        if self._operations_client is None:
+            self._operations_client = operations_v1.OperationsClient(self.grpc_channel)
+
+        # Return the client from cache.
+        return self._operations_client
+
+    @property
+    def get_model(self) -> Callable[[model_service.GetModelRequest], model.Model]:
+        r"""Return a callable for the get model method over gRPC.
+
+        Gets information about a specific Model.
+
+        Returns:
+            Callable[[~.GetModelRequest],
+                    ~.Model]:
+                A function that, when called, will call the underlying RPC
+                on the server.
+        """
+        # Generate a "stub function" on-the-fly which will actually make
+        # the request.
+        # gRPC handles serialization and deserialization, so we just need
+        # to pass in the functions for each.
+        if "get_model" not in self._stubs:
+            self._stubs["get_model"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.ModelService/GetModel",
+                request_serializer=model_service.GetModelRequest.serialize,
+                response_deserializer=model.Model.deserialize,
+            )
+        return self._stubs["get_model"]
+
+    @property
+    def list_models(
         self,
-    ) -> Callable[
-        [generative_service.GenerateContentRequest],
-        generative_service.GenerateContentResponse,
-    ]:
-        r"""Return a callable for the generate content method over gRPC.
+    ) -> Callable[[model_service.ListModelsRequest], model_service.ListModelsResponse]:
+        r"""Return a callable for the list models method over gRPC.
 
-        Generates a response from the model given an input
-        ``GenerateContentRequest``.
+        Lists models available through the API.
 
         Returns:
-            Callable[[~.GenerateContentRequest],
-                    ~.GenerateContentResponse]:
+            Callable[[~.ListModelsRequest],
+                    ~.ListModelsResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "generate_content" not in self._stubs:
-            self._stubs["generate_content"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.GenerativeService/GenerateContent",
-                request_serializer=generative_service.GenerateContentRequest.serialize,
-                response_deserializer=generative_service.GenerateContentResponse.deserialize,
+        if "list_models" not in self._stubs:
+            self._stubs["list_models"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.ModelService/ListModels",
+                request_serializer=model_service.ListModelsRequest.serialize,
+                response_deserializer=model_service.ListModelsResponse.deserialize,
             )
-        return self._stubs["generate_content"]
+        return self._stubs["list_models"]
 
     @property
-    def generate_answer(
+    def get_tuned_model(
         self,
-    ) -> Callable[
-        [generative_service.GenerateAnswerRequest],
-        generative_service.GenerateAnswerResponse,
-    ]:
-        r"""Return a callable for the generate answer method over gRPC.
+    ) -> Callable[[model_service.GetTunedModelRequest], tuned_model.TunedModel]:
+        r"""Return a callable for the get tuned model method over gRPC.
 
-        Generates a grounded answer from the model given an input
-        ``GenerateAnswerRequest``.
+        Gets information about a specific TunedModel.
 
         Returns:
-            Callable[[~.GenerateAnswerRequest],
-                    ~.GenerateAnswerResponse]:
+            Callable[[~.GetTunedModelRequest],
+                    ~.TunedModel]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "generate_answer" not in self._stubs:
-            self._stubs["generate_answer"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.GenerativeService/GenerateAnswer",
-                request_serializer=generative_service.GenerateAnswerRequest.serialize,
-                response_deserializer=generative_service.GenerateAnswerResponse.deserialize,
+        if "get_tuned_model" not in self._stubs:
+            self._stubs["get_tuned_model"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.ModelService/GetTunedModel",
+                request_serializer=model_service.GetTunedModelRequest.serialize,
+                response_deserializer=tuned_model.TunedModel.deserialize,
             )
-        return self._stubs["generate_answer"]
+        return self._stubs["get_tuned_model"]
 
     @property
-    def stream_generate_content(
+    def list_tuned_models(
         self,
     ) -> Callable[
-        [generative_service.GenerateContentRequest],
-        generative_service.GenerateContentResponse,
+        [model_service.ListTunedModelsRequest], model_service.ListTunedModelsResponse
     ]:
-        r"""Return a callable for the stream generate content method over gRPC.
+        r"""Return a callable for the list tuned models method over gRPC.
 
-        Generates a streamed response from the model given an input
-        ``GenerateContentRequest``.
+        Lists tuned models owned by the user.
 
         Returns:
-            Callable[[~.GenerateContentRequest],
-                    ~.GenerateContentResponse]:
+            Callable[[~.ListTunedModelsRequest],
+                    ~.ListTunedModelsResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "stream_generate_content" not in self._stubs:
-            self._stubs["stream_generate_content"] = self.grpc_channel.unary_stream(
-                "/google.ai.generativelanguage.v1beta.GenerativeService/StreamGenerateContent",
-                request_serializer=generative_service.GenerateContentRequest.serialize,
-                response_deserializer=generative_service.GenerateContentResponse.deserialize,
+        if "list_tuned_models" not in self._stubs:
+            self._stubs["list_tuned_models"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.ModelService/ListTunedModels",
+                request_serializer=model_service.ListTunedModelsRequest.serialize,
+                response_deserializer=model_service.ListTunedModelsResponse.deserialize,
             )
-        return self._stubs["stream_generate_content"]
+        return self._stubs["list_tuned_models"]
 
     @property
-    def embed_content(
+    def create_tuned_model(
         self,
-    ) -> Callable[
-        [generative_service.EmbedContentRequest],
-        generative_service.EmbedContentResponse,
-    ]:
-        r"""Return a callable for the embed content method over gRPC.
+    ) -> Callable[[model_service.CreateTunedModelRequest], operations_pb2.Operation]:
+        r"""Return a callable for the create tuned model method over gRPC.
 
-        Generates an embedding from the model given an input
-        ``Content``.
+        Creates a tuned model. Intermediate tuning progress (if any) is
+        accessed through the [google.longrunning.Operations] service.
+
+        Status and results can be accessed through the Operations
+        service. Example: GET
+        /v1/tunedModels/az2mb0bpw6i/operations/000-111-222
 
         Returns:
-            Callable[[~.EmbedContentRequest],
-                    ~.EmbedContentResponse]:
+            Callable[[~.CreateTunedModelRequest],
+                    ~.Operation]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "embed_content" not in self._stubs:
-            self._stubs["embed_content"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.GenerativeService/EmbedContent",
-                request_serializer=generative_service.EmbedContentRequest.serialize,
-                response_deserializer=generative_service.EmbedContentResponse.deserialize,
+        if "create_tuned_model" not in self._stubs:
+            self._stubs["create_tuned_model"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.ModelService/CreateTunedModel",
+                request_serializer=model_service.CreateTunedModelRequest.serialize,
+                response_deserializer=operations_pb2.Operation.FromString,
             )
-        return self._stubs["embed_content"]
+        return self._stubs["create_tuned_model"]
 
     @property
-    def batch_embed_contents(
+    def update_tuned_model(
         self,
-    ) -> Callable[
-        [generative_service.BatchEmbedContentsRequest],
-        generative_service.BatchEmbedContentsResponse,
-    ]:
-        r"""Return a callable for the batch embed contents method over gRPC.
+    ) -> Callable[[model_service.UpdateTunedModelRequest], gag_tuned_model.TunedModel]:
+        r"""Return a callable for the update tuned model method over gRPC.
 
-        Generates multiple embeddings from the model given
-        input text in a synchronous call.
+        Updates a tuned model.
 
         Returns:
-            Callable[[~.BatchEmbedContentsRequest],
-                    ~.BatchEmbedContentsResponse]:
+            Callable[[~.UpdateTunedModelRequest],
+                    ~.TunedModel]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "batch_embed_contents" not in self._stubs:
-            self._stubs["batch_embed_contents"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.GenerativeService/BatchEmbedContents",
-                request_serializer=generative_service.BatchEmbedContentsRequest.serialize,
-                response_deserializer=generative_service.BatchEmbedContentsResponse.deserialize,
+        if "update_tuned_model" not in self._stubs:
+            self._stubs["update_tuned_model"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.ModelService/UpdateTunedModel",
+                request_serializer=model_service.UpdateTunedModelRequest.serialize,
+                response_deserializer=gag_tuned_model.TunedModel.deserialize,
             )
-        return self._stubs["batch_embed_contents"]
+        return self._stubs["update_tuned_model"]
 
     @property
-    def count_tokens(
+    def delete_tuned_model(
         self,
-    ) -> Callable[
-        [generative_service.CountTokensRequest], generative_service.CountTokensResponse
-    ]:
-        r"""Return a callable for the count tokens method over gRPC.
+    ) -> Callable[[model_service.DeleteTunedModelRequest], empty_pb2.Empty]:
+        r"""Return a callable for the delete tuned model method over gRPC.
 
-        Runs a model's tokenizer on input content and returns
-        the token count.
+        Deletes a tuned model.
 
         Returns:
-            Callable[[~.CountTokensRequest],
-                    ~.CountTokensResponse]:
+            Callable[[~.DeleteTunedModelRequest],
+                    ~.Empty]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "count_tokens" not in self._stubs:
-            self._stubs["count_tokens"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.GenerativeService/CountTokens",
-                request_serializer=generative_service.CountTokensRequest.serialize,
-                response_deserializer=generative_service.CountTokensResponse.deserialize,
+        if "delete_tuned_model" not in self._stubs:
+            self._stubs["delete_tuned_model"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.ModelService/DeleteTunedModel",
+                request_serializer=model_service.DeleteTunedModelRequest.serialize,
+                response_deserializer=empty_pb2.Empty.FromString,
             )
-        return self._stubs["count_tokens"]
+        return self._stubs["delete_tuned_model"]
 
     def close(self):
         self.grpc_channel.close()
 
     @property
     def kind(self) -> str:
         return "grpc"
 
 
-__all__ = ("GenerativeServiceGrpcTransport",)
+__all__ = ("ModelServiceGrpcTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc_asyncio.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
+from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
 from grpc.experimental import aio  # type: ignore
 
 from google.ai.generativelanguage_v1beta.types import generative_service
@@ -62,15 +64,14 @@
             credentials (Optional[~.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify this application to the service. If
                 none are specified, the client will attempt to ascertain
                 the credentials from the environment.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             kwargs (Optional[dict]): Keyword arguments, which are passed to the
                 channel creation.
@@ -92,15 +93,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -112,37 +113,40 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -160,15 +164,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
         else:
             if api_mtls_endpoint:
@@ -200,15 +204,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -241,14 +247,20 @@
         Awaitable[generative_service.GenerateContentResponse],
     ]:
         r"""Return a callable for the generate content method over gRPC.
 
         Generates a response from the model given an input
         ``GenerateContentRequest``.
 
+        Input capabilities differ between models, including tuned
+        models. See the `model
+        guide <https://ai.google.dev/models/gemini>`__ and `tuning
+        guide <https://ai.google.dev/docs/model_tuning_guidance>`__ for
+        details.
+
         Returns:
             Callable[[~.GenerateContentRequest],
                     Awaitable[~.GenerateContentResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
@@ -409,12 +421,101 @@
             self._stubs["count_tokens"] = self.grpc_channel.unary_unary(
                 "/google.ai.generativelanguage.v1beta.GenerativeService/CountTokens",
                 request_serializer=generative_service.CountTokensRequest.serialize,
                 response_deserializer=generative_service.CountTokensResponse.deserialize,
             )
         return self._stubs["count_tokens"]
 
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.generate_content: gapic_v1.method_async.wrap_method(
+                self.generate_content,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.generate_answer: gapic_v1.method_async.wrap_method(
+                self.generate_answer,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.stream_generate_content: gapic_v1.method_async.wrap_method(
+                self.stream_generate_content,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.embed_content: gapic_v1.method_async.wrap_method(
+                self.embed_content,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.batch_embed_contents: gapic_v1.method_async.wrap_method(
+                self.batch_embed_contents,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.count_tokens: gapic_v1.method_async.wrap_method(
+                self.count_tokens,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+        }
+
     def close(self):
         return self.grpc_channel.close()
 
 
 __all__ = ("GenerativeServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/async_client.py`

 * *Files 9% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -198,29 +199,33 @@
         type(ModelServiceClient).get_transport_class, type(ModelServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, ModelServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, ModelServiceTransport, Callable[..., ModelServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the model service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.ModelServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,ModelServiceTransport,Callable[..., ModelServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the ModelServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -321,46 +326,38 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Model:
                 Information about a Generative
                 Language Model.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.GetModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.GetModelRequest):
+            request = model_service.GetModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_model,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -456,48 +453,40 @@
                 Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.ListModelsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.ListModelsRequest):
+            request = model_service.ListModelsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if page_size is not None:
             request.page_size = page_size
         if page_token is not None:
             request.page_token = page_token
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_models,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_models
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -576,46 +565,38 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.TunedModel:
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.GetTunedModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.GetTunedModelRequest):
+            request = model_service.GetTunedModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_tuned_model,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_tuned_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -712,48 +693,40 @@
                 list of Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.ListTunedModelsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.ListTunedModelsRequest):
+            request = model_service.ListTunedModelsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if page_size is not None:
             request.page_size = page_size
         if page_token is not None:
             request.page_token = page_token
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_tuned_models,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_tuned_models
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -856,48 +829,40 @@
                 The result type for the operation will be
                 :class:`google.ai.generativelanguage_v1beta.types.TunedModel`
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([tuned_model, tuned_model_id])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.CreateTunedModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.CreateTunedModelRequest):
+            request = model_service.CreateTunedModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if tuned_model is not None:
             request.tuned_model = tuned_model
         if tuned_model_id is not None:
             request.tuned_model_id = tuned_model_id
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.create_tuned_model,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.create_tuned_model
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -983,48 +948,40 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.TunedModel:
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([tuned_model, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.UpdateTunedModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.UpdateTunedModelRequest):
+            request = model_service.UpdateTunedModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if tuned_model is not None:
             request.tuned_model = tuned_model
         if update_mask is not None:
             request.update_mask = update_mask
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.update_tuned_model,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.update_tuned_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata(
                 (("tuned_model.name", request.tuned_model.name),)
             ),
@@ -1091,46 +1048,38 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.DeleteTunedModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.DeleteTunedModelRequest):
+            request = model_service.DeleteTunedModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.delete_tuned_model,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.delete_tuned_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/client.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -36,31 +37,31 @@
 from google.api_core import retry as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.exceptions import MutualTLSChannelError  # type: ignore
 from google.auth.transport import mtls  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.oauth2 import service_account  # type: ignore
 
-from google.ai.generativelanguage_v1beta import gapic_version as package_version
+from google.ai.generativelanguage_v1beta3 import gapic_version as package_version
 
 try:
     OptionalRetry = Union[retries.Retry, gapic_v1.method._MethodDefault, None]
 except AttributeError:  # pragma: NO COVER
     OptionalRetry = Union[retries.Retry, object, None]  # type: ignore
 
 from google.api_core import operation  # type: ignore
 from google.api_core import operation_async  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import field_mask_pb2  # type: ignore
 from google.protobuf import timestamp_pb2  # type: ignore
 
-from google.ai.generativelanguage_v1beta.services.model_service import pagers
-from google.ai.generativelanguage_v1beta.types import tuned_model as gag_tuned_model
-from google.ai.generativelanguage_v1beta.types import model, model_service
-from google.ai.generativelanguage_v1beta.types import tuned_model
+from google.ai.generativelanguage_v1beta3.services.model_service import pagers
+from google.ai.generativelanguage_v1beta3.types import tuned_model as gag_tuned_model
+from google.ai.generativelanguage_v1beta3.types import model, model_service
+from google.ai.generativelanguage_v1beta3.types import tuned_model
 
 from .transports.base import DEFAULT_CLIENT_INFO, ModelServiceTransport
 from .transports.grpc import ModelServiceGrpcTransport
 from .transports.grpc_asyncio import ModelServiceGrpcAsyncIOTransport
 from .transports.rest import ModelServiceRestTransport
 
 
@@ -541,29 +542,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, ModelServiceTransport]] = None,
+        transport: Optional[
+            Union[str, ModelServiceTransport, Callable[..., ModelServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the model service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ModelServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,ModelServiceTransport,Callable[..., ModelServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the ModelServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -661,16 +666,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[ModelServiceTransport], Callable[..., ModelServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., ModelServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -694,33 +706,33 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             def sample_get_model():
                 # Create a client
-                client = generativelanguage_v1beta.ModelServiceClient()
+                client = generativelanguage_v1beta3.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.GetModelRequest(
+                request = generativelanguage_v1beta3.GetModelRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 response = client.get_model(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta.types.GetModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta3.types.GetModelRequest, dict]):
                 The request object. Request for getting information about
                 a specific Model.
             name (str):
                 Required. The resource name of the model.
 
                 This name should match a model name returned by the
                 ``ListModels`` method.
@@ -733,33 +745,31 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.Model:
+            google.ai.generativelanguage_v1beta3.types.Model:
                 Information about a Generative
                 Language Model.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.GetModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.GetModelRequest):
             request = model_service.GetModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -804,33 +814,33 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             def sample_list_models():
                 # Create a client
-                client = generativelanguage_v1beta.ModelServiceClient()
+                client = generativelanguage_v1beta3.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.ListModelsRequest(
+                request = generativelanguage_v1beta3.ListModelsRequest(
                 )
 
                 # Make the request
                 page_result = client.list_models(request=request)
 
                 # Handle the response
                 for response in page_result:
                     print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta.types.ListModelsRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta3.types.ListModelsRequest, dict]):
                 The request object. Request for listing all Models.
             page_size (int):
                 The maximum number of ``Models`` to return (per page).
 
                 The service may return fewer models. If unspecified, at
                 most 50 models will be returned per page. This method
                 returns at most 1000 models per page, even if you pass a
@@ -856,36 +866,34 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.services.model_service.pagers.ListModelsPager:
+            google.ai.generativelanguage_v1beta3.services.model_service.pagers.ListModelsPager:
                 Response from ListModel containing a paginated list of
                 Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.ListModelsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.ListModelsRequest):
             request = model_service.ListModelsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if page_size is not None:
                 request.page_size = page_size
             if page_token is not None:
@@ -934,33 +942,33 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             def sample_get_tuned_model():
                 # Create a client
-                client = generativelanguage_v1beta.ModelServiceClient()
+                client = generativelanguage_v1beta3.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.GetTunedModelRequest(
+                request = generativelanguage_v1beta3.GetTunedModelRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 response = client.get_tuned_model(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta.types.GetTunedModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta3.types.GetTunedModelRequest, dict]):
                 The request object. Request for getting information about
                 a specific Model.
             name (str):
                 Required. The resource name of the model.
 
                 Format: ``tunedModels/my-model-id``
 
@@ -970,33 +978,31 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.TunedModel:
+            google.ai.generativelanguage_v1beta3.types.TunedModel:
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.GetTunedModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.GetTunedModelRequest):
             request = model_service.GetTunedModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -1041,33 +1047,33 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             def sample_list_tuned_models():
                 # Create a client
-                client = generativelanguage_v1beta.ModelServiceClient()
+                client = generativelanguage_v1beta3.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.ListTunedModelsRequest(
+                request = generativelanguage_v1beta3.ListTunedModelsRequest(
                 )
 
                 # Make the request
                 page_result = client.list_tuned_models(request=request)
 
                 # Handle the response
                 for response in page_result:
                     print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta.types.ListTunedModelsRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta3.types.ListTunedModelsRequest, dict]):
                 The request object. Request for listing TunedModels.
             page_size (int):
                 Optional. The maximum number of ``TunedModels`` to
                 return (per page). The service may return fewer tuned
                 models.
 
                 If unspecified, at most 10 tuned models will be
@@ -1094,36 +1100,34 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.services.model_service.pagers.ListTunedModelsPager:
+            google.ai.generativelanguage_v1beta3.services.model_service.pagers.ListTunedModelsPager:
                 Response from ListTunedModels containing a paginated
                 list of Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.ListTunedModelsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.ListTunedModelsRequest):
             request = model_service.ListTunedModelsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if page_size is not None:
                 request.page_size = page_size
             if page_token is not None:
@@ -1178,43 +1182,43 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             def sample_create_tuned_model():
                 # Create a client
-                client = generativelanguage_v1beta.ModelServiceClient()
+                client = generativelanguage_v1beta3.ModelServiceClient()
 
                 # Initialize request argument(s)
-                tuned_model = generativelanguage_v1beta.TunedModel()
+                tuned_model = generativelanguage_v1beta3.TunedModel()
                 tuned_model.tuning_task.training_data.examples.examples.text_input = "text_input_value"
                 tuned_model.tuning_task.training_data.examples.examples.output = "output_value"
 
-                request = generativelanguage_v1beta.CreateTunedModelRequest(
+                request = generativelanguage_v1beta3.CreateTunedModelRequest(
                     tuned_model=tuned_model,
                 )
 
                 # Make the request
                 operation = client.create_tuned_model(request=request)
 
                 print("Waiting for operation to complete...")
 
                 response = operation.result()
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta.types.CreateTunedModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta3.types.CreateTunedModelRequest, dict]):
                 The request object. Request to create a TunedModel.
-            tuned_model (google.ai.generativelanguage_v1beta.types.TunedModel):
+            tuned_model (google.ai.generativelanguage_v1beta3.types.TunedModel):
                 Required. The tuned model to create.
                 This corresponds to the ``tuned_model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             tuned_model_id (str):
                 Optional. The unique id for the tuned model if
                 specified. This value should be up to 40 characters, the
@@ -1232,33 +1236,31 @@
                 sent along with the request as metadata.
 
         Returns:
             google.api_core.operation.Operation:
                 An object representing a long-running operation.
 
                 The result type for the operation will be
-                :class:`google.ai.generativelanguage_v1beta.types.TunedModel`
+                :class:`google.ai.generativelanguage_v1beta3.types.TunedModel`
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([tuned_model, tuned_model_id])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.CreateTunedModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.CreateTunedModelRequest):
             request = model_service.CreateTunedModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if tuned_model is not None:
                 request.tuned_model = tuned_model
             if tuned_model_id is not None:
@@ -1307,39 +1309,39 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             def sample_update_tuned_model():
                 # Create a client
-                client = generativelanguage_v1beta.ModelServiceClient()
+                client = generativelanguage_v1beta3.ModelServiceClient()
 
                 # Initialize request argument(s)
-                tuned_model = generativelanguage_v1beta.TunedModel()
+                tuned_model = generativelanguage_v1beta3.TunedModel()
                 tuned_model.tuning_task.training_data.examples.examples.text_input = "text_input_value"
                 tuned_model.tuning_task.training_data.examples.examples.output = "output_value"
 
-                request = generativelanguage_v1beta.UpdateTunedModelRequest(
+                request = generativelanguage_v1beta3.UpdateTunedModelRequest(
                     tuned_model=tuned_model,
                 )
 
                 # Make the request
                 response = client.update_tuned_model(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta.types.UpdateTunedModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta3.types.UpdateTunedModelRequest, dict]):
                 The request object. Request to update a TunedModel.
-            tuned_model (google.ai.generativelanguage_v1beta.types.TunedModel):
+            tuned_model (google.ai.generativelanguage_v1beta3.types.TunedModel):
                 Required. The tuned model to update.
                 This corresponds to the ``tuned_model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             update_mask (google.protobuf.field_mask_pb2.FieldMask):
                 Required. The list of fields to
                 update.
@@ -1350,33 +1352,31 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.TunedModel:
+            google.ai.generativelanguage_v1beta3.types.TunedModel:
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([tuned_model, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.UpdateTunedModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.UpdateTunedModelRequest):
             request = model_service.UpdateTunedModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if tuned_model is not None:
                 request.tuned_model = tuned_model
             if update_mask is not None:
@@ -1424,30 +1424,30 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             def sample_delete_tuned_model():
                 # Create a client
-                client = generativelanguage_v1beta.ModelServiceClient()
+                client = generativelanguage_v1beta3.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.DeleteTunedModelRequest(
+                request = generativelanguage_v1beta3.DeleteTunedModelRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 client.delete_tuned_model(request=request)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta.types.DeleteTunedModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta3.types.DeleteTunedModelRequest, dict]):
                 The request object. Request to delete a TunedModel.
             name (str):
                 Required. The resource name of the model. Format:
                 ``tunedModels/my-model-id``
 
                 This corresponds to the ``name`` field
                 on the ``request`` instance; if ``request`` is provided, this
@@ -1455,27 +1455,25 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.DeleteTunedModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.DeleteTunedModelRequest):
             request = model_service.DeleteTunedModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/pagers.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/pagers.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/grpc.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,17 +20,17 @@
 import google.auth  # type: ignore
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
 
-from google.ai.generativelanguage_v1beta.types import tuned_model as gag_tuned_model
-from google.ai.generativelanguage_v1beta.types import model, model_service
-from google.ai.generativelanguage_v1beta.types import tuned_model
+from google.ai.generativelanguage_v1beta3.types import tuned_model as gag_tuned_model
+from google.ai.generativelanguage_v1beta3.types import model, model_service
+from google.ai.generativelanguage_v1beta3.types import tuned_model
 
 from .base import DEFAULT_CLIENT_INFO, ModelServiceTransport
 
 
 class ModelServiceGrpcTransport(ModelServiceTransport):
     """gRPC backend transport for ModelService.
 
@@ -50,15 +50,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -70,36 +70,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -118,15 +121,15 @@
         self._operations_client: Optional[operations_v1.OperationsClient] = None
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -159,15 +162,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -262,15 +267,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "get_model" not in self._stubs:
             self._stubs["get_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/GetModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/GetModel",
                 request_serializer=model_service.GetModelRequest.serialize,
                 response_deserializer=model.Model.deserialize,
             )
         return self._stubs["get_model"]
 
     @property
     def list_models(
@@ -288,15 +293,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "list_models" not in self._stubs:
             self._stubs["list_models"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/ListModels",
+                "/google.ai.generativelanguage.v1beta3.ModelService/ListModels",
                 request_serializer=model_service.ListModelsRequest.serialize,
                 response_deserializer=model_service.ListModelsResponse.deserialize,
             )
         return self._stubs["list_models"]
 
     @property
     def get_tuned_model(
@@ -314,15 +319,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "get_tuned_model" not in self._stubs:
             self._stubs["get_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/GetTunedModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/GetTunedModel",
                 request_serializer=model_service.GetTunedModelRequest.serialize,
                 response_deserializer=tuned_model.TunedModel.deserialize,
             )
         return self._stubs["get_tuned_model"]
 
     @property
     def list_tuned_models(
@@ -342,15 +347,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "list_tuned_models" not in self._stubs:
             self._stubs["list_tuned_models"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/ListTunedModels",
+                "/google.ai.generativelanguage.v1beta3.ModelService/ListTunedModels",
                 request_serializer=model_service.ListTunedModelsRequest.serialize,
                 response_deserializer=model_service.ListTunedModelsResponse.deserialize,
             )
         return self._stubs["list_tuned_models"]
 
     @property
     def create_tuned_model(
@@ -373,15 +378,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "create_tuned_model" not in self._stubs:
             self._stubs["create_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/CreateTunedModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/CreateTunedModel",
                 request_serializer=model_service.CreateTunedModelRequest.serialize,
                 response_deserializer=operations_pb2.Operation.FromString,
             )
         return self._stubs["create_tuned_model"]
 
     @property
     def update_tuned_model(
@@ -399,15 +404,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "update_tuned_model" not in self._stubs:
             self._stubs["update_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/UpdateTunedModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/UpdateTunedModel",
                 request_serializer=model_service.UpdateTunedModelRequest.serialize,
                 response_deserializer=gag_tuned_model.TunedModel.deserialize,
             )
         return self._stubs["update_tuned_model"]
 
     @property
     def delete_tuned_model(
@@ -425,15 +430,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "delete_tuned_model" not in self._stubs:
             self._stubs["delete_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/DeleteTunedModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/DeleteTunedModel",
                 request_serializer=model_service.DeleteTunedModelRequest.serialize,
                 response_deserializer=empty_pb2.Empty.FromString,
             )
         return self._stubs["delete_tuned_model"]
 
     def close(self):
         self.grpc_channel.close()
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/grpc_asyncio.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,25 +12,27 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
+from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1, grpc_helpers_async, operations_v1
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
 from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta.types import tuned_model as gag_tuned_model
-from google.ai.generativelanguage_v1beta.types import model, model_service
-from google.ai.generativelanguage_v1beta.types import tuned_model
+from google.ai.generativelanguage_v1beta3.types import tuned_model as gag_tuned_model
+from google.ai.generativelanguage_v1beta3.types import model, model_service
+from google.ai.generativelanguage_v1beta3.types import tuned_model
 
 from .base import DEFAULT_CLIENT_INFO, ModelServiceTransport
 from .grpc import ModelServiceGrpcTransport
 
 
 class ModelServiceGrpcAsyncIOTransport(ModelServiceTransport):
     """gRPC AsyncIO backend transport for ModelService.
@@ -65,15 +67,14 @@
             credentials (Optional[~.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify this application to the service. If
                 none are specified, the client will attempt to ascertain
                 the credentials from the environment.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             kwargs (Optional[dict]): Keyword arguments, which are passed to the
                 channel creation.
@@ -95,15 +96,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -115,37 +116,40 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -164,15 +168,15 @@
         self._operations_client: Optional[operations_v1.OperationsAsyncClient] = None
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
         else:
             if api_mtls_endpoint:
@@ -204,15 +208,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -269,15 +275,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "get_model" not in self._stubs:
             self._stubs["get_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/GetModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/GetModel",
                 request_serializer=model_service.GetModelRequest.serialize,
                 response_deserializer=model.Model.deserialize,
             )
         return self._stubs["get_model"]
 
     @property
     def list_models(
@@ -297,15 +303,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "list_models" not in self._stubs:
             self._stubs["list_models"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/ListModels",
+                "/google.ai.generativelanguage.v1beta3.ModelService/ListModels",
                 request_serializer=model_service.ListModelsRequest.serialize,
                 response_deserializer=model_service.ListModelsResponse.deserialize,
             )
         return self._stubs["list_models"]
 
     @property
     def get_tuned_model(
@@ -325,15 +331,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "get_tuned_model" not in self._stubs:
             self._stubs["get_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/GetTunedModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/GetTunedModel",
                 request_serializer=model_service.GetTunedModelRequest.serialize,
                 response_deserializer=tuned_model.TunedModel.deserialize,
             )
         return self._stubs["get_tuned_model"]
 
     @property
     def list_tuned_models(
@@ -354,15 +360,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "list_tuned_models" not in self._stubs:
             self._stubs["list_tuned_models"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/ListTunedModels",
+                "/google.ai.generativelanguage.v1beta3.ModelService/ListTunedModels",
                 request_serializer=model_service.ListTunedModelsRequest.serialize,
                 response_deserializer=model_service.ListTunedModelsResponse.deserialize,
             )
         return self._stubs["list_tuned_models"]
 
     @property
     def create_tuned_model(
@@ -387,15 +393,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "create_tuned_model" not in self._stubs:
             self._stubs["create_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/CreateTunedModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/CreateTunedModel",
                 request_serializer=model_service.CreateTunedModelRequest.serialize,
                 response_deserializer=operations_pb2.Operation.FromString,
             )
         return self._stubs["create_tuned_model"]
 
     @property
     def update_tuned_model(
@@ -415,15 +421,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "update_tuned_model" not in self._stubs:
             self._stubs["update_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/UpdateTunedModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/UpdateTunedModel",
                 request_serializer=model_service.UpdateTunedModelRequest.serialize,
                 response_deserializer=gag_tuned_model.TunedModel.deserialize,
             )
         return self._stubs["update_tuned_model"]
 
     @property
     def delete_tuned_model(
@@ -441,18 +447,58 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "delete_tuned_model" not in self._stubs:
             self._stubs["delete_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.ModelService/DeleteTunedModel",
+                "/google.ai.generativelanguage.v1beta3.ModelService/DeleteTunedModel",
                 request_serializer=model_service.DeleteTunedModelRequest.serialize,
                 response_deserializer=empty_pb2.Empty.FromString,
             )
         return self._stubs["delete_tuned_model"]
 
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.get_model: gapic_v1.method_async.wrap_method(
+                self.get_model,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.list_models: gapic_v1.method_async.wrap_method(
+                self.list_models,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.get_tuned_model: gapic_v1.method_async.wrap_method(
+                self.get_tuned_model,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.list_tuned_models: gapic_v1.method_async.wrap_method(
+                self.list_tuned_models,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.create_tuned_model: gapic_v1.method_async.wrap_method(
+                self.create_tuned_model,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.update_tuned_model: gapic_v1.method_async.wrap_method(
+                self.update_tuned_model,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.delete_tuned_model: gapic_v1.method_async.wrap_method(
+                self.delete_tuned_model,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+        }
+
     def close(self):
         return self.grpc_channel.close()
 
 
 __all__ = ("ModelServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/async_client.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -31,28 +32,28 @@
 from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1
 from google.api_core import retry_async as retries
 from google.api_core.client_options import ClientOptions
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.oauth2 import service_account  # type: ignore
 
-from google.ai.generativelanguage_v1beta import gapic_version as package_version
+from google.ai.generativelanguage_v1beta3 import gapic_version as package_version
 
 try:
     OptionalRetry = Union[retries.AsyncRetry, gapic_v1.method._MethodDefault, None]
 except AttributeError:  # pragma: NO COVER
     OptionalRetry = Union[retries.AsyncRetry, object, None]  # type: ignore
 
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import field_mask_pb2  # type: ignore
 
-from google.ai.generativelanguage_v1beta.services.permission_service import pagers
-from google.ai.generativelanguage_v1beta.types import permission as gag_permission
-from google.ai.generativelanguage_v1beta.types import permission
-from google.ai.generativelanguage_v1beta.types import permission_service
+from google.ai.generativelanguage_v1beta3.services.permission_service import pagers
+from google.ai.generativelanguage_v1beta3.types import permission as gag_permission
+from google.ai.generativelanguage_v1beta3.types import permission
+from google.ai.generativelanguage_v1beta3.types import permission_service
 
 from .client import PermissionServiceClient
 from .transports.base import DEFAULT_CLIENT_INFO, PermissionServiceTransport
 from .transports.grpc_asyncio import PermissionServiceGrpcAsyncIOTransport
 
 
 class PermissionServiceAsyncClient:
@@ -67,14 +68,18 @@
     DEFAULT_ENDPOINT = PermissionServiceClient.DEFAULT_ENDPOINT
     DEFAULT_MTLS_ENDPOINT = PermissionServiceClient.DEFAULT_MTLS_ENDPOINT
     _DEFAULT_ENDPOINT_TEMPLATE = PermissionServiceClient._DEFAULT_ENDPOINT_TEMPLATE
     _DEFAULT_UNIVERSE = PermissionServiceClient._DEFAULT_UNIVERSE
 
     permission_path = staticmethod(PermissionServiceClient.permission_path)
     parse_permission_path = staticmethod(PermissionServiceClient.parse_permission_path)
+    tuned_model_path = staticmethod(PermissionServiceClient.tuned_model_path)
+    parse_tuned_model_path = staticmethod(
+        PermissionServiceClient.parse_tuned_model_path
+    )
     common_billing_account_path = staticmethod(
         PermissionServiceClient.common_billing_account_path
     )
     parse_common_billing_account_path = staticmethod(
         PermissionServiceClient.parse_common_billing_account_path
     )
     common_folder_path = staticmethod(PermissionServiceClient.common_folder_path)
@@ -197,29 +202,37 @@
         type(PermissionServiceClient).get_transport_class, type(PermissionServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, PermissionServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[
+                str,
+                PermissionServiceTransport,
+                Callable[..., PermissionServiceTransport],
+            ]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the permission service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.PermissionServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,PermissionServiceTransport,Callable[..., PermissionServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the PermissionServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -278,115 +291,106 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_create_permission():
                 # Create a client
-                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.CreatePermissionRequest(
+                request = generativelanguage_v1beta3.CreatePermissionRequest(
                     parent="parent_value",
                 )
 
                 # Make the request
                 response = await client.create_permission(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.CreatePermissionRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.CreatePermissionRequest, dict]]):
                 The request object. Request to create a ``Permission``.
             parent (:class:`str`):
                 Required. The parent resource of the ``Permission``.
-                Formats: ``tunedModels/{tuned_model}``
-                ``corpora/{corpus}``
+                Format: tunedModels/{tuned_model}
 
                 This corresponds to the ``parent`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
-            permission (:class:`google.ai.generativelanguage_v1beta.types.Permission`):
+            permission (:class:`google.ai.generativelanguage_v1beta3.types.Permission`):
                 Required. The permission to create.
                 This corresponds to the ``permission`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.Permission:
+            google.ai.generativelanguage_v1beta3.types.Permission:
                 Permission resource grants user,
                 group or the rest of the world access to
                 the PaLM API resource (e.g. a tuned
-                model, corpus).
+                model, file).
 
                 A role is a collection of permitted
                 operations that allows users to perform
                 specific actions on PaLM API resources.
                 To make them available to users, groups,
                 or service accounts, you assign roles.
                 When you assign a role, you grant
                 permissions that the role contains.
 
                 There are three concentric roles. Each
                 role is a superset of the previous
                 role's permitted operations:
 
                 - reader can use the resource (e.g.
-                  tuned model, corpus) for inference
+                  tuned model) for inference
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent, permission])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.CreatePermissionRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.CreatePermissionRequest):
+            request = permission_service.CreatePermissionRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if parent is not None:
             request.parent = parent
         if permission is not None:
             request.permission = permission
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.create_permission,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.create_permission
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -420,111 +424,102 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_get_permission():
                 # Create a client
-                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.GetPermissionRequest(
+                request = generativelanguage_v1beta3.GetPermissionRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 response = await client.get_permission(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.GetPermissionRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.GetPermissionRequest, dict]]):
                 The request object. Request for getting information about a specific
                 ``Permission``.
             name (:class:`str`):
                 Required. The resource name of the permission.
 
-                Formats:
-                ``tunedModels/{tuned_model}/permissions/{permission}``
-                ``corpora/{corpus}/permissions/{permission}``
+                Format:
+                ``tunedModels/{tuned_model}permissions/{permission}``
 
                 This corresponds to the ``name`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.Permission:
+            google.ai.generativelanguage_v1beta3.types.Permission:
                 Permission resource grants user,
                 group or the rest of the world access to
                 the PaLM API resource (e.g. a tuned
-                model, corpus).
+                model, file).
 
                 A role is a collection of permitted
                 operations that allows users to perform
                 specific actions on PaLM API resources.
                 To make them available to users, groups,
                 or service accounts, you assign roles.
                 When you assign a role, you grant
                 permissions that the role contains.
 
                 There are three concentric roles. Each
                 role is a superset of the previous
                 role's permitted operations:
 
                 - reader can use the resource (e.g.
-                  tuned model, corpus) for inference
+                  tuned model) for inference
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.GetPermissionRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.GetPermissionRequest):
+            request = permission_service.GetPermissionRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_permission,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_permission
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -560,82 +555,82 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_list_permissions():
                 # Create a client
-                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.ListPermissionsRequest(
+                request = generativelanguage_v1beta3.ListPermissionsRequest(
                     parent="parent_value",
                 )
 
                 # Make the request
                 page_result = client.list_permissions(request=request)
 
                 # Handle the response
                 async for response in page_result:
                     print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.ListPermissionsRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.ListPermissionsRequest, dict]]):
                 The request object. Request for listing permissions.
             parent (:class:`str`):
                 Required. The parent resource of the permissions.
-                Formats: ``tunedModels/{tuned_model}``
-                ``corpora/{corpus}``
+                Format: tunedModels/{tuned_model}
 
                 This corresponds to the ``parent`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.services.permission_service.pagers.ListPermissionsAsyncPager:
+            google.ai.generativelanguage_v1beta3.services.permission_service.pagers.ListPermissionsAsyncPager:
                 Response from ListPermissions containing a paginated list of
                    permissions.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.ListPermissionsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.ListPermissionsRequest):
+            request = permission_service.ListPermissionsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if parent is not None:
             request.parent = parent
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_permissions,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_permissions
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -681,34 +676,34 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_update_permission():
                 # Create a client
-                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.UpdatePermissionRequest(
+                request = generativelanguage_v1beta3.UpdatePermissionRequest(
                 )
 
                 # Make the request
                 response = await client.update_permission(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.UpdatePermissionRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.UpdatePermissionRequest, dict]]):
                 The request object. Request to update the ``Permission``.
-            permission (:class:`google.ai.generativelanguage_v1beta.types.Permission`):
+            permission (:class:`google.ai.generativelanguage_v1beta3.types.Permission`):
                 Required. The permission to update.
 
                 The permission's ``name`` field is used to identify the
                 permission to update.
 
                 This corresponds to the ``permission`` field
                 on the ``request`` instance; if ``request`` is provided, this
@@ -724,75 +719,67 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.Permission:
+            google.ai.generativelanguage_v1beta3.types.Permission:
                 Permission resource grants user,
                 group or the rest of the world access to
                 the PaLM API resource (e.g. a tuned
-                model, corpus).
+                model, file).
 
                 A role is a collection of permitted
                 operations that allows users to perform
                 specific actions on PaLM API resources.
                 To make them available to users, groups,
                 or service accounts, you assign roles.
                 When you assign a role, you grant
                 permissions that the role contains.
 
                 There are three concentric roles. Each
                 role is a superset of the previous
                 role's permitted operations:
 
                 - reader can use the resource (e.g.
-                  tuned model, corpus) for inference
+                  tuned model) for inference
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([permission, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.UpdatePermissionRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.UpdatePermissionRequest):
+            request = permission_service.UpdatePermissionRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if permission is not None:
             request.permission = permission
         if update_mask is not None:
             request.update_mask = update_mask
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.update_permission,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.update_permission
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata(
                 (("permission.name", request.permission.name),)
             ),
@@ -830,78 +817,69 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_delete_permission():
                 # Create a client
-                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.DeletePermissionRequest(
+                request = generativelanguage_v1beta3.DeletePermissionRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 await client.delete_permission(request=request)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.DeletePermissionRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.DeletePermissionRequest, dict]]):
                 The request object. Request to delete the ``Permission``.
             name (:class:`str`):
-                Required. The resource name of the permission. Formats:
+                Required. The resource name of the permission. Format:
                 ``tunedModels/{tuned_model}/permissions/{permission}``
-                ``corpora/{corpus}/permissions/{permission}``
 
                 This corresponds to the ``name`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.DeletePermissionRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.DeletePermissionRequest):
+            request = permission_service.DeletePermissionRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.delete_permission,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.delete_permission
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -936,65 +914,57 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_transfer_ownership():
                 # Create a client
-                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.TransferOwnershipRequest(
+                request = generativelanguage_v1beta3.TransferOwnershipRequest(
                     name="name_value",
                     email_address="email_address_value",
                 )
 
                 # Make the request
                 response = await client.transfer_ownership(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.TransferOwnershipRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.TransferOwnershipRequest, dict]]):
                 The request object. Request to transfer the ownership of
                 the tuned model.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.TransferOwnershipResponse:
+            google.ai.generativelanguage_v1beta3.types.TransferOwnershipResponse:
                 Response from TransferOwnership.
         """
         # Create or coerce a protobuf request object.
-        request = permission_service.TransferOwnershipRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.TransferOwnershipRequest):
+            request = permission_service.TransferOwnershipRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.transfer_ownership,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.transfer_ownership
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/client.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -529,29 +530,37 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, PermissionServiceTransport]] = None,
+        transport: Optional[
+            Union[
+                str,
+                PermissionServiceTransport,
+                Callable[..., PermissionServiceTransport],
+            ]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the permission service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, PermissionServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,PermissionServiceTransport,Callable[..., PermissionServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the PermissionServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -652,16 +661,24 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[PermissionServiceTransport],
+                Callable[..., PermissionServiceTransport],
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., PermissionServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -755,27 +772,25 @@
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent, permission])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.CreatePermissionRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.CreatePermissionRequest):
             request = permission_service.CreatePermissionRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if parent is not None:
                 request.parent = parent
             if permission is not None:
@@ -886,27 +901,25 @@
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.GetPermissionRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.GetPermissionRequest):
             request = permission_service.GetPermissionRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -997,27 +1010,25 @@
                    permissions.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.ListPermissionsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.ListPermissionsRequest):
             request = permission_service.ListPermissionsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if parent is not None:
                 request.parent = parent
 
@@ -1143,27 +1154,25 @@
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([permission, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.UpdatePermissionRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.UpdatePermissionRequest):
             request = permission_service.UpdatePermissionRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if permission is not None:
                 request.permission = permission
             if update_mask is not None:
@@ -1245,27 +1254,25 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.DeletePermissionRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.DeletePermissionRequest):
             request = permission_service.DeletePermissionRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -1343,18 +1350,16 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.TransferOwnershipResponse:
                 Response from TransferOwnership.
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.TransferOwnershipRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.TransferOwnershipRequest):
             request = permission_service.TransferOwnershipRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.transfer_ownership]
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/pagers.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/pagers.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/grpc.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,15 +50,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -70,36 +70,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -117,15 +120,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -158,15 +161,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/grpc_asyncio.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,25 +12,27 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
+from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
 from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta.types import permission as gag_permission
-from google.ai.generativelanguage_v1beta.types import permission
-from google.ai.generativelanguage_v1beta.types import permission_service
+from google.ai.generativelanguage_v1beta3.types import permission as gag_permission
+from google.ai.generativelanguage_v1beta3.types import permission
+from google.ai.generativelanguage_v1beta3.types import permission_service
 
 from .base import DEFAULT_CLIENT_INFO, PermissionServiceTransport
 from .grpc import PermissionServiceGrpcTransport
 
 
 class PermissionServiceGrpcAsyncIOTransport(PermissionServiceTransport):
     """gRPC AsyncIO backend transport for PermissionService.
@@ -65,15 +67,14 @@
             credentials (Optional[~.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify this application to the service. If
                 none are specified, the client will attempt to ascertain
                 the credentials from the environment.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             kwargs (Optional[dict]): Keyword arguments, which are passed to the
                 channel creation.
@@ -95,15 +96,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -115,37 +116,40 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -163,15 +167,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
         else:
             if api_mtls_endpoint:
@@ -203,15 +207,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -255,15 +261,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "create_permission" not in self._stubs:
             self._stubs["create_permission"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.PermissionService/CreatePermission",
+                "/google.ai.generativelanguage.v1beta3.PermissionService/CreatePermission",
                 request_serializer=permission_service.CreatePermissionRequest.serialize,
                 response_deserializer=gag_permission.Permission.deserialize,
             )
         return self._stubs["create_permission"]
 
     @property
     def get_permission(
@@ -283,15 +289,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "get_permission" not in self._stubs:
             self._stubs["get_permission"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.PermissionService/GetPermission",
+                "/google.ai.generativelanguage.v1beta3.PermissionService/GetPermission",
                 request_serializer=permission_service.GetPermissionRequest.serialize,
                 response_deserializer=permission.Permission.deserialize,
             )
         return self._stubs["get_permission"]
 
     @property
     def list_permissions(
@@ -312,15 +318,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "list_permissions" not in self._stubs:
             self._stubs["list_permissions"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.PermissionService/ListPermissions",
+                "/google.ai.generativelanguage.v1beta3.PermissionService/ListPermissions",
                 request_serializer=permission_service.ListPermissionsRequest.serialize,
                 response_deserializer=permission_service.ListPermissionsResponse.deserialize,
             )
         return self._stubs["list_permissions"]
 
     @property
     def update_permission(
@@ -341,15 +347,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "update_permission" not in self._stubs:
             self._stubs["update_permission"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.PermissionService/UpdatePermission",
+                "/google.ai.generativelanguage.v1beta3.PermissionService/UpdatePermission",
                 request_serializer=permission_service.UpdatePermissionRequest.serialize,
                 response_deserializer=gag_permission.Permission.deserialize,
             )
         return self._stubs["update_permission"]
 
     @property
     def delete_permission(
@@ -369,15 +375,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "delete_permission" not in self._stubs:
             self._stubs["delete_permission"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.PermissionService/DeletePermission",
+                "/google.ai.generativelanguage.v1beta3.PermissionService/DeletePermission",
                 request_serializer=permission_service.DeletePermissionRequest.serialize,
                 response_deserializer=empty_pb2.Empty.FromString,
             )
         return self._stubs["delete_permission"]
 
     @property
     def transfer_ownership(
@@ -401,18 +407,53 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "transfer_ownership" not in self._stubs:
             self._stubs["transfer_ownership"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.PermissionService/TransferOwnership",
+                "/google.ai.generativelanguage.v1beta3.PermissionService/TransferOwnership",
                 request_serializer=permission_service.TransferOwnershipRequest.serialize,
                 response_deserializer=permission_service.TransferOwnershipResponse.deserialize,
             )
         return self._stubs["transfer_ownership"]
 
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.create_permission: gapic_v1.method_async.wrap_method(
+                self.create_permission,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.get_permission: gapic_v1.method_async.wrap_method(
+                self.get_permission,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.list_permissions: gapic_v1.method_async.wrap_method(
+                self.list_permissions,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.update_permission: gapic_v1.method_async.wrap_method(
+                self.update_permission,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.delete_permission: gapic_v1.method_async.wrap_method(
+                self.delete_permission,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.transfer_ownership: gapic_v1.method_async.wrap_method(
+                self.transfer_ownership,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+        }
+
     def close(self):
         return self.grpc_channel.close()
 
 
 __all__ = ("PermissionServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/permission_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/async_client.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -200,29 +201,35 @@
         type(RetrieverServiceClient).get_transport_class, type(RetrieverServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, RetrieverServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[
+                str, RetrieverServiceTransport, Callable[..., RetrieverServiceTransport]
+            ]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the retriever service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.RetrieverServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,RetrieverServiceTransport,Callable[..., RetrieverServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the RetrieverServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -315,46 +322,38 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Corpus:
                 A Corpus is a collection of Documents.
                    A project can create up to 5 corpora.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([corpus])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.CreateCorpusRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.CreateCorpusRequest):
+            request = retriever_service.CreateCorpusRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if corpus is not None:
             request.corpus = corpus
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.create_corpus,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.create_corpus
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -423,46 +422,38 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Corpus:
                 A Corpus is a collection of Documents.
                    A project can create up to 5 corpora.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.GetCorpusRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.GetCorpusRequest):
+            request = retriever_service.GetCorpusRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_corpus,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_corpus
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -541,48 +532,40 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Corpus:
                 A Corpus is a collection of Documents.
                    A project can create up to 5 corpora.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([corpus, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.UpdateCorpusRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.UpdateCorpusRequest):
+            request = retriever_service.UpdateCorpusRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if corpus is not None:
             request.corpus = corpus
         if update_mask is not None:
             request.update_mask = update_mask
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.update_corpus,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.update_corpus
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata(
                 (("corpus.name", request.corpus.name),)
             ),
@@ -649,46 +632,38 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.DeleteCorpusRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.DeleteCorpusRequest):
+            request = retriever_service.DeleteCorpusRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.delete_corpus,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.delete_corpus
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -755,32 +730,24 @@
                    corpus.create_time.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        request = retriever_service.ListCorporaRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.ListCorporaRequest):
+            request = retriever_service.ListCorporaRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_corpora,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_corpora
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -850,32 +817,24 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.QueryCorpusResponse:
                 Response from QueryCorpus containing a list of relevant
                 chunks.
 
         """
         # Create or coerce a protobuf request object.
-        request = retriever_service.QueryCorpusRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.QueryCorpusRequest):
+            request = retriever_service.QueryCorpusRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.query_corpus,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.query_corpus
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -956,48 +915,40 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Document:
                 A Document is a collection of Chunks.
                    A Corpus can have a maximum of 10,000 Documents.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent, document])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.CreateDocumentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.CreateDocumentRequest):
+            request = retriever_service.CreateDocumentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if parent is not None:
             request.parent = parent
         if document is not None:
             request.document = document
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.create_document,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.create_document
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -1072,46 +1023,38 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Document:
                 A Document is a collection of Chunks.
                    A Corpus can have a maximum of 10,000 Documents.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.GetDocumentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.GetDocumentRequest):
+            request = retriever_service.GetDocumentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_document,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_document
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -1191,48 +1134,40 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Document:
                 A Document is a collection of Chunks.
                    A Corpus can have a maximum of 10,000 Documents.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([document, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.UpdateDocumentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.UpdateDocumentRequest):
+            request = retriever_service.UpdateDocumentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if document is not None:
             request.document = document
         if update_mask is not None:
             request.update_mask = update_mask
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.update_document,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.update_document
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata(
                 (("document.name", request.document.name),)
             ),
@@ -1300,46 +1235,38 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.DeleteDocumentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.DeleteDocumentRequest):
+            request = retriever_service.DeleteDocumentRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.delete_document,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.delete_document
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -1415,46 +1342,38 @@
                    document.create_time.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.ListDocumentsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.ListDocumentsRequest):
+            request = retriever_service.ListDocumentsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if parent is not None:
             request.parent = parent
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_documents,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_documents
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -1530,32 +1449,24 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.QueryDocumentResponse:
                 Response from QueryDocument containing a list of
                 relevant chunks.
 
         """
         # Create or coerce a protobuf request object.
-        request = retriever_service.QueryDocumentRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.QueryDocumentRequest):
+            request = retriever_service.QueryDocumentRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.query_document,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.query_document
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -1642,48 +1553,40 @@
                 A Chunk is a subpart of a Document that is treated as an independent unit
                    for the purposes of vector representation and
                    storage. A Corpus can have a maximum of 1 million
                    Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent, chunk])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.CreateChunkRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.CreateChunkRequest):
+            request = retriever_service.CreateChunkRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if parent is not None:
             request.parent = parent
         if chunk is not None:
             request.chunk = chunk
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.create_chunk,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.create_chunk
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -1755,23 +1658,24 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.BatchCreateChunksResponse:
                 Response from BatchCreateChunks containing a list of
                 created Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        request = retriever_service.BatchCreateChunksRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.BatchCreateChunksRequest):
+            request = retriever_service.BatchCreateChunksRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.batch_create_chunks,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.batch_create_chunks
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -1849,46 +1753,38 @@
                 A Chunk is a subpart of a Document that is treated as an independent unit
                    for the purposes of vector representation and
                    storage. A Corpus can have a maximum of 1 million
                    Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.GetChunkRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.GetChunkRequest):
+            request = retriever_service.GetChunkRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_chunk,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_chunk
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -1973,48 +1869,40 @@
                 A Chunk is a subpart of a Document that is treated as an independent unit
                    for the purposes of vector representation and
                    storage. A Corpus can have a maximum of 1 million
                    Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([chunk, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.UpdateChunkRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.UpdateChunkRequest):
+            request = retriever_service.UpdateChunkRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if chunk is not None:
             request.chunk = chunk
         if update_mask is not None:
             request.update_mask = update_mask
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.update_chunk,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.update_chunk
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata(
                 (("chunk.name", request.chunk.name),)
             ),
@@ -2087,23 +1975,24 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.BatchUpdateChunksResponse:
                 Response from BatchUpdateChunks containing a list of
                 updated Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        request = retriever_service.BatchUpdateChunksRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.BatchUpdateChunksRequest):
+            request = retriever_service.BatchUpdateChunksRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.batch_update_chunks,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.batch_update_chunks
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -2169,46 +2058,38 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.DeleteChunkRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.DeleteChunkRequest):
+            request = retriever_service.DeleteChunkRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.delete_chunk,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.delete_chunk
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -2267,23 +2148,24 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        request = retriever_service.BatchDeleteChunksRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.BatchDeleteChunksRequest):
+            request = retriever_service.BatchDeleteChunksRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.batch_delete_chunks,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.batch_delete_chunks
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -2359,37 +2241,38 @@
                    The Chunks are sorted by ascending chunk.create_time.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = retriever_service.ListChunksRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, retriever_service.ListChunksRequest):
+            request = retriever_service.ListChunksRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if parent is not None:
             request.parent = parent
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_chunks,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_chunks
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/client.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -563,29 +564,35 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, RetrieverServiceTransport]] = None,
+        transport: Optional[
+            Union[
+                str, RetrieverServiceTransport, Callable[..., RetrieverServiceTransport]
+            ]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the retriever service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, RetrieverServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,RetrieverServiceTransport,Callable[..., RetrieverServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the RetrieverServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -686,16 +693,24 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[RetrieverServiceTransport],
+                Callable[..., RetrieverServiceTransport],
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., RetrieverServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -756,27 +771,25 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Corpus:
                 A Corpus is a collection of Documents.
                    A project can create up to 5 corpora.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([corpus])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.CreateCorpusRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.CreateCorpusRequest):
             request = retriever_service.CreateCorpusRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if corpus is not None:
                 request.corpus = corpus
 
@@ -855,27 +868,25 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Corpus:
                 A Corpus is a collection of Documents.
                    A project can create up to 5 corpora.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.GetCorpusRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.GetCorpusRequest):
             request = retriever_service.GetCorpusRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -964,27 +975,25 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Corpus:
                 A Corpus is a collection of Documents.
                    A project can create up to 5 corpora.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([corpus, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.UpdateCorpusRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.UpdateCorpusRequest):
             request = retriever_service.UpdateCorpusRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if corpus is not None:
                 request.corpus = corpus
             if update_mask is not None:
@@ -1063,27 +1072,25 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.DeleteCorpusRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.DeleteCorpusRequest):
             request = retriever_service.DeleteCorpusRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -1160,18 +1167,16 @@
                    corpus.create_time.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.ListCorporaRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.ListCorporaRequest):
             request = retriever_service.ListCorporaRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.list_corpora]
 
@@ -1247,18 +1252,16 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.QueryCorpusResponse:
                 Response from QueryCorpus containing a list of relevant
                 chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.QueryCorpusRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.QueryCorpusRequest):
             request = retriever_service.QueryCorpusRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.query_corpus]
 
@@ -1345,27 +1348,25 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Document:
                 A Document is a collection of Chunks.
                    A Corpus can have a maximum of 10,000 Documents.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent, document])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.CreateDocumentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.CreateDocumentRequest):
             request = retriever_service.CreateDocumentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if parent is not None:
                 request.parent = parent
             if document is not None:
@@ -1452,27 +1453,25 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Document:
                 A Document is a collection of Chunks.
                    A Corpus can have a maximum of 10,000 Documents.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.GetDocumentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.GetDocumentRequest):
             request = retriever_service.GetDocumentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -1562,27 +1561,25 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.Document:
                 A Document is a collection of Chunks.
                    A Corpus can have a maximum of 10,000 Documents.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([document, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.UpdateDocumentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.UpdateDocumentRequest):
             request = retriever_service.UpdateDocumentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if document is not None:
                 request.document = document
             if update_mask is not None:
@@ -1662,27 +1659,25 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.DeleteDocumentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.DeleteDocumentRequest):
             request = retriever_service.DeleteDocumentRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -1768,27 +1763,25 @@
                    document.create_time.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.ListDocumentsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.ListDocumentsRequest):
             request = retriever_service.ListDocumentsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if parent is not None:
                 request.parent = parent
 
@@ -1874,18 +1867,16 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.QueryDocumentResponse:
                 Response from QueryDocument containing a list of
                 relevant chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.QueryDocumentRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.QueryDocumentRequest):
             request = retriever_service.QueryDocumentRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.query_document]
 
@@ -1978,27 +1969,25 @@
                 A Chunk is a subpart of a Document that is treated as an independent unit
                    for the purposes of vector representation and
                    storage. A Corpus can have a maximum of 1 million
                    Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent, chunk])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.CreateChunkRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.CreateChunkRequest):
             request = retriever_service.CreateChunkRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if parent is not None:
                 request.parent = parent
             if chunk is not None:
@@ -2082,18 +2071,16 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.BatchCreateChunksResponse:
                 Response from BatchCreateChunks containing a list of
                 created Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.BatchCreateChunksRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.BatchCreateChunksRequest):
             request = retriever_service.BatchCreateChunksRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.batch_create_chunks]
 
@@ -2177,27 +2164,25 @@
                 A Chunk is a subpart of a Document that is treated as an independent unit
                    for the purposes of vector representation and
                    storage. A Corpus can have a maximum of 1 million
                    Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.GetChunkRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.GetChunkRequest):
             request = retriever_service.GetChunkRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -2292,27 +2277,25 @@
                 A Chunk is a subpart of a Document that is treated as an independent unit
                    for the purposes of vector representation and
                    storage. A Corpus can have a maximum of 1 million
                    Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([chunk, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.UpdateChunkRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.UpdateChunkRequest):
             request = retriever_service.UpdateChunkRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if chunk is not None:
                 request.chunk = chunk
             if update_mask is not None:
@@ -2397,18 +2380,16 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.BatchUpdateChunksResponse:
                 Response from BatchUpdateChunks containing a list of
                 updated Chunks.
 
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.BatchUpdateChunksRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.BatchUpdateChunksRequest):
             request = retriever_service.BatchUpdateChunksRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.batch_update_chunks]
 
@@ -2480,27 +2461,25 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.DeleteChunkRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.DeleteChunkRequest):
             request = retriever_service.DeleteChunkRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -2569,18 +2548,16 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.BatchDeleteChunksRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.BatchDeleteChunksRequest):
             request = retriever_service.BatchDeleteChunksRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.batch_delete_chunks]
 
@@ -2662,27 +2639,25 @@
                    The Chunks are sorted by ascending chunk.create_time.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a retriever_service.ListChunksRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, retriever_service.ListChunksRequest):
             request = retriever_service.ListChunksRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if parent is not None:
                 request.parent = parent
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/pagers.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/pagers.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/grpc.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,15 +48,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -68,36 +68,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -115,15 +118,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -156,15 +159,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/grpc_asyncio.py`

 * *Files 17% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
+from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
 from grpc.experimental import aio  # type: ignore
 
@@ -63,15 +65,14 @@
             credentials (Optional[~.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify this application to the service. If
                 none are specified, the client will attempt to ascertain
                 the credentials from the environment.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             kwargs (Optional[dict]): Keyword arguments, which are passed to the
                 channel creation.
@@ -93,15 +94,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -113,37 +114,40 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -161,15 +165,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
         else:
             if api_mtls_endpoint:
@@ -201,15 +205,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -781,12 +787,261 @@
             self._stubs["list_chunks"] = self.grpc_channel.unary_unary(
                 "/google.ai.generativelanguage.v1beta.RetrieverService/ListChunks",
                 request_serializer=retriever_service.ListChunksRequest.serialize,
                 response_deserializer=retriever_service.ListChunksResponse.deserialize,
             )
         return self._stubs["list_chunks"]
 
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.create_corpus: gapic_v1.method_async.wrap_method(
+                self.create_corpus,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.get_corpus: gapic_v1.method_async.wrap_method(
+                self.get_corpus,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.update_corpus: gapic_v1.method_async.wrap_method(
+                self.update_corpus,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.delete_corpus: gapic_v1.method_async.wrap_method(
+                self.delete_corpus,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.list_corpora: gapic_v1.method_async.wrap_method(
+                self.list_corpora,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.query_corpus: gapic_v1.method_async.wrap_method(
+                self.query_corpus,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.create_document: gapic_v1.method_async.wrap_method(
+                self.create_document,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.get_document: gapic_v1.method_async.wrap_method(
+                self.get_document,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.update_document: gapic_v1.method_async.wrap_method(
+                self.update_document,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.delete_document: gapic_v1.method_async.wrap_method(
+                self.delete_document,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.list_documents: gapic_v1.method_async.wrap_method(
+                self.list_documents,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.query_document: gapic_v1.method_async.wrap_method(
+                self.query_document,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.create_chunk: gapic_v1.method_async.wrap_method(
+                self.create_chunk,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.batch_create_chunks: gapic_v1.method_async.wrap_method(
+                self.batch_create_chunks,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.get_chunk: gapic_v1.method_async.wrap_method(
+                self.get_chunk,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.update_chunk: gapic_v1.method_async.wrap_method(
+                self.update_chunk,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.batch_update_chunks: gapic_v1.method_async.wrap_method(
+                self.batch_update_chunks,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.delete_chunk: gapic_v1.method_async.wrap_method(
+                self.delete_chunk,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.batch_delete_chunks: gapic_v1.method_async.wrap_method(
+                self.batch_delete_chunks,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.list_chunks: gapic_v1.method_async.wrap_method(
+                self.list_chunks,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+        }
+
     def close(self):
         return self.grpc_channel.close()
 
 
 __all__ = ("RetrieverServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/retriever_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/retriever_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/async_client.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -31,24 +32,24 @@
 from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1
 from google.api_core import retry_async as retries
 from google.api_core.client_options import ClientOptions
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.oauth2 import service_account  # type: ignore
 
-from google.ai.generativelanguage_v1beta import gapic_version as package_version
+from google.ai.generativelanguage_v1beta3 import gapic_version as package_version
 
 try:
     OptionalRetry = Union[retries.AsyncRetry, gapic_v1.method._MethodDefault, None]
 except AttributeError:  # pragma: NO COVER
     OptionalRetry = Union[retries.AsyncRetry, object, None]  # type: ignore
 
 from google.longrunning import operations_pb2  # type: ignore
 
-from google.ai.generativelanguage_v1beta.types import safety, text_service
+from google.ai.generativelanguage_v1beta3.types import safety, text_service
 
 from .client import TextServiceClient
 from .transports.base import DEFAULT_CLIENT_INFO, TextServiceTransport
 from .transports.grpc_asyncio import TextServiceGrpcAsyncIOTransport
 
 
 class TextServiceAsyncClient:
@@ -191,29 +192,33 @@
         type(TextServiceClient).get_transport_class, type(TextServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, TextServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, TextServiceTransport, Callable[..., TextServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the text service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.TextServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,TextServiceTransport,Callable[..., TextServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the TextServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -276,49 +281,49 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_generate_text():
                 # Create a client
-                client = generativelanguage_v1beta.TextServiceAsyncClient()
+                client = generativelanguage_v1beta3.TextServiceAsyncClient()
 
                 # Initialize request argument(s)
-                prompt = generativelanguage_v1beta.TextPrompt()
+                prompt = generativelanguage_v1beta3.TextPrompt()
                 prompt.text = "text_value"
 
-                request = generativelanguage_v1beta.GenerateTextRequest(
+                request = generativelanguage_v1beta3.GenerateTextRequest(
                     model="model_value",
                     prompt=prompt,
                 )
 
                 # Make the request
                 response = await client.generate_text(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.GenerateTextRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.GenerateTextRequest, dict]]):
                 The request object. Request to generate a text completion
                 response from the model.
             model (:class:`str`):
                 Required. The name of the ``Model`` or ``TunedModel`` to
                 use for generating the completion. Examples:
                 models/text-bison-001
                 tunedModels/sentence-translator-u3b7m
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
-            prompt (:class:`google.ai.generativelanguage_v1beta.types.TextPrompt`):
+            prompt (:class:`google.ai.generativelanguage_v1beta3.types.TextPrompt`):
                 Required. The free-form input text
                 given to the model as a prompt.
                 Given a prompt, the model will generate
                 a TextCompletion response it predicts as
                 the completion of the input text.
 
                 This corresponds to the ``prompt`` field
@@ -396,22 +401,22 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.GenerateTextResponse:
+            google.ai.generativelanguage_v1beta3.types.GenerateTextResponse:
                 The response from the model,
                 including candidate completions.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [
                 model,
                 prompt,
                 temperature,
                 candidate_count,
                 max_output_tokens,
@@ -421,15 +426,18 @@
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.GenerateTextRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.GenerateTextRequest):
+            request = text_service.GenerateTextRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
@@ -442,28 +450,17 @@
         if top_p is not None:
             request.top_p = top_p
         if top_k is not None:
             request.top_k = top_k
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.generate_text,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.generate_text
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -499,95 +496,88 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_embed_text():
                 # Create a client
-                client = generativelanguage_v1beta.TextServiceAsyncClient()
+                client = generativelanguage_v1beta3.TextServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.EmbedTextRequest(
+                request = generativelanguage_v1beta3.EmbedTextRequest(
                     model="model_value",
+                    text="text_value",
                 )
 
                 # Make the request
                 response = await client.embed_text(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.EmbedTextRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.EmbedTextRequest, dict]]):
                 The request object. Request to get a text embedding from
                 the model.
             model (:class:`str`):
                 Required. The model name to use with
                 the format model=models/{model}.
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             text (:class:`str`):
-                Optional. The free-form input text
+                Required. The free-form input text
                 that the model will turn into an
                 embedding.
 
                 This corresponds to the ``text`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.EmbedTextResponse:
+            google.ai.generativelanguage_v1beta3.types.EmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, text])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.EmbedTextRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.EmbedTextRequest):
+            request = text_service.EmbedTextRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if text is not None:
             request.text = text
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.embed_text,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.embed_text
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -623,98 +613,91 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_batch_embed_text():
                 # Create a client
-                client = generativelanguage_v1beta.TextServiceAsyncClient()
+                client = generativelanguage_v1beta3.TextServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta.BatchEmbedTextRequest(
+                request = generativelanguage_v1beta3.BatchEmbedTextRequest(
                     model="model_value",
+                    texts=['texts_value1', 'texts_value2'],
                 )
 
                 # Make the request
                 response = await client.batch_embed_text(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.BatchEmbedTextRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.BatchEmbedTextRequest, dict]]):
                 The request object. Batch request to get a text embedding
                 from the model.
             model (:class:`str`):
                 Required. The name of the ``Model`` to use for
                 generating the embedding. Examples:
                 models/embedding-gecko-001
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             texts (:class:`MutableSequence[str]`):
-                Optional. The free-form input texts
+                Required. The free-form input texts
                 that the model will turn into an
-                embedding. The current limit is 100
+                embedding.  The current limit is 100
                 texts, over which an error will be
                 thrown.
 
                 This corresponds to the ``texts`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.BatchEmbedTextResponse:
+            google.ai.generativelanguage_v1beta3.types.BatchEmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, texts])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.BatchEmbedTextRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.BatchEmbedTextRequest):
+            request = text_service.BatchEmbedTextRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if texts:
             request.texts.extend(texts)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.batch_embed_text,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.batch_embed_text
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -750,37 +733,37 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta
+            from google.ai import generativelanguage_v1beta3
 
             async def sample_count_text_tokens():
                 # Create a client
-                client = generativelanguage_v1beta.TextServiceAsyncClient()
+                client = generativelanguage_v1beta3.TextServiceAsyncClient()
 
                 # Initialize request argument(s)
-                prompt = generativelanguage_v1beta.TextPrompt()
+                prompt = generativelanguage_v1beta3.TextPrompt()
                 prompt.text = "text_value"
 
-                request = generativelanguage_v1beta.CountTextTokensRequest(
+                request = generativelanguage_v1beta3.CountTextTokensRequest(
                     model="model_value",
                     prompt=prompt,
                 )
 
                 # Make the request
                 response = await client.count_text_tokens(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta.types.CountTextTokensRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.CountTextTokensRequest, dict]]):
                 The request object. Counts the number of tokens in the ``prompt`` sent to a
                 model.
 
                 Models may tokenize text differently, so each model may
                 return a different ``token_count``.
             model (:class:`str`):
                 Required. The model's resource name. This serves as an
@@ -790,69 +773,61 @@
                 ``ListModels`` method.
 
                 Format: ``models/{model}``
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
-            prompt (:class:`google.ai.generativelanguage_v1beta.types.TextPrompt`):
+            prompt (:class:`google.ai.generativelanguage_v1beta3.types.TextPrompt`):
                 Required. The free-form input text
                 given to the model as a prompt.
 
                 This corresponds to the ``prompt`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta.types.CountTextTokensResponse:
+            google.ai.generativelanguage_v1beta3.types.CountTextTokensResponse:
                 A response from CountTextTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.CountTextTokensRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.CountTextTokensRequest):
+            request = text_service.CountTextTokensRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.count_text_tokens,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.count_text_tokens
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/client.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -521,29 +522,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, TextServiceTransport]] = None,
+        transport: Optional[
+            Union[str, TextServiceTransport, Callable[..., TextServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the text service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, TextServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,TextServiceTransport,Callable[..., TextServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the TextServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -641,16 +646,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[TextServiceTransport], Callable[..., TextServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., TextServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -807,16 +819,16 @@
         Returns:
             google.ai.generativelanguage_v1beta.types.GenerateTextResponse:
                 The response from the model,
                 including candidate completions.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [
                 model,
                 prompt,
                 temperature,
                 candidate_count,
                 max_output_tokens,
@@ -826,18 +838,16 @@
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.GenerateTextRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.GenerateTextRequest):
             request = text_service.GenerateTextRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
@@ -942,27 +952,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.EmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, text])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.EmbedTextRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.EmbedTextRequest):
             request = text_service.EmbedTextRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if text is not None:
@@ -1060,27 +1068,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta.types.BatchEmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, texts])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.BatchEmbedTextRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.BatchEmbedTextRequest):
             request = text_service.BatchEmbedTextRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if texts is not None:
@@ -1189,27 +1195,25 @@
             google.ai.generativelanguage_v1beta.types.CountTextTokensResponse:
                 A response from CountTextTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.CountTextTokensRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.CountTextTokensRequest):
             request = text_service.CountTextTokensRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/grpc.py`

 * *Files 3% similar despite different names*

```diff
@@ -49,15 +49,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -69,36 +69,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -116,15 +119,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -157,15 +160,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/grpc.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,100 +9,55 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import gapic_v1, grpc_helpers
+import google.auth  # type: ignore
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
-from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta.types import text_service
+from google.ai.generativelanguage_v1beta3.types import text_service
 
 from .base import DEFAULT_CLIENT_INFO, TextServiceTransport
-from .grpc import TextServiceGrpcTransport
 
 
-class TextServiceGrpcAsyncIOTransport(TextServiceTransport):
-    """gRPC AsyncIO backend transport for TextService.
+class TextServiceGrpcTransport(TextServiceTransport):
+    """gRPC backend transport for TextService.
 
     API for using Generative Language Models (GLMs) trained to
     generate text.
     Also known as Large Language Models (LLM)s, these generate text
     given an input prompt from the user.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _grpc_channel: aio.Channel
-    _stubs: Dict[str, Callable] = {}
-
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> aio.Channel:
-        """Create and return a gRPC AsyncIO channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            aio.Channel: A gRPC AsyncIO channel object.
-        """
-
-        return grpc_helpers_async.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
+    _stubs: Dict[str, Callable]
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -114,68 +69,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional(Sequence[str])): A list of scopes. This argument is
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
+          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
+
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -202,15 +160,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -221,140 +181,182 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @property
-    def grpc_channel(self) -> aio.Channel:
-        """Create the channel designed to connect to this service.
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> grpc.Channel:
+        """Create and return a gRPC channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+                This argument is mutually exclusive with credentials.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            grpc.Channel: A gRPC channel object.
 
-        This property caches on the instance; repeated calls return
-        the same channel.
+        Raises:
+            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
+              and ``credentials_file`` are passed.
         """
-        # Return the channel from cache.
+
+        return grpc_helpers.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
+
+    @property
+    def grpc_channel(self) -> grpc.Channel:
+        """Return the channel designed to connect to this service."""
         return self._grpc_channel
 
     @property
     def generate_text(
         self,
     ) -> Callable[
-        [text_service.GenerateTextRequest], Awaitable[text_service.GenerateTextResponse]
+        [text_service.GenerateTextRequest], text_service.GenerateTextResponse
     ]:
         r"""Return a callable for the generate text method over gRPC.
 
         Generates a response from the model given an input
         message.
 
         Returns:
             Callable[[~.GenerateTextRequest],
-                    Awaitable[~.GenerateTextResponse]]:
+                    ~.GenerateTextResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "generate_text" not in self._stubs:
             self._stubs["generate_text"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.TextService/GenerateText",
+                "/google.ai.generativelanguage.v1beta3.TextService/GenerateText",
                 request_serializer=text_service.GenerateTextRequest.serialize,
                 response_deserializer=text_service.GenerateTextResponse.deserialize,
             )
         return self._stubs["generate_text"]
 
     @property
     def embed_text(
         self,
-    ) -> Callable[
-        [text_service.EmbedTextRequest], Awaitable[text_service.EmbedTextResponse]
-    ]:
+    ) -> Callable[[text_service.EmbedTextRequest], text_service.EmbedTextResponse]:
         r"""Return a callable for the embed text method over gRPC.
 
         Generates an embedding from the model given an input
         message.
 
         Returns:
             Callable[[~.EmbedTextRequest],
-                    Awaitable[~.EmbedTextResponse]]:
+                    ~.EmbedTextResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "embed_text" not in self._stubs:
             self._stubs["embed_text"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.TextService/EmbedText",
+                "/google.ai.generativelanguage.v1beta3.TextService/EmbedText",
                 request_serializer=text_service.EmbedTextRequest.serialize,
                 response_deserializer=text_service.EmbedTextResponse.deserialize,
             )
         return self._stubs["embed_text"]
 
     @property
     def batch_embed_text(
         self,
     ) -> Callable[
-        [text_service.BatchEmbedTextRequest],
-        Awaitable[text_service.BatchEmbedTextResponse],
+        [text_service.BatchEmbedTextRequest], text_service.BatchEmbedTextResponse
     ]:
         r"""Return a callable for the batch embed text method over gRPC.
 
         Generates multiple embeddings from the model given
         input text in a synchronous call.
 
         Returns:
             Callable[[~.BatchEmbedTextRequest],
-                    Awaitable[~.BatchEmbedTextResponse]]:
+                    ~.BatchEmbedTextResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "batch_embed_text" not in self._stubs:
             self._stubs["batch_embed_text"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.TextService/BatchEmbedText",
+                "/google.ai.generativelanguage.v1beta3.TextService/BatchEmbedText",
                 request_serializer=text_service.BatchEmbedTextRequest.serialize,
                 response_deserializer=text_service.BatchEmbedTextResponse.deserialize,
             )
         return self._stubs["batch_embed_text"]
 
     @property
     def count_text_tokens(
         self,
     ) -> Callable[
-        [text_service.CountTextTokensRequest],
-        Awaitable[text_service.CountTextTokensResponse],
+        [text_service.CountTextTokensRequest], text_service.CountTextTokensResponse
     ]:
         r"""Return a callable for the count text tokens method over gRPC.
 
         Runs a model's tokenizer on a text and returns the
         token count.
 
         Returns:
             Callable[[~.CountTextTokensRequest],
-                    Awaitable[~.CountTextTokensResponse]]:
+                    ~.CountTextTokensResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "count_text_tokens" not in self._stubs:
             self._stubs["count_text_tokens"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta.TextService/CountTextTokens",
+                "/google.ai.generativelanguage.v1beta3.TextService/CountTextTokens",
                 request_serializer=text_service.CountTextTokensRequest.serialize,
                 response_deserializer=text_service.CountTextTokensResponse.deserialize,
             )
         return self._stubs["count_text_tokens"]
 
     def close(self):
-        return self.grpc_channel.close()
+        self.grpc_channel.close()
+
+    @property
+    def kind(self) -> str:
+        return "grpc"
 
 
-__all__ = ("TextServiceGrpcAsyncIOTransport",)
+__all__ = ("TextServiceGrpcTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/services/text_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/citation.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/citation.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/content.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/content.py`

 * *Files 2% similar despite different names*

```diff
@@ -182,18 +182,21 @@
 class Blob(proto.Message):
     r"""Raw media bytes.
 
     Text should not be sent as raw bytes, use the 'text' field.
 
     Attributes:
         mime_type (str):
-            The IANA standard MIME type of the source
-            data. Accepted types include: "image/png",
-            "image/jpeg", "image/heic", "image/heif",
-            "image/webp".
+            The IANA standard MIME type of the source data. Examples:
+
+            -  image/png
+            -  image/jpeg If an unsupported MIME type is provided, an
+               error will be returned. For a complete list of supported
+               types, see `Supported file
+               formats <https://ai.google.dev/gemini-api/docs/prompting_with_media#supported_file_formats>`__.
         data (bytes):
             Raw bytes for media formats.
     """
 
     mime_type: str = proto.Field(
         proto.STRING,
         number=1,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/discuss_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/discuss_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/file.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/file_service.py`

 * *Files 23% similar despite different names*

```diff
@@ -13,94 +13,133 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from __future__ import annotations
 
 from typing import MutableMapping, MutableSequence
 
-from google.protobuf import timestamp_pb2  # type: ignore
 import proto  # type: ignore
 
+from google.ai.generativelanguage_v1beta.types import file as gag_file
+
 __protobuf__ = proto.module(
     package="google.ai.generativelanguage.v1beta",
     manifest={
-        "File",
+        "CreateFileRequest",
+        "CreateFileResponse",
+        "ListFilesRequest",
+        "ListFilesResponse",
+        "GetFileRequest",
+        "DeleteFileRequest",
     },
 )
 
 
-class File(proto.Message):
-    r"""A file uploaded to the API.
+class CreateFileRequest(proto.Message):
+    r"""Request for ``CreateFile``.
 
     Attributes:
-        name (str):
-            Immutable. Identifier. The ``File`` resource name. The ID
-            (name excluding the "files/" prefix) can contain up to 40
-            characters that are lowercase alphanumeric or dashes (-).
-            The ID cannot start or end with a dash. If the name is empty
-            on create, a unique name will be generated. Example:
-            ``files/123-456``
-        display_name (str):
-            Optional. The human-readable display name for the ``File``.
-            The display name must be no more than 512 characters in
-            length, including spaces. Example: "Welcome Image".
-        mime_type (str):
-            Output only. MIME type of the file.
-        size_bytes (int):
-            Output only. Size of the file in bytes.
-        create_time (google.protobuf.timestamp_pb2.Timestamp):
-            Output only. The timestamp of when the ``File`` was created.
-        update_time (google.protobuf.timestamp_pb2.Timestamp):
-            Output only. The timestamp of when the ``File`` was last
-            updated.
-        expiration_time (google.protobuf.timestamp_pb2.Timestamp):
-            Output only. The timestamp of when the ``File`` will be
-            deleted. Only set if the ``File`` is scheduled to expire.
-        sha256_hash (bytes):
-            Output only. SHA-256 hash of the uploaded
-            bytes.
-        uri (str):
-            Output only. The uri of the ``File``.
+        file (google.ai.generativelanguage_v1beta.types.File):
+            Optional. Metadata for the file to create.
     """
 
-    name: str = proto.Field(
-        proto.STRING,
+    file: gag_file.File = proto.Field(
+        proto.MESSAGE,
         number=1,
+        message=gag_file.File,
     )
-    display_name: str = proto.Field(
-        proto.STRING,
-        number=2,
+
+
+class CreateFileResponse(proto.Message):
+    r"""Response for ``CreateFile``.
+
+    Attributes:
+        file (google.ai.generativelanguage_v1beta.types.File):
+            Metadata for the created file.
+    """
+
+    file: gag_file.File = proto.Field(
+        proto.MESSAGE,
+        number=1,
+        message=gag_file.File,
+    )
+
+
+class ListFilesRequest(proto.Message):
+    r"""Request for ``ListFiles``.
+
+    Attributes:
+        page_size (int):
+            Optional. Maximum number of ``File``\ s to return per page.
+            If unspecified, defaults to 10. Maximum ``page_size`` is
+            100.
+        page_token (str):
+            Optional. A page token from a previous ``ListFiles`` call.
+    """
+
+    page_size: int = proto.Field(
+        proto.INT32,
+        number=1,
     )
-    mime_type: str = proto.Field(
+    page_token: str = proto.Field(
         proto.STRING,
         number=3,
     )
-    size_bytes: int = proto.Field(
-        proto.INT64,
-        number=4,
-    )
-    create_time: timestamp_pb2.Timestamp = proto.Field(
-        proto.MESSAGE,
-        number=5,
-        message=timestamp_pb2.Timestamp,
-    )
-    update_time: timestamp_pb2.Timestamp = proto.Field(
+
+
+class ListFilesResponse(proto.Message):
+    r"""Response for ``ListFiles``.
+
+    Attributes:
+        files (MutableSequence[google.ai.generativelanguage_v1beta.types.File]):
+            The list of ``File``\ s.
+        next_page_token (str):
+            A token that can be sent as a ``page_token`` into a
+            subsequent ``ListFiles`` call.
+    """
+
+    @property
+    def raw_page(self):
+        return self
+
+    files: MutableSequence[gag_file.File] = proto.RepeatedField(
         proto.MESSAGE,
-        number=6,
-        message=timestamp_pb2.Timestamp,
+        number=1,
+        message=gag_file.File,
     )
-    expiration_time: timestamp_pb2.Timestamp = proto.Field(
-        proto.MESSAGE,
-        number=7,
-        message=timestamp_pb2.Timestamp,
+    next_page_token: str = proto.Field(
+        proto.STRING,
+        number=2,
     )
-    sha256_hash: bytes = proto.Field(
-        proto.BYTES,
-        number=8,
+
+
+class GetFileRequest(proto.Message):
+    r"""Request for ``GetFile``.
+
+    Attributes:
+        name (str):
+            Required. The name of the ``File`` to get. Example:
+            ``files/abc-123``
+    """
+
+    name: str = proto.Field(
+        proto.STRING,
+        number=1,
     )
-    uri: str = proto.Field(
+
+
+class DeleteFileRequest(proto.Message):
+    r"""Request for ``DeleteFile``.
+
+    Attributes:
+        name (str):
+            Required. The name of the ``File`` to delete. Example:
+            ``files/abc-123``
+    """
+
+    name: str = proto.Field(
         proto.STRING,
-        number=9,
+        number=1,
     )
 
 
 __all__ = tuple(sorted(__protobuf__.manifest))
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/generative_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/generative_service.py`

 * *Files 2% similar despite different names*

```diff
@@ -257,14 +257,23 @@
 
             This field is a member of `oneof`_ ``_top_k``.
         response_mime_type (str):
             Optional. Output response mimetype of the generated
             candidate text. Supported mimetype: ``text/plain``:
             (default) Text output. ``application/json``: JSON response
             in the candidates.
+        response_schema (google.ai.generativelanguage_v1beta.types.Schema):
+            Optional. Output response schema of the generated candidate
+            text when response mime type can have schema. Schema can be
+            objects, primitives or arrays and is a subset of `OpenAPI
+            schema <https://spec.openapis.org/oas/v3.0.3#schema>`__.
+
+            If set, a compatible response_mime_type must also be set.
+            Compatible mimetypes: ``application/json``: Schema for JSON
+            response.
     """
 
     candidate_count: int = proto.Field(
         proto.INT32,
         number=1,
         optional=True,
     )
@@ -292,14 +301,19 @@
         number=7,
         optional=True,
     )
     response_mime_type: str = proto.Field(
         proto.STRING,
         number=13,
     )
+    response_schema: gag_content.Schema = proto.Field(
+        proto.MESSAGE,
+        number=14,
+        message=gag_content.Schema,
+    )
 
 
 class SemanticRetrieverConfig(proto.Message):
     r"""Configuration for retrieving grounding content from a ``Corpus`` or
     ``Document`` created using the Semantic Retriever API.
 
 
@@ -370,14 +384,17 @@
 
     Attributes:
         candidates (MutableSequence[google.ai.generativelanguage_v1beta.types.Candidate]):
             Candidate responses from the model.
         prompt_feedback (google.ai.generativelanguage_v1beta.types.GenerateContentResponse.PromptFeedback):
             Returns the prompt's feedback related to the
             content filters.
+        usage_metadata (google.ai.generativelanguage_v1beta.types.GenerateContentResponse.UsageMetadata):
+            Output only. Metadata on the generation
+            requests' token usage.
     """
 
     class PromptFeedback(proto.Message):
         r"""A set of the feedback metadata the prompt specified in
         ``GenerateContentRequest.content``.
 
         Attributes:
@@ -416,24 +433,56 @@
         )
         safety_ratings: MutableSequence[safety.SafetyRating] = proto.RepeatedField(
             proto.MESSAGE,
             number=2,
             message=safety.SafetyRating,
         )
 
+    class UsageMetadata(proto.Message):
+        r"""Metadata on the generation request's token usage.
+
+        Attributes:
+            prompt_token_count (int):
+                Number of tokens in the prompt.
+            candidates_token_count (int):
+                Total number of tokens across the generated
+                candidates.
+            total_token_count (int):
+                Total token count for the generation request
+                (prompt + candidates).
+        """
+
+        prompt_token_count: int = proto.Field(
+            proto.INT32,
+            number=1,
+        )
+        candidates_token_count: int = proto.Field(
+            proto.INT32,
+            number=2,
+        )
+        total_token_count: int = proto.Field(
+            proto.INT32,
+            number=3,
+        )
+
     candidates: MutableSequence["Candidate"] = proto.RepeatedField(
         proto.MESSAGE,
         number=1,
         message="Candidate",
     )
     prompt_feedback: PromptFeedback = proto.Field(
         proto.MESSAGE,
         number=2,
         message=PromptFeedback,
     )
+    usage_metadata: UsageMetadata = proto.Field(
+        proto.MESSAGE,
+        number=3,
+        message=UsageMetadata,
+    )
 
 
 class Candidate(proto.Message):
     r"""A response candidate generated from the model.
 
     .. _oneof: https://proto-plus-python.readthedocs.io/en/stable/fields.html#oneofs-mutually-exclusive-fields
 
@@ -914,16 +963,17 @@
             Note: Specifying a ``title`` for ``RETRIEVAL_DOCUMENT``
             provides better quality embeddings for retrieval.
 
             This field is a member of `oneof`_ ``_title``.
         output_dimensionality (int):
             Optional. Optional reduced dimension for the output
             embedding. If set, excessive values in the output embedding
-            are truncated from the end. Supported by
-            ``models/text-embedding-latest``.
+            are truncated from the end. Supported by newer models since
+            2024, and the earlier model (``models/embedding-001``)
+            cannot specify this value.
 
             This field is a member of `oneof`_ ``_output_dimensionality``.
     """
 
     model: str = proto.Field(
         proto.STRING,
         number=1,
@@ -1040,27 +1090,36 @@
             for the Model to use.
 
             This name should match a model name returned by the
             ``ListModels`` method.
 
             Format: ``models/{model}``
         contents (MutableSequence[google.ai.generativelanguage_v1beta.types.Content]):
-            Required. The input given to the model as a
+            Optional. The input given to the model as a
             prompt.
+        generate_content_request (google.ai.generativelanguage_v1beta.types.GenerateContentRequest):
+            Optional. The overall input given to the
+            model. CountTokens will count prompt, function
+            calling, etc.
     """
 
     model: str = proto.Field(
         proto.STRING,
         number=1,
     )
     contents: MutableSequence[gag_content.Content] = proto.RepeatedField(
         proto.MESSAGE,
         number=2,
         message=gag_content.Content,
     )
+    generate_content_request: "GenerateContentRequest" = proto.Field(
+        proto.MESSAGE,
+        number=3,
+        message="GenerateContentRequest",
+    )
 
 
 class CountTokensResponse(proto.Message):
     r"""A response from ``CountTokens``.
 
     It returns the model's ``token_count`` for the ``prompt``.
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/model.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/model.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/model_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/model_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/permission.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/permission.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/permission_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/permission_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/retriever.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/retriever.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/retriever_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/retriever_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/safety.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/safety.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/text_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/text_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta/types/tuned_model.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/types/tuned_model.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/gapic_metadata.json` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/gapic_metadata.json`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/gapic_version.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/gapic_version.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,8 +9,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-__version__ = "0.6.2"  # {x-release-please-version}
+__version__ = "0.6.3"  # {x-release-please-version}
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/async_client.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -193,29 +194,33 @@
         type(DiscussServiceClient).get_transport_class, type(DiscussServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, DiscussServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, DiscussServiceTransport, Callable[..., DiscussServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the discuss service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.DiscussServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,DiscussServiceTransport,Callable[..., DiscussServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the DiscussServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -382,26 +387,29 @@
 
                 This includes candidate messages and
                 conversation history in the form of
                 chronologically-ordered messages.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [model, prompt, temperature, candidate_count, top_p, top_k]
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = discuss_service.GenerateMessageRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, discuss_service.GenerateMessageRequest):
+            request = discuss_service.GenerateMessageRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
@@ -412,28 +420,17 @@
         if top_p is not None:
             request.top_p = top_p
         if top_k is not None:
             request.top_k = top_k
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.generate_message,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.generate_message
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -532,48 +529,40 @@
             google.ai.generativelanguage_v1beta2.types.CountMessageTokensResponse:
                 A response from CountMessageTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = discuss_service.CountMessageTokensRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, discuss_service.CountMessageTokensRequest):
+            request = discuss_service.CountMessageTokensRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.count_message_tokens,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.count_message_tokens
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/client.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -521,29 +522,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, DiscussServiceTransport]] = None,
+        transport: Optional[
+            Union[str, DiscussServiceTransport, Callable[..., DiscussServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the discuss service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, DiscussServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,DiscussServiceTransport,Callable[..., DiscussServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the DiscussServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -644,16 +649,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[DiscussServiceTransport], Callable[..., DiscussServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., DiscussServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -788,29 +800,27 @@
 
                 This includes candidate messages and
                 conversation history in the form of
                 chronologically-ordered messages.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [model, prompt, temperature, candidate_count, top_p, top_k]
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a discuss_service.GenerateMessageRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, discuss_service.GenerateMessageRequest):
             request = discuss_service.GenerateMessageRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
@@ -929,27 +939,25 @@
             google.ai.generativelanguage_v1beta2.types.CountMessageTokensResponse:
                 A response from CountMessageTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a discuss_service.CountMessageTokensRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, discuss_service.CountMessageTokensRequest):
             request = discuss_service.CountMessageTokensRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/grpc_asyncio.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,54 +9,101 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers
-import google.auth  # type: ignore
+from google.api_core import exceptions as core_exceptions
+from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
+from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
+from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta2.types import discuss_service
+from google.ai.generativelanguage_v1beta3.types import discuss_service
 
 from .base import DEFAULT_CLIENT_INFO, DiscussServiceTransport
+from .grpc import DiscussServiceGrpcTransport
 
 
-class DiscussServiceGrpcTransport(DiscussServiceTransport):
-    """gRPC backend transport for DiscussService.
+class DiscussServiceGrpcAsyncIOTransport(DiscussServiceTransport):
+    """gRPC AsyncIO backend transport for DiscussService.
 
     An API for using Generative Language Models (GLMs) in dialog
     applications.
     Also known as large language models (LLMs), this API provides
     models that are trained for multi-turn dialog.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _stubs: Dict[str, Callable]
+    _grpc_channel: aio.Channel
+    _stubs: Dict[str, Callable] = {}
+
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> aio.Channel:
+        """Create and return a gRPC AsyncIO channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            aio.Channel: A gRPC AsyncIO channel object.
+        """
+
+        return grpc_helpers_async.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -68,68 +115,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
+            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
-
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -156,15 +206,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -175,128 +227,97 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> grpc.Channel:
-        """Create and return a gRPC channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is mutually exclusive with credentials.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            grpc.Channel: A gRPC channel object.
+    @property
+    def grpc_channel(self) -> aio.Channel:
+        """Create the channel designed to connect to this service.
 
-        Raises:
-            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
-              and ``credentials_file`` are passed.
+        This property caches on the instance; repeated calls return
+        the same channel.
         """
-
-        return grpc_helpers.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
-
-    @property
-    def grpc_channel(self) -> grpc.Channel:
-        """Return the channel designed to connect to this service."""
+        # Return the channel from cache.
         return self._grpc_channel
 
     @property
     def generate_message(
         self,
     ) -> Callable[
         [discuss_service.GenerateMessageRequest],
-        discuss_service.GenerateMessageResponse,
+        Awaitable[discuss_service.GenerateMessageResponse],
     ]:
         r"""Return a callable for the generate message method over gRPC.
 
         Generates a response from the model given an input
         ``MessagePrompt``.
 
         Returns:
             Callable[[~.GenerateMessageRequest],
-                    ~.GenerateMessageResponse]:
+                    Awaitable[~.GenerateMessageResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "generate_message" not in self._stubs:
             self._stubs["generate_message"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta2.DiscussService/GenerateMessage",
+                "/google.ai.generativelanguage.v1beta3.DiscussService/GenerateMessage",
                 request_serializer=discuss_service.GenerateMessageRequest.serialize,
                 response_deserializer=discuss_service.GenerateMessageResponse.deserialize,
             )
         return self._stubs["generate_message"]
 
     @property
     def count_message_tokens(
         self,
     ) -> Callable[
         [discuss_service.CountMessageTokensRequest],
-        discuss_service.CountMessageTokensResponse,
+        Awaitable[discuss_service.CountMessageTokensResponse],
     ]:
         r"""Return a callable for the count message tokens method over gRPC.
 
         Runs a model's tokenizer on a string and returns the
         token count.
 
         Returns:
             Callable[[~.CountMessageTokensRequest],
-                    ~.CountMessageTokensResponse]:
+                    Awaitable[~.CountMessageTokensResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "count_message_tokens" not in self._stubs:
             self._stubs["count_message_tokens"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta2.DiscussService/CountMessageTokens",
+                "/google.ai.generativelanguage.v1beta3.DiscussService/CountMessageTokens",
                 request_serializer=discuss_service.CountMessageTokensRequest.serialize,
                 response_deserializer=discuss_service.CountMessageTokensResponse.deserialize,
             )
         return self._stubs["count_message_tokens"]
 
-    def close(self):
-        self.grpc_channel.close()
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.generate_message: gapic_v1.method_async.wrap_method(
+                self.generate_message,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.count_message_tokens: gapic_v1.method_async.wrap_method(
+                self.count_message_tokens,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+        }
 
-    @property
-    def kind(self) -> str:
-        return "grpc"
+    def close(self):
+        return self.grpc_channel.close()
 
 
-__all__ = ("DiscussServiceGrpcTransport",)
+__all__ = ("DiscussServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/grpc.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,99 +9,54 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import gapic_v1, grpc_helpers
+import google.auth  # type: ignore
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 import grpc  # type: ignore
-from grpc.experimental import aio  # type: ignore
 
 from google.ai.generativelanguage_v1beta2.types import discuss_service
 
 from .base import DEFAULT_CLIENT_INFO, DiscussServiceTransport
-from .grpc import DiscussServiceGrpcTransport
 
 
-class DiscussServiceGrpcAsyncIOTransport(DiscussServiceTransport):
-    """gRPC AsyncIO backend transport for DiscussService.
+class DiscussServiceGrpcTransport(DiscussServiceTransport):
+    """gRPC backend transport for DiscussService.
 
     An API for using Generative Language Models (GLMs) in dialog
     applications.
     Also known as large language models (LLMs), this API provides
     models that are trained for multi-turn dialog.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _grpc_channel: aio.Channel
-    _stubs: Dict[str, Callable] = {}
-
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> aio.Channel:
-        """Create and return a gRPC AsyncIO channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            aio.Channel: A gRPC AsyncIO channel object.
-        """
-
-        return grpc_helpers_async.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
+    _stubs: Dict[str, Callable]
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -113,68 +68,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional(Sequence[str])): A list of scopes. This argument is
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
+          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
+
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -201,15 +159,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -220,39 +180,81 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @property
-    def grpc_channel(self) -> aio.Channel:
-        """Create the channel designed to connect to this service.
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> grpc.Channel:
+        """Create and return a gRPC channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+                This argument is mutually exclusive with credentials.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            grpc.Channel: A gRPC channel object.
 
-        This property caches on the instance; repeated calls return
-        the same channel.
+        Raises:
+            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
+              and ``credentials_file`` are passed.
         """
-        # Return the channel from cache.
+
+        return grpc_helpers.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
+
+    @property
+    def grpc_channel(self) -> grpc.Channel:
+        """Return the channel designed to connect to this service."""
         return self._grpc_channel
 
     @property
     def generate_message(
         self,
     ) -> Callable[
         [discuss_service.GenerateMessageRequest],
-        Awaitable[discuss_service.GenerateMessageResponse],
+        discuss_service.GenerateMessageResponse,
     ]:
         r"""Return a callable for the generate message method over gRPC.
 
         Generates a response from the model given an input
         ``MessagePrompt``.
 
         Returns:
             Callable[[~.GenerateMessageRequest],
-                    Awaitable[~.GenerateMessageResponse]]:
+                    ~.GenerateMessageResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -265,24 +267,24 @@
         return self._stubs["generate_message"]
 
     @property
     def count_message_tokens(
         self,
     ) -> Callable[
         [discuss_service.CountMessageTokensRequest],
-        Awaitable[discuss_service.CountMessageTokensResponse],
+        discuss_service.CountMessageTokensResponse,
     ]:
         r"""Return a callable for the count message tokens method over gRPC.
 
         Runs a model's tokenizer on a string and returns the
         token count.
 
         Returns:
             Callable[[~.CountMessageTokensRequest],
-                    Awaitable[~.CountMessageTokensResponse]]:
+                    ~.CountMessageTokensResponse]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
@@ -291,11 +293,15 @@
                 "/google.ai.generativelanguage.v1beta2.DiscussService/CountMessageTokens",
                 request_serializer=discuss_service.CountMessageTokensRequest.serialize,
                 response_deserializer=discuss_service.CountMessageTokensResponse.deserialize,
             )
         return self._stubs["count_message_tokens"]
 
     def close(self):
-        return self.grpc_channel.close()
+        self.grpc_channel.close()
+
+    @property
+    def kind(self) -> str:
+        return "grpc"
 
 
-__all__ = ("DiscussServiceGrpcAsyncIOTransport",)
+__all__ = ("DiscussServiceGrpcTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/async_client.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -188,29 +189,33 @@
         type(ModelServiceClient).get_transport_class, type(ModelServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, ModelServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, ModelServiceTransport, Callable[..., ModelServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the model service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.ModelServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,ModelServiceTransport,Callable[..., ModelServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the ModelServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -311,46 +316,38 @@
         Returns:
             google.ai.generativelanguage_v1beta2.types.Model:
                 Information about a Generative
                 Language Model.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.GetModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.GetModelRequest):
+            request = model_service.GetModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_model,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -446,48 +443,40 @@
                 Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.ListModelsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.ListModelsRequest):
+            request = model_service.ListModelsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if page_size is not None:
             request.page_size = page_size
         if page_token is not None:
             request.page_token = page_token
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_models,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_models
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/client.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -518,29 +519,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, ModelServiceTransport]] = None,
+        transport: Optional[
+            Union[str, ModelServiceTransport, Callable[..., ModelServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the model service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ModelServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,ModelServiceTransport,Callable[..., ModelServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the ModelServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -638,16 +643,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[ModelServiceTransport], Callable[..., ModelServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., ModelServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -716,27 +728,25 @@
         Returns:
             google.ai.generativelanguage_v1beta2.types.Model:
                 Information about a Generative
                 Language Model.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.GetModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.GetModelRequest):
             request = model_service.GetModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -842,27 +852,25 @@
                 Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.ListModelsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.ListModelsRequest):
             request = model_service.ListModelsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if page_size is not None:
                 request.page_size = page_size
             if page_token is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/pagers.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/pagers.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/grpc.py`

 * *Files 4% similar despite different names*

```diff
@@ -46,15 +46,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -66,36 +66,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -113,15 +116,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -154,15 +157,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/grpc_asyncio.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
+from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 import grpc  # type: ignore
 from grpc.experimental import aio  # type: ignore
 
 from google.ai.generativelanguage_v1beta2.types import model, model_service
 
@@ -61,15 +63,14 @@
             credentials (Optional[~.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify this application to the service. If
                 none are specified, the client will attempt to ascertain
                 the credentials from the environment.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             kwargs (Optional[dict]): Keyword arguments, which are passed to the
                 channel creation.
@@ -91,15 +92,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -111,37 +112,40 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -159,15 +163,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
         else:
             if api_mtls_endpoint:
@@ -199,15 +203,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -282,12 +288,45 @@
             self._stubs["list_models"] = self.grpc_channel.unary_unary(
                 "/google.ai.generativelanguage.v1beta2.ModelService/ListModels",
                 request_serializer=model_service.ListModelsRequest.serialize,
                 response_deserializer=model_service.ListModelsResponse.deserialize,
             )
         return self._stubs["list_models"]
 
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.get_model: gapic_v1.method_async.wrap_method(
+                self.get_model,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.list_models: gapic_v1.method_async.wrap_method(
+                self.list_models,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+        }
+
     def close(self):
         return self.grpc_channel.close()
 
 
 __all__ = ("ModelServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/model_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/model_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/async_client.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -189,29 +190,33 @@
         type(TextServiceClient).get_transport_class, type(TextServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, TextServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, TextServiceTransport, Callable[..., TextServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the text service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.TextServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,TextServiceTransport,Callable[..., TextServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the TextServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -395,16 +400,16 @@
         Returns:
             google.ai.generativelanguage_v1beta2.types.GenerateTextResponse:
                 The response from the model,
                 including candidate completions.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [
                 model,
                 prompt,
                 temperature,
                 candidate_count,
                 max_output_tokens,
@@ -414,15 +419,18 @@
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.GenerateTextRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.GenerateTextRequest):
+            request = text_service.GenerateTextRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
@@ -435,28 +443,17 @@
         if top_p is not None:
             request.top_p = top_p
         if top_k is not None:
             request.top_k = top_k
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.generate_text,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.generate_text
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -540,48 +537,40 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta2.types.EmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, text])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.EmbedTextRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.EmbedTextRequest):
+            request = text_service.EmbedTextRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if text is not None:
             request.text = text
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.embed_text,
-            default_retry=retries.AsyncRetry(
-                initial=1.0,
-                maximum=10.0,
-                multiplier=1.3,
-                predicate=retries.if_exception_type(
-                    core_exceptions.ServiceUnavailable,
-                ),
-                deadline=60.0,
-            ),
-            default_timeout=60.0,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.embed_text
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/client.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -519,29 +520,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, TextServiceTransport]] = None,
+        transport: Optional[
+            Union[str, TextServiceTransport, Callable[..., TextServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the text service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, TextServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,TextServiceTransport,Callable[..., TextServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the TextServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -639,16 +644,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[TextServiceTransport], Callable[..., TextServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., TextServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -800,16 +812,16 @@
         Returns:
             google.ai.generativelanguage_v1beta2.types.GenerateTextResponse:
                 The response from the model,
                 including candidate completions.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [
                 model,
                 prompt,
                 temperature,
                 candidate_count,
                 max_output_tokens,
@@ -819,18 +831,16 @@
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.GenerateTextRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.GenerateTextRequest):
             request = text_service.GenerateTextRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
@@ -936,27 +946,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta2.types.EmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, text])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.EmbedTextRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.EmbedTextRequest):
             request = text_service.EmbedTextRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if text is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/grpc.py`

 * *Files 4% similar despite different names*

```diff
@@ -48,15 +48,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -68,36 +68,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -115,15 +118,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -156,15 +159,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/grpc_asyncio.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
+from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 import grpc  # type: ignore
 from grpc.experimental import aio  # type: ignore
 
 from google.ai.generativelanguage_v1beta2.types import text_service
 
@@ -63,15 +65,14 @@
             credentials (Optional[~.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify this application to the service. If
                 none are specified, the client will attempt to ascertain
                 the credentials from the environment.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             kwargs (Optional[dict]): Keyword arguments, which are passed to the
                 channel creation.
@@ -93,15 +94,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -113,37 +114,40 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -161,15 +165,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
         else:
             if api_mtls_endpoint:
@@ -201,15 +205,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -288,12 +294,45 @@
             self._stubs["embed_text"] = self.grpc_channel.unary_unary(
                 "/google.ai.generativelanguage.v1beta2.TextService/EmbedText",
                 request_serializer=text_service.EmbedTextRequest.serialize,
                 response_deserializer=text_service.EmbedTextResponse.deserialize,
             )
         return self._stubs["embed_text"]
 
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.generate_text: gapic_v1.method_async.wrap_method(
+                self.generate_text,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.embed_text: gapic_v1.method_async.wrap_method(
+                self.embed_text,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+        }
+
     def close(self):
         return self.grpc_channel.close()
 
 
 __all__ = ("TextServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/services/text_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/text_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/citation.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/citation.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/discuss_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/discuss_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/model.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/model.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/model_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/model_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/safety.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/safety.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta2/types/text_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/types/text_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/gapic_metadata.json` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/gapic_metadata.json`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/gapic_version.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/gapic_version.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,8 +9,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-__version__ = "0.6.2"  # {x-release-please-version}
+__version__ = "0.6.3"  # {x-release-please-version}
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/async_client.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -195,29 +196,33 @@
         type(DiscussServiceClient).get_transport_class, type(DiscussServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, DiscussServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, DiscussServiceTransport, Callable[..., DiscussServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the discuss service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.DiscussServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,DiscussServiceTransport,Callable[..., DiscussServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the DiscussServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -384,26 +389,29 @@
 
                 This includes candidate messages and
                 conversation history in the form of
                 chronologically-ordered messages.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [model, prompt, temperature, candidate_count, top_p, top_k]
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = discuss_service.GenerateMessageRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, discuss_service.GenerateMessageRequest):
+            request = discuss_service.GenerateMessageRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
@@ -414,19 +422,17 @@
         if top_p is not None:
             request.top_p = top_p
         if top_k is not None:
             request.top_k = top_k
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.generate_message,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.generate_message
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -525,39 +531,40 @@
             google.ai.generativelanguage_v1beta3.types.CountMessageTokensResponse:
                 A response from CountMessageTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = discuss_service.CountMessageTokensRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, discuss_service.CountMessageTokensRequest):
+            request = discuss_service.CountMessageTokensRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.count_message_tokens,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.count_message_tokens
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/client.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -523,29 +524,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, DiscussServiceTransport]] = None,
+        transport: Optional[
+            Union[str, DiscussServiceTransport, Callable[..., DiscussServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the discuss service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, DiscussServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,DiscussServiceTransport,Callable[..., DiscussServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the DiscussServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -646,16 +651,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[DiscussServiceTransport], Callable[..., DiscussServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., DiscussServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -790,29 +802,27 @@
 
                 This includes candidate messages and
                 conversation history in the form of
                 chronologically-ordered messages.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [model, prompt, temperature, candidate_count, top_p, top_k]
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a discuss_service.GenerateMessageRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, discuss_service.GenerateMessageRequest):
             request = discuss_service.GenerateMessageRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
@@ -931,27 +941,25 @@
             google.ai.generativelanguage_v1beta3.types.CountMessageTokensResponse:
                 A response from CountMessageTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a discuss_service.CountMessageTokensRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, discuss_service.CountMessageTokensRequest):
             request = discuss_service.CountMessageTokensRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/discuss_service/transports/grpc_asyncio.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,55 +9,101 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers
-import google.auth  # type: ignore
+from google.api_core import exceptions as core_exceptions
+from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
+from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta3.types import discuss_service
+from google.ai.generativelanguage_v1beta.types import discuss_service
 
 from .base import DEFAULT_CLIENT_INFO, DiscussServiceTransport
+from .grpc import DiscussServiceGrpcTransport
 
 
-class DiscussServiceGrpcTransport(DiscussServiceTransport):
-    """gRPC backend transport for DiscussService.
+class DiscussServiceGrpcAsyncIOTransport(DiscussServiceTransport):
+    """gRPC AsyncIO backend transport for DiscussService.
 
     An API for using Generative Language Models (GLMs) in dialog
     applications.
     Also known as large language models (LLMs), this API provides
     models that are trained for multi-turn dialog.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _stubs: Dict[str, Callable]
+    _grpc_channel: aio.Channel
+    _stubs: Dict[str, Callable] = {}
+
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> aio.Channel:
+        """Create and return a gRPC AsyncIO channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            aio.Channel: A gRPC AsyncIO channel object.
+        """
+
+        return grpc_helpers_async.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -69,68 +115,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
+            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
-
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -157,15 +206,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -176,128 +227,115 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> grpc.Channel:
-        """Create and return a gRPC channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is mutually exclusive with credentials.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            grpc.Channel: A gRPC channel object.
+    @property
+    def grpc_channel(self) -> aio.Channel:
+        """Create the channel designed to connect to this service.
 
-        Raises:
-            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
-              and ``credentials_file`` are passed.
+        This property caches on the instance; repeated calls return
+        the same channel.
         """
-
-        return grpc_helpers.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
-
-    @property
-    def grpc_channel(self) -> grpc.Channel:
-        """Return the channel designed to connect to this service."""
+        # Return the channel from cache.
         return self._grpc_channel
 
     @property
     def generate_message(
         self,
     ) -> Callable[
         [discuss_service.GenerateMessageRequest],
-        discuss_service.GenerateMessageResponse,
+        Awaitable[discuss_service.GenerateMessageResponse],
     ]:
         r"""Return a callable for the generate message method over gRPC.
 
         Generates a response from the model given an input
         ``MessagePrompt``.
 
         Returns:
             Callable[[~.GenerateMessageRequest],
-                    ~.GenerateMessageResponse]:
+                    Awaitable[~.GenerateMessageResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "generate_message" not in self._stubs:
             self._stubs["generate_message"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.DiscussService/GenerateMessage",
+                "/google.ai.generativelanguage.v1beta.DiscussService/GenerateMessage",
                 request_serializer=discuss_service.GenerateMessageRequest.serialize,
                 response_deserializer=discuss_service.GenerateMessageResponse.deserialize,
             )
         return self._stubs["generate_message"]
 
     @property
     def count_message_tokens(
         self,
     ) -> Callable[
         [discuss_service.CountMessageTokensRequest],
-        discuss_service.CountMessageTokensResponse,
+        Awaitable[discuss_service.CountMessageTokensResponse],
     ]:
         r"""Return a callable for the count message tokens method over gRPC.
 
         Runs a model's tokenizer on a string and returns the
         token count.
 
         Returns:
             Callable[[~.CountMessageTokensRequest],
-                    ~.CountMessageTokensResponse]:
+                    Awaitable[~.CountMessageTokensResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "count_message_tokens" not in self._stubs:
             self._stubs["count_message_tokens"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.DiscussService/CountMessageTokens",
+                "/google.ai.generativelanguage.v1beta.DiscussService/CountMessageTokens",
                 request_serializer=discuss_service.CountMessageTokensRequest.serialize,
                 response_deserializer=discuss_service.CountMessageTokensResponse.deserialize,
             )
         return self._stubs["count_message_tokens"]
 
-    def close(self):
-        self.grpc_channel.close()
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.generate_message: gapic_v1.method_async.wrap_method(
+                self.generate_message,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.count_message_tokens: gapic_v1.method_async.wrap_method(
+                self.count_message_tokens,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+        }
 
-    @property
-    def kind(self) -> str:
-        return "grpc"
+    def close(self):
+        return self.grpc_channel.close()
 
 
-__all__ = ("DiscussServiceGrpcTransport",)
+__all__ = ("DiscussServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta2/services/discuss_service/transports/grpc_asyncio.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,22 +12,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
+from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
-from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
 from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta3.types import discuss_service
+from google.ai.generativelanguage_v1beta2.types import discuss_service
 
 from .base import DEFAULT_CLIENT_INFO, DiscussServiceTransport
 from .grpc import DiscussServiceGrpcTransport
 
 
 class DiscussServiceGrpcAsyncIOTransport(DiscussServiceTransport):
     """gRPC AsyncIO backend transport for DiscussService.
@@ -64,15 +65,14 @@
             credentials (Optional[~.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify this application to the service. If
                 none are specified, the client will attempt to ascertain
                 the credentials from the environment.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             kwargs (Optional[dict]): Keyword arguments, which are passed to the
                 channel creation.
@@ -94,15 +94,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -114,37 +114,40 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -162,15 +165,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
         else:
             if api_mtls_endpoint:
@@ -202,15 +205,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -255,15 +260,15 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "generate_message" not in self._stubs:
             self._stubs["generate_message"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.DiscussService/GenerateMessage",
+                "/google.ai.generativelanguage.v1beta2.DiscussService/GenerateMessage",
                 request_serializer=discuss_service.GenerateMessageRequest.serialize,
                 response_deserializer=discuss_service.GenerateMessageResponse.deserialize,
             )
         return self._stubs["generate_message"]
 
     @property
     def count_message_tokens(
@@ -285,18 +290,51 @@
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
         if "count_message_tokens" not in self._stubs:
             self._stubs["count_message_tokens"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.DiscussService/CountMessageTokens",
+                "/google.ai.generativelanguage.v1beta2.DiscussService/CountMessageTokens",
                 request_serializer=discuss_service.CountMessageTokensRequest.serialize,
                 response_deserializer=discuss_service.CountMessageTokensResponse.deserialize,
             )
         return self._stubs["count_message_tokens"]
 
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.generate_message: gapic_v1.method_async.wrap_method(
+                self.generate_message,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.count_message_tokens: gapic_v1.method_async.wrap_method(
+                self.count_message_tokens,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+        }
+
     def close(self):
         return self.grpc_channel.close()
 
 
 __all__ = ("DiscussServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/discuss_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/async_client.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -198,29 +199,33 @@
         type(ModelServiceClient).get_transport_class, type(ModelServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, ModelServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, ModelServiceTransport, Callable[..., ModelServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the model service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.ModelServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,ModelServiceTransport,Callable[..., ModelServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the ModelServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -321,37 +326,38 @@
         Returns:
             google.ai.generativelanguage_v1beta3.types.Model:
                 Information about a Generative
                 Language Model.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.GetModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.GetModelRequest):
+            request = model_service.GetModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_model,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -447,39 +453,40 @@
                 Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.ListModelsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.ListModelsRequest):
+            request = model_service.ListModelsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if page_size is not None:
             request.page_size = page_size
         if page_token is not None:
             request.page_token = page_token
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_models,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_models
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -558,37 +565,38 @@
         Returns:
             google.ai.generativelanguage_v1beta3.types.TunedModel:
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.GetTunedModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.GetTunedModelRequest):
+            request = model_service.GetTunedModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_tuned_model,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_tuned_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -685,39 +693,40 @@
                 list of Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.ListTunedModelsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.ListTunedModelsRequest):
+            request = model_service.ListTunedModelsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if page_size is not None:
             request.page_size = page_size
         if page_token is not None:
             request.page_token = page_token
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_tuned_models,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_tuned_models
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -820,39 +829,40 @@
                 The result type for the operation will be
                 :class:`google.ai.generativelanguage_v1beta3.types.TunedModel`
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([tuned_model, tuned_model_id])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.CreateTunedModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.CreateTunedModelRequest):
+            request = model_service.CreateTunedModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if tuned_model is not None:
             request.tuned_model = tuned_model
         if tuned_model_id is not None:
             request.tuned_model_id = tuned_model_id
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.create_tuned_model,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.create_tuned_model
+        ]
 
         # Validate the universe domain.
         self._client._validate_universe_domain()
 
         # Send the request.
         response = await rpc(
             request,
@@ -938,39 +948,40 @@
         Returns:
             google.ai.generativelanguage_v1beta3.types.TunedModel:
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([tuned_model, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.UpdateTunedModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.UpdateTunedModelRequest):
+            request = model_service.UpdateTunedModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if tuned_model is not None:
             request.tuned_model = tuned_model
         if update_mask is not None:
             request.update_mask = update_mask
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.update_tuned_model,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.update_tuned_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata(
                 (("tuned_model.name", request.tuned_model.name),)
             ),
@@ -1037,37 +1048,38 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = model_service.DeleteTunedModelRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, model_service.DeleteTunedModelRequest):
+            request = model_service.DeleteTunedModelRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.delete_tuned_model,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.delete_tuned_model
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/model_service/client.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -36,31 +37,31 @@
 from google.api_core import retry as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.exceptions import MutualTLSChannelError  # type: ignore
 from google.auth.transport import mtls  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.oauth2 import service_account  # type: ignore
 
-from google.ai.generativelanguage_v1beta3 import gapic_version as package_version
+from google.ai.generativelanguage_v1beta import gapic_version as package_version
 
 try:
     OptionalRetry = Union[retries.Retry, gapic_v1.method._MethodDefault, None]
 except AttributeError:  # pragma: NO COVER
     OptionalRetry = Union[retries.Retry, object, None]  # type: ignore
 
 from google.api_core import operation  # type: ignore
 from google.api_core import operation_async  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import field_mask_pb2  # type: ignore
 from google.protobuf import timestamp_pb2  # type: ignore
 
-from google.ai.generativelanguage_v1beta3.services.model_service import pagers
-from google.ai.generativelanguage_v1beta3.types import tuned_model as gag_tuned_model
-from google.ai.generativelanguage_v1beta3.types import model, model_service
-from google.ai.generativelanguage_v1beta3.types import tuned_model
+from google.ai.generativelanguage_v1beta.services.model_service import pagers
+from google.ai.generativelanguage_v1beta.types import tuned_model as gag_tuned_model
+from google.ai.generativelanguage_v1beta.types import model, model_service
+from google.ai.generativelanguage_v1beta.types import tuned_model
 
 from .transports.base import DEFAULT_CLIENT_INFO, ModelServiceTransport
 from .transports.grpc import ModelServiceGrpcTransport
 from .transports.grpc_asyncio import ModelServiceGrpcAsyncIOTransport
 from .transports.rest import ModelServiceRestTransport
 
 
@@ -541,29 +542,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, ModelServiceTransport]] = None,
+        transport: Optional[
+            Union[str, ModelServiceTransport, Callable[..., ModelServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the model service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ModelServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,ModelServiceTransport,Callable[..., ModelServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the ModelServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -661,16 +666,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[ModelServiceTransport], Callable[..., ModelServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., ModelServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -694,33 +706,33 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             def sample_get_model():
                 # Create a client
-                client = generativelanguage_v1beta3.ModelServiceClient()
+                client = generativelanguage_v1beta.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.GetModelRequest(
+                request = generativelanguage_v1beta.GetModelRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 response = client.get_model(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta3.types.GetModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta.types.GetModelRequest, dict]):
                 The request object. Request for getting information about
                 a specific Model.
             name (str):
                 Required. The resource name of the model.
 
                 This name should match a model name returned by the
                 ``ListModels`` method.
@@ -733,33 +745,31 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.Model:
+            google.ai.generativelanguage_v1beta.types.Model:
                 Information about a Generative
                 Language Model.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.GetModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.GetModelRequest):
             request = model_service.GetModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -804,33 +814,33 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             def sample_list_models():
                 # Create a client
-                client = generativelanguage_v1beta3.ModelServiceClient()
+                client = generativelanguage_v1beta.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.ListModelsRequest(
+                request = generativelanguage_v1beta.ListModelsRequest(
                 )
 
                 # Make the request
                 page_result = client.list_models(request=request)
 
                 # Handle the response
                 for response in page_result:
                     print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta3.types.ListModelsRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta.types.ListModelsRequest, dict]):
                 The request object. Request for listing all Models.
             page_size (int):
                 The maximum number of ``Models`` to return (per page).
 
                 The service may return fewer models. If unspecified, at
                 most 50 models will be returned per page. This method
                 returns at most 1000 models per page, even if you pass a
@@ -856,36 +866,34 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.services.model_service.pagers.ListModelsPager:
+            google.ai.generativelanguage_v1beta.services.model_service.pagers.ListModelsPager:
                 Response from ListModel containing a paginated list of
                 Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.ListModelsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.ListModelsRequest):
             request = model_service.ListModelsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if page_size is not None:
                 request.page_size = page_size
             if page_token is not None:
@@ -934,33 +942,33 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             def sample_get_tuned_model():
                 # Create a client
-                client = generativelanguage_v1beta3.ModelServiceClient()
+                client = generativelanguage_v1beta.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.GetTunedModelRequest(
+                request = generativelanguage_v1beta.GetTunedModelRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 response = client.get_tuned_model(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta3.types.GetTunedModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta.types.GetTunedModelRequest, dict]):
                 The request object. Request for getting information about
                 a specific Model.
             name (str):
                 Required. The resource name of the model.
 
                 Format: ``tunedModels/my-model-id``
 
@@ -970,33 +978,31 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.TunedModel:
+            google.ai.generativelanguage_v1beta.types.TunedModel:
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.GetTunedModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.GetTunedModelRequest):
             request = model_service.GetTunedModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -1041,33 +1047,33 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             def sample_list_tuned_models():
                 # Create a client
-                client = generativelanguage_v1beta3.ModelServiceClient()
+                client = generativelanguage_v1beta.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.ListTunedModelsRequest(
+                request = generativelanguage_v1beta.ListTunedModelsRequest(
                 )
 
                 # Make the request
                 page_result = client.list_tuned_models(request=request)
 
                 # Handle the response
                 for response in page_result:
                     print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta3.types.ListTunedModelsRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta.types.ListTunedModelsRequest, dict]):
                 The request object. Request for listing TunedModels.
             page_size (int):
                 Optional. The maximum number of ``TunedModels`` to
                 return (per page). The service may return fewer tuned
                 models.
 
                 If unspecified, at most 10 tuned models will be
@@ -1094,36 +1100,34 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.services.model_service.pagers.ListTunedModelsPager:
+            google.ai.generativelanguage_v1beta.services.model_service.pagers.ListTunedModelsPager:
                 Response from ListTunedModels containing a paginated
                 list of Models.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([page_size, page_token])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.ListTunedModelsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.ListTunedModelsRequest):
             request = model_service.ListTunedModelsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if page_size is not None:
                 request.page_size = page_size
             if page_token is not None:
@@ -1178,43 +1182,43 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             def sample_create_tuned_model():
                 # Create a client
-                client = generativelanguage_v1beta3.ModelServiceClient()
+                client = generativelanguage_v1beta.ModelServiceClient()
 
                 # Initialize request argument(s)
-                tuned_model = generativelanguage_v1beta3.TunedModel()
+                tuned_model = generativelanguage_v1beta.TunedModel()
                 tuned_model.tuning_task.training_data.examples.examples.text_input = "text_input_value"
                 tuned_model.tuning_task.training_data.examples.examples.output = "output_value"
 
-                request = generativelanguage_v1beta3.CreateTunedModelRequest(
+                request = generativelanguage_v1beta.CreateTunedModelRequest(
                     tuned_model=tuned_model,
                 )
 
                 # Make the request
                 operation = client.create_tuned_model(request=request)
 
                 print("Waiting for operation to complete...")
 
                 response = operation.result()
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta3.types.CreateTunedModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta.types.CreateTunedModelRequest, dict]):
                 The request object. Request to create a TunedModel.
-            tuned_model (google.ai.generativelanguage_v1beta3.types.TunedModel):
+            tuned_model (google.ai.generativelanguage_v1beta.types.TunedModel):
                 Required. The tuned model to create.
                 This corresponds to the ``tuned_model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             tuned_model_id (str):
                 Optional. The unique id for the tuned model if
                 specified. This value should be up to 40 characters, the
@@ -1232,33 +1236,31 @@
                 sent along with the request as metadata.
 
         Returns:
             google.api_core.operation.Operation:
                 An object representing a long-running operation.
 
                 The result type for the operation will be
-                :class:`google.ai.generativelanguage_v1beta3.types.TunedModel`
+                :class:`google.ai.generativelanguage_v1beta.types.TunedModel`
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([tuned_model, tuned_model_id])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.CreateTunedModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.CreateTunedModelRequest):
             request = model_service.CreateTunedModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if tuned_model is not None:
                 request.tuned_model = tuned_model
             if tuned_model_id is not None:
@@ -1307,39 +1309,39 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             def sample_update_tuned_model():
                 # Create a client
-                client = generativelanguage_v1beta3.ModelServiceClient()
+                client = generativelanguage_v1beta.ModelServiceClient()
 
                 # Initialize request argument(s)
-                tuned_model = generativelanguage_v1beta3.TunedModel()
+                tuned_model = generativelanguage_v1beta.TunedModel()
                 tuned_model.tuning_task.training_data.examples.examples.text_input = "text_input_value"
                 tuned_model.tuning_task.training_data.examples.examples.output = "output_value"
 
-                request = generativelanguage_v1beta3.UpdateTunedModelRequest(
+                request = generativelanguage_v1beta.UpdateTunedModelRequest(
                     tuned_model=tuned_model,
                 )
 
                 # Make the request
                 response = client.update_tuned_model(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta3.types.UpdateTunedModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta.types.UpdateTunedModelRequest, dict]):
                 The request object. Request to update a TunedModel.
-            tuned_model (google.ai.generativelanguage_v1beta3.types.TunedModel):
+            tuned_model (google.ai.generativelanguage_v1beta.types.TunedModel):
                 Required. The tuned model to update.
                 This corresponds to the ``tuned_model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             update_mask (google.protobuf.field_mask_pb2.FieldMask):
                 Required. The list of fields to
                 update.
@@ -1350,33 +1352,31 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.TunedModel:
+            google.ai.generativelanguage_v1beta.types.TunedModel:
                 A fine-tuned model created using
                 ModelService.CreateTunedModel.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([tuned_model, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.UpdateTunedModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.UpdateTunedModelRequest):
             request = model_service.UpdateTunedModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if tuned_model is not None:
                 request.tuned_model = tuned_model
             if update_mask is not None:
@@ -1424,30 +1424,30 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             def sample_delete_tuned_model():
                 # Create a client
-                client = generativelanguage_v1beta3.ModelServiceClient()
+                client = generativelanguage_v1beta.ModelServiceClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.DeleteTunedModelRequest(
+                request = generativelanguage_v1beta.DeleteTunedModelRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 client.delete_tuned_model(request=request)
 
         Args:
-            request (Union[google.ai.generativelanguage_v1beta3.types.DeleteTunedModelRequest, dict]):
+            request (Union[google.ai.generativelanguage_v1beta.types.DeleteTunedModelRequest, dict]):
                 The request object. Request to delete a TunedModel.
             name (str):
                 Required. The resource name of the model. Format:
                 ``tunedModels/my-model-id``
 
                 This corresponds to the ``name`` field
                 on the ``request`` instance; if ``request`` is provided, this
@@ -1455,27 +1455,25 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a model_service.DeleteTunedModelRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, model_service.DeleteTunedModelRequest):
             request = model_service.DeleteTunedModelRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/pagers.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/pagers.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/transports/grpc_asyncio.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,56 +9,101 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers, operations_v1
-import google.auth  # type: ignore
+from google.api_core import exceptions as core_exceptions
+from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
-from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
+from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta3.types import tuned_model as gag_tuned_model
-from google.ai.generativelanguage_v1beta3.types import model, model_service
-from google.ai.generativelanguage_v1beta3.types import tuned_model
+from google.ai.generativelanguage_v1beta.types import text_service
 
-from .base import DEFAULT_CLIENT_INFO, ModelServiceTransport
+from .base import DEFAULT_CLIENT_INFO, TextServiceTransport
+from .grpc import TextServiceGrpcTransport
 
 
-class ModelServiceGrpcTransport(ModelServiceTransport):
-    """gRPC backend transport for ModelService.
+class TextServiceGrpcAsyncIOTransport(TextServiceTransport):
+    """gRPC AsyncIO backend transport for TextService.
 
-    Provides methods for getting metadata information about
-    Generative Models.
+    API for using Generative Language Models (GLMs) trained to
+    generate text.
+    Also known as Large Language Models (LLM)s, these generate text
+    given an input prompt from the user.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _stubs: Dict[str, Callable]
+    _grpc_channel: aio.Channel
+    _stubs: Dict[str, Callable] = {}
+
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> aio.Channel:
+        """Create and return a gRPC AsyncIO channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            aio.Channel: A gRPC AsyncIO channel object.
+        """
+
+        return grpc_helpers_async.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -70,69 +115,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
+            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
-        self._operations_client: Optional[operations_v1.OperationsClient] = None
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
-
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -159,15 +206,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -178,269 +227,201 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> grpc.Channel:
-        """Create and return a gRPC channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is mutually exclusive with credentials.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            grpc.Channel: A gRPC channel object.
-
-        Raises:
-            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
-              and ``credentials_file`` are passed.
-        """
-
-        return grpc_helpers.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
-
-    @property
-    def grpc_channel(self) -> grpc.Channel:
-        """Return the channel designed to connect to this service."""
-        return self._grpc_channel
-
-    @property
-    def operations_client(self) -> operations_v1.OperationsClient:
-        """Create the client designed to process long-running operations.
-
-        This property caches on the instance; repeated calls return the same
-        client.
-        """
-        # Quick check: Only create a new client if we do not already have one.
-        if self._operations_client is None:
-            self._operations_client = operations_v1.OperationsClient(self.grpc_channel)
-
-        # Return the client from cache.
-        return self._operations_client
-
-    @property
-    def get_model(self) -> Callable[[model_service.GetModelRequest], model.Model]:
-        r"""Return a callable for the get model method over gRPC.
-
-        Gets information about a specific Model.
-
-        Returns:
-            Callable[[~.GetModelRequest],
-                    ~.Model]:
-                A function that, when called, will call the underlying RPC
-                on the server.
-        """
-        # Generate a "stub function" on-the-fly which will actually make
-        # the request.
-        # gRPC handles serialization and deserialization, so we just need
-        # to pass in the functions for each.
-        if "get_model" not in self._stubs:
-            self._stubs["get_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/GetModel",
-                request_serializer=model_service.GetModelRequest.serialize,
-                response_deserializer=model.Model.deserialize,
-            )
-        return self._stubs["get_model"]
-
     @property
-    def list_models(
-        self,
-    ) -> Callable[[model_service.ListModelsRequest], model_service.ListModelsResponse]:
-        r"""Return a callable for the list models method over gRPC.
-
-        Lists models available through the API.
+    def grpc_channel(self) -> aio.Channel:
+        """Create the channel designed to connect to this service.
 
-        Returns:
-            Callable[[~.ListModelsRequest],
-                    ~.ListModelsResponse]:
-                A function that, when called, will call the underlying RPC
-                on the server.
+        This property caches on the instance; repeated calls return
+        the same channel.
         """
-        # Generate a "stub function" on-the-fly which will actually make
-        # the request.
-        # gRPC handles serialization and deserialization, so we just need
-        # to pass in the functions for each.
-        if "list_models" not in self._stubs:
-            self._stubs["list_models"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/ListModels",
-                request_serializer=model_service.ListModelsRequest.serialize,
-                response_deserializer=model_service.ListModelsResponse.deserialize,
-            )
-        return self._stubs["list_models"]
+        # Return the channel from cache.
+        return self._grpc_channel
 
     @property
-    def get_tuned_model(
+    def generate_text(
         self,
-    ) -> Callable[[model_service.GetTunedModelRequest], tuned_model.TunedModel]:
-        r"""Return a callable for the get tuned model method over gRPC.
+    ) -> Callable[
+        [text_service.GenerateTextRequest], Awaitable[text_service.GenerateTextResponse]
+    ]:
+        r"""Return a callable for the generate text method over gRPC.
 
-        Gets information about a specific TunedModel.
+        Generates a response from the model given an input
+        message.
 
         Returns:
-            Callable[[~.GetTunedModelRequest],
-                    ~.TunedModel]:
+            Callable[[~.GenerateTextRequest],
+                    Awaitable[~.GenerateTextResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "get_tuned_model" not in self._stubs:
-            self._stubs["get_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/GetTunedModel",
-                request_serializer=model_service.GetTunedModelRequest.serialize,
-                response_deserializer=tuned_model.TunedModel.deserialize,
+        if "generate_text" not in self._stubs:
+            self._stubs["generate_text"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.TextService/GenerateText",
+                request_serializer=text_service.GenerateTextRequest.serialize,
+                response_deserializer=text_service.GenerateTextResponse.deserialize,
             )
-        return self._stubs["get_tuned_model"]
+        return self._stubs["generate_text"]
 
     @property
-    def list_tuned_models(
+    def embed_text(
         self,
     ) -> Callable[
-        [model_service.ListTunedModelsRequest], model_service.ListTunedModelsResponse
+        [text_service.EmbedTextRequest], Awaitable[text_service.EmbedTextResponse]
     ]:
-        r"""Return a callable for the list tuned models method over gRPC.
+        r"""Return a callable for the embed text method over gRPC.
 
-        Lists tuned models owned by the user.
+        Generates an embedding from the model given an input
+        message.
 
         Returns:
-            Callable[[~.ListTunedModelsRequest],
-                    ~.ListTunedModelsResponse]:
+            Callable[[~.EmbedTextRequest],
+                    Awaitable[~.EmbedTextResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "list_tuned_models" not in self._stubs:
-            self._stubs["list_tuned_models"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/ListTunedModels",
-                request_serializer=model_service.ListTunedModelsRequest.serialize,
-                response_deserializer=model_service.ListTunedModelsResponse.deserialize,
+        if "embed_text" not in self._stubs:
+            self._stubs["embed_text"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.TextService/EmbedText",
+                request_serializer=text_service.EmbedTextRequest.serialize,
+                response_deserializer=text_service.EmbedTextResponse.deserialize,
             )
-        return self._stubs["list_tuned_models"]
+        return self._stubs["embed_text"]
 
     @property
-    def create_tuned_model(
+    def batch_embed_text(
         self,
-    ) -> Callable[[model_service.CreateTunedModelRequest], operations_pb2.Operation]:
-        r"""Return a callable for the create tuned model method over gRPC.
-
-        Creates a tuned model. Intermediate tuning progress (if any) is
-        accessed through the [google.longrunning.Operations] service.
+    ) -> Callable[
+        [text_service.BatchEmbedTextRequest],
+        Awaitable[text_service.BatchEmbedTextResponse],
+    ]:
+        r"""Return a callable for the batch embed text method over gRPC.
 
-        Status and results can be accessed through the Operations
-        service. Example: GET
-        /v1/tunedModels/az2mb0bpw6i/operations/000-111-222
+        Generates multiple embeddings from the model given
+        input text in a synchronous call.
 
         Returns:
-            Callable[[~.CreateTunedModelRequest],
-                    ~.Operation]:
+            Callable[[~.BatchEmbedTextRequest],
+                    Awaitable[~.BatchEmbedTextResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "create_tuned_model" not in self._stubs:
-            self._stubs["create_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/CreateTunedModel",
-                request_serializer=model_service.CreateTunedModelRequest.serialize,
-                response_deserializer=operations_pb2.Operation.FromString,
+        if "batch_embed_text" not in self._stubs:
+            self._stubs["batch_embed_text"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.TextService/BatchEmbedText",
+                request_serializer=text_service.BatchEmbedTextRequest.serialize,
+                response_deserializer=text_service.BatchEmbedTextResponse.deserialize,
             )
-        return self._stubs["create_tuned_model"]
+        return self._stubs["batch_embed_text"]
 
     @property
-    def update_tuned_model(
+    def count_text_tokens(
         self,
-    ) -> Callable[[model_service.UpdateTunedModelRequest], gag_tuned_model.TunedModel]:
-        r"""Return a callable for the update tuned model method over gRPC.
+    ) -> Callable[
+        [text_service.CountTextTokensRequest],
+        Awaitable[text_service.CountTextTokensResponse],
+    ]:
+        r"""Return a callable for the count text tokens method over gRPC.
 
-        Updates a tuned model.
+        Runs a model's tokenizer on a text and returns the
+        token count.
 
         Returns:
-            Callable[[~.UpdateTunedModelRequest],
-                    ~.TunedModel]:
+            Callable[[~.CountTextTokensRequest],
+                    Awaitable[~.CountTextTokensResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "update_tuned_model" not in self._stubs:
-            self._stubs["update_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/UpdateTunedModel",
-                request_serializer=model_service.UpdateTunedModelRequest.serialize,
-                response_deserializer=gag_tuned_model.TunedModel.deserialize,
+        if "count_text_tokens" not in self._stubs:
+            self._stubs["count_text_tokens"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.TextService/CountTextTokens",
+                request_serializer=text_service.CountTextTokensRequest.serialize,
+                response_deserializer=text_service.CountTextTokensResponse.deserialize,
             )
-        return self._stubs["update_tuned_model"]
-
-    @property
-    def delete_tuned_model(
-        self,
-    ) -> Callable[[model_service.DeleteTunedModelRequest], empty_pb2.Empty]:
-        r"""Return a callable for the delete tuned model method over gRPC.
+        return self._stubs["count_text_tokens"]
 
-        Deletes a tuned model.
-
-        Returns:
-            Callable[[~.DeleteTunedModelRequest],
-                    ~.Empty]:
-                A function that, when called, will call the underlying RPC
-                on the server.
-        """
-        # Generate a "stub function" on-the-fly which will actually make
-        # the request.
-        # gRPC handles serialization and deserialization, so we just need
-        # to pass in the functions for each.
-        if "delete_tuned_model" not in self._stubs:
-            self._stubs["delete_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/DeleteTunedModel",
-                request_serializer=model_service.DeleteTunedModelRequest.serialize,
-                response_deserializer=empty_pb2.Empty.FromString,
-            )
-        return self._stubs["delete_tuned_model"]
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.generate_text: gapic_v1.method_async.wrap_method(
+                self.generate_text,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.embed_text: gapic_v1.method_async.wrap_method(
+                self.embed_text,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.batch_embed_text: gapic_v1.method_async.wrap_method(
+                self.batch_embed_text,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.count_text_tokens: gapic_v1.method_async.wrap_method(
+                self.count_text_tokens,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+        }
 
     def close(self):
-        self.grpc_channel.close()
-
-    @property
-    def kind(self) -> str:
-        return "grpc"
+        return self.grpc_channel.close()
 
 
-__all__ = ("ModelServiceGrpcTransport",)
+__all__ = ("TextServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/transports/grpc_asyncio.py`

 * *Files 14% similar despite different names*

```diff
@@ -12,35 +12,37 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers_async, operations_v1
+from google.api_core import exceptions as core_exceptions
+from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
 from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta3.types import tuned_model as gag_tuned_model
-from google.ai.generativelanguage_v1beta3.types import model, model_service
-from google.ai.generativelanguage_v1beta3.types import tuned_model
+from google.ai.generativelanguage_v1beta.types import permission as gag_permission
+from google.ai.generativelanguage_v1beta.types import permission
+from google.ai.generativelanguage_v1beta.types import permission_service
 
-from .base import DEFAULT_CLIENT_INFO, ModelServiceTransport
-from .grpc import ModelServiceGrpcTransport
+from .base import DEFAULT_CLIENT_INFO, PermissionServiceTransport
+from .grpc import PermissionServiceGrpcTransport
 
 
-class ModelServiceGrpcAsyncIOTransport(ModelServiceTransport):
-    """gRPC AsyncIO backend transport for ModelService.
+class PermissionServiceGrpcAsyncIOTransport(PermissionServiceTransport):
+    """gRPC AsyncIO backend transport for PermissionService.
 
-    Provides methods for getting metadata information about
-    Generative Models.
+    Provides methods for managing permissions to PaLM API
+    resources.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
@@ -65,15 +67,14 @@
             credentials (Optional[~.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify this application to the service. If
                 none are specified, the client will attempt to ascertain
                 the credentials from the environment.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             kwargs (Optional[dict]): Keyword arguments, which are passed to the
                 channel creation.
@@ -95,15 +96,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -115,37 +116,40 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -157,22 +161,21 @@
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
-        self._operations_client: Optional[operations_v1.OperationsAsyncClient] = None
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
         else:
             if api_mtls_endpoint:
@@ -204,15 +207,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -234,225 +239,266 @@
         This property caches on the instance; repeated calls return
         the same channel.
         """
         # Return the channel from cache.
         return self._grpc_channel
 
     @property
-    def operations_client(self) -> operations_v1.OperationsAsyncClient:
-        """Create the client designed to process long-running operations.
-
-        This property caches on the instance; repeated calls return the same
-        client.
-        """
-        # Quick check: Only create a new client if we do not already have one.
-        if self._operations_client is None:
-            self._operations_client = operations_v1.OperationsAsyncClient(
-                self.grpc_channel
-            )
-
-        # Return the client from cache.
-        return self._operations_client
-
-    @property
-    def get_model(
+    def create_permission(
         self,
-    ) -> Callable[[model_service.GetModelRequest], Awaitable[model.Model]]:
-        r"""Return a callable for the get model method over gRPC.
+    ) -> Callable[
+        [permission_service.CreatePermissionRequest],
+        Awaitable[gag_permission.Permission],
+    ]:
+        r"""Return a callable for the create permission method over gRPC.
 
-        Gets information about a specific Model.
+        Create a permission to a specific resource.
 
         Returns:
-            Callable[[~.GetModelRequest],
-                    Awaitable[~.Model]]:
+            Callable[[~.CreatePermissionRequest],
+                    Awaitable[~.Permission]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "get_model" not in self._stubs:
-            self._stubs["get_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/GetModel",
-                request_serializer=model_service.GetModelRequest.serialize,
-                response_deserializer=model.Model.deserialize,
+        if "create_permission" not in self._stubs:
+            self._stubs["create_permission"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.PermissionService/CreatePermission",
+                request_serializer=permission_service.CreatePermissionRequest.serialize,
+                response_deserializer=gag_permission.Permission.deserialize,
             )
-        return self._stubs["get_model"]
+        return self._stubs["create_permission"]
 
     @property
-    def list_models(
+    def get_permission(
         self,
     ) -> Callable[
-        [model_service.ListModelsRequest], Awaitable[model_service.ListModelsResponse]
+        [permission_service.GetPermissionRequest], Awaitable[permission.Permission]
     ]:
-        r"""Return a callable for the list models method over gRPC.
+        r"""Return a callable for the get permission method over gRPC.
 
-        Lists models available through the API.
+        Gets information about a specific Permission.
 
         Returns:
-            Callable[[~.ListModelsRequest],
-                    Awaitable[~.ListModelsResponse]]:
+            Callable[[~.GetPermissionRequest],
+                    Awaitable[~.Permission]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "list_models" not in self._stubs:
-            self._stubs["list_models"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/ListModels",
-                request_serializer=model_service.ListModelsRequest.serialize,
-                response_deserializer=model_service.ListModelsResponse.deserialize,
+        if "get_permission" not in self._stubs:
+            self._stubs["get_permission"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.PermissionService/GetPermission",
+                request_serializer=permission_service.GetPermissionRequest.serialize,
+                response_deserializer=permission.Permission.deserialize,
             )
-        return self._stubs["list_models"]
+        return self._stubs["get_permission"]
 
     @property
-    def get_tuned_model(
+    def list_permissions(
         self,
     ) -> Callable[
-        [model_service.GetTunedModelRequest], Awaitable[tuned_model.TunedModel]
+        [permission_service.ListPermissionsRequest],
+        Awaitable[permission_service.ListPermissionsResponse],
     ]:
-        r"""Return a callable for the get tuned model method over gRPC.
+        r"""Return a callable for the list permissions method over gRPC.
 
-        Gets information about a specific TunedModel.
+        Lists permissions for the specific resource.
 
         Returns:
-            Callable[[~.GetTunedModelRequest],
-                    Awaitable[~.TunedModel]]:
+            Callable[[~.ListPermissionsRequest],
+                    Awaitable[~.ListPermissionsResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "get_tuned_model" not in self._stubs:
-            self._stubs["get_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/GetTunedModel",
-                request_serializer=model_service.GetTunedModelRequest.serialize,
-                response_deserializer=tuned_model.TunedModel.deserialize,
+        if "list_permissions" not in self._stubs:
+            self._stubs["list_permissions"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.PermissionService/ListPermissions",
+                request_serializer=permission_service.ListPermissionsRequest.serialize,
+                response_deserializer=permission_service.ListPermissionsResponse.deserialize,
             )
-        return self._stubs["get_tuned_model"]
+        return self._stubs["list_permissions"]
 
     @property
-    def list_tuned_models(
+    def update_permission(
         self,
     ) -> Callable[
-        [model_service.ListTunedModelsRequest],
-        Awaitable[model_service.ListTunedModelsResponse],
+        [permission_service.UpdatePermissionRequest],
+        Awaitable[gag_permission.Permission],
     ]:
-        r"""Return a callable for the list tuned models method over gRPC.
+        r"""Return a callable for the update permission method over gRPC.
 
-        Lists tuned models owned by the user.
+        Updates the permission.
 
         Returns:
-            Callable[[~.ListTunedModelsRequest],
-                    Awaitable[~.ListTunedModelsResponse]]:
+            Callable[[~.UpdatePermissionRequest],
+                    Awaitable[~.Permission]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "list_tuned_models" not in self._stubs:
-            self._stubs["list_tuned_models"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/ListTunedModels",
-                request_serializer=model_service.ListTunedModelsRequest.serialize,
-                response_deserializer=model_service.ListTunedModelsResponse.deserialize,
+        if "update_permission" not in self._stubs:
+            self._stubs["update_permission"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.PermissionService/UpdatePermission",
+                request_serializer=permission_service.UpdatePermissionRequest.serialize,
+                response_deserializer=gag_permission.Permission.deserialize,
             )
-        return self._stubs["list_tuned_models"]
+        return self._stubs["update_permission"]
 
     @property
-    def create_tuned_model(
+    def delete_permission(
         self,
     ) -> Callable[
-        [model_service.CreateTunedModelRequest], Awaitable[operations_pb2.Operation]
+        [permission_service.DeletePermissionRequest], Awaitable[empty_pb2.Empty]
     ]:
-        r"""Return a callable for the create tuned model method over gRPC.
-
-        Creates a tuned model. Intermediate tuning progress (if any) is
-        accessed through the [google.longrunning.Operations] service.
+        r"""Return a callable for the delete permission method over gRPC.
 
-        Status and results can be accessed through the Operations
-        service. Example: GET
-        /v1/tunedModels/az2mb0bpw6i/operations/000-111-222
+        Deletes the permission.
 
         Returns:
-            Callable[[~.CreateTunedModelRequest],
-                    Awaitable[~.Operation]]:
+            Callable[[~.DeletePermissionRequest],
+                    Awaitable[~.Empty]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "create_tuned_model" not in self._stubs:
-            self._stubs["create_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/CreateTunedModel",
-                request_serializer=model_service.CreateTunedModelRequest.serialize,
-                response_deserializer=operations_pb2.Operation.FromString,
+        if "delete_permission" not in self._stubs:
+            self._stubs["delete_permission"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.PermissionService/DeletePermission",
+                request_serializer=permission_service.DeletePermissionRequest.serialize,
+                response_deserializer=empty_pb2.Empty.FromString,
             )
-        return self._stubs["create_tuned_model"]
+        return self._stubs["delete_permission"]
 
     @property
-    def update_tuned_model(
+    def transfer_ownership(
         self,
     ) -> Callable[
-        [model_service.UpdateTunedModelRequest], Awaitable[gag_tuned_model.TunedModel]
+        [permission_service.TransferOwnershipRequest],
+        Awaitable[permission_service.TransferOwnershipResponse],
     ]:
-        r"""Return a callable for the update tuned model method over gRPC.
+        r"""Return a callable for the transfer ownership method over gRPC.
 
-        Updates a tuned model.
+        Transfers ownership of the tuned model.
+        This is the only way to change ownership of the tuned
+        model. The current owner will be downgraded to writer
+        role.
 
         Returns:
-            Callable[[~.UpdateTunedModelRequest],
-                    Awaitable[~.TunedModel]]:
+            Callable[[~.TransferOwnershipRequest],
+                    Awaitable[~.TransferOwnershipResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "update_tuned_model" not in self._stubs:
-            self._stubs["update_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/UpdateTunedModel",
-                request_serializer=model_service.UpdateTunedModelRequest.serialize,
-                response_deserializer=gag_tuned_model.TunedModel.deserialize,
+        if "transfer_ownership" not in self._stubs:
+            self._stubs["transfer_ownership"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta.PermissionService/TransferOwnership",
+                request_serializer=permission_service.TransferOwnershipRequest.serialize,
+                response_deserializer=permission_service.TransferOwnershipResponse.deserialize,
             )
-        return self._stubs["update_tuned_model"]
-
-    @property
-    def delete_tuned_model(
-        self,
-    ) -> Callable[[model_service.DeleteTunedModelRequest], Awaitable[empty_pb2.Empty]]:
-        r"""Return a callable for the delete tuned model method over gRPC.
-
-        Deletes a tuned model.
+        return self._stubs["transfer_ownership"]
 
-        Returns:
-            Callable[[~.DeleteTunedModelRequest],
-                    Awaitable[~.Empty]]:
-                A function that, when called, will call the underlying RPC
-                on the server.
-        """
-        # Generate a "stub function" on-the-fly which will actually make
-        # the request.
-        # gRPC handles serialization and deserialization, so we just need
-        # to pass in the functions for each.
-        if "delete_tuned_model" not in self._stubs:
-            self._stubs["delete_tuned_model"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.ModelService/DeleteTunedModel",
-                request_serializer=model_service.DeleteTunedModelRequest.serialize,
-                response_deserializer=empty_pb2.Empty.FromString,
-            )
-        return self._stubs["delete_tuned_model"]
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.create_permission: gapic_v1.method_async.wrap_method(
+                self.create_permission,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.get_permission: gapic_v1.method_async.wrap_method(
+                self.get_permission,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.list_permissions: gapic_v1.method_async.wrap_method(
+                self.list_permissions,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.update_permission: gapic_v1.method_async.wrap_method(
+                self.update_permission,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.delete_permission: gapic_v1.method_async.wrap_method(
+                self.delete_permission,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.transfer_ownership: gapic_v1.method_async.wrap_method(
+                self.transfer_ownership,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+        }
 
     def close(self):
         return self.grpc_channel.close()
 
 
-__all__ = ("ModelServiceGrpcAsyncIOTransport",)
+__all__ = ("PermissionServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/model_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/model_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/permission_service/async_client.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -31,28 +32,28 @@
 from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1
 from google.api_core import retry_async as retries
 from google.api_core.client_options import ClientOptions
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.oauth2 import service_account  # type: ignore
 
-from google.ai.generativelanguage_v1beta3 import gapic_version as package_version
+from google.ai.generativelanguage_v1beta import gapic_version as package_version
 
 try:
     OptionalRetry = Union[retries.AsyncRetry, gapic_v1.method._MethodDefault, None]
 except AttributeError:  # pragma: NO COVER
     OptionalRetry = Union[retries.AsyncRetry, object, None]  # type: ignore
 
 from google.longrunning import operations_pb2  # type: ignore
 from google.protobuf import field_mask_pb2  # type: ignore
 
-from google.ai.generativelanguage_v1beta3.services.permission_service import pagers
-from google.ai.generativelanguage_v1beta3.types import permission as gag_permission
-from google.ai.generativelanguage_v1beta3.types import permission
-from google.ai.generativelanguage_v1beta3.types import permission_service
+from google.ai.generativelanguage_v1beta.services.permission_service import pagers
+from google.ai.generativelanguage_v1beta.types import permission as gag_permission
+from google.ai.generativelanguage_v1beta.types import permission
+from google.ai.generativelanguage_v1beta.types import permission_service
 
 from .client import PermissionServiceClient
 from .transports.base import DEFAULT_CLIENT_INFO, PermissionServiceTransport
 from .transports.grpc_asyncio import PermissionServiceGrpcAsyncIOTransport
 
 
 class PermissionServiceAsyncClient:
@@ -67,18 +68,14 @@
     DEFAULT_ENDPOINT = PermissionServiceClient.DEFAULT_ENDPOINT
     DEFAULT_MTLS_ENDPOINT = PermissionServiceClient.DEFAULT_MTLS_ENDPOINT
     _DEFAULT_ENDPOINT_TEMPLATE = PermissionServiceClient._DEFAULT_ENDPOINT_TEMPLATE
     _DEFAULT_UNIVERSE = PermissionServiceClient._DEFAULT_UNIVERSE
 
     permission_path = staticmethod(PermissionServiceClient.permission_path)
     parse_permission_path = staticmethod(PermissionServiceClient.parse_permission_path)
-    tuned_model_path = staticmethod(PermissionServiceClient.tuned_model_path)
-    parse_tuned_model_path = staticmethod(
-        PermissionServiceClient.parse_tuned_model_path
-    )
     common_billing_account_path = staticmethod(
         PermissionServiceClient.common_billing_account_path
     )
     parse_common_billing_account_path = staticmethod(
         PermissionServiceClient.parse_common_billing_account_path
     )
     common_folder_path = staticmethod(PermissionServiceClient.common_folder_path)
@@ -201,29 +198,37 @@
         type(PermissionServiceClient).get_transport_class, type(PermissionServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, PermissionServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[
+                str,
+                PermissionServiceTransport,
+                Callable[..., PermissionServiceTransport],
+            ]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the permission service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.PermissionServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,PermissionServiceTransport,Callable[..., PermissionServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the PermissionServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -282,105 +287,107 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_create_permission():
                 # Create a client
-                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.CreatePermissionRequest(
+                request = generativelanguage_v1beta.CreatePermissionRequest(
                     parent="parent_value",
                 )
 
                 # Make the request
                 response = await client.create_permission(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.CreatePermissionRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.CreatePermissionRequest, dict]]):
                 The request object. Request to create a ``Permission``.
             parent (:class:`str`):
                 Required. The parent resource of the ``Permission``.
-                Format: tunedModels/{tuned_model}
+                Formats: ``tunedModels/{tuned_model}``
+                ``corpora/{corpus}``
 
                 This corresponds to the ``parent`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
-            permission (:class:`google.ai.generativelanguage_v1beta3.types.Permission`):
+            permission (:class:`google.ai.generativelanguage_v1beta.types.Permission`):
                 Required. The permission to create.
                 This corresponds to the ``permission`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.Permission:
+            google.ai.generativelanguage_v1beta.types.Permission:
                 Permission resource grants user,
                 group or the rest of the world access to
                 the PaLM API resource (e.g. a tuned
-                model, file).
+                model, corpus).
 
                 A role is a collection of permitted
                 operations that allows users to perform
                 specific actions on PaLM API resources.
                 To make them available to users, groups,
                 or service accounts, you assign roles.
                 When you assign a role, you grant
                 permissions that the role contains.
 
                 There are three concentric roles. Each
                 role is a superset of the previous
                 role's permitted operations:
 
                 - reader can use the resource (e.g.
-                  tuned model) for inference
+                  tuned model, corpus) for inference
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent, permission])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.CreatePermissionRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.CreatePermissionRequest):
+            request = permission_service.CreatePermissionRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if parent is not None:
             request.parent = parent
         if permission is not None:
             request.permission = permission
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.create_permission,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.create_permission
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -414,101 +421,103 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_get_permission():
                 # Create a client
-                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.GetPermissionRequest(
+                request = generativelanguage_v1beta.GetPermissionRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 response = await client.get_permission(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.GetPermissionRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.GetPermissionRequest, dict]]):
                 The request object. Request for getting information about a specific
                 ``Permission``.
             name (:class:`str`):
                 Required. The resource name of the permission.
 
-                Format:
-                ``tunedModels/{tuned_model}permissions/{permission}``
+                Formats:
+                ``tunedModels/{tuned_model}/permissions/{permission}``
+                ``corpora/{corpus}/permissions/{permission}``
 
                 This corresponds to the ``name`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.Permission:
+            google.ai.generativelanguage_v1beta.types.Permission:
                 Permission resource grants user,
                 group or the rest of the world access to
                 the PaLM API resource (e.g. a tuned
-                model, file).
+                model, corpus).
 
                 A role is a collection of permitted
                 operations that allows users to perform
                 specific actions on PaLM API resources.
                 To make them available to users, groups,
                 or service accounts, you assign roles.
                 When you assign a role, you grant
                 permissions that the role contains.
 
                 There are three concentric roles. Each
                 role is a superset of the previous
                 role's permitted operations:
 
                 - reader can use the resource (e.g.
-                  tuned model) for inference
+                  tuned model, corpus) for inference
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.GetPermissionRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.GetPermissionRequest):
+            request = permission_service.GetPermissionRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.get_permission,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.get_permission
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -544,81 +553,83 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_list_permissions():
                 # Create a client
-                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.ListPermissionsRequest(
+                request = generativelanguage_v1beta.ListPermissionsRequest(
                     parent="parent_value",
                 )
 
                 # Make the request
                 page_result = client.list_permissions(request=request)
 
                 # Handle the response
                 async for response in page_result:
                     print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.ListPermissionsRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.ListPermissionsRequest, dict]]):
                 The request object. Request for listing permissions.
             parent (:class:`str`):
                 Required. The parent resource of the permissions.
-                Format: tunedModels/{tuned_model}
+                Formats: ``tunedModels/{tuned_model}``
+                ``corpora/{corpus}``
 
                 This corresponds to the ``parent`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.services.permission_service.pagers.ListPermissionsAsyncPager:
+            google.ai.generativelanguage_v1beta.services.permission_service.pagers.ListPermissionsAsyncPager:
                 Response from ListPermissions containing a paginated list of
                    permissions.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.ListPermissionsRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.ListPermissionsRequest):
+            request = permission_service.ListPermissionsRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if parent is not None:
             request.parent = parent
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.list_permissions,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.list_permissions
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("parent", request.parent),)),
         )
 
@@ -664,34 +675,34 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_update_permission():
                 # Create a client
-                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.UpdatePermissionRequest(
+                request = generativelanguage_v1beta.UpdatePermissionRequest(
                 )
 
                 # Make the request
                 response = await client.update_permission(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.UpdatePermissionRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.UpdatePermissionRequest, dict]]):
                 The request object. Request to update the ``Permission``.
-            permission (:class:`google.ai.generativelanguage_v1beta3.types.Permission`):
+            permission (:class:`google.ai.generativelanguage_v1beta.types.Permission`):
                 Required. The permission to update.
 
                 The permission's ``name`` field is used to identify the
                 permission to update.
 
                 This corresponds to the ``permission`` field
                 on the ``request`` instance; if ``request`` is provided, this
@@ -707,66 +718,67 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.Permission:
+            google.ai.generativelanguage_v1beta.types.Permission:
                 Permission resource grants user,
                 group or the rest of the world access to
                 the PaLM API resource (e.g. a tuned
-                model, file).
+                model, corpus).
 
                 A role is a collection of permitted
                 operations that allows users to perform
                 specific actions on PaLM API resources.
                 To make them available to users, groups,
                 or service accounts, you assign roles.
                 When you assign a role, you grant
                 permissions that the role contains.
 
                 There are three concentric roles. Each
                 role is a superset of the previous
                 role's permitted operations:
 
                 - reader can use the resource (e.g.
-                  tuned model) for inference
+                  tuned model, corpus) for inference
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([permission, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.UpdatePermissionRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.UpdatePermissionRequest):
+            request = permission_service.UpdatePermissionRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if permission is not None:
             request.permission = permission
         if update_mask is not None:
             request.update_mask = update_mask
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.update_permission,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.update_permission
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata(
                 (("permission.name", request.permission.name),)
             ),
@@ -804,68 +816,70 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_delete_permission():
                 # Create a client
-                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.DeletePermissionRequest(
+                request = generativelanguage_v1beta.DeletePermissionRequest(
                     name="name_value",
                 )
 
                 # Make the request
                 await client.delete_permission(request=request)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.DeletePermissionRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.DeletePermissionRequest, dict]]):
                 The request object. Request to delete the ``Permission``.
             name (:class:`str`):
-                Required. The resource name of the permission. Format:
+                Required. The resource name of the permission. Formats:
                 ``tunedModels/{tuned_model}/permissions/{permission}``
+                ``corpora/{corpus}/permissions/{permission}``
 
                 This corresponds to the ``name`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = permission_service.DeletePermissionRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.DeletePermissionRequest):
+            request = permission_service.DeletePermissionRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if name is not None:
             request.name = name
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.delete_permission,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.delete_permission
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
 
@@ -900,56 +914,57 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_transfer_ownership():
                 # Create a client
-                client = generativelanguage_v1beta3.PermissionServiceAsyncClient()
+                client = generativelanguage_v1beta.PermissionServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.TransferOwnershipRequest(
+                request = generativelanguage_v1beta.TransferOwnershipRequest(
                     name="name_value",
                     email_address="email_address_value",
                 )
 
                 # Make the request
                 response = await client.transfer_ownership(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.TransferOwnershipRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.TransferOwnershipRequest, dict]]):
                 The request object. Request to transfer the ownership of
                 the tuned model.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.TransferOwnershipResponse:
+            google.ai.generativelanguage_v1beta.types.TransferOwnershipResponse:
                 Response from TransferOwnership.
         """
         # Create or coerce a protobuf request object.
-        request = permission_service.TransferOwnershipRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, permission_service.TransferOwnershipRequest):
+            request = permission_service.TransferOwnershipRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.transfer_ownership,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.transfer_ownership
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("name", request.name),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/client.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -544,29 +545,37 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, PermissionServiceTransport]] = None,
+        transport: Optional[
+            Union[
+                str,
+                PermissionServiceTransport,
+                Callable[..., PermissionServiceTransport],
+            ]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the permission service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, PermissionServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,PermissionServiceTransport,Callable[..., PermissionServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the PermissionServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -667,16 +676,24 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[PermissionServiceTransport],
+                Callable[..., PermissionServiceTransport],
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., PermissionServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -769,27 +786,25 @@
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent, permission])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.CreatePermissionRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.CreatePermissionRequest):
             request = permission_service.CreatePermissionRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if parent is not None:
                 request.parent = parent
             if permission is not None:
@@ -899,27 +914,25 @@
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.GetPermissionRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.GetPermissionRequest):
             request = permission_service.GetPermissionRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -1009,27 +1022,25 @@
                    permissions.
 
                 Iterating over this object will yield results and
                 resolve additional pages automatically.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([parent])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.ListPermissionsRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.ListPermissionsRequest):
             request = permission_service.ListPermissionsRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if parent is not None:
                 request.parent = parent
 
@@ -1155,27 +1166,25 @@
                 - writer has reader's permissions and
                   additionally can edit and share
                 - owner has writer's permissions and
                   additionally can delete
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([permission, update_mask])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.UpdatePermissionRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.UpdatePermissionRequest):
             request = permission_service.UpdatePermissionRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if permission is not None:
                 request.permission = permission
             if update_mask is not None:
@@ -1256,27 +1265,25 @@
             retry (google.api_core.retry.Retry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([name])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.DeletePermissionRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.DeletePermissionRequest):
             request = permission_service.DeletePermissionRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if name is not None:
                 request.name = name
 
@@ -1354,18 +1361,16 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta3.types.TransferOwnershipResponse:
                 Response from TransferOwnership.
         """
         # Create or coerce a protobuf request object.
-        # Minor optimization to avoid making a copy if the user passes
-        # in a permission_service.TransferOwnershipRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, permission_service.TransferOwnershipRequest):
             request = permission_service.TransferOwnershipRequest(request)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
         rpc = self._transport._wrapped_methods[self._transport.transfer_ownership]
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/pagers.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/pagers.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/grpc.py`

 * *Files 3% similar despite different names*

```diff
@@ -50,15 +50,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -70,36 +70,39 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                ignored if a ``channel`` instance is provided.
+            channel (Optional[Union[grpc.Channel, Callable[..., grpc.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -117,15 +120,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, grpc.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
 
         else:
@@ -158,15 +161,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/grpc_asyncio.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/grpc_asyncio.py`

 * *Files 18% similar despite different names*

```diff
@@ -12,35 +12,36 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
+from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
-from google.protobuf import empty_pb2  # type: ignore
 import grpc  # type: ignore
 from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta3.types import permission as gag_permission
-from google.ai.generativelanguage_v1beta3.types import permission
-from google.ai.generativelanguage_v1beta3.types import permission_service
+from google.ai.generativelanguage_v1beta3.types import text_service
 
-from .base import DEFAULT_CLIENT_INFO, PermissionServiceTransport
-from .grpc import PermissionServiceGrpcTransport
+from .base import DEFAULT_CLIENT_INFO, TextServiceTransport
+from .grpc import TextServiceGrpcTransport
 
 
-class PermissionServiceGrpcAsyncIOTransport(PermissionServiceTransport):
-    """gRPC AsyncIO backend transport for PermissionService.
+class TextServiceGrpcAsyncIOTransport(TextServiceTransport):
+    """gRPC AsyncIO backend transport for TextService.
 
-    Provides methods for managing permissions to PaLM API
-    resources.
+    API for using Generative Language Models (GLMs) trained to
+    generate text.
+    Also known as Large Language Models (LLM)s, these generate text
+    given an input prompt from the user.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
@@ -65,15 +66,14 @@
             credentials (Optional[~.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify this application to the service. If
                 none are specified, the client will attempt to ascertain
                 the credentials from the environment.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             kwargs (Optional[dict]): Keyword arguments, which are passed to the
                 channel creation.
@@ -95,15 +95,15 @@
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[aio.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -115,37 +115,40 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
                 service. These are only used when credentials are not specified and
                 are passed to :func:`google.auth.default`.
-            channel (Optional[aio.Channel]): A ``Channel`` instance through
-                which to make calls.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
@@ -163,15 +166,15 @@
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
         else:
             if api_mtls_endpoint:
@@ -203,15 +206,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -233,186 +238,154 @@
         This property caches on the instance; repeated calls return
         the same channel.
         """
         # Return the channel from cache.
         return self._grpc_channel
 
     @property
-    def create_permission(
+    def generate_text(
         self,
     ) -> Callable[
-        [permission_service.CreatePermissionRequest],
-        Awaitable[gag_permission.Permission],
+        [text_service.GenerateTextRequest], Awaitable[text_service.GenerateTextResponse]
     ]:
-        r"""Return a callable for the create permission method over gRPC.
+        r"""Return a callable for the generate text method over gRPC.
 
-        Create a permission to a specific resource.
+        Generates a response from the model given an input
+        message.
 
         Returns:
-            Callable[[~.CreatePermissionRequest],
-                    Awaitable[~.Permission]]:
+            Callable[[~.GenerateTextRequest],
+                    Awaitable[~.GenerateTextResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "create_permission" not in self._stubs:
-            self._stubs["create_permission"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.PermissionService/CreatePermission",
-                request_serializer=permission_service.CreatePermissionRequest.serialize,
-                response_deserializer=gag_permission.Permission.deserialize,
+        if "generate_text" not in self._stubs:
+            self._stubs["generate_text"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta3.TextService/GenerateText",
+                request_serializer=text_service.GenerateTextRequest.serialize,
+                response_deserializer=text_service.GenerateTextResponse.deserialize,
             )
-        return self._stubs["create_permission"]
+        return self._stubs["generate_text"]
 
     @property
-    def get_permission(
+    def embed_text(
         self,
     ) -> Callable[
-        [permission_service.GetPermissionRequest], Awaitable[permission.Permission]
+        [text_service.EmbedTextRequest], Awaitable[text_service.EmbedTextResponse]
     ]:
-        r"""Return a callable for the get permission method over gRPC.
+        r"""Return a callable for the embed text method over gRPC.
 
-        Gets information about a specific Permission.
+        Generates an embedding from the model given an input
+        message.
 
         Returns:
-            Callable[[~.GetPermissionRequest],
-                    Awaitable[~.Permission]]:
+            Callable[[~.EmbedTextRequest],
+                    Awaitable[~.EmbedTextResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "get_permission" not in self._stubs:
-            self._stubs["get_permission"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.PermissionService/GetPermission",
-                request_serializer=permission_service.GetPermissionRequest.serialize,
-                response_deserializer=permission.Permission.deserialize,
+        if "embed_text" not in self._stubs:
+            self._stubs["embed_text"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta3.TextService/EmbedText",
+                request_serializer=text_service.EmbedTextRequest.serialize,
+                response_deserializer=text_service.EmbedTextResponse.deserialize,
             )
-        return self._stubs["get_permission"]
+        return self._stubs["embed_text"]
 
     @property
-    def list_permissions(
+    def batch_embed_text(
         self,
     ) -> Callable[
-        [permission_service.ListPermissionsRequest],
-        Awaitable[permission_service.ListPermissionsResponse],
+        [text_service.BatchEmbedTextRequest],
+        Awaitable[text_service.BatchEmbedTextResponse],
     ]:
-        r"""Return a callable for the list permissions method over gRPC.
+        r"""Return a callable for the batch embed text method over gRPC.
 
-        Lists permissions for the specific resource.
+        Generates multiple embeddings from the model given
+        input text in a synchronous call.
 
         Returns:
-            Callable[[~.ListPermissionsRequest],
-                    Awaitable[~.ListPermissionsResponse]]:
+            Callable[[~.BatchEmbedTextRequest],
+                    Awaitable[~.BatchEmbedTextResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "list_permissions" not in self._stubs:
-            self._stubs["list_permissions"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.PermissionService/ListPermissions",
-                request_serializer=permission_service.ListPermissionsRequest.serialize,
-                response_deserializer=permission_service.ListPermissionsResponse.deserialize,
+        if "batch_embed_text" not in self._stubs:
+            self._stubs["batch_embed_text"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta3.TextService/BatchEmbedText",
+                request_serializer=text_service.BatchEmbedTextRequest.serialize,
+                response_deserializer=text_service.BatchEmbedTextResponse.deserialize,
             )
-        return self._stubs["list_permissions"]
+        return self._stubs["batch_embed_text"]
 
     @property
-    def update_permission(
+    def count_text_tokens(
         self,
     ) -> Callable[
-        [permission_service.UpdatePermissionRequest],
-        Awaitable[gag_permission.Permission],
+        [text_service.CountTextTokensRequest],
+        Awaitable[text_service.CountTextTokensResponse],
     ]:
-        r"""Return a callable for the update permission method over gRPC.
+        r"""Return a callable for the count text tokens method over gRPC.
 
-        Updates the permission.
+        Runs a model's tokenizer on a text and returns the
+        token count.
 
         Returns:
-            Callable[[~.UpdatePermissionRequest],
-                    Awaitable[~.Permission]]:
+            Callable[[~.CountTextTokensRequest],
+                    Awaitable[~.CountTextTokensResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "update_permission" not in self._stubs:
-            self._stubs["update_permission"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.PermissionService/UpdatePermission",
-                request_serializer=permission_service.UpdatePermissionRequest.serialize,
-                response_deserializer=gag_permission.Permission.deserialize,
+        if "count_text_tokens" not in self._stubs:
+            self._stubs["count_text_tokens"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1beta3.TextService/CountTextTokens",
+                request_serializer=text_service.CountTextTokensRequest.serialize,
+                response_deserializer=text_service.CountTextTokensResponse.deserialize,
             )
-        return self._stubs["update_permission"]
+        return self._stubs["count_text_tokens"]
 
-    @property
-    def delete_permission(
-        self,
-    ) -> Callable[
-        [permission_service.DeletePermissionRequest], Awaitable[empty_pb2.Empty]
-    ]:
-        r"""Return a callable for the delete permission method over gRPC.
-
-        Deletes the permission.
-
-        Returns:
-            Callable[[~.DeletePermissionRequest],
-                    Awaitable[~.Empty]]:
-                A function that, when called, will call the underlying RPC
-                on the server.
-        """
-        # Generate a "stub function" on-the-fly which will actually make
-        # the request.
-        # gRPC handles serialization and deserialization, so we just need
-        # to pass in the functions for each.
-        if "delete_permission" not in self._stubs:
-            self._stubs["delete_permission"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.PermissionService/DeletePermission",
-                request_serializer=permission_service.DeletePermissionRequest.serialize,
-                response_deserializer=empty_pb2.Empty.FromString,
-            )
-        return self._stubs["delete_permission"]
-
-    @property
-    def transfer_ownership(
-        self,
-    ) -> Callable[
-        [permission_service.TransferOwnershipRequest],
-        Awaitable[permission_service.TransferOwnershipResponse],
-    ]:
-        r"""Return a callable for the transfer ownership method over gRPC.
-
-        Transfers ownership of the tuned model.
-        This is the only way to change ownership of the tuned
-        model. The current owner will be downgraded to writer
-        role.
-
-        Returns:
-            Callable[[~.TransferOwnershipRequest],
-                    Awaitable[~.TransferOwnershipResponse]]:
-                A function that, when called, will call the underlying RPC
-                on the server.
-        """
-        # Generate a "stub function" on-the-fly which will actually make
-        # the request.
-        # gRPC handles serialization and deserialization, so we just need
-        # to pass in the functions for each.
-        if "transfer_ownership" not in self._stubs:
-            self._stubs["transfer_ownership"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.PermissionService/TransferOwnership",
-                request_serializer=permission_service.TransferOwnershipRequest.serialize,
-                response_deserializer=permission_service.TransferOwnershipResponse.deserialize,
-            )
-        return self._stubs["transfer_ownership"]
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.generate_text: gapic_v1.method_async.wrap_method(
+                self.generate_text,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.embed_text: gapic_v1.method_async.wrap_method(
+                self.embed_text,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.batch_embed_text: gapic_v1.method_async.wrap_method(
+                self.batch_embed_text,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+            self.count_text_tokens: gapic_v1.method_async.wrap_method(
+                self.count_text_tokens,
+                default_timeout=None,
+                client_info=client_info,
+            ),
+        }
 
     def close(self):
         return self.grpc_channel.close()
 
 
-__all__ = ("PermissionServiceGrpcAsyncIOTransport",)
+__all__ = ("TextServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/permission_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/permission_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/async_client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta/services/text_service/async_client.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import functools
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -31,24 +32,24 @@
 from google.api_core import exceptions as core_exceptions
 from google.api_core import gapic_v1
 from google.api_core import retry_async as retries
 from google.api_core.client_options import ClientOptions
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.oauth2 import service_account  # type: ignore
 
-from google.ai.generativelanguage_v1beta3 import gapic_version as package_version
+from google.ai.generativelanguage_v1beta import gapic_version as package_version
 
 try:
     OptionalRetry = Union[retries.AsyncRetry, gapic_v1.method._MethodDefault, None]
 except AttributeError:  # pragma: NO COVER
     OptionalRetry = Union[retries.AsyncRetry, object, None]  # type: ignore
 
 from google.longrunning import operations_pb2  # type: ignore
 
-from google.ai.generativelanguage_v1beta3.types import safety, text_service
+from google.ai.generativelanguage_v1beta.types import safety, text_service
 
 from .client import TextServiceClient
 from .transports.base import DEFAULT_CLIENT_INFO, TextServiceTransport
 from .transports.grpc_asyncio import TextServiceGrpcAsyncIOTransport
 
 
 class TextServiceAsyncClient:
@@ -191,29 +192,33 @@
         type(TextServiceClient).get_transport_class, type(TextServiceClient)
     )
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Union[str, TextServiceTransport] = "grpc_asyncio",
+        transport: Optional[
+            Union[str, TextServiceTransport, Callable[..., TextServiceTransport]]
+        ] = "grpc_asyncio",
         client_options: Optional[ClientOptions] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the text service async client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, ~.TextServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,TextServiceTransport,Callable[..., TextServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport to use.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the TextServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -276,49 +281,49 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_generate_text():
                 # Create a client
-                client = generativelanguage_v1beta3.TextServiceAsyncClient()
+                client = generativelanguage_v1beta.TextServiceAsyncClient()
 
                 # Initialize request argument(s)
-                prompt = generativelanguage_v1beta3.TextPrompt()
+                prompt = generativelanguage_v1beta.TextPrompt()
                 prompt.text = "text_value"
 
-                request = generativelanguage_v1beta3.GenerateTextRequest(
+                request = generativelanguage_v1beta.GenerateTextRequest(
                     model="model_value",
                     prompt=prompt,
                 )
 
                 # Make the request
                 response = await client.generate_text(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.GenerateTextRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.GenerateTextRequest, dict]]):
                 The request object. Request to generate a text completion
                 response from the model.
             model (:class:`str`):
                 Required. The name of the ``Model`` or ``TunedModel`` to
                 use for generating the completion. Examples:
                 models/text-bison-001
                 tunedModels/sentence-translator-u3b7m
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
-            prompt (:class:`google.ai.generativelanguage_v1beta3.types.TextPrompt`):
+            prompt (:class:`google.ai.generativelanguage_v1beta.types.TextPrompt`):
                 Required. The free-form input text
                 given to the model as a prompt.
                 Given a prompt, the model will generate
                 a TextCompletion response it predicts as
                 the completion of the input text.
 
                 This corresponds to the ``prompt`` field
@@ -396,22 +401,22 @@
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.GenerateTextResponse:
+            google.ai.generativelanguage_v1beta.types.GenerateTextResponse:
                 The response from the model,
                 including candidate completions.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [
                 model,
                 prompt,
                 temperature,
                 candidate_count,
                 max_output_tokens,
@@ -421,15 +426,18 @@
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.GenerateTextRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.GenerateTextRequest):
+            request = text_service.GenerateTextRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
@@ -442,19 +450,17 @@
         if top_p is not None:
             request.top_p = top_p
         if top_k is not None:
             request.top_k = top_k
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.generate_text,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.generate_text
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -490,87 +496,87 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_embed_text():
                 # Create a client
-                client = generativelanguage_v1beta3.TextServiceAsyncClient()
+                client = generativelanguage_v1beta.TextServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.EmbedTextRequest(
+                request = generativelanguage_v1beta.EmbedTextRequest(
                     model="model_value",
-                    text="text_value",
                 )
 
                 # Make the request
                 response = await client.embed_text(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.EmbedTextRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.EmbedTextRequest, dict]]):
                 The request object. Request to get a text embedding from
                 the model.
             model (:class:`str`):
                 Required. The model name to use with
                 the format model=models/{model}.
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             text (:class:`str`):
-                Required. The free-form input text
+                Optional. The free-form input text
                 that the model will turn into an
                 embedding.
 
                 This corresponds to the ``text`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.EmbedTextResponse:
+            google.ai.generativelanguage_v1beta.types.EmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, text])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.EmbedTextRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.EmbedTextRequest):
+            request = text_service.EmbedTextRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if text is not None:
             request.text = text
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.embed_text,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.embed_text
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -606,90 +612,90 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_batch_embed_text():
                 # Create a client
-                client = generativelanguage_v1beta3.TextServiceAsyncClient()
+                client = generativelanguage_v1beta.TextServiceAsyncClient()
 
                 # Initialize request argument(s)
-                request = generativelanguage_v1beta3.BatchEmbedTextRequest(
+                request = generativelanguage_v1beta.BatchEmbedTextRequest(
                     model="model_value",
-                    texts=['texts_value1', 'texts_value2'],
                 )
 
                 # Make the request
                 response = await client.batch_embed_text(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.BatchEmbedTextRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.BatchEmbedTextRequest, dict]]):
                 The request object. Batch request to get a text embedding
                 from the model.
             model (:class:`str`):
                 Required. The name of the ``Model`` to use for
                 generating the embedding. Examples:
                 models/embedding-gecko-001
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             texts (:class:`MutableSequence[str]`):
-                Required. The free-form input texts
+                Optional. The free-form input texts
                 that the model will turn into an
-                embedding.  The current limit is 100
+                embedding. The current limit is 100
                 texts, over which an error will be
                 thrown.
 
                 This corresponds to the ``texts`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.BatchEmbedTextResponse:
+            google.ai.generativelanguage_v1beta.types.BatchEmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, texts])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.BatchEmbedTextRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.BatchEmbedTextRequest):
+            request = text_service.BatchEmbedTextRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if texts:
             request.texts.extend(texts)
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.batch_embed_text,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.batch_embed_text
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
 
@@ -725,37 +731,37 @@
             # This snippet has been automatically generated and should be regarded as a
             # code template only.
             # It will require modifications to work:
             # - It may require correct/in-range values for request initialization.
             # - It may require specifying regional endpoints when creating the service
             #   client as shown in:
             #   https://googleapis.dev/python/google-api-core/latest/client_options.html
-            from google.ai import generativelanguage_v1beta3
+            from google.ai import generativelanguage_v1beta
 
             async def sample_count_text_tokens():
                 # Create a client
-                client = generativelanguage_v1beta3.TextServiceAsyncClient()
+                client = generativelanguage_v1beta.TextServiceAsyncClient()
 
                 # Initialize request argument(s)
-                prompt = generativelanguage_v1beta3.TextPrompt()
+                prompt = generativelanguage_v1beta.TextPrompt()
                 prompt.text = "text_value"
 
-                request = generativelanguage_v1beta3.CountTextTokensRequest(
+                request = generativelanguage_v1beta.CountTextTokensRequest(
                     model="model_value",
                     prompt=prompt,
                 )
 
                 # Make the request
                 response = await client.count_text_tokens(request=request)
 
                 # Handle the response
                 print(response)
 
         Args:
-            request (Optional[Union[google.ai.generativelanguage_v1beta3.types.CountTextTokensRequest, dict]]):
+            request (Optional[Union[google.ai.generativelanguage_v1beta.types.CountTextTokensRequest, dict]]):
                 The request object. Counts the number of tokens in the ``prompt`` sent to a
                 model.
 
                 Models may tokenize text differently, so each model may
                 return a different ``token_count``.
             model (:class:`str`):
                 Required. The model's resource name. This serves as an
@@ -765,60 +771,61 @@
                 ``ListModels`` method.
 
                 Format: ``models/{model}``
 
                 This corresponds to the ``model`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
-            prompt (:class:`google.ai.generativelanguage_v1beta3.types.TextPrompt`):
+            prompt (:class:`google.ai.generativelanguage_v1beta.types.TextPrompt`):
                 Required. The free-form input text
                 given to the model as a prompt.
 
                 This corresponds to the ``prompt`` field
                 on the ``request`` instance; if ``request`` is provided, this
                 should not be set.
             retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,
                 should be retried.
             timeout (float): The timeout for this request.
             metadata (Sequence[Tuple[str, str]]): Strings which should be
                 sent along with the request as metadata.
 
         Returns:
-            google.ai.generativelanguage_v1beta3.types.CountTextTokensResponse:
+            google.ai.generativelanguage_v1beta.types.CountTextTokensResponse:
                 A response from CountTextTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        request = text_service.CountTextTokensRequest(request)
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
+        if not isinstance(request, text_service.CountTextTokensRequest):
+            request = text_service.CountTextTokensRequest(request)
 
         # If we have keyword arguments corresponding to fields on the
         # request, apply these.
         if model is not None:
             request.model = model
         if prompt is not None:
             request.prompt = prompt
 
         # Wrap the RPC method; this adds retry and timeout information,
         # and friendly error handling.
-        rpc = gapic_v1.method_async.wrap_method(
-            self._client._transport.count_text_tokens,
-            default_timeout=None,
-            client_info=DEFAULT_CLIENT_INFO,
-        )
+        rpc = self._client._transport._wrapped_methods[
+            self._client._transport.count_text_tokens
+        ]
 
         # Certain fields should be provided within the metadata header;
         # add these here.
         metadata = tuple(metadata) + (
             gapic_v1.routing_header.to_grpc_metadata((("model", request.model),)),
         )
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/client.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/client.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from collections import OrderedDict
 import os
 import re
 from typing import (
+    Callable,
     Dict,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Sequence,
     Tuple,
@@ -521,29 +522,33 @@
         """
         return self._universe_domain
 
     def __init__(
         self,
         *,
         credentials: Optional[ga_credentials.Credentials] = None,
-        transport: Optional[Union[str, TextServiceTransport]] = None,
+        transport: Optional[
+            Union[str, TextServiceTransport, Callable[..., TextServiceTransport]]
+        ] = None,
         client_options: Optional[Union[client_options_lib.ClientOptions, dict]] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
     ) -> None:
         """Instantiates the text service client.
 
         Args:
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-            transport (Union[str, TextServiceTransport]): The
-                transport to use. If set to None, a transport is chosen
-                automatically.
+            transport (Optional[Union[str,TextServiceTransport,Callable[..., TextServiceTransport]]]):
+                The transport to use, or a Callable that constructs and returns a new transport.
+                If a Callable is given, it will be called with the same set of initialization
+                arguments as used in the TextServiceTransport constructor.
+                If set to None, a transport is chosen automatically.
             client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):
                 Custom options for the client.
 
                 1. The ``api_endpoint`` property can be used to override the
                 default endpoint provided by the client when ``transport`` is
                 not explicitly provided. Only if this property is not set and
                 ``transport`` was not explicitly provided, the endpoint is
@@ -641,16 +646,23 @@
             if api_key_value and hasattr(
                 google.auth._default, "get_api_key_credentials"
             ):
                 credentials = google.auth._default.get_api_key_credentials(
                     api_key_value
                 )
 
-            Transport = type(self).get_transport_class(cast(str, transport))
-            self._transport = Transport(
+            transport_init: Union[
+                Type[TextServiceTransport], Callable[..., TextServiceTransport]
+            ] = (
+                type(self).get_transport_class(transport)
+                if isinstance(transport, str) or transport is None
+                else cast(Callable[..., TextServiceTransport], transport)
+            )
+            # initialize with the provided callable or the passed in class
+            self._transport = transport_init(
                 credentials=credentials,
                 credentials_file=self._client_options.credentials_file,
                 host=self._api_endpoint,
                 scopes=self._client_options.scopes,
                 client_cert_source_for_mtls=self._client_cert_source,
                 quota_project_id=self._client_options.quota_project_id,
                 client_info=client_info,
@@ -807,16 +819,16 @@
         Returns:
             google.ai.generativelanguage_v1beta3.types.GenerateTextResponse:
                 The response from the model,
                 including candidate completions.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any(
             [
                 model,
                 prompt,
                 temperature,
                 candidate_count,
                 max_output_tokens,
@@ -826,18 +838,16 @@
         )
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.GenerateTextRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.GenerateTextRequest):
             request = text_service.GenerateTextRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
@@ -943,27 +953,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta3.types.EmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, text])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.EmbedTextRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.EmbedTextRequest):
             request = text_service.EmbedTextRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if text is not None:
@@ -1062,27 +1070,25 @@
                 sent along with the request as metadata.
 
         Returns:
             google.ai.generativelanguage_v1beta3.types.BatchEmbedTextResponse:
                 The response to a EmbedTextRequest.
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, texts])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.BatchEmbedTextRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.BatchEmbedTextRequest):
             request = text_service.BatchEmbedTextRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if texts is not None:
@@ -1191,27 +1197,25 @@
             google.ai.generativelanguage_v1beta3.types.CountTextTokensResponse:
                 A response from CountTextTokens.
 
                    It returns the model's token_count for the prompt.
 
         """
         # Create or coerce a protobuf request object.
-        # Quick check: If we got a request object, we should *not* have
-        # gotten any keyword arguments that map to the request.
+        # - Quick check: If we got a request object, we should *not* have
+        #   gotten any keyword arguments that map to the request.
         has_flattened_params = any([model, prompt])
         if request is not None and has_flattened_params:
             raise ValueError(
                 "If the `request` argument is set, then none of "
                 "the individual field arguments should be set."
             )
 
-        # Minor optimization to avoid making a copy if the user passes
-        # in a text_service.CountTextTokensRequest.
-        # There's no risk of modifying the input as we've already verified
-        # there are no flattened fields.
+        # - Use the request object if provided (there's no risk of modifying the input as
+        #   there are no flattened fields), or create one.
         if not isinstance(request, text_service.CountTextTokensRequest):
             request = text_service.CountTextTokensRequest(request)
             # If we have keyword arguments corresponding to fields on the
             # request, apply these.
             if model is not None:
                 request.model = model
             if prompt is not None:
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/base.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/base.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/grpc.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1/services/generative_service/transports/grpc_asyncio.py`

 * *Files 26% similar despite different names*

```diff
@@ -9,55 +9,99 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from typing import Callable, Dict, Optional, Sequence, Tuple, Union
+from typing import Awaitable, Callable, Dict, Optional, Sequence, Tuple, Union
 import warnings
 
-from google.api_core import gapic_v1, grpc_helpers
-import google.auth  # type: ignore
+from google.api_core import exceptions as core_exceptions
+from google.api_core import gapic_v1, grpc_helpers_async
+from google.api_core import retry_async as retries
 from google.auth import credentials as ga_credentials  # type: ignore
 from google.auth.transport.grpc import SslCredentials  # type: ignore
 from google.longrunning import operations_pb2  # type: ignore
 import grpc  # type: ignore
+from grpc.experimental import aio  # type: ignore
 
-from google.ai.generativelanguage_v1beta3.types import text_service
+from google.ai.generativelanguage_v1.types import generative_service
 
-from .base import DEFAULT_CLIENT_INFO, TextServiceTransport
+from .base import DEFAULT_CLIENT_INFO, GenerativeServiceTransport
+from .grpc import GenerativeServiceGrpcTransport
 
 
-class TextServiceGrpcTransport(TextServiceTransport):
-    """gRPC backend transport for TextService.
+class GenerativeServiceGrpcAsyncIOTransport(GenerativeServiceTransport):
+    """gRPC AsyncIO backend transport for GenerativeService.
 
-    API for using Generative Language Models (GLMs) trained to
-    generate text.
-    Also known as Large Language Models (LLM)s, these generate text
-    given an input prompt from the user.
+    API for using Large Models that generate multimodal content
+    and have additional capabilities beyond text generation.
 
     This class defines the same methods as the primary client, so the
     primary client can load the underlying transport implementation
     and call it.
 
     It sends protocol buffers over the wire using gRPC (which is built on
     top of HTTP/2); the ``grpcio`` package must be installed.
     """
 
-    _stubs: Dict[str, Callable]
+    _grpc_channel: aio.Channel
+    _stubs: Dict[str, Callable] = {}
+
+    @classmethod
+    def create_channel(
+        cls,
+        host: str = "generativelanguage.googleapis.com",
+        credentials: Optional[ga_credentials.Credentials] = None,
+        credentials_file: Optional[str] = None,
+        scopes: Optional[Sequence[str]] = None,
+        quota_project_id: Optional[str] = None,
+        **kwargs,
+    ) -> aio.Channel:
+        """Create and return a gRPC AsyncIO channel object.
+        Args:
+            host (Optional[str]): The host for the channel to use.
+            credentials (Optional[~.Credentials]): The
+                authorization credentials to attach to requests. These
+                credentials identify this application to the service. If
+                none are specified, the client will attempt to ascertain
+                the credentials from the environment.
+            credentials_file (Optional[str]): A file with credentials that can
+                be loaded with :func:`google.auth.load_credentials_from_file`.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            quota_project_id (Optional[str]): An optional project to use for billing
+                and quota.
+            kwargs (Optional[dict]): Keyword arguments, which are passed to the
+                channel creation.
+        Returns:
+            aio.Channel: A gRPC AsyncIO channel object.
+        """
+
+        return grpc_helpers_async.create_channel(
+            host,
+            credentials=credentials,
+            credentials_file=credentials_file,
+            quota_project_id=quota_project_id,
+            default_scopes=cls.AUTH_SCOPES,
+            scopes=scopes,
+            default_host=cls.DEFAULT_HOST,
+            **kwargs,
+        )
 
     def __init__(
         self,
         *,
         host: str = "generativelanguage.googleapis.com",
         credentials: Optional[ga_credentials.Credentials] = None,
         credentials_file: Optional[str] = None,
         scopes: Optional[Sequence[str]] = None,
-        channel: Optional[grpc.Channel] = None,
+        channel: Optional[Union[aio.Channel, Callable[..., aio.Channel]]] = None,
         api_mtls_endpoint: Optional[str] = None,
         client_cert_source: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         ssl_channel_credentials: Optional[grpc.ChannelCredentials] = None,
         client_cert_source_for_mtls: Optional[Callable[[], Tuple[bytes, bytes]]] = None,
         quota_project_id: Optional[str] = None,
         client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,
         always_use_jwt_access: Optional[bool] = False,
@@ -69,68 +113,71 @@
             host (Optional[str]):
                  The hostname to connect to (default: 'generativelanguage.googleapis.com').
             credentials (Optional[google.auth.credentials.Credentials]): The
                 authorization credentials to attach to requests. These
                 credentials identify the application to the service; if none
                 are specified, the client will attempt to ascertain the
                 credentials from the environment.
-                This argument is ignored if ``channel`` is provided.
+                This argument is ignored if a ``channel`` instance is provided.
             credentials_file (Optional[str]): A file with credentials that can
                 be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is ignored if ``channel`` is provided.
-            scopes (Optional(Sequence[str])): A list of scopes. This argument is
-                ignored if ``channel`` is provided.
-            channel (Optional[grpc.Channel]): A ``Channel`` instance through
-                which to make calls.
+                This argument is ignored if a ``channel`` instance is provided.
+            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
+                service. These are only used when credentials are not specified and
+                are passed to :func:`google.auth.default`.
+            channel (Optional[Union[aio.Channel, Callable[..., aio.Channel]]]):
+                A ``Channel`` instance through which to make calls, or a Callable
+                that constructs and returns one. If set to None, ``self.create_channel``
+                is used to create the channel. If a Callable is given, it will be called
+                with the same arguments as used in ``self.create_channel``.
             api_mtls_endpoint (Optional[str]): Deprecated. The mutual TLS endpoint.
                 If provided, it overrides the ``host`` argument and tries to create
                 a mutual TLS channel with client SSL credentials from
                 ``client_cert_source`` or application default SSL credentials.
             client_cert_source (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 Deprecated. A callback to provide client SSL certificate bytes and
                 private key bytes, both in PEM format. It is ignored if
                 ``api_mtls_endpoint`` is None.
             ssl_channel_credentials (grpc.ChannelCredentials): SSL credentials
-                for the grpc channel. It is ignored if ``channel`` is provided.
+                for the grpc channel. It is ignored if a ``channel`` instance is provided.
             client_cert_source_for_mtls (Optional[Callable[[], Tuple[bytes, bytes]]]):
                 A callback to provide client certificate bytes and private key bytes,
                 both in PEM format. It is used to configure a mutual TLS channel. It is
-                ignored if ``channel`` or ``ssl_channel_credentials`` is provided.
+                ignored if a ``channel`` instance or ``ssl_channel_credentials`` is provided.
             quota_project_id (Optional[str]): An optional project to use for billing
                 and quota.
             client_info (google.api_core.gapic_v1.client_info.ClientInfo):
                 The client info used to send a user-agent string along with
                 API requests. If ``None``, then default info will be used.
                 Generally, you only need to set this if you're developing
                 your own client library.
             always_use_jwt_access (Optional[bool]): Whether self signed JWT should
                 be used for service account credentials.
 
         Raises:
-          google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport
+            google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport
               creation failed for any reason.
           google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
               and ``credentials_file`` are passed.
         """
         self._grpc_channel = None
         self._ssl_channel_credentials = ssl_channel_credentials
         self._stubs: Dict[str, Callable] = {}
 
         if api_mtls_endpoint:
             warnings.warn("api_mtls_endpoint is deprecated", DeprecationWarning)
         if client_cert_source:
             warnings.warn("client_cert_source is deprecated", DeprecationWarning)
 
-        if channel:
+        if isinstance(channel, aio.Channel):
             # Ignore credentials if a channel was passed.
             credentials = False
             # If a channel was explicitly provided, set it.
             self._grpc_channel = channel
             self._ssl_channel_credentials = None
-
         else:
             if api_mtls_endpoint:
                 host = api_mtls_endpoint
 
                 # Create SSL credentials with client_cert_source or application
                 # default SSL credentials.
                 if client_cert_source:
@@ -157,15 +204,17 @@
             quota_project_id=quota_project_id,
             client_info=client_info,
             always_use_jwt_access=always_use_jwt_access,
             api_audience=api_audience,
         )
 
         if not self._grpc_channel:
-            self._grpc_channel = type(self).create_channel(
+            # initialize with the provided callable or the default channel
+            channel_init = channel or type(self).create_channel
+            self._grpc_channel = channel_init(
                 self._host,
                 # use the credentials which are saved
                 credentials=self._credentials,
                 # Set ``credentials_file`` to ``None`` here as
                 # the credentials that we saved earlier should be used.
                 credentials_file=None,
                 scopes=self._scopes,
@@ -176,182 +225,300 @@
                     ("grpc.max_receive_message_length", -1),
                 ],
             )
 
         # Wrap messages. This must be done after self._grpc_channel exists
         self._prep_wrapped_messages(client_info)
 
-    @classmethod
-    def create_channel(
-        cls,
-        host: str = "generativelanguage.googleapis.com",
-        credentials: Optional[ga_credentials.Credentials] = None,
-        credentials_file: Optional[str] = None,
-        scopes: Optional[Sequence[str]] = None,
-        quota_project_id: Optional[str] = None,
-        **kwargs,
-    ) -> grpc.Channel:
-        """Create and return a gRPC channel object.
-        Args:
-            host (Optional[str]): The host for the channel to use.
-            credentials (Optional[~.Credentials]): The
-                authorization credentials to attach to requests. These
-                credentials identify this application to the service. If
-                none are specified, the client will attempt to ascertain
-                the credentials from the environment.
-            credentials_file (Optional[str]): A file with credentials that can
-                be loaded with :func:`google.auth.load_credentials_from_file`.
-                This argument is mutually exclusive with credentials.
-            scopes (Optional[Sequence[str]]): A optional list of scopes needed for this
-                service. These are only used when credentials are not specified and
-                are passed to :func:`google.auth.default`.
-            quota_project_id (Optional[str]): An optional project to use for billing
-                and quota.
-            kwargs (Optional[dict]): Keyword arguments, which are passed to the
-                channel creation.
-        Returns:
-            grpc.Channel: A gRPC channel object.
+    @property
+    def grpc_channel(self) -> aio.Channel:
+        """Create the channel designed to connect to this service.
 
-        Raises:
-            google.api_core.exceptions.DuplicateCredentialArgs: If both ``credentials``
-              and ``credentials_file`` are passed.
+        This property caches on the instance; repeated calls return
+        the same channel.
         """
-
-        return grpc_helpers.create_channel(
-            host,
-            credentials=credentials,
-            credentials_file=credentials_file,
-            quota_project_id=quota_project_id,
-            default_scopes=cls.AUTH_SCOPES,
-            scopes=scopes,
-            default_host=cls.DEFAULT_HOST,
-            **kwargs,
-        )
-
-    @property
-    def grpc_channel(self) -> grpc.Channel:
-        """Return the channel designed to connect to this service."""
+        # Return the channel from cache.
         return self._grpc_channel
 
     @property
-    def generate_text(
+    def generate_content(
         self,
     ) -> Callable[
-        [text_service.GenerateTextRequest], text_service.GenerateTextResponse
+        [generative_service.GenerateContentRequest],
+        Awaitable[generative_service.GenerateContentResponse],
     ]:
-        r"""Return a callable for the generate text method over gRPC.
+        r"""Return a callable for the generate content method over gRPC.
 
         Generates a response from the model given an input
-        message.
+        ``GenerateContentRequest``.
 
         Returns:
-            Callable[[~.GenerateTextRequest],
-                    ~.GenerateTextResponse]:
+            Callable[[~.GenerateContentRequest],
+                    Awaitable[~.GenerateContentResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "generate_text" not in self._stubs:
-            self._stubs["generate_text"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.TextService/GenerateText",
-                request_serializer=text_service.GenerateTextRequest.serialize,
-                response_deserializer=text_service.GenerateTextResponse.deserialize,
+        if "generate_content" not in self._stubs:
+            self._stubs["generate_content"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1.GenerativeService/GenerateContent",
+                request_serializer=generative_service.GenerateContentRequest.serialize,
+                response_deserializer=generative_service.GenerateContentResponse.deserialize,
             )
-        return self._stubs["generate_text"]
+        return self._stubs["generate_content"]
 
     @property
-    def embed_text(
+    def stream_generate_content(
         self,
-    ) -> Callable[[text_service.EmbedTextRequest], text_service.EmbedTextResponse]:
-        r"""Return a callable for the embed text method over gRPC.
+    ) -> Callable[
+        [generative_service.GenerateContentRequest],
+        Awaitable[generative_service.GenerateContentResponse],
+    ]:
+        r"""Return a callable for the stream generate content method over gRPC.
+
+        Generates a streamed response from the model given an input
+        ``GenerateContentRequest``.
+
+        Returns:
+            Callable[[~.GenerateContentRequest],
+                    Awaitable[~.GenerateContentResponse]]:
+                A function that, when called, will call the underlying RPC
+                on the server.
+        """
+        # Generate a "stub function" on-the-fly which will actually make
+        # the request.
+        # gRPC handles serialization and deserialization, so we just need
+        # to pass in the functions for each.
+        if "stream_generate_content" not in self._stubs:
+            self._stubs["stream_generate_content"] = self.grpc_channel.unary_stream(
+                "/google.ai.generativelanguage.v1.GenerativeService/StreamGenerateContent",
+                request_serializer=generative_service.GenerateContentRequest.serialize,
+                response_deserializer=generative_service.GenerateContentResponse.deserialize,
+            )
+        return self._stubs["stream_generate_content"]
+
+    @property
+    def embed_content(
+        self,
+    ) -> Callable[
+        [generative_service.EmbedContentRequest],
+        Awaitable[generative_service.EmbedContentResponse],
+    ]:
+        r"""Return a callable for the embed content method over gRPC.
 
         Generates an embedding from the model given an input
-        message.
+        ``Content``.
 
         Returns:
-            Callable[[~.EmbedTextRequest],
-                    ~.EmbedTextResponse]:
+            Callable[[~.EmbedContentRequest],
+                    Awaitable[~.EmbedContentResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "embed_text" not in self._stubs:
-            self._stubs["embed_text"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.TextService/EmbedText",
-                request_serializer=text_service.EmbedTextRequest.serialize,
-                response_deserializer=text_service.EmbedTextResponse.deserialize,
+        if "embed_content" not in self._stubs:
+            self._stubs["embed_content"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1.GenerativeService/EmbedContent",
+                request_serializer=generative_service.EmbedContentRequest.serialize,
+                response_deserializer=generative_service.EmbedContentResponse.deserialize,
             )
-        return self._stubs["embed_text"]
+        return self._stubs["embed_content"]
 
     @property
-    def batch_embed_text(
+    def batch_embed_contents(
         self,
     ) -> Callable[
-        [text_service.BatchEmbedTextRequest], text_service.BatchEmbedTextResponse
+        [generative_service.BatchEmbedContentsRequest],
+        Awaitable[generative_service.BatchEmbedContentsResponse],
     ]:
-        r"""Return a callable for the batch embed text method over gRPC.
+        r"""Return a callable for the batch embed contents method over gRPC.
 
         Generates multiple embeddings from the model given
         input text in a synchronous call.
 
         Returns:
-            Callable[[~.BatchEmbedTextRequest],
-                    ~.BatchEmbedTextResponse]:
+            Callable[[~.BatchEmbedContentsRequest],
+                    Awaitable[~.BatchEmbedContentsResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "batch_embed_text" not in self._stubs:
-            self._stubs["batch_embed_text"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.TextService/BatchEmbedText",
-                request_serializer=text_service.BatchEmbedTextRequest.serialize,
-                response_deserializer=text_service.BatchEmbedTextResponse.deserialize,
+        if "batch_embed_contents" not in self._stubs:
+            self._stubs["batch_embed_contents"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1.GenerativeService/BatchEmbedContents",
+                request_serializer=generative_service.BatchEmbedContentsRequest.serialize,
+                response_deserializer=generative_service.BatchEmbedContentsResponse.deserialize,
             )
-        return self._stubs["batch_embed_text"]
+        return self._stubs["batch_embed_contents"]
 
     @property
-    def count_text_tokens(
+    def count_tokens(
         self,
     ) -> Callable[
-        [text_service.CountTextTokensRequest], text_service.CountTextTokensResponse
+        [generative_service.CountTokensRequest],
+        Awaitable[generative_service.CountTokensResponse],
     ]:
-        r"""Return a callable for the count text tokens method over gRPC.
+        r"""Return a callable for the count tokens method over gRPC.
 
-        Runs a model's tokenizer on a text and returns the
-        token count.
+        Runs a model's tokenizer on input content and returns
+        the token count.
 
         Returns:
-            Callable[[~.CountTextTokensRequest],
-                    ~.CountTextTokensResponse]:
+            Callable[[~.CountTokensRequest],
+                    Awaitable[~.CountTokensResponse]]:
                 A function that, when called, will call the underlying RPC
                 on the server.
         """
         # Generate a "stub function" on-the-fly which will actually make
         # the request.
         # gRPC handles serialization and deserialization, so we just need
         # to pass in the functions for each.
-        if "count_text_tokens" not in self._stubs:
-            self._stubs["count_text_tokens"] = self.grpc_channel.unary_unary(
-                "/google.ai.generativelanguage.v1beta3.TextService/CountTextTokens",
-                request_serializer=text_service.CountTextTokensRequest.serialize,
-                response_deserializer=text_service.CountTextTokensResponse.deserialize,
+        if "count_tokens" not in self._stubs:
+            self._stubs["count_tokens"] = self.grpc_channel.unary_unary(
+                "/google.ai.generativelanguage.v1.GenerativeService/CountTokens",
+                request_serializer=generative_service.CountTokensRequest.serialize,
+                response_deserializer=generative_service.CountTokensResponse.deserialize,
             )
-        return self._stubs["count_text_tokens"]
+        return self._stubs["count_tokens"]
+
+    def _prep_wrapped_messages(self, client_info):
+        """Precompute the wrapped methods, overriding the base class method to use async wrappers."""
+        self._wrapped_methods = {
+            self.generate_content: gapic_v1.method_async.wrap_method(
+                self.generate_content,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.stream_generate_content: gapic_v1.method_async.wrap_method(
+                self.stream_generate_content,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.embed_content: gapic_v1.method_async.wrap_method(
+                self.embed_content,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.batch_embed_contents: gapic_v1.method_async.wrap_method(
+                self.batch_embed_contents,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+            self.count_tokens: gapic_v1.method_async.wrap_method(
+                self.count_tokens,
+                default_retry=retries.AsyncRetry(
+                    initial=1.0,
+                    maximum=10.0,
+                    multiplier=1.3,
+                    predicate=retries.if_exception_type(
+                        core_exceptions.ServiceUnavailable,
+                    ),
+                    deadline=60.0,
+                ),
+                default_timeout=60.0,
+                client_info=client_info,
+            ),
+        }
 
     def close(self):
-        self.grpc_channel.close()
+        return self.grpc_channel.close()
+
+    @property
+    def cancel_operation(
+        self,
+    ) -> Callable[[operations_pb2.CancelOperationRequest], None]:
+        r"""Return a callable for the cancel_operation method over gRPC."""
+        # Generate a "stub function" on-the-fly which will actually make
+        # the request.
+        # gRPC handles serialization and deserialization, so we just need
+        # to pass in the functions for each.
+        if "cancel_operation" not in self._stubs:
+            self._stubs["cancel_operation"] = self.grpc_channel.unary_unary(
+                "/google.longrunning.Operations/CancelOperation",
+                request_serializer=operations_pb2.CancelOperationRequest.SerializeToString,
+                response_deserializer=None,
+            )
+        return self._stubs["cancel_operation"]
 
     @property
-    def kind(self) -> str:
-        return "grpc"
+    def get_operation(
+        self,
+    ) -> Callable[[operations_pb2.GetOperationRequest], operations_pb2.Operation]:
+        r"""Return a callable for the get_operation method over gRPC."""
+        # Generate a "stub function" on-the-fly which will actually make
+        # the request.
+        # gRPC handles serialization and deserialization, so we just need
+        # to pass in the functions for each.
+        if "get_operation" not in self._stubs:
+            self._stubs["get_operation"] = self.grpc_channel.unary_unary(
+                "/google.longrunning.Operations/GetOperation",
+                request_serializer=operations_pb2.GetOperationRequest.SerializeToString,
+                response_deserializer=operations_pb2.Operation.FromString,
+            )
+        return self._stubs["get_operation"]
+
+    @property
+    def list_operations(
+        self,
+    ) -> Callable[
+        [operations_pb2.ListOperationsRequest], operations_pb2.ListOperationsResponse
+    ]:
+        r"""Return a callable for the list_operations method over gRPC."""
+        # Generate a "stub function" on-the-fly which will actually make
+        # the request.
+        # gRPC handles serialization and deserialization, so we just need
+        # to pass in the functions for each.
+        if "list_operations" not in self._stubs:
+            self._stubs["list_operations"] = self.grpc_channel.unary_unary(
+                "/google.longrunning.Operations/ListOperations",
+                request_serializer=operations_pb2.ListOperationsRequest.SerializeToString,
+                response_deserializer=operations_pb2.ListOperationsResponse.FromString,
+            )
+        return self._stubs["list_operations"]
 
 
-__all__ = ("TextServiceGrpcTransport",)
+__all__ = ("GenerativeServiceGrpcAsyncIOTransport",)
```

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/services/text_service/transports/rest.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/services/text_service/transports/rest.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/__init__.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/citation.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/citation.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/discuss_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/discuss_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/model.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/model.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/model_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/model_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/permission.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/permission.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/permission_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/permission_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/safety.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/safety.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/text_service.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/text_service.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google/ai/generativelanguage_v1beta3/types/tuned_model.py` & `google-ai-generativelanguage-0.6.3/google/ai/generativelanguage_v1beta3/types/tuned_model.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/google_ai_generativelanguage.egg-info/PKG-INFO` & `google-ai-generativelanguage-0.6.3/google_ai_generativelanguage.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: google-ai-generativelanguage
-Version: 0.6.2
+Version: 0.6.3
 Summary: Google Ai Generativelanguage API client library
 Home-page: https://github.com/googleapis/google-cloud-python/tree/main/packages/google-ai-generativelanguage
 Author: Google LLC
 Author-email: googleapis-packages@google.com
 License: Apache 2.0
 Platform: Posix; MacOS X; Windows
 Classifier: Development Status :: 4 - Beta
```

### Comparing `google-ai-generativelanguage-0.6.2/google_ai_generativelanguage.egg-info/SOURCES.txt` & `google-ai-generativelanguage-0.6.3/google_ai_generativelanguage.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/setup.py` & `google-ai-generativelanguage-0.6.3/setup.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/tests/__init__.py` & `google-ai-generativelanguage-0.6.3/tests/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/__init__.py` & `google-ai-generativelanguage-0.6.3/tests/unit/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/__init__.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1/__init__.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1/test_generative_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1/test_generative_service.py`

 * *Files 4% similar despite different names*

```diff
@@ -1195,14 +1195,17 @@
     client = GenerativeServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_content), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest()
 
 
 def test_generate_content_non_empty_request_with_auto_populated_field():
@@ -1218,22 +1221,62 @@
     # if they meet the requirements of AIP 4235.
     request = generative_service.GenerateContentRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_content), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_content(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest(
             model="model_value",
         )
 
 
+def test_generate_content_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_content in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_content
+        ] = mock_rpc
+        request = {}
+        client.generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_generate_content_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1248,14 +1291,60 @@
         response = await client.generate_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest()
 
 
 @pytest.mark.asyncio
+async def test_generate_content_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.generate_content
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.generate_content
+        ] = mock_object
+
+        request = {}
+        await client.generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_generate_content_async(
     transport: str = "grpc_asyncio",
     request_type=generative_service.GenerateContentRequest,
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1485,14 +1574,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.stream_generate_content), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.stream_generate_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest()
 
 
 def test_stream_generate_content_non_empty_request_with_auto_populated_field():
@@ -1510,22 +1602,65 @@
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.stream_generate_content), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.stream_generate_content(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest(
             model="model_value",
         )
 
 
+def test_stream_generate_content_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.stream_generate_content
+            in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.stream_generate_content
+        ] = mock_rpc
+        request = {}
+        client.stream_generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.stream_generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_stream_generate_content_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1543,14 +1678,60 @@
         response = await client.stream_generate_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest()
 
 
 @pytest.mark.asyncio
+async def test_stream_generate_content_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.stream_generate_content
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.stream_generate_content
+        ] = mock_object
+
+        request = {}
+        await client.stream_generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.stream_generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_stream_generate_content_async(
     transport: str = "grpc_asyncio",
     request_type=generative_service.GenerateContentRequest,
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1786,14 +1967,17 @@
     client = GenerativeServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_content), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.EmbedContentRequest()
 
 
 def test_embed_content_non_empty_request_with_auto_populated_field():
@@ -1810,23 +1994,61 @@
     request = generative_service.EmbedContentRequest(
         model="model_value",
         title="title_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_content), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_content(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.EmbedContentRequest(
             model="model_value",
             title="title_value",
         )
 
 
+def test_embed_content_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_content in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_content] = mock_rpc
+        request = {}
+        client.embed_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_embed_content_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1841,14 +2063,60 @@
         response = await client.embed_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.EmbedContentRequest()
 
 
 @pytest.mark.asyncio
+async def test_embed_content_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.embed_content
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.embed_content
+        ] = mock_object
+
+        request = {}
+        await client.embed_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.embed_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_embed_content_async(
     transport: str = "grpc_asyncio", request_type=generative_service.EmbedContentRequest
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2076,14 +2344,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_embed_contents), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_embed_contents()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.BatchEmbedContentsRequest()
 
 
 def test_batch_embed_contents_non_empty_request_with_auto_populated_field():
@@ -2101,22 +2372,64 @@
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_embed_contents), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_embed_contents(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.BatchEmbedContentsRequest(
             model="model_value",
         )
 
 
+def test_batch_embed_contents_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_embed_contents in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_embed_contents
+        ] = mock_rpc
+        request = {}
+        client.batch_embed_contents(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_embed_contents(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_batch_embed_contents_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2133,14 +2446,60 @@
         response = await client.batch_embed_contents()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.BatchEmbedContentsRequest()
 
 
 @pytest.mark.asyncio
+async def test_batch_embed_contents_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.batch_embed_contents
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.batch_embed_contents
+        ] = mock_object
+
+        request = {}
+        await client.batch_embed_contents(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.batch_embed_contents(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_batch_embed_contents_async(
     transport: str = "grpc_asyncio",
     request_type=generative_service.BatchEmbedContentsRequest,
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -2378,14 +2737,17 @@
     client = GenerativeServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.count_tokens), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.CountTokensRequest()
 
 
 def test_count_tokens_non_empty_request_with_auto_populated_field():
@@ -2401,22 +2763,60 @@
     # if they meet the requirements of AIP 4235.
     request = generative_service.CountTokensRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.count_tokens), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_tokens(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.CountTokensRequest(
             model="model_value",
         )
 
 
+def test_count_tokens_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.count_tokens in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.count_tokens] = mock_rpc
+        request = {}
+        client.count_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_count_tokens_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2433,14 +2833,60 @@
         response = await client.count_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.CountTokensRequest()
 
 
 @pytest.mark.asyncio
+async def test_count_tokens_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.count_tokens
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.count_tokens
+        ] = mock_object
+
+        request = {}
+        await client.count_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.count_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_count_tokens_async(
     transport: str = "grpc_asyncio", request_type=generative_service.CountTokensRequest
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2661,14 +3107,52 @@
         req.return_value = response_value
         response = client.generate_content(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.GenerateContentResponse)
 
 
+def test_generate_content_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_content in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_content
+        ] = mock_rpc
+
+        request = {}
+        client.generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_generate_content_rest_required_fields(
     request_type=generative_service.GenerateContentRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -2941,14 +3425,55 @@
     assert isinstance(response, Iterable)
     response = next(response)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.GenerateContentResponse)
 
 
+def test_stream_generate_content_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.stream_generate_content
+            in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.stream_generate_content
+        ] = mock_rpc
+
+        request = {}
+        client.stream_generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.stream_generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_stream_generate_content_rest_required_fields(
     request_type=generative_service.GenerateContentRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -3222,14 +3747,50 @@
         req.return_value = response_value
         response = client.embed_content(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.EmbedContentResponse)
 
 
+def test_embed_content_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_content in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_content] = mock_rpc
+
+        request = {}
+        client.embed_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_embed_content_rest_required_fields(
     request_type=generative_service.EmbedContentRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -3495,14 +4056,54 @@
         req.return_value = response_value
         response = client.batch_embed_contents(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.BatchEmbedContentsResponse)
 
 
+def test_batch_embed_contents_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_embed_contents in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_embed_contents
+        ] = mock_rpc
+
+        request = {}
+        client.batch_embed_contents(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_embed_contents(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_batch_embed_contents_rest_required_fields(
     request_type=generative_service.BatchEmbedContentsRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -3776,14 +4377,50 @@
         response = client.count_tokens(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.CountTokensResponse)
     assert response.total_tokens == 1303
 
 
+def test_count_tokens_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.count_tokens in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.count_tokens] = mock_rpc
+
+        request = {}
+        client.count_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_count_tokens_rest_required_fields(
     request_type=generative_service.CountTokensRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1/test_model_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1/test_model_service.py`

 * *Files 4% similar despite different names*

```diff
@@ -1158,14 +1158,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest()
 
 
 def test_get_model_non_empty_request_with_auto_populated_field():
@@ -1181,22 +1184,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.GetModelRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest(
             name="name_value",
         )
 
 
+def test_get_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_model] = mock_rpc
+        request = {}
+        client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1223,14 +1264,58 @@
         response = await client.get_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_model_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_model
+        ] = mock_object
+
+        request = {}
+        await client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.GetModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1468,14 +1553,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_models), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest()
 
 
 def test_list_models_non_empty_request_with_auto_populated_field():
@@ -1491,22 +1579,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.ListModelsRequest(
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_models), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_models(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest(
             page_token="page_token_value",
         )
 
 
+def test_list_models_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_models] = mock_rpc
+        request = {}
+        client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_models_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1523,14 +1649,60 @@
         response = await client.list_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_models_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_models
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_models
+        ] = mock_object
+
+        request = {}
+        await client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_models_async(
     transport: str = "grpc_asyncio", request_type=model_service.ListModelsRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1902,14 +2074,50 @@
         "supported_generation_methods_value"
     ]
     assert math.isclose(response.temperature, 0.1198, rel_tol=1e-6)
     assert math.isclose(response.top_p, 0.546, rel_tol=1e-6)
     assert response.top_k == 541
 
 
+def test_get_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_model] = mock_rpc
+
+        request = {}
+        client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_model_rest_required_fields(request_type=model_service.GetModelRequest):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
     request = request_type(**request_init)
     pb_request = request_type.pb(request)
@@ -2161,14 +2369,50 @@
         response = client.list_models(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListModelsPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_models_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_models] = mock_rpc
+
+        request = {}
+        client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_list_models_rest_interceptors(null_interceptor):
     transport = transports.ModelServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
         if null_interceptor
         else transports.ModelServiceRestInterceptor(),
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/__init__.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_discuss_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta2/test_discuss_service.py`

 * *Files 5% similar despite different names*

```diff
@@ -28,31 +28,30 @@
 
 from google.api_core import gapic_v1, grpc_helpers, grpc_helpers_async, path_template
 from google.api_core import api_core_version, client_options
 from google.api_core import exceptions as core_exceptions
 import google.auth
 from google.auth import credentials as ga_credentials
 from google.auth.exceptions import MutualTLSChannelError
-from google.longrunning import operations_pb2  # type: ignore
 from google.oauth2 import service_account
 from google.protobuf import json_format
 import grpc
 from grpc.experimental import aio
 from proto.marshal.rules import wrappers
 from proto.marshal.rules.dates import DurationRule, TimestampRule
 import pytest
 from requests import PreparedRequest, Request, Response
 from requests.sessions import Session
 
-from google.ai.generativelanguage_v1beta.services.discuss_service import (
+from google.ai.generativelanguage_v1beta2.services.discuss_service import (
     DiscussServiceAsyncClient,
     DiscussServiceClient,
     transports,
 )
-from google.ai.generativelanguage_v1beta.types import citation, discuss_service, safety
+from google.ai.generativelanguage_v1beta2.types import citation, discuss_service, safety
 
 
 def client_cert_source_callback():
     return b"cert bytes", b"key bytes"
 
 
 # If default endpoint is localhost, then default mtls endpoint will be the same.
@@ -1024,15 +1023,15 @@
             always_use_jwt_access=True,
             api_audience=None,
         )
 
 
 def test_discuss_service_client_client_options_from_dict():
     with mock.patch(
-        "google.ai.generativelanguage_v1beta.services.discuss_service.transports.DiscussServiceGrpcTransport.__init__"
+        "google.ai.generativelanguage_v1beta2.services.discuss_service.transports.DiscussServiceGrpcTransport.__init__"
     ) as grpc_transport:
         grpc_transport.return_value = None
         client = DiscussServiceClient(
             client_options={"api_endpoint": "squid.clam.whelk"}
         )
         grpc_transport.assert_called_once_with(
             credentials=None,
@@ -1155,14 +1154,17 @@
     client = DiscussServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_message), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_message()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.GenerateMessageRequest()
 
 
 def test_generate_message_non_empty_request_with_auto_populated_field():
@@ -1178,22 +1180,62 @@
     # if they meet the requirements of AIP 4235.
     request = discuss_service.GenerateMessageRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_message), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_message(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.GenerateMessageRequest(
             model="model_value",
         )
 
 
+def test_generate_message_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_message in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_message
+        ] = mock_rpc
+        request = {}
+        client.generate_message(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_message(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_generate_message_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1208,14 +1250,60 @@
         response = await client.generate_message()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.GenerateMessageRequest()
 
 
 @pytest.mark.asyncio
+async def test_generate_message_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = DiscussServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.generate_message
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.generate_message
+        ] = mock_object
+
+        request = {}
+        await client.generate_message(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.generate_message(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_generate_message_async(
     transport: str = "grpc_asyncio", request_type=discuss_service.GenerateMessageRequest
 ):
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1478,14 +1566,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_message_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_message_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.CountMessageTokensRequest()
 
 
 def test_count_message_tokens_non_empty_request_with_auto_populated_field():
@@ -1503,22 +1594,64 @@
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_message_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_message_tokens(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.CountMessageTokensRequest(
             model="model_value",
         )
 
 
+def test_count_message_tokens_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.count_message_tokens in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_message_tokens
+        ] = mock_rpc
+        request = {}
+        client.count_message_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_message_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_count_message_tokens_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1537,14 +1670,60 @@
         response = await client.count_message_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.CountMessageTokensRequest()
 
 
 @pytest.mark.asyncio
+async def test_count_message_tokens_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = DiscussServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.count_message_tokens
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.count_message_tokens
+        ] = mock_object
+
+        request = {}
+        await client.count_message_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.count_message_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_count_message_tokens_async(
     transport: str = "grpc_asyncio",
     request_type=discuss_service.CountMessageTokensRequest,
 ):
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1776,14 +1955,52 @@
         req.return_value = response_value
         response = client.generate_message(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, discuss_service.GenerateMessageResponse)
 
 
+def test_generate_message_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_message in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_message
+        ] = mock_rpc
+
+        request = {}
+        client.generate_message(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_message(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_generate_message_rest_required_fields(
     request_type=discuss_service.GenerateMessageRequest,
 ):
     transport_class = transports.DiscussServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -1990,15 +2207,15 @@
         client.generate_message(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta/{model=models/*}:generateMessage" % client.transport._host,
+            "%s/v1beta2/{model=models/*}:generateMessage" % client.transport._host,
             args[1],
         )
 
 
 def test_generate_message_rest_flattened_error(transport: str = "rest"):
     client = DiscussServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -2061,14 +2278,54 @@
         response = client.count_message_tokens(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, discuss_service.CountMessageTokensResponse)
     assert response.token_count == 1193
 
 
+def test_count_message_tokens_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.count_message_tokens in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_message_tokens
+        ] = mock_rpc
+
+        request = {}
+        client.count_message_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_message_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_count_message_tokens_rest_required_fields(
     request_type=discuss_service.CountMessageTokensRequest,
 ):
     transport_class = transports.DiscussServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -2271,15 +2528,15 @@
         client.count_message_tokens(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta/{model=models/*}:countMessageTokens" % client.transport._host,
+            "%s/v1beta2/{model=models/*}:countMessageTokens" % client.transport._host,
             args[1],
         )
 
 
 def test_count_message_tokens_rest_flattened_error(transport: str = "rest"):
     client = DiscussServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -2427,15 +2684,15 @@
             credentials_file="credentials.json",
         )
 
 
 def test_discuss_service_base_transport():
     # Instantiate the base transport.
     with mock.patch(
-        "google.ai.generativelanguage_v1beta.services.discuss_service.transports.DiscussServiceTransport.__init__"
+        "google.ai.generativelanguage_v1beta2.services.discuss_service.transports.DiscussServiceTransport.__init__"
     ) as Transport:
         Transport.return_value = None
         transport = transports.DiscussServiceTransport(
             credentials=ga_credentials.AnonymousCredentials(),
         )
 
     # Every method on the transport should just blindly
@@ -2461,15 +2718,15 @@
 
 
 def test_discuss_service_base_transport_with_credentials_file():
     # Instantiate the base transport with a credentials file
     with mock.patch.object(
         google.auth, "load_credentials_from_file", autospec=True
     ) as load_creds, mock.patch(
-        "google.ai.generativelanguage_v1beta.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta2.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         load_creds.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.DiscussServiceTransport(
             credentials_file="credentials.json",
             quota_project_id="octopus",
         )
@@ -2480,15 +2737,15 @@
             quota_project_id="octopus",
         )
 
 
 def test_discuss_service_base_transport_with_adc():
     # Test the default credentials are used if credentials and credentials_file are None.
     with mock.patch.object(google.auth, "default", autospec=True) as adc, mock.patch(
-        "google.ai.generativelanguage_v1beta.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta2.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         adc.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.DiscussServiceTransport()
         adc.assert_called_once()
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_file_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_file_service.py`

 * *Files 4% similar despite different names*

```diff
@@ -1120,14 +1120,17 @@
     client = FileServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.create_file), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_file()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.CreateFileRequest()
 
 
 def test_create_file_non_empty_request_with_auto_populated_field():
@@ -1141,20 +1144,58 @@
     # Populate all string fields in the request which are not UUID4
     # since we want to check that UUID4 are populated automatically
     # if they meet the requirements of AIP 4235.
     request = file_service.CreateFileRequest()
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.create_file), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_file(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.CreateFileRequest()
 
 
+def test_create_file_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = FileServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_file in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.create_file] = mock_rpc
+        request = {}
+        client.create_file(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_file(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_create_file_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = FileServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1169,14 +1210,60 @@
         response = await client.create_file()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.CreateFileRequest()
 
 
 @pytest.mark.asyncio
+async def test_create_file_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = FileServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.create_file
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.create_file
+        ] = mock_object
+
+        request = {}
+        await client.create_file(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.create_file(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_create_file_async(
     transport: str = "grpc_asyncio", request_type=file_service.CreateFileRequest
 ):
     client = FileServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1250,14 +1337,17 @@
     client = FileServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_files), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_files()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.ListFilesRequest()
 
 
 def test_list_files_non_empty_request_with_auto_populated_field():
@@ -1273,22 +1363,60 @@
     # if they meet the requirements of AIP 4235.
     request = file_service.ListFilesRequest(
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_files), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_files(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.ListFilesRequest(
             page_token="page_token_value",
         )
 
 
+def test_list_files_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = FileServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_files in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_files] = mock_rpc
+        request = {}
+        client.list_files(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_files(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_files_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = FileServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1305,14 +1433,58 @@
         response = await client.list_files()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.ListFilesRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_files_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = FileServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_files
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_files
+        ] = mock_object
+
+        request = {}
+        await client.list_files(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_files(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_files_async(
     transport: str = "grpc_asyncio", request_type=file_service.ListFilesRequest
 ):
     client = FileServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1557,14 +1729,15 @@
         call.return_value = file.File(
             name="name_value",
             display_name="display_name_value",
             mime_type="mime_type_value",
             size_bytes=1089,
             sha256_hash=b"sha256_hash_blob",
             uri="uri_value",
+            state=file.File.State.PROCESSING,
         )
         response = client.get_file(request)
 
         # Establish that the underlying gRPC stub method was called.
         assert len(call.mock_calls) == 1
         _, args, _ = call.mock_calls[0]
         request = file_service.GetFileRequest()
@@ -1574,26 +1747,30 @@
     assert isinstance(response, file.File)
     assert response.name == "name_value"
     assert response.display_name == "display_name_value"
     assert response.mime_type == "mime_type_value"
     assert response.size_bytes == 1089
     assert response.sha256_hash == b"sha256_hash_blob"
     assert response.uri == "uri_value"
+    assert response.state == file.File.State.PROCESSING
 
 
 def test_get_file_empty_call():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = FileServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_file), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_file()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.GetFileRequest()
 
 
 def test_get_file_non_empty_request_with_auto_populated_field():
@@ -1609,22 +1786,60 @@
     # if they meet the requirements of AIP 4235.
     request = file_service.GetFileRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_file), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_file(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.GetFileRequest(
             name="name_value",
         )
 
 
+def test_get_file_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = FileServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_file in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_file] = mock_rpc
+        request = {}
+        client.get_file(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_file(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_file_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = FileServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1637,23 +1852,68 @@
             file.File(
                 name="name_value",
                 display_name="display_name_value",
                 mime_type="mime_type_value",
                 size_bytes=1089,
                 sha256_hash=b"sha256_hash_blob",
                 uri="uri_value",
+                state=file.File.State.PROCESSING,
             )
         )
         response = await client.get_file()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.GetFileRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_file_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = FileServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_file
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_file
+        ] = mock_object
+
+        request = {}
+        await client.get_file(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_file(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_file_async(
     transport: str = "grpc_asyncio", request_type=file_service.GetFileRequest
 ):
     client = FileServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1669,14 +1929,15 @@
             file.File(
                 name="name_value",
                 display_name="display_name_value",
                 mime_type="mime_type_value",
                 size_bytes=1089,
                 sha256_hash=b"sha256_hash_blob",
                 uri="uri_value",
+                state=file.File.State.PROCESSING,
             )
         )
         response = await client.get_file(request)
 
         # Establish that the underlying gRPC stub method was called.
         assert len(call.mock_calls)
         _, args, _ = call.mock_calls[0]
@@ -1687,14 +1948,15 @@
     assert isinstance(response, file.File)
     assert response.name == "name_value"
     assert response.display_name == "display_name_value"
     assert response.mime_type == "mime_type_value"
     assert response.size_bytes == 1089
     assert response.sha256_hash == b"sha256_hash_blob"
     assert response.uri == "uri_value"
+    assert response.state == file.File.State.PROCESSING
 
 
 @pytest.mark.asyncio
 async def test_get_file_async_from_dict():
     await test_get_file_async(request_type=dict)
 
 
@@ -1876,14 +2138,17 @@
     client = FileServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.delete_file), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_file()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.DeleteFileRequest()
 
 
 def test_delete_file_non_empty_request_with_auto_populated_field():
@@ -1899,22 +2164,60 @@
     # if they meet the requirements of AIP 4235.
     request = file_service.DeleteFileRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.delete_file), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_file(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.DeleteFileRequest(
             name="name_value",
         )
 
 
+def test_delete_file_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = FileServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_file in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.delete_file] = mock_rpc
+        request = {}
+        client.delete_file(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_file(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_delete_file_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = FileServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1927,14 +2230,60 @@
         response = await client.delete_file()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == file_service.DeleteFileRequest()
 
 
 @pytest.mark.asyncio
+async def test_delete_file_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = FileServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.delete_file
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.delete_file
+        ] = mock_object
+
+        request = {}
+        await client.delete_file(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.delete_file(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_delete_file_async(
     transport: str = "grpc_asyncio", request_type=file_service.DeleteFileRequest
 ):
     client = FileServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2136,14 +2485,50 @@
         req.return_value = response_value
         response = client.create_file(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, file_service.CreateFileResponse)
 
 
+def test_create_file_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = FileServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_file in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.create_file] = mock_rpc
+
+        request = {}
+        client.create_file(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_file(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_create_file_rest_interceptors(null_interceptor):
     transport = transports.FileServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
         if null_interceptor
         else transports.FileServiceRestInterceptor(),
@@ -2261,14 +2646,50 @@
         response = client.list_files(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListFilesPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_files_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = FileServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_files in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_files] = mock_rpc
+
+        request = {}
+        client.list_files(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_files(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_list_files_rest_interceptors(null_interceptor):
     transport = transports.FileServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
         if null_interceptor
         else transports.FileServiceRestInterceptor(),
@@ -2428,14 +2849,15 @@
         return_value = file.File(
             name="name_value",
             display_name="display_name_value",
             mime_type="mime_type_value",
             size_bytes=1089,
             sha256_hash=b"sha256_hash_blob",
             uri="uri_value",
+            state=file.File.State.PROCESSING,
         )
 
         # Wrap the value into a proper Response obj
         response_value = Response()
         response_value.status_code = 200
         # Convert return value to protobuf type
         return_value = file.File.pb(return_value)
@@ -2449,14 +2871,51 @@
     assert isinstance(response, file.File)
     assert response.name == "name_value"
     assert response.display_name == "display_name_value"
     assert response.mime_type == "mime_type_value"
     assert response.size_bytes == 1089
     assert response.sha256_hash == b"sha256_hash_blob"
     assert response.uri == "uri_value"
+    assert response.state == file.File.State.PROCESSING
+
+
+def test_get_file_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = FileServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_file in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_file] = mock_rpc
+
+        request = {}
+        client.get_file(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_file(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
 
 
 def test_get_file_rest_required_fields(request_type=file_service.GetFileRequest):
     transport_class = transports.FileServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -2705,14 +3164,50 @@
         req.return_value = response_value
         response = client.delete_file(request)
 
     # Establish that the response is the type that we expect.
     assert response is None
 
 
+def test_delete_file_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = FileServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_file in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.delete_file] = mock_rpc
+
+        request = {}
+        client.delete_file(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_file(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_delete_file_rest_required_fields(request_type=file_service.DeleteFileRequest):
     transport_class = transports.FileServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
     request = request_type(**request_init)
     pb_request = request_type.pb(request)
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_generative_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_generative_service.py`

 * *Files 4% similar despite different names*

```diff
@@ -1200,14 +1200,17 @@
     client = GenerativeServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_content), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest()
 
 
 def test_generate_content_non_empty_request_with_auto_populated_field():
@@ -1223,22 +1226,62 @@
     # if they meet the requirements of AIP 4235.
     request = generative_service.GenerateContentRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_content), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_content(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest(
             model="model_value",
         )
 
 
+def test_generate_content_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_content in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_content
+        ] = mock_rpc
+        request = {}
+        client.generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_generate_content_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1253,14 +1296,60 @@
         response = await client.generate_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest()
 
 
 @pytest.mark.asyncio
+async def test_generate_content_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.generate_content
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.generate_content
+        ] = mock_object
+
+        request = {}
+        await client.generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_generate_content_async(
     transport: str = "grpc_asyncio",
     request_type=generative_service.GenerateContentRequest,
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1488,14 +1577,17 @@
     client = GenerativeServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_answer), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_answer()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateAnswerRequest()
 
 
 def test_generate_answer_non_empty_request_with_auto_populated_field():
@@ -1511,22 +1603,60 @@
     # if they meet the requirements of AIP 4235.
     request = generative_service.GenerateAnswerRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_answer), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_answer(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateAnswerRequest(
             model="model_value",
         )
 
 
+def test_generate_answer_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_answer in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.generate_answer] = mock_rpc
+        request = {}
+        client.generate_answer(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_answer(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_generate_answer_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1543,14 +1673,60 @@
         response = await client.generate_answer()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateAnswerRequest()
 
 
 @pytest.mark.asyncio
+async def test_generate_answer_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.generate_answer
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.generate_answer
+        ] = mock_object
+
+        request = {}
+        await client.generate_answer(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.generate_answer(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_generate_answer_async(
     transport: str = "grpc_asyncio",
     request_type=generative_service.GenerateAnswerRequest,
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1823,14 +1999,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.stream_generate_content), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.stream_generate_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest()
 
 
 def test_stream_generate_content_non_empty_request_with_auto_populated_field():
@@ -1848,22 +2027,65 @@
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.stream_generate_content), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.stream_generate_content(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest(
             model="model_value",
         )
 
 
+def test_stream_generate_content_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.stream_generate_content
+            in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.stream_generate_content
+        ] = mock_rpc
+        request = {}
+        client.stream_generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.stream_generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_stream_generate_content_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1881,14 +2103,60 @@
         response = await client.stream_generate_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.GenerateContentRequest()
 
 
 @pytest.mark.asyncio
+async def test_stream_generate_content_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.stream_generate_content
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.stream_generate_content
+        ] = mock_object
+
+        request = {}
+        await client.stream_generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.stream_generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_stream_generate_content_async(
     transport: str = "grpc_asyncio",
     request_type=generative_service.GenerateContentRequest,
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -2124,14 +2392,17 @@
     client = GenerativeServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_content), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.EmbedContentRequest()
 
 
 def test_embed_content_non_empty_request_with_auto_populated_field():
@@ -2148,23 +2419,61 @@
     request = generative_service.EmbedContentRequest(
         model="model_value",
         title="title_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_content), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_content(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.EmbedContentRequest(
             model="model_value",
             title="title_value",
         )
 
 
+def test_embed_content_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_content in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_content] = mock_rpc
+        request = {}
+        client.embed_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_embed_content_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2179,14 +2488,60 @@
         response = await client.embed_content()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.EmbedContentRequest()
 
 
 @pytest.mark.asyncio
+async def test_embed_content_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.embed_content
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.embed_content
+        ] = mock_object
+
+        request = {}
+        await client.embed_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.embed_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_embed_content_async(
     transport: str = "grpc_asyncio", request_type=generative_service.EmbedContentRequest
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2414,14 +2769,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_embed_contents), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_embed_contents()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.BatchEmbedContentsRequest()
 
 
 def test_batch_embed_contents_non_empty_request_with_auto_populated_field():
@@ -2439,22 +2797,64 @@
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_embed_contents), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_embed_contents(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.BatchEmbedContentsRequest(
             model="model_value",
         )
 
 
+def test_batch_embed_contents_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_embed_contents in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_embed_contents
+        ] = mock_rpc
+        request = {}
+        client.batch_embed_contents(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_embed_contents(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_batch_embed_contents_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2471,14 +2871,60 @@
         response = await client.batch_embed_contents()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.BatchEmbedContentsRequest()
 
 
 @pytest.mark.asyncio
+async def test_batch_embed_contents_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.batch_embed_contents
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.batch_embed_contents
+        ] = mock_object
+
+        request = {}
+        await client.batch_embed_contents(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.batch_embed_contents(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_batch_embed_contents_async(
     transport: str = "grpc_asyncio",
     request_type=generative_service.BatchEmbedContentsRequest,
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -2716,14 +3162,17 @@
     client = GenerativeServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.count_tokens), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.CountTokensRequest()
 
 
 def test_count_tokens_non_empty_request_with_auto_populated_field():
@@ -2739,22 +3188,60 @@
     # if they meet the requirements of AIP 4235.
     request = generative_service.CountTokensRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.count_tokens), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_tokens(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.CountTokensRequest(
             model="model_value",
         )
 
 
+def test_count_tokens_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.count_tokens in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.count_tokens] = mock_rpc
+        request = {}
+        client.count_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_count_tokens_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2771,14 +3258,60 @@
         response = await client.count_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == generative_service.CountTokensRequest()
 
 
 @pytest.mark.asyncio
+async def test_count_tokens_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = GenerativeServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.count_tokens
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.count_tokens
+        ] = mock_object
+
+        request = {}
+        await client.count_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.count_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_count_tokens_async(
     transport: str = "grpc_asyncio", request_type=generative_service.CountTokensRequest
 ):
     client = GenerativeServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2999,14 +3532,52 @@
         req.return_value = response_value
         response = client.generate_content(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.GenerateContentResponse)
 
 
+def test_generate_content_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_content in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_content
+        ] = mock_rpc
+
+        request = {}
+        client.generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_generate_content_rest_required_fields(
     request_type=generative_service.GenerateContentRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -3276,14 +3847,50 @@
         response = client.generate_answer(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.GenerateAnswerResponse)
     assert math.isclose(response.answerable_probability, 0.234, rel_tol=1e-6)
 
 
+def test_generate_answer_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_answer in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.generate_answer] = mock_rpc
+
+        request = {}
+        client.generate_answer(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_answer(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_generate_answer_rest_required_fields(
     request_type=generative_service.GenerateAnswerRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -3570,14 +4177,55 @@
     assert isinstance(response, Iterable)
     response = next(response)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.GenerateContentResponse)
 
 
+def test_stream_generate_content_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.stream_generate_content
+            in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.stream_generate_content
+        ] = mock_rpc
+
+        request = {}
+        client.stream_generate_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.stream_generate_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_stream_generate_content_rest_required_fields(
     request_type=generative_service.GenerateContentRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -3851,14 +4499,50 @@
         req.return_value = response_value
         response = client.embed_content(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.EmbedContentResponse)
 
 
+def test_embed_content_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_content in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_content] = mock_rpc
+
+        request = {}
+        client.embed_content(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_content(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_embed_content_rest_required_fields(
     request_type=generative_service.EmbedContentRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -4124,14 +4808,54 @@
         req.return_value = response_value
         response = client.batch_embed_contents(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.BatchEmbedContentsResponse)
 
 
+def test_batch_embed_contents_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_embed_contents in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_embed_contents
+        ] = mock_rpc
+
+        request = {}
+        client.batch_embed_contents(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_embed_contents(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_batch_embed_contents_rest_required_fields(
     request_type=generative_service.BatchEmbedContentsRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -4405,14 +5129,50 @@
         response = client.count_tokens(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, generative_service.CountTokensResponse)
     assert response.total_tokens == 1303
 
 
+def test_count_tokens_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = GenerativeServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.count_tokens in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.count_tokens] = mock_rpc
+
+        request = {}
+        client.count_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_count_tokens_rest_required_fields(
     request_type=generative_service.CountTokensRequest,
 ):
     transport_class = transports.GenerativeServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -4486,23 +5246,15 @@
 
 def test_count_tokens_rest_unset_required_fields():
     transport = transports.GenerativeServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials
     )
 
     unset_fields = transport.count_tokens._get_unset_required_fields({})
-    assert set(unset_fields) == (
-        set(())
-        & set(
-            (
-                "model",
-                "contents",
-            )
-        )
-    )
+    assert set(unset_fields) == (set(()) & set(("model",)))
 
 
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_count_tokens_rest_interceptors(null_interceptor):
     transport = transports.GenerativeServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_model_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_model_service.py`

 * *Files 8% similar despite different names*

```diff
@@ -1171,14 +1171,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest()
 
 
 def test_get_model_non_empty_request_with_auto_populated_field():
@@ -1194,22 +1197,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.GetModelRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest(
             name="name_value",
         )
 
 
+def test_get_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_model] = mock_rpc
+        request = {}
+        client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1236,14 +1277,58 @@
         response = await client.get_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_model_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_model
+        ] = mock_object
+
+        request = {}
+        await client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.GetModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1481,14 +1566,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_models), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest()
 
 
 def test_list_models_non_empty_request_with_auto_populated_field():
@@ -1504,22 +1592,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.ListModelsRequest(
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_models), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_models(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest(
             page_token="page_token_value",
         )
 
 
+def test_list_models_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_models] = mock_rpc
+        request = {}
+        client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_models_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1536,14 +1662,60 @@
         response = await client.list_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_models_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_models
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_models
+        ] = mock_object
+
+        request = {}
+        await client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_models_async(
     transport: str = "grpc_asyncio", request_type=model_service.ListModelsRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1912,14 +2084,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_tuned_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetTunedModelRequest()
 
 
 def test_get_tuned_model_non_empty_request_with_auto_populated_field():
@@ -1935,22 +2110,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.GetTunedModelRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_tuned_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_tuned_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetTunedModelRequest(
             name="name_value",
         )
 
 
+def test_get_tuned_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_tuned_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_tuned_model] = mock_rpc
+        request = {}
+        client.get_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_tuned_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1973,14 +2186,60 @@
         response = await client.get_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetTunedModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_tuned_model_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_tuned_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_tuned_model
+        ] = mock_object
+
+        request = {}
+        await client.get_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_tuned_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.GetTunedModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2216,14 +2475,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.list_tuned_models), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_tuned_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListTunedModelsRequest()
 
 
 def test_list_tuned_models_non_empty_request_with_auto_populated_field():
@@ -2242,23 +2504,63 @@
         filter="filter_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.list_tuned_models), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_tuned_models(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListTunedModelsRequest(
             page_token="page_token_value",
             filter="filter_value",
         )
 
 
+def test_list_tuned_models_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_tuned_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.list_tuned_models
+        ] = mock_rpc
+        request = {}
+        client.list_tuned_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_tuned_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_tuned_models_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2277,14 +2579,60 @@
         response = await client.list_tuned_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListTunedModelsRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_tuned_models_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_tuned_models
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_tuned_models
+        ] = mock_object
+
+        request = {}
+        await client.list_tuned_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_tuned_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_tuned_models_async(
     transport: str = "grpc_asyncio", request_type=model_service.ListTunedModelsRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2655,14 +3003,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.create_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.CreateTunedModelRequest()
 
 
 def test_create_tuned_model_non_empty_request_with_auto_populated_field():
@@ -2680,22 +3031,68 @@
         tuned_model_id="tuned_model_id_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.create_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_tuned_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.CreateTunedModelRequest(
             tuned_model_id="tuned_model_id_value",
         )
 
 
+def test_create_tuned_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.create_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.create_tuned_model
+        ] = mock_rpc
+        request = {}
+        client.create_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        # Operation methods build a cached wrapper on first rpc call
+        # subsequent calls should use the cached wrapper
+        wrapper_fn.reset_mock()
+
+        client.create_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_create_tuned_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2712,14 +3109,64 @@
         response = await client.create_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.CreateTunedModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_create_tuned_model_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.create_tuned_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.create_tuned_model
+        ] = mock_object
+
+        request = {}
+        await client.create_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        # Operation methods build a cached wrapper on first rpc call
+        # subsequent calls should use the cached wrapper
+        wrapper_fn.reset_mock()
+
+        await client.create_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_create_tuned_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.CreateTunedModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2932,14 +3379,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.update_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.UpdateTunedModelRequest()
 
 
 def test_update_tuned_model_non_empty_request_with_auto_populated_field():
@@ -2955,20 +3405,62 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.UpdateTunedModelRequest()
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.update_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_tuned_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.UpdateTunedModelRequest()
 
 
+def test_update_tuned_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.update_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.update_tuned_model
+        ] = mock_rpc
+        request = {}
+        client.update_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_update_tuned_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2993,14 +3485,60 @@
         response = await client.update_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.UpdateTunedModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_update_tuned_model_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.update_tuned_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.update_tuned_model
+        ] = mock_object
+
+        request = {}
+        await client.update_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.update_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_update_tuned_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.UpdateTunedModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -3277,14 +3815,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.delete_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.DeleteTunedModelRequest()
 
 
 def test_delete_tuned_model_non_empty_request_with_auto_populated_field():
@@ -3302,22 +3843,64 @@
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.delete_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_tuned_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.DeleteTunedModelRequest(
             name="name_value",
         )
 
 
+def test_delete_tuned_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.delete_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.delete_tuned_model
+        ] = mock_rpc
+        request = {}
+        client.delete_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_delete_tuned_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -3332,14 +3915,60 @@
         response = await client.delete_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.DeleteTunedModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_delete_tuned_model_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.delete_tuned_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.delete_tuned_model
+        ] = mock_object
+
+        request = {}
+        await client.delete_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.delete_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_delete_tuned_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.DeleteTunedModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -3576,14 +4205,50 @@
         "supported_generation_methods_value"
     ]
     assert math.isclose(response.temperature, 0.1198, rel_tol=1e-6)
     assert math.isclose(response.top_p, 0.546, rel_tol=1e-6)
     assert response.top_k == 541
 
 
+def test_get_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_model] = mock_rpc
+
+        request = {}
+        client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_model_rest_required_fields(request_type=model_service.GetModelRequest):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
     request = request_type(**request_init)
     pb_request = request_type.pb(request)
@@ -3835,14 +4500,50 @@
         response = client.list_models(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListModelsPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_models_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_models] = mock_rpc
+
+        request = {}
+        client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_list_models_rest_interceptors(null_interceptor):
     transport = transports.ModelServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
         if null_interceptor
         else transports.ModelServiceRestInterceptor(),
@@ -4087,14 +4788,50 @@
     assert response.description == "description_value"
     assert math.isclose(response.temperature, 0.1198, rel_tol=1e-6)
     assert math.isclose(response.top_p, 0.546, rel_tol=1e-6)
     assert response.top_k == 541
     assert response.state == tuned_model.TunedModel.State.CREATING
 
 
+def test_get_tuned_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_tuned_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_tuned_model] = mock_rpc
+
+        request = {}
+        client.get_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_tuned_model_rest_required_fields(
     request_type=model_service.GetTunedModelRequest,
 ):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -4352,14 +5089,52 @@
         response = client.list_tuned_models(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListTunedModelsPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_tuned_models_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_tuned_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.list_tuned_models
+        ] = mock_rpc
+
+        request = {}
+        client.list_tuned_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_tuned_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_list_tuned_models_rest_interceptors(null_interceptor):
     transport = transports.ModelServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
         if null_interceptor
         else transports.ModelServiceRestInterceptor(),
@@ -4691,14 +5466,58 @@
         req.return_value = response_value
         response = client.create_tuned_model(request)
 
     # Establish that the response is the type that we expect.
     assert response.operation.name == "operations/spam"
 
 
+def test_create_tuned_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.create_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.create_tuned_model
+        ] = mock_rpc
+
+        request = {}
+        client.create_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        # Operation methods build a cached wrapper on first rpc call
+        # subsequent calls should use the cached wrapper
+        wrapper_fn.reset_mock()
+
+        client.create_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_create_tuned_model_rest_required_fields(
     request_type=model_service.CreateTunedModelRequest,
 ):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -5077,14 +5896,54 @@
     assert response.description == "description_value"
     assert math.isclose(response.temperature, 0.1198, rel_tol=1e-6)
     assert math.isclose(response.top_p, 0.546, rel_tol=1e-6)
     assert response.top_k == 541
     assert response.state == gag_tuned_model.TunedModel.State.CREATING
 
 
+def test_update_tuned_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.update_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.update_tuned_model
+        ] = mock_rpc
+
+        request = {}
+        client.update_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_update_tuned_model_rest_required_fields(
     request_type=model_service.UpdateTunedModelRequest,
 ):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -5354,14 +6213,54 @@
         req.return_value = response_value
         response = client.delete_tuned_model(request)
 
     # Establish that the response is the type that we expect.
     assert response is None
 
 
+def test_delete_tuned_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.delete_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.delete_tuned_model
+        ] = mock_rpc
+
+        request = {}
+        client.delete_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_delete_tuned_model_rest_required_fields(
     request_type=model_service.DeleteTunedModelRequest,
 ):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_permission_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/test_permission_service.py`

 * *Files 9% similar despite different names*

```diff
@@ -40,23 +40,23 @@
 from grpc.experimental import aio
 from proto.marshal.rules import wrappers
 from proto.marshal.rules.dates import DurationRule, TimestampRule
 import pytest
 from requests import PreparedRequest, Request, Response
 from requests.sessions import Session
 
-from google.ai.generativelanguage_v1beta.services.permission_service import (
+from google.ai.generativelanguage_v1beta3.services.permission_service import (
     PermissionServiceAsyncClient,
     PermissionServiceClient,
     pagers,
     transports,
 )
-from google.ai.generativelanguage_v1beta.types import permission as gag_permission
-from google.ai.generativelanguage_v1beta.types import permission
-from google.ai.generativelanguage_v1beta.types import permission_service
+from google.ai.generativelanguage_v1beta3.types import permission as gag_permission
+from google.ai.generativelanguage_v1beta3.types import permission
+from google.ai.generativelanguage_v1beta3.types import permission_service
 
 
 def client_cert_source_callback():
     return b"cert bytes", b"key bytes"
 
 
 # If default endpoint is localhost, then default mtls endpoint will be the same.
@@ -1066,15 +1066,15 @@
             always_use_jwt_access=True,
             api_audience=None,
         )
 
 
 def test_permission_service_client_client_options_from_dict():
     with mock.patch(
-        "google.ai.generativelanguage_v1beta.services.permission_service.transports.PermissionServiceGrpcTransport.__init__"
+        "google.ai.generativelanguage_v1beta3.services.permission_service.transports.PermissionServiceGrpcTransport.__init__"
     ) as grpc_transport:
         grpc_transport.return_value = None
         client = PermissionServiceClient(
             client_options={"api_endpoint": "squid.clam.whelk"}
         )
         grpc_transport.assert_called_once_with(
             credentials=None,
@@ -1210,14 +1210,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.create_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.CreatePermissionRequest()
 
 
 def test_create_permission_non_empty_request_with_auto_populated_field():
@@ -1235,22 +1238,62 @@
         parent="parent_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.create_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_permission(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.CreatePermissionRequest(
             parent="parent_value",
         )
 
 
+def test_create_permission_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.create_permission
+        ] = mock_rpc
+        request = {}
+        client.create_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_create_permission_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1272,14 +1315,60 @@
         response = await client.create_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.CreatePermissionRequest()
 
 
 @pytest.mark.asyncio
+async def test_create_permission_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.create_permission
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.create_permission
+        ] = mock_object
+
+        request = {}
+        await client.create_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.create_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_create_permission_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.CreatePermissionRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1532,14 +1621,17 @@
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_permission), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.GetPermissionRequest()
 
 
 def test_get_permission_non_empty_request_with_auto_populated_field():
@@ -1555,22 +1647,60 @@
     # if they meet the requirements of AIP 4235.
     request = permission_service.GetPermissionRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_permission), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_permission(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.GetPermissionRequest(
             name="name_value",
         )
 
 
+def test_get_permission_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_permission] = mock_rpc
+        request = {}
+        client.get_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_permission_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1590,14 +1720,60 @@
         response = await client.get_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.GetPermissionRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_permission_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_permission
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_permission
+        ] = mock_object
+
+        request = {}
+        await client.get_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_permission_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.GetPermissionRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1824,14 +2000,17 @@
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_permissions), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_permissions()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.ListPermissionsRequest()
 
 
 def test_list_permissions_non_empty_request_with_auto_populated_field():
@@ -1848,23 +2027,63 @@
     request = permission_service.ListPermissionsRequest(
         parent="parent_value",
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_permissions), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_permissions(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.ListPermissionsRequest(
             parent="parent_value",
             page_token="page_token_value",
         )
 
 
+def test_list_permissions_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_permissions in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.list_permissions
+        ] = mock_rpc
+        request = {}
+        client.list_permissions(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_permissions(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_permissions_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1881,14 +2100,60 @@
         response = await client.list_permissions()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.ListPermissionsRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_permissions_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_permissions
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_permissions
+        ] = mock_object
+
+        request = {}
+        await client.list_permissions(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_permissions(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_permissions_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.ListPermissionsRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -2309,14 +2574,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.update_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.UpdatePermissionRequest()
 
 
 def test_update_permission_non_empty_request_with_auto_populated_field():
@@ -2332,20 +2600,60 @@
     # if they meet the requirements of AIP 4235.
     request = permission_service.UpdatePermissionRequest()
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.update_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_permission(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.UpdatePermissionRequest()
 
 
+def test_update_permission_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.update_permission
+        ] = mock_rpc
+        request = {}
+        client.update_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_update_permission_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2367,14 +2675,60 @@
         response = await client.update_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.UpdatePermissionRequest()
 
 
 @pytest.mark.asyncio
+async def test_update_permission_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.update_permission
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.update_permission
+        ] = mock_object
+
+        request = {}
+        await client.update_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.update_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_update_permission_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.UpdatePermissionRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -2622,14 +2976,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.delete_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.DeletePermissionRequest()
 
 
 def test_delete_permission_non_empty_request_with_auto_populated_field():
@@ -2647,22 +3004,62 @@
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.delete_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_permission(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.DeletePermissionRequest(
             name="name_value",
         )
 
 
+def test_delete_permission_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.delete_permission
+        ] = mock_rpc
+        request = {}
+        client.delete_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_delete_permission_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2677,14 +3074,60 @@
         response = await client.delete_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.DeletePermissionRequest()
 
 
 @pytest.mark.asyncio
+async def test_delete_permission_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.delete_permission
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.delete_permission
+        ] = mock_object
+
+        request = {}
+        await client.delete_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.delete_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_delete_permission_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.DeletePermissionRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -2907,14 +3350,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.transfer_ownership), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.transfer_ownership()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.TransferOwnershipRequest()
 
 
 def test_transfer_ownership_non_empty_request_with_auto_populated_field():
@@ -2933,23 +3379,65 @@
         email_address="email_address_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.transfer_ownership), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.transfer_ownership(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.TransferOwnershipRequest(
             name="name_value",
             email_address="email_address_value",
         )
 
 
+def test_transfer_ownership_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.transfer_ownership in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.transfer_ownership
+        ] = mock_rpc
+        request = {}
+        client.transfer_ownership(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.transfer_ownership(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_transfer_ownership_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2966,14 +3454,60 @@
         response = await client.transfer_ownership()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.TransferOwnershipRequest()
 
 
 @pytest.mark.asyncio
+async def test_transfer_ownership_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.transfer_ownership
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.transfer_ownership
+        ] = mock_object
+
+        request = {}
+        await client.transfer_ownership(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.transfer_ownership(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_transfer_ownership_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.TransferOwnershipRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -3188,14 +3722,52 @@
     assert isinstance(response, gag_permission.Permission)
     assert response.name == "name_value"
     assert response.grantee_type == gag_permission.Permission.GranteeType.USER
     assert response.email_address == "email_address_value"
     assert response.role == gag_permission.Permission.Role.OWNER
 
 
+def test_create_permission_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.create_permission
+        ] = mock_rpc
+
+        request = {}
+        client.create_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_create_permission_rest_required_fields(
     request_type=permission_service.CreatePermissionRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["parent"] = ""
@@ -3398,15 +3970,15 @@
         client.create_permission(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta/{parent=tunedModels/*}/permissions" % client.transport._host,
+            "%s/v1beta3/{parent=tunedModels/*}/permissions" % client.transport._host,
             args[1],
         )
 
 
 def test_create_permission_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -3471,14 +4043,50 @@
     assert isinstance(response, permission.Permission)
     assert response.name == "name_value"
     assert response.grantee_type == permission.Permission.GranteeType.USER
     assert response.email_address == "email_address_value"
     assert response.role == permission.Permission.Role.OWNER
 
 
+def test_get_permission_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_permission] = mock_rpc
+
+        request = {}
+        client.get_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_permission_rest_required_fields(
     request_type=permission_service.GetPermissionRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -3671,15 +4279,15 @@
         client.get_permission(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta/{name=tunedModels/*/permissions/*}" % client.transport._host,
+            "%s/v1beta3/{name=tunedModels/*/permissions/*}" % client.transport._host,
             args[1],
         )
 
 
 def test_get_permission_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -3737,14 +4345,52 @@
         response = client.list_permissions(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListPermissionsPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_permissions_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_permissions in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.list_permissions
+        ] = mock_rpc
+
+        request = {}
+        client.list_permissions(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_permissions(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_list_permissions_rest_required_fields(
     request_type=permission_service.ListPermissionsRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["parent"] = ""
@@ -3952,15 +4598,15 @@
         client.list_permissions(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta/{parent=tunedModels/*}/permissions" % client.transport._host,
+            "%s/v1beta3/{parent=tunedModels/*}/permissions" % client.transport._host,
             args[1],
         )
 
 
 def test_list_permissions_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -4154,14 +4800,52 @@
     assert isinstance(response, gag_permission.Permission)
     assert response.name == "name_value"
     assert response.grantee_type == gag_permission.Permission.GranteeType.USER
     assert response.email_address == "email_address_value"
     assert response.role == gag_permission.Permission.Role.OWNER
 
 
+def test_update_permission_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.update_permission
+        ] = mock_rpc
+
+        request = {}
+        client.update_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_update_permission_rest_required_fields(
     request_type=permission_service.UpdatePermissionRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -4363,15 +5047,15 @@
         client.update_permission(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta/{permission.name=tunedModels/*/permissions/*}"
+            "%s/v1beta3/{permission.name=tunedModels/*/permissions/*}"
             % client.transport._host,
             args[1],
         )
 
 
 def test_update_permission_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
@@ -4426,14 +5110,52 @@
         req.return_value = response_value
         response = client.delete_permission(request)
 
     # Establish that the response is the type that we expect.
     assert response is None
 
 
+def test_delete_permission_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.delete_permission
+        ] = mock_rpc
+
+        request = {}
+        client.delete_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_delete_permission_rest_required_fields(
     request_type=permission_service.DeletePermissionRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -4613,15 +5335,15 @@
         client.delete_permission(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta/{name=tunedModels/*/permissions/*}" % client.transport._host,
+            "%s/v1beta3/{name=tunedModels/*/permissions/*}" % client.transport._host,
             args[1],
         )
 
 
 def test_delete_permission_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -4676,14 +5398,54 @@
         req.return_value = response_value
         response = client.transfer_ownership(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, permission_service.TransferOwnershipResponse)
 
 
+def test_transfer_ownership_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.transfer_ownership in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.transfer_ownership
+        ] = mock_rpc
+
+        request = {}
+        client.transfer_ownership(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.transfer_ownership(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_transfer_ownership_rest_required_fields(
     request_type=permission_service.TransferOwnershipRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -4990,15 +5752,15 @@
             credentials_file="credentials.json",
         )
 
 
 def test_permission_service_base_transport():
     # Instantiate the base transport.
     with mock.patch(
-        "google.ai.generativelanguage_v1beta.services.permission_service.transports.PermissionServiceTransport.__init__"
+        "google.ai.generativelanguage_v1beta3.services.permission_service.transports.PermissionServiceTransport.__init__"
     ) as Transport:
         Transport.return_value = None
         transport = transports.PermissionServiceTransport(
             credentials=ga_credentials.AnonymousCredentials(),
         )
 
     # Every method on the transport should just blindly
@@ -5028,15 +5790,15 @@
 
 
 def test_permission_service_base_transport_with_credentials_file():
     # Instantiate the base transport with a credentials file
     with mock.patch.object(
         google.auth, "load_credentials_from_file", autospec=True
     ) as load_creds, mock.patch(
-        "google.ai.generativelanguage_v1beta.services.permission_service.transports.PermissionServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta3.services.permission_service.transports.PermissionServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         load_creds.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.PermissionServiceTransport(
             credentials_file="credentials.json",
             quota_project_id="octopus",
         )
@@ -5047,15 +5809,15 @@
             quota_project_id="octopus",
         )
 
 
 def test_permission_service_base_transport_with_adc():
     # Test the default credentials are used if credentials and credentials_file are None.
     with mock.patch.object(google.auth, "default", autospec=True) as adc, mock.patch(
-        "google.ai.generativelanguage_v1beta.services.permission_service.transports.PermissionServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta3.services.permission_service.transports.PermissionServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         adc.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.PermissionServiceTransport()
         adc.assert_called_once()
 
 
@@ -5433,109 +6195,129 @@
     path = PermissionServiceClient.permission_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_permission_path(path)
     assert expected == actual
 
 
+def test_tuned_model_path():
+    tuned_model = "oyster"
+    expected = "tunedModels/{tuned_model}".format(
+        tuned_model=tuned_model,
+    )
+    actual = PermissionServiceClient.tuned_model_path(tuned_model)
+    assert expected == actual
+
+
+def test_parse_tuned_model_path():
+    expected = {
+        "tuned_model": "nudibranch",
+    }
+    path = PermissionServiceClient.tuned_model_path(**expected)
+
+    # Check that the path construction is reversible.
+    actual = PermissionServiceClient.parse_tuned_model_path(path)
+    assert expected == actual
+
+
 def test_common_billing_account_path():
-    billing_account = "oyster"
+    billing_account = "cuttlefish"
     expected = "billingAccounts/{billing_account}".format(
         billing_account=billing_account,
     )
     actual = PermissionServiceClient.common_billing_account_path(billing_account)
     assert expected == actual
 
 
 def test_parse_common_billing_account_path():
     expected = {
-        "billing_account": "nudibranch",
+        "billing_account": "mussel",
     }
     path = PermissionServiceClient.common_billing_account_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_billing_account_path(path)
     assert expected == actual
 
 
 def test_common_folder_path():
-    folder = "cuttlefish"
+    folder = "winkle"
     expected = "folders/{folder}".format(
         folder=folder,
     )
     actual = PermissionServiceClient.common_folder_path(folder)
     assert expected == actual
 
 
 def test_parse_common_folder_path():
     expected = {
-        "folder": "mussel",
+        "folder": "nautilus",
     }
     path = PermissionServiceClient.common_folder_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_folder_path(path)
     assert expected == actual
 
 
 def test_common_organization_path():
-    organization = "winkle"
+    organization = "scallop"
     expected = "organizations/{organization}".format(
         organization=organization,
     )
     actual = PermissionServiceClient.common_organization_path(organization)
     assert expected == actual
 
 
 def test_parse_common_organization_path():
     expected = {
-        "organization": "nautilus",
+        "organization": "abalone",
     }
     path = PermissionServiceClient.common_organization_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_organization_path(path)
     assert expected == actual
 
 
 def test_common_project_path():
-    project = "scallop"
+    project = "squid"
     expected = "projects/{project}".format(
         project=project,
     )
     actual = PermissionServiceClient.common_project_path(project)
     assert expected == actual
 
 
 def test_parse_common_project_path():
     expected = {
-        "project": "abalone",
+        "project": "clam",
     }
     path = PermissionServiceClient.common_project_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_project_path(path)
     assert expected == actual
 
 
 def test_common_location_path():
-    project = "squid"
-    location = "clam"
+    project = "whelk"
+    location = "octopus"
     expected = "projects/{project}/locations/{location}".format(
         project=project,
         location=location,
     )
     actual = PermissionServiceClient.common_location_path(project, location)
     assert expected == actual
 
 
 def test_parse_common_location_path():
     expected = {
-        "project": "whelk",
-        "location": "octopus",
+        "project": "oyster",
+        "location": "nudibranch",
     }
     path = PermissionServiceClient.common_location_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_location_path(path)
     assert expected == actual
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_retriever_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_retriever_service.py`

 * *Files 10% similar despite different names*

```diff
@@ -1193,14 +1193,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.create_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.CreateCorpusRequest()
 
 
 def test_create_corpus_non_empty_request_with_auto_populated_field():
@@ -1214,20 +1217,58 @@
     # Populate all string fields in the request which are not UUID4
     # since we want to check that UUID4 are populated automatically
     # if they meet the requirements of AIP 4235.
     request = retriever_service.CreateCorpusRequest()
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.create_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_corpus(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.CreateCorpusRequest()
 
 
+def test_create_corpus_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.create_corpus] = mock_rpc
+        request = {}
+        client.create_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_create_corpus_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1245,14 +1286,60 @@
         response = await client.create_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.CreateCorpusRequest()
 
 
 @pytest.mark.asyncio
+async def test_create_corpus_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.create_corpus
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.create_corpus
+        ] = mock_object
+
+        request = {}
+        await client.create_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.create_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_create_corpus_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.CreateCorpusRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1413,14 +1500,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.GetCorpusRequest()
 
 
 def test_get_corpus_non_empty_request_with_auto_populated_field():
@@ -1436,22 +1526,60 @@
     # if they meet the requirements of AIP 4235.
     request = retriever_service.GetCorpusRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_corpus(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.GetCorpusRequest(
             name="name_value",
         )
 
 
+def test_get_corpus_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_corpus] = mock_rpc
+        request = {}
+        client.get_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_corpus_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1469,14 +1597,58 @@
         response = await client.get_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.GetCorpusRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_corpus_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_corpus
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_corpus
+        ] = mock_object
+
+        request = {}
+        await client.get_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_corpus_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.GetCorpusRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1696,14 +1868,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.update_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.UpdateCorpusRequest()
 
 
 def test_update_corpus_non_empty_request_with_auto_populated_field():
@@ -1717,20 +1892,58 @@
     # Populate all string fields in the request which are not UUID4
     # since we want to check that UUID4 are populated automatically
     # if they meet the requirements of AIP 4235.
     request = retriever_service.UpdateCorpusRequest()
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.update_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_corpus(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.UpdateCorpusRequest()
 
 
+def test_update_corpus_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.update_corpus] = mock_rpc
+        request = {}
+        client.update_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_update_corpus_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1748,14 +1961,60 @@
         response = await client.update_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.UpdateCorpusRequest()
 
 
 @pytest.mark.asyncio
+async def test_update_corpus_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.update_corpus
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.update_corpus
+        ] = mock_object
+
+        request = {}
+        await client.update_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.update_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_update_corpus_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.UpdateCorpusRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1980,14 +2239,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.delete_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.DeleteCorpusRequest()
 
 
 def test_delete_corpus_non_empty_request_with_auto_populated_field():
@@ -2003,22 +2265,60 @@
     # if they meet the requirements of AIP 4235.
     request = retriever_service.DeleteCorpusRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.delete_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_corpus(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.DeleteCorpusRequest(
             name="name_value",
         )
 
 
+def test_delete_corpus_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.delete_corpus] = mock_rpc
+        request = {}
+        client.delete_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_delete_corpus_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2031,14 +2331,60 @@
         response = await client.delete_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.DeleteCorpusRequest()
 
 
 @pytest.mark.asyncio
+async def test_delete_corpus_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.delete_corpus
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.delete_corpus
+        ] = mock_object
+
+        request = {}
+        await client.delete_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.delete_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_delete_corpus_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.DeleteCorpusRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2249,14 +2595,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_corpora), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_corpora()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.ListCorporaRequest()
 
 
 def test_list_corpora_non_empty_request_with_auto_populated_field():
@@ -2272,22 +2621,60 @@
     # if they meet the requirements of AIP 4235.
     request = retriever_service.ListCorporaRequest(
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_corpora), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_corpora(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.ListCorporaRequest(
             page_token="page_token_value",
         )
 
 
+def test_list_corpora_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_corpora in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_corpora] = mock_rpc
+        request = {}
+        client.list_corpora(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_corpora(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_corpora_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2304,14 +2691,60 @@
         response = await client.list_corpora()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.ListCorporaRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_corpora_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_corpora
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_corpora
+        ] = mock_object
+
+        request = {}
+        await client.list_corpora(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_corpora(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_corpora_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.ListCorporaRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2572,14 +3005,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.query_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.query_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.QueryCorpusRequest()
 
 
 def test_query_corpus_non_empty_request_with_auto_populated_field():
@@ -2596,23 +3032,61 @@
     request = retriever_service.QueryCorpusRequest(
         name="name_value",
         query="query_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.query_corpus), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.query_corpus(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.QueryCorpusRequest(
             name="name_value",
             query="query_value",
         )
 
 
+def test_query_corpus_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.query_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.query_corpus] = mock_rpc
+        request = {}
+        client.query_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.query_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_query_corpus_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2627,14 +3101,60 @@
         response = await client.query_corpus()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.QueryCorpusRequest()
 
 
 @pytest.mark.asyncio
+async def test_query_corpus_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.query_corpus
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.query_corpus
+        ] = mock_object
+
+        request = {}
+        await client.query_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.query_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_query_corpus_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.QueryCorpusRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2771,14 +3291,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.create_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.CreateDocumentRequest()
 
 
 def test_create_document_non_empty_request_with_auto_populated_field():
@@ -2794,22 +3317,60 @@
     # if they meet the requirements of AIP 4235.
     request = retriever_service.CreateDocumentRequest(
         parent="parent_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.create_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_document(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.CreateDocumentRequest(
             parent="parent_value",
         )
 
 
+def test_create_document_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.create_document] = mock_rpc
+        request = {}
+        client.create_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_create_document_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2827,14 +3388,60 @@
         response = await client.create_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.CreateDocumentRequest()
 
 
 @pytest.mark.asyncio
+async def test_create_document_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.create_document
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.create_document
+        ] = mock_object
+
+        request = {}
+        await client.create_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.create_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_create_document_async(
     transport: str = "grpc_asyncio",
     request_type=retriever_service.CreateDocumentRequest,
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -3065,14 +3672,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.GetDocumentRequest()
 
 
 def test_get_document_non_empty_request_with_auto_populated_field():
@@ -3088,22 +3698,60 @@
     # if they meet the requirements of AIP 4235.
     request = retriever_service.GetDocumentRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_document(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.GetDocumentRequest(
             name="name_value",
         )
 
 
+def test_get_document_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_document] = mock_rpc
+        request = {}
+        client.get_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_document_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -3121,14 +3769,60 @@
         response = await client.get_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.GetDocumentRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_document_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_document
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_document
+        ] = mock_object
+
+        request = {}
+        await client.get_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_document_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.GetDocumentRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -3348,14 +4042,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.update_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.UpdateDocumentRequest()
 
 
 def test_update_document_non_empty_request_with_auto_populated_field():
@@ -3369,20 +4066,58 @@
     # Populate all string fields in the request which are not UUID4
     # since we want to check that UUID4 are populated automatically
     # if they meet the requirements of AIP 4235.
     request = retriever_service.UpdateDocumentRequest()
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.update_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_document(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.UpdateDocumentRequest()
 
 
+def test_update_document_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.update_document] = mock_rpc
+        request = {}
+        client.update_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_update_document_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -3400,14 +4135,60 @@
         response = await client.update_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.UpdateDocumentRequest()
 
 
 @pytest.mark.asyncio
+async def test_update_document_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.update_document
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.update_document
+        ] = mock_object
+
+        request = {}
+        await client.update_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.update_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_update_document_async(
     transport: str = "grpc_asyncio",
     request_type=retriever_service.UpdateDocumentRequest,
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -3633,14 +4414,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.delete_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.DeleteDocumentRequest()
 
 
 def test_delete_document_non_empty_request_with_auto_populated_field():
@@ -3656,22 +4440,60 @@
     # if they meet the requirements of AIP 4235.
     request = retriever_service.DeleteDocumentRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.delete_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_document(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.DeleteDocumentRequest(
             name="name_value",
         )
 
 
+def test_delete_document_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.delete_document] = mock_rpc
+        request = {}
+        client.delete_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_delete_document_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -3684,14 +4506,60 @@
         response = await client.delete_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.DeleteDocumentRequest()
 
 
 @pytest.mark.asyncio
+async def test_delete_document_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.delete_document
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.delete_document
+        ] = mock_object
+
+        request = {}
+        await client.delete_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.delete_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_delete_document_async(
     transport: str = "grpc_asyncio",
     request_type=retriever_service.DeleteDocumentRequest,
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -3903,14 +4771,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_documents), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_documents()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.ListDocumentsRequest()
 
 
 def test_list_documents_non_empty_request_with_auto_populated_field():
@@ -3927,23 +4798,61 @@
     request = retriever_service.ListDocumentsRequest(
         parent="parent_value",
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_documents), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_documents(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.ListDocumentsRequest(
             parent="parent_value",
             page_token="page_token_value",
         )
 
 
+def test_list_documents_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_documents in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_documents] = mock_rpc
+        request = {}
+        client.list_documents(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_documents(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_documents_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -3960,14 +4869,60 @@
         response = await client.list_documents()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.ListDocumentsRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_documents_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_documents
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_documents
+        ] = mock_object
+
+        request = {}
+        await client.list_documents(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_documents(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_documents_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.ListDocumentsRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -4374,14 +5329,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.query_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.query_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.QueryDocumentRequest()
 
 
 def test_query_document_non_empty_request_with_auto_populated_field():
@@ -4398,23 +5356,61 @@
     request = retriever_service.QueryDocumentRequest(
         name="name_value",
         query="query_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.query_document), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.query_document(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.QueryDocumentRequest(
             name="name_value",
             query="query_value",
         )
 
 
+def test_query_document_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.query_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.query_document] = mock_rpc
+        request = {}
+        client.query_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.query_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_query_document_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -4429,14 +5425,60 @@
         response = await client.query_document()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.QueryDocumentRequest()
 
 
 @pytest.mark.asyncio
+async def test_query_document_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.query_document
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.query_document
+        ] = mock_object
+
+        request = {}
+        await client.query_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.query_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_query_document_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.QueryDocumentRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -4573,14 +5615,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.create_chunk), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_chunk()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.CreateChunkRequest()
 
 
 def test_create_chunk_non_empty_request_with_auto_populated_field():
@@ -4596,22 +5641,60 @@
     # if they meet the requirements of AIP 4235.
     request = retriever_service.CreateChunkRequest(
         parent="parent_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.create_chunk), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_chunk(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.CreateChunkRequest(
             parent="parent_value",
         )
 
 
+def test_create_chunk_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_chunk in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.create_chunk] = mock_rpc
+        request = {}
+        client.create_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_create_chunk_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -4629,14 +5712,60 @@
         response = await client.create_chunk()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.CreateChunkRequest()
 
 
 @pytest.mark.asyncio
+async def test_create_chunk_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.create_chunk
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.create_chunk
+        ] = mock_object
+
+        request = {}
+        await client.create_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.create_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_create_chunk_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.CreateChunkRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -4865,14 +5994,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_create_chunks), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_create_chunks()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.BatchCreateChunksRequest()
 
 
 def test_batch_create_chunks_non_empty_request_with_auto_populated_field():
@@ -4890,22 +6022,64 @@
         parent="parent_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_create_chunks), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_create_chunks(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.BatchCreateChunksRequest(
             parent="parent_value",
         )
 
 
+def test_batch_create_chunks_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_create_chunks in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_create_chunks
+        ] = mock_rpc
+        request = {}
+        client.batch_create_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_create_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_batch_create_chunks_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -4922,14 +6096,60 @@
         response = await client.batch_create_chunks()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.BatchCreateChunksRequest()
 
 
 @pytest.mark.asyncio
+async def test_batch_create_chunks_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.batch_create_chunks
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.batch_create_chunks
+        ] = mock_object
+
+        request = {}
+        await client.batch_create_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.batch_create_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_batch_create_chunks_async(
     transport: str = "grpc_asyncio",
     request_type=retriever_service.BatchCreateChunksRequest,
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -5073,14 +6293,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_chunk), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_chunk()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.GetChunkRequest()
 
 
 def test_get_chunk_non_empty_request_with_auto_populated_field():
@@ -5096,22 +6319,60 @@
     # if they meet the requirements of AIP 4235.
     request = retriever_service.GetChunkRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_chunk), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_chunk(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.GetChunkRequest(
             name="name_value",
         )
 
 
+def test_get_chunk_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_chunk in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_chunk] = mock_rpc
+        request = {}
+        client.get_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_chunk_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -5129,14 +6390,58 @@
         response = await client.get_chunk()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.GetChunkRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_chunk_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_chunk
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_chunk
+        ] = mock_object
+
+        request = {}
+        await client.get_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_chunk_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.GetChunkRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -5356,14 +6661,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.update_chunk), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_chunk()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.UpdateChunkRequest()
 
 
 def test_update_chunk_non_empty_request_with_auto_populated_field():
@@ -5377,20 +6685,58 @@
     # Populate all string fields in the request which are not UUID4
     # since we want to check that UUID4 are populated automatically
     # if they meet the requirements of AIP 4235.
     request = retriever_service.UpdateChunkRequest()
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.update_chunk), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_chunk(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.UpdateChunkRequest()
 
 
+def test_update_chunk_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_chunk in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.update_chunk] = mock_rpc
+        request = {}
+        client.update_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_update_chunk_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -5408,14 +6754,60 @@
         response = await client.update_chunk()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.UpdateChunkRequest()
 
 
 @pytest.mark.asyncio
+async def test_update_chunk_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.update_chunk
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.update_chunk
+        ] = mock_object
+
+        request = {}
+        await client.update_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.update_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_update_chunk_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.UpdateChunkRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -5644,14 +7036,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_update_chunks), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_update_chunks()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.BatchUpdateChunksRequest()
 
 
 def test_batch_update_chunks_non_empty_request_with_auto_populated_field():
@@ -5669,22 +7064,64 @@
         parent="parent_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_update_chunks), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_update_chunks(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.BatchUpdateChunksRequest(
             parent="parent_value",
         )
 
 
+def test_batch_update_chunks_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_update_chunks in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_update_chunks
+        ] = mock_rpc
+        request = {}
+        client.batch_update_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_update_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_batch_update_chunks_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -5701,14 +7138,60 @@
         response = await client.batch_update_chunks()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.BatchUpdateChunksRequest()
 
 
 @pytest.mark.asyncio
+async def test_batch_update_chunks_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.batch_update_chunks
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.batch_update_chunks
+        ] = mock_object
+
+        request = {}
+        await client.batch_update_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.batch_update_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_batch_update_chunks_async(
     transport: str = "grpc_asyncio",
     request_type=retriever_service.BatchUpdateChunksRequest,
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -5847,14 +7330,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.delete_chunk), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_chunk()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.DeleteChunkRequest()
 
 
 def test_delete_chunk_non_empty_request_with_auto_populated_field():
@@ -5870,22 +7356,60 @@
     # if they meet the requirements of AIP 4235.
     request = retriever_service.DeleteChunkRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.delete_chunk), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_chunk(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.DeleteChunkRequest(
             name="name_value",
         )
 
 
+def test_delete_chunk_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_chunk in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.delete_chunk] = mock_rpc
+        request = {}
+        client.delete_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_delete_chunk_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -5898,14 +7422,60 @@
         response = await client.delete_chunk()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.DeleteChunkRequest()
 
 
 @pytest.mark.asyncio
+async def test_delete_chunk_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.delete_chunk
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.delete_chunk
+        ] = mock_object
+
+        request = {}
+        await client.delete_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.delete_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_delete_chunk_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.DeleteChunkRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -6117,14 +7687,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_delete_chunks), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_delete_chunks()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.BatchDeleteChunksRequest()
 
 
 def test_batch_delete_chunks_non_empty_request_with_auto_populated_field():
@@ -6142,22 +7715,64 @@
         parent="parent_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.batch_delete_chunks), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_delete_chunks(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.BatchDeleteChunksRequest(
             parent="parent_value",
         )
 
 
+def test_batch_delete_chunks_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_delete_chunks in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_delete_chunks
+        ] = mock_rpc
+        request = {}
+        client.batch_delete_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_delete_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_batch_delete_chunks_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -6172,14 +7787,60 @@
         response = await client.batch_delete_chunks()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.BatchDeleteChunksRequest()
 
 
 @pytest.mark.asyncio
+async def test_batch_delete_chunks_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.batch_delete_chunks
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.batch_delete_chunks
+        ] = mock_object
+
+        request = {}
+        await client.batch_delete_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.batch_delete_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_batch_delete_chunks_async(
     transport: str = "grpc_asyncio",
     request_type=retriever_service.BatchDeleteChunksRequest,
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -6317,14 +7978,17 @@
     client = RetrieverServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_chunks), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_chunks()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.ListChunksRequest()
 
 
 def test_list_chunks_non_empty_request_with_auto_populated_field():
@@ -6341,23 +8005,61 @@
     request = retriever_service.ListChunksRequest(
         parent="parent_value",
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_chunks), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_chunks(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.ListChunksRequest(
             parent="parent_value",
             page_token="page_token_value",
         )
 
 
+def test_list_chunks_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_chunks in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_chunks] = mock_rpc
+        request = {}
+        client.list_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_chunks_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -6374,14 +8076,60 @@
         response = await client.list_chunks()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == retriever_service.ListChunksRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_chunks_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = RetrieverServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_chunks
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_chunks
+        ] = mock_object
+
+        request = {}
+        await client.list_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_chunks_async(
     transport: str = "grpc_asyncio", request_type=retriever_service.ListChunksRequest
 ):
     client = RetrieverServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -6860,14 +8608,50 @@
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever.Corpus)
     assert response.name == "name_value"
     assert response.display_name == "display_name_value"
 
 
+def test_create_corpus_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.create_corpus] = mock_rpc
+
+        request = {}
+        client.create_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_create_corpus_rest_required_fields(
     request_type=retriever_service.CreateCorpusRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -7121,14 +8905,50 @@
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever.Corpus)
     assert response.name == "name_value"
     assert response.display_name == "display_name_value"
 
 
+def test_get_corpus_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_corpus] = mock_rpc
+
+        request = {}
+        client.get_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_corpus_rest_required_fields(
     request_type=retriever_service.GetCorpusRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -7459,14 +9279,50 @@
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever.Corpus)
     assert response.name == "name_value"
     assert response.display_name == "display_name_value"
 
 
+def test_update_corpus_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.update_corpus] = mock_rpc
+
+        request = {}
+        client.update_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_update_corpus_rest_required_fields(
     request_type=retriever_service.UpdateCorpusRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -7725,14 +9581,50 @@
         req.return_value = response_value
         response = client.delete_corpus(request)
 
     # Establish that the response is the type that we expect.
     assert response is None
 
 
+def test_delete_corpus_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.delete_corpus] = mock_rpc
+
+        request = {}
+        client.delete_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_delete_corpus_rest_required_fields(
     request_type=retriever_service.DeleteCorpusRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -7979,14 +9871,50 @@
         response = client.list_corpora(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListCorporaPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_corpora_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_corpora in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_corpora] = mock_rpc
+
+        request = {}
+        client.list_corpora(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_corpora(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_list_corpora_rest_interceptors(null_interceptor):
     transport = transports.RetrieverServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
         if null_interceptor
         else transports.RetrieverServiceRestInterceptor(),
@@ -8160,14 +10088,50 @@
         req.return_value = response_value
         response = client.query_corpus(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever_service.QueryCorpusResponse)
 
 
+def test_query_corpus_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.query_corpus in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.query_corpus] = mock_rpc
+
+        request = {}
+        client.query_corpus(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.query_corpus(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_query_corpus_rest_required_fields(
     request_type=retriever_service.QueryCorpusRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -8466,14 +10430,50 @@
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever.Document)
     assert response.name == "name_value"
     assert response.display_name == "display_name_value"
 
 
+def test_create_document_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.create_document] = mock_rpc
+
+        request = {}
+        client.create_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_create_document_rest_required_fields(
     request_type=retriever_service.CreateDocumentRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["parent"] = ""
@@ -8742,14 +10742,50 @@
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever.Document)
     assert response.name == "name_value"
     assert response.display_name == "display_name_value"
 
 
+def test_get_document_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_document] = mock_rpc
+
+        request = {}
+        client.get_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_document_rest_required_fields(
     request_type=retriever_service.GetDocumentRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -9088,14 +11124,50 @@
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever.Document)
     assert response.name == "name_value"
     assert response.display_name == "display_name_value"
 
 
+def test_update_document_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.update_document] = mock_rpc
+
+        request = {}
+        client.update_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_update_document_rest_required_fields(
     request_type=retriever_service.UpdateDocumentRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -9355,14 +11427,50 @@
         req.return_value = response_value
         response = client.delete_document(request)
 
     # Establish that the response is the type that we expect.
     assert response is None
 
 
+def test_delete_document_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.delete_document] = mock_rpc
+
+        request = {}
+        client.delete_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_delete_document_rest_required_fields(
     request_type=retriever_service.DeleteDocumentRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -9609,14 +11717,50 @@
         response = client.list_documents(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListDocumentsPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_documents_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_documents in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_documents] = mock_rpc
+
+        request = {}
+        client.list_documents(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_documents(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_list_documents_rest_required_fields(
     request_type=retriever_service.ListDocumentsRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["parent"] = ""
@@ -9943,14 +12087,50 @@
         req.return_value = response_value
         response = client.query_document(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever_service.QueryDocumentResponse)
 
 
+def test_query_document_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.query_document in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.query_document] = mock_rpc
+
+        request = {}
+        client.query_document(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.query_document(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_query_document_rest_required_fields(
     request_type=retriever_service.QueryDocumentRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -10250,14 +12430,50 @@
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever.Chunk)
     assert response.name == "name_value"
     assert response.state == retriever.Chunk.State.STATE_PENDING_PROCESSING
 
 
+def test_create_chunk_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_chunk in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.create_chunk] = mock_rpc
+
+        request = {}
+        client.create_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_create_chunk_rest_required_fields(
     request_type=retriever_service.CreateChunkRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["parent"] = ""
@@ -10522,14 +12738,54 @@
         req.return_value = response_value
         response = client.batch_create_chunks(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever_service.BatchCreateChunksResponse)
 
 
+def test_batch_create_chunks_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_create_chunks in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_create_chunks
+        ] = mock_rpc
+
+        request = {}
+        client.batch_create_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_create_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_batch_create_chunks_rest_required_fields(
     request_type=retriever_service.BatchCreateChunksRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -10730,14 +12986,50 @@
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever.Chunk)
     assert response.name == "name_value"
     assert response.state == retriever.Chunk.State.STATE_PENDING_PROCESSING
 
 
+def test_get_chunk_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_chunk in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_chunk] = mock_rpc
+
+        request = {}
+        client.get_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_chunk_rest_required_fields(request_type=retriever_service.GetChunkRequest):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
     request = request_type(**request_init)
     pb_request = request_type.pb(request)
@@ -11078,14 +13370,50 @@
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever.Chunk)
     assert response.name == "name_value"
     assert response.state == retriever.Chunk.State.STATE_PENDING_PROCESSING
 
 
+def test_update_chunk_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_chunk in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.update_chunk] = mock_rpc
+
+        request = {}
+        client.update_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_update_chunk_rest_required_fields(
     request_type=retriever_service.UpdateChunkRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -11352,14 +13680,54 @@
         req.return_value = response_value
         response = client.batch_update_chunks(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, retriever_service.BatchUpdateChunksResponse)
 
 
+def test_batch_update_chunks_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_update_chunks in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_update_chunks
+        ] = mock_rpc
+
+        request = {}
+        client.batch_update_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_update_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_batch_update_chunks_rest_required_fields(
     request_type=retriever_service.BatchUpdateChunksRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -11553,14 +13921,50 @@
         req.return_value = response_value
         response = client.delete_chunk(request)
 
     # Establish that the response is the type that we expect.
     assert response is None
 
 
+def test_delete_chunk_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_chunk in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.delete_chunk] = mock_rpc
+
+        request = {}
+        client.delete_chunk(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_chunk(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_delete_chunk_rest_required_fields(
     request_type=retriever_service.DeleteChunkRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -11801,14 +14205,54 @@
         req.return_value = response_value
         response = client.batch_delete_chunks(request)
 
     # Establish that the response is the type that we expect.
     assert response is None
 
 
+def test_batch_delete_chunks_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.batch_delete_chunks in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_delete_chunks
+        ] = mock_rpc
+
+        request = {}
+        client.batch_delete_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_delete_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_batch_delete_chunks_rest_required_fields(
     request_type=retriever_service.BatchDeleteChunksRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -11996,14 +14440,50 @@
         response = client.list_chunks(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListChunksPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_chunks_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = RetrieverServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_chunks in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_chunks] = mock_rpc
+
+        request = {}
+        client.list_chunks(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_chunks(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_list_chunks_rest_required_fields(
     request_type=retriever_service.ListChunksRequest,
 ):
     transport_class = transports.RetrieverServiceRestTransport
 
     request_init = {}
     request_init["parent"] = ""
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta/test_text_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_text_service.py`

 * *Files 4% similar despite different names*

```diff
@@ -1118,14 +1118,17 @@
     client = TextServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.GenerateTextRequest()
 
 
 def test_generate_text_non_empty_request_with_auto_populated_field():
@@ -1141,22 +1144,60 @@
     # if they meet the requirements of AIP 4235.
     request = text_service.GenerateTextRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_text(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.GenerateTextRequest(
             model="model_value",
         )
 
 
+def test_generate_text_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.generate_text] = mock_rpc
+        request = {}
+        client.generate_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_generate_text_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1171,14 +1212,60 @@
         response = await client.generate_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.GenerateTextRequest()
 
 
 @pytest.mark.asyncio
+async def test_generate_text_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.generate_text
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.generate_text
+        ] = mock_object
+
+        request = {}
+        await client.generate_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.generate_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_generate_text_async(
     transport: str = "grpc_asyncio", request_type=text_service.GenerateTextRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1444,14 +1531,17 @@
     client = TextServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.EmbedTextRequest()
 
 
 def test_embed_text_non_empty_request_with_auto_populated_field():
@@ -1468,23 +1558,61 @@
     request = text_service.EmbedTextRequest(
         model="model_value",
         text="text_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_text(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.EmbedTextRequest(
             model="model_value",
             text="text_value",
         )
 
 
+def test_embed_text_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_text] = mock_rpc
+        request = {}
+        client.embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_embed_text_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1499,14 +1627,58 @@
         response = await client.embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.EmbedTextRequest()
 
 
 @pytest.mark.asyncio
+async def test_embed_text_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.embed_text
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.embed_text
+        ] = mock_object
+
+        request = {}
+        await client.embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_embed_text_async(
     transport: str = "grpc_asyncio", request_type=text_service.EmbedTextRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1730,14 +1902,17 @@
     client = TextServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.batch_embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.BatchEmbedTextRequest()
 
 
 def test_batch_embed_text_non_empty_request_with_auto_populated_field():
@@ -1753,22 +1928,62 @@
     # if they meet the requirements of AIP 4235.
     request = text_service.BatchEmbedTextRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.batch_embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_embed_text(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.BatchEmbedTextRequest(
             model="model_value",
         )
 
 
+def test_batch_embed_text_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.batch_embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_embed_text
+        ] = mock_rpc
+        request = {}
+        client.batch_embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_batch_embed_text_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1783,14 +1998,60 @@
         response = await client.batch_embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.BatchEmbedTextRequest()
 
 
 @pytest.mark.asyncio
+async def test_batch_embed_text_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.batch_embed_text
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.batch_embed_text
+        ] = mock_object
+
+        request = {}
+        await client.batch_embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.batch_embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_batch_embed_text_async(
     transport: str = "grpc_asyncio", request_type=text_service.BatchEmbedTextRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2021,14 +2282,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_text_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_text_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.CountTextTokensRequest()
 
 
 def test_count_text_tokens_non_empty_request_with_auto_populated_field():
@@ -2046,22 +2310,62 @@
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_text_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_text_tokens(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.CountTextTokensRequest(
             model="model_value",
         )
 
 
+def test_count_text_tokens_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.count_text_tokens in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_text_tokens
+        ] = mock_rpc
+        request = {}
+        client.count_text_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_text_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_count_text_tokens_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2080,14 +2384,60 @@
         response = await client.count_text_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.CountTextTokensRequest()
 
 
 @pytest.mark.asyncio
+async def test_count_text_tokens_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.count_text_tokens
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.count_text_tokens
+        ] = mock_object
+
+        request = {}
+        await client.count_text_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.count_text_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_count_text_tokens_async(
     transport: str = "grpc_asyncio", request_type=text_service.CountTextTokensRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2318,14 +2668,50 @@
         req.return_value = response_value
         response = client.generate_text(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.GenerateTextResponse)
 
 
+def test_generate_text_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.generate_text] = mock_rpc
+
+        request = {}
+        client.generate_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_generate_text_rest_required_fields(
     request_type=text_service.GenerateTextRequest,
 ):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -2601,14 +2987,50 @@
         req.return_value = response_value
         response = client.embed_text(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.EmbedTextResponse)
 
 
+def test_embed_text_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_text] = mock_rpc
+
+        request = {}
+        client.embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_embed_text_rest_required_fields(request_type=text_service.EmbedTextRequest):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
     request = request_type(**request_init)
     pb_request = request_type.pb(request)
@@ -2862,14 +3284,52 @@
         req.return_value = response_value
         response = client.batch_embed_text(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.BatchEmbedTextResponse)
 
 
+def test_batch_embed_text_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.batch_embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_embed_text
+        ] = mock_rpc
+
+        request = {}
+        client.batch_embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_batch_embed_text_rest_required_fields(
     request_type=text_service.BatchEmbedTextRequest,
 ):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -3131,14 +3591,52 @@
         response = client.count_text_tokens(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.CountTextTokensResponse)
     assert response.token_count == 1193
 
 
+def test_count_text_tokens_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.count_text_tokens in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_text_tokens
+        ] = mock_rpc
+
+        request = {}
+        client.count_text_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_text_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_count_text_tokens_rest_required_fields(
     request_type=text_service.CountTextTokensRequest,
 ):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta2/__init__.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta2/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta2/test_discuss_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/test_discuss_service.py`

 * *Files 5% similar despite different names*

```diff
@@ -28,30 +28,31 @@
 
 from google.api_core import gapic_v1, grpc_helpers, grpc_helpers_async, path_template
 from google.api_core import api_core_version, client_options
 from google.api_core import exceptions as core_exceptions
 import google.auth
 from google.auth import credentials as ga_credentials
 from google.auth.exceptions import MutualTLSChannelError
+from google.longrunning import operations_pb2  # type: ignore
 from google.oauth2 import service_account
 from google.protobuf import json_format
 import grpc
 from grpc.experimental import aio
 from proto.marshal.rules import wrappers
 from proto.marshal.rules.dates import DurationRule, TimestampRule
 import pytest
 from requests import PreparedRequest, Request, Response
 from requests.sessions import Session
 
-from google.ai.generativelanguage_v1beta2.services.discuss_service import (
+from google.ai.generativelanguage_v1beta3.services.discuss_service import (
     DiscussServiceAsyncClient,
     DiscussServiceClient,
     transports,
 )
-from google.ai.generativelanguage_v1beta2.types import citation, discuss_service, safety
+from google.ai.generativelanguage_v1beta3.types import citation, discuss_service, safety
 
 
 def client_cert_source_callback():
     return b"cert bytes", b"key bytes"
 
 
 # If default endpoint is localhost, then default mtls endpoint will be the same.
@@ -1023,15 +1024,15 @@
             always_use_jwt_access=True,
             api_audience=None,
         )
 
 
 def test_discuss_service_client_client_options_from_dict():
     with mock.patch(
-        "google.ai.generativelanguage_v1beta2.services.discuss_service.transports.DiscussServiceGrpcTransport.__init__"
+        "google.ai.generativelanguage_v1beta3.services.discuss_service.transports.DiscussServiceGrpcTransport.__init__"
     ) as grpc_transport:
         grpc_transport.return_value = None
         client = DiscussServiceClient(
             client_options={"api_endpoint": "squid.clam.whelk"}
         )
         grpc_transport.assert_called_once_with(
             credentials=None,
@@ -1154,14 +1155,17 @@
     client = DiscussServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_message), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_message()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.GenerateMessageRequest()
 
 
 def test_generate_message_non_empty_request_with_auto_populated_field():
@@ -1177,22 +1181,62 @@
     # if they meet the requirements of AIP 4235.
     request = discuss_service.GenerateMessageRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_message), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_message(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.GenerateMessageRequest(
             model="model_value",
         )
 
 
+def test_generate_message_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_message in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_message
+        ] = mock_rpc
+        request = {}
+        client.generate_message(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_message(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_generate_message_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1207,14 +1251,60 @@
         response = await client.generate_message()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.GenerateMessageRequest()
 
 
 @pytest.mark.asyncio
+async def test_generate_message_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = DiscussServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.generate_message
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.generate_message
+        ] = mock_object
+
+        request = {}
+        await client.generate_message(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.generate_message(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_generate_message_async(
     transport: str = "grpc_asyncio", request_type=discuss_service.GenerateMessageRequest
 ):
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1477,14 +1567,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_message_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_message_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.CountMessageTokensRequest()
 
 
 def test_count_message_tokens_non_empty_request_with_auto_populated_field():
@@ -1502,22 +1595,64 @@
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_message_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_message_tokens(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.CountMessageTokensRequest(
             model="model_value",
         )
 
 
+def test_count_message_tokens_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.count_message_tokens in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_message_tokens
+        ] = mock_rpc
+        request = {}
+        client.count_message_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_message_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_count_message_tokens_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1536,14 +1671,60 @@
         response = await client.count_message_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.CountMessageTokensRequest()
 
 
 @pytest.mark.asyncio
+async def test_count_message_tokens_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = DiscussServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.count_message_tokens
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.count_message_tokens
+        ] = mock_object
+
+        request = {}
+        await client.count_message_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.count_message_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_count_message_tokens_async(
     transport: str = "grpc_asyncio",
     request_type=discuss_service.CountMessageTokensRequest,
 ):
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1775,14 +1956,52 @@
         req.return_value = response_value
         response = client.generate_message(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, discuss_service.GenerateMessageResponse)
 
 
+def test_generate_message_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_message in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_message
+        ] = mock_rpc
+
+        request = {}
+        client.generate_message(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_message(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_generate_message_rest_required_fields(
     request_type=discuss_service.GenerateMessageRequest,
 ):
     transport_class = transports.DiscussServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -1989,15 +2208,15 @@
         client.generate_message(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta2/{model=models/*}:generateMessage" % client.transport._host,
+            "%s/v1beta3/{model=models/*}:generateMessage" % client.transport._host,
             args[1],
         )
 
 
 def test_generate_message_rest_flattened_error(transport: str = "rest"):
     client = DiscussServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -2060,14 +2279,54 @@
         response = client.count_message_tokens(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, discuss_service.CountMessageTokensResponse)
     assert response.token_count == 1193
 
 
+def test_count_message_tokens_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.count_message_tokens in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_message_tokens
+        ] = mock_rpc
+
+        request = {}
+        client.count_message_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_message_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_count_message_tokens_rest_required_fields(
     request_type=discuss_service.CountMessageTokensRequest,
 ):
     transport_class = transports.DiscussServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -2270,15 +2529,15 @@
         client.count_message_tokens(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta2/{model=models/*}:countMessageTokens" % client.transport._host,
+            "%s/v1beta3/{model=models/*}:countMessageTokens" % client.transport._host,
             args[1],
         )
 
 
 def test_count_message_tokens_rest_flattened_error(transport: str = "rest"):
     client = DiscussServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -2426,15 +2685,15 @@
             credentials_file="credentials.json",
         )
 
 
 def test_discuss_service_base_transport():
     # Instantiate the base transport.
     with mock.patch(
-        "google.ai.generativelanguage_v1beta2.services.discuss_service.transports.DiscussServiceTransport.__init__"
+        "google.ai.generativelanguage_v1beta3.services.discuss_service.transports.DiscussServiceTransport.__init__"
     ) as Transport:
         Transport.return_value = None
         transport = transports.DiscussServiceTransport(
             credentials=ga_credentials.AnonymousCredentials(),
         )
 
     # Every method on the transport should just blindly
@@ -2460,15 +2719,15 @@
 
 
 def test_discuss_service_base_transport_with_credentials_file():
     # Instantiate the base transport with a credentials file
     with mock.patch.object(
         google.auth, "load_credentials_from_file", autospec=True
     ) as load_creds, mock.patch(
-        "google.ai.generativelanguage_v1beta2.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta3.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         load_creds.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.DiscussServiceTransport(
             credentials_file="credentials.json",
             quota_project_id="octopus",
         )
@@ -2479,15 +2738,15 @@
             quota_project_id="octopus",
         )
 
 
 def test_discuss_service_base_transport_with_adc():
     # Test the default credentials are used if credentials and credentials_file are None.
     with mock.patch.object(google.auth, "default", autospec=True) as adc, mock.patch(
-        "google.ai.generativelanguage_v1beta2.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta3.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         adc.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.DiscussServiceTransport()
         adc.assert_called_once()
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta2/test_model_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta2/test_model_service.py`

 * *Files 3% similar despite different names*

```diff
@@ -1157,14 +1157,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest()
 
 
 def test_get_model_non_empty_request_with_auto_populated_field():
@@ -1180,22 +1183,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.GetModelRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest(
             name="name_value",
         )
 
 
+def test_get_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_model] = mock_rpc
+        request = {}
+        client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1222,14 +1263,58 @@
         response = await client.get_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_model_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_model
+        ] = mock_object
+
+        request = {}
+        await client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.GetModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1467,14 +1552,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_models), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest()
 
 
 def test_list_models_non_empty_request_with_auto_populated_field():
@@ -1490,22 +1578,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.ListModelsRequest(
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_models), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_models(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest(
             page_token="page_token_value",
         )
 
 
+def test_list_models_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_models] = mock_rpc
+        request = {}
+        client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_models_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1522,14 +1648,60 @@
         response = await client.list_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_models_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_models
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_models
+        ] = mock_object
+
+        request = {}
+        await client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_models_async(
     transport: str = "grpc_asyncio", request_type=model_service.ListModelsRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1901,14 +2073,50 @@
         "supported_generation_methods_value"
     ]
     assert math.isclose(response.temperature, 0.1198, rel_tol=1e-6)
     assert math.isclose(response.top_p, 0.546, rel_tol=1e-6)
     assert response.top_k == 541
 
 
+def test_get_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_model] = mock_rpc
+
+        request = {}
+        client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_model_rest_required_fields(request_type=model_service.GetModelRequest):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
     request = request_type(**request_init)
     pb_request = request_type.pb(request)
@@ -2160,14 +2368,50 @@
         response = client.list_models(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListModelsPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_models_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_models] = mock_rpc
+
+        request = {}
+        client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_list_models_rest_interceptors(null_interceptor):
     transport = transports.ModelServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
         if null_interceptor
         else transports.ModelServiceRestInterceptor(),
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta2/test_text_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta2/test_text_service.py`

 * *Files 6% similar despite different names*

```diff
@@ -1117,14 +1117,17 @@
     client = TextServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.GenerateTextRequest()
 
 
 def test_generate_text_non_empty_request_with_auto_populated_field():
@@ -1140,22 +1143,60 @@
     # if they meet the requirements of AIP 4235.
     request = text_service.GenerateTextRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_text(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.GenerateTextRequest(
             model="model_value",
         )
 
 
+def test_generate_text_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.generate_text] = mock_rpc
+        request = {}
+        client.generate_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_generate_text_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1170,14 +1211,60 @@
         response = await client.generate_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.GenerateTextRequest()
 
 
 @pytest.mark.asyncio
+async def test_generate_text_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.generate_text
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.generate_text
+        ] = mock_object
+
+        request = {}
+        await client.generate_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.generate_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_generate_text_async(
     transport: str = "grpc_asyncio", request_type=text_service.GenerateTextRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1443,14 +1530,17 @@
     client = TextServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.EmbedTextRequest()
 
 
 def test_embed_text_non_empty_request_with_auto_populated_field():
@@ -1467,23 +1557,61 @@
     request = text_service.EmbedTextRequest(
         model="model_value",
         text="text_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_text(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.EmbedTextRequest(
             model="model_value",
             text="text_value",
         )
 
 
+def test_embed_text_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_text] = mock_rpc
+        request = {}
+        client.embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_embed_text_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1498,14 +1626,58 @@
         response = await client.embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.EmbedTextRequest()
 
 
 @pytest.mark.asyncio
+async def test_embed_text_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.embed_text
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.embed_text
+        ] = mock_object
+
+        request = {}
+        await client.embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_embed_text_async(
     transport: str = "grpc_asyncio", request_type=text_service.EmbedTextRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1723,14 +1895,50 @@
         req.return_value = response_value
         response = client.generate_text(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.GenerateTextResponse)
 
 
+def test_generate_text_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.generate_text] = mock_rpc
+
+        request = {}
+        client.generate_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_generate_text_rest_required_fields(
     request_type=text_service.GenerateTextRequest,
 ):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -2006,14 +2214,50 @@
         req.return_value = response_value
         response = client.embed_text(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.EmbedTextResponse)
 
 
+def test_embed_text_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_text] = mock_rpc
+
+        request = {}
+        client.embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_embed_text_rest_required_fields(request_type=text_service.EmbedTextRequest):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
     request_init["text"] = ""
     request = request_type(**request_init)
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/__init__.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/__init__.py`

 * *Files identical despite different names*

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/test_discuss_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_discuss_service.py`

 * *Files 5% similar despite different names*

```diff
@@ -39,20 +39,20 @@
 from grpc.experimental import aio
 from proto.marshal.rules import wrappers
 from proto.marshal.rules.dates import DurationRule, TimestampRule
 import pytest
 from requests import PreparedRequest, Request, Response
 from requests.sessions import Session
 
-from google.ai.generativelanguage_v1beta3.services.discuss_service import (
+from google.ai.generativelanguage_v1beta.services.discuss_service import (
     DiscussServiceAsyncClient,
     DiscussServiceClient,
     transports,
 )
-from google.ai.generativelanguage_v1beta3.types import citation, discuss_service, safety
+from google.ai.generativelanguage_v1beta.types import citation, discuss_service, safety
 
 
 def client_cert_source_callback():
     return b"cert bytes", b"key bytes"
 
 
 # If default endpoint is localhost, then default mtls endpoint will be the same.
@@ -1024,15 +1024,15 @@
             always_use_jwt_access=True,
             api_audience=None,
         )
 
 
 def test_discuss_service_client_client_options_from_dict():
     with mock.patch(
-        "google.ai.generativelanguage_v1beta3.services.discuss_service.transports.DiscussServiceGrpcTransport.__init__"
+        "google.ai.generativelanguage_v1beta.services.discuss_service.transports.DiscussServiceGrpcTransport.__init__"
     ) as grpc_transport:
         grpc_transport.return_value = None
         client = DiscussServiceClient(
             client_options={"api_endpoint": "squid.clam.whelk"}
         )
         grpc_transport.assert_called_once_with(
             credentials=None,
@@ -1155,14 +1155,17 @@
     client = DiscussServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_message), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_message()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.GenerateMessageRequest()
 
 
 def test_generate_message_non_empty_request_with_auto_populated_field():
@@ -1178,22 +1181,62 @@
     # if they meet the requirements of AIP 4235.
     request = discuss_service.GenerateMessageRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_message), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_message(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.GenerateMessageRequest(
             model="model_value",
         )
 
 
+def test_generate_message_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_message in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_message
+        ] = mock_rpc
+        request = {}
+        client.generate_message(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_message(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_generate_message_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1208,14 +1251,60 @@
         response = await client.generate_message()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.GenerateMessageRequest()
 
 
 @pytest.mark.asyncio
+async def test_generate_message_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = DiscussServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.generate_message
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.generate_message
+        ] = mock_object
+
+        request = {}
+        await client.generate_message(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.generate_message(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_generate_message_async(
     transport: str = "grpc_asyncio", request_type=discuss_service.GenerateMessageRequest
 ):
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1478,14 +1567,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_message_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_message_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.CountMessageTokensRequest()
 
 
 def test_count_message_tokens_non_empty_request_with_auto_populated_field():
@@ -1503,22 +1595,64 @@
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_message_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_message_tokens(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.CountMessageTokensRequest(
             model="model_value",
         )
 
 
+def test_count_message_tokens_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.count_message_tokens in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_message_tokens
+        ] = mock_rpc
+        request = {}
+        client.count_message_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_message_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_count_message_tokens_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1537,14 +1671,60 @@
         response = await client.count_message_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == discuss_service.CountMessageTokensRequest()
 
 
 @pytest.mark.asyncio
+async def test_count_message_tokens_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = DiscussServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.count_message_tokens
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.count_message_tokens
+        ] = mock_object
+
+        request = {}
+        await client.count_message_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.count_message_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_count_message_tokens_async(
     transport: str = "grpc_asyncio",
     request_type=discuss_service.CountMessageTokensRequest,
 ):
     client = DiscussServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1776,14 +1956,52 @@
         req.return_value = response_value
         response = client.generate_message(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, discuss_service.GenerateMessageResponse)
 
 
+def test_generate_message_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_message in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.generate_message
+        ] = mock_rpc
+
+        request = {}
+        client.generate_message(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_message(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_generate_message_rest_required_fields(
     request_type=discuss_service.GenerateMessageRequest,
 ):
     transport_class = transports.DiscussServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -1990,15 +2208,15 @@
         client.generate_message(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta3/{model=models/*}:generateMessage" % client.transport._host,
+            "%s/v1beta/{model=models/*}:generateMessage" % client.transport._host,
             args[1],
         )
 
 
 def test_generate_message_rest_flattened_error(transport: str = "rest"):
     client = DiscussServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -2061,14 +2279,54 @@
         response = client.count_message_tokens(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, discuss_service.CountMessageTokensResponse)
     assert response.token_count == 1193
 
 
+def test_count_message_tokens_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = DiscussServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.count_message_tokens in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_message_tokens
+        ] = mock_rpc
+
+        request = {}
+        client.count_message_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_message_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_count_message_tokens_rest_required_fields(
     request_type=discuss_service.CountMessageTokensRequest,
 ):
     transport_class = transports.DiscussServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -2271,15 +2529,15 @@
         client.count_message_tokens(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta3/{model=models/*}:countMessageTokens" % client.transport._host,
+            "%s/v1beta/{model=models/*}:countMessageTokens" % client.transport._host,
             args[1],
         )
 
 
 def test_count_message_tokens_rest_flattened_error(transport: str = "rest"):
     client = DiscussServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -2427,15 +2685,15 @@
             credentials_file="credentials.json",
         )
 
 
 def test_discuss_service_base_transport():
     # Instantiate the base transport.
     with mock.patch(
-        "google.ai.generativelanguage_v1beta3.services.discuss_service.transports.DiscussServiceTransport.__init__"
+        "google.ai.generativelanguage_v1beta.services.discuss_service.transports.DiscussServiceTransport.__init__"
     ) as Transport:
         Transport.return_value = None
         transport = transports.DiscussServiceTransport(
             credentials=ga_credentials.AnonymousCredentials(),
         )
 
     # Every method on the transport should just blindly
@@ -2461,15 +2719,15 @@
 
 
 def test_discuss_service_base_transport_with_credentials_file():
     # Instantiate the base transport with a credentials file
     with mock.patch.object(
         google.auth, "load_credentials_from_file", autospec=True
     ) as load_creds, mock.patch(
-        "google.ai.generativelanguage_v1beta3.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         load_creds.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.DiscussServiceTransport(
             credentials_file="credentials.json",
             quota_project_id="octopus",
         )
@@ -2480,15 +2738,15 @@
             quota_project_id="octopus",
         )
 
 
 def test_discuss_service_base_transport_with_adc():
     # Test the default credentials are used if credentials and credentials_file are None.
     with mock.patch.object(google.auth, "default", autospec=True) as adc, mock.patch(
-        "google.ai.generativelanguage_v1beta3.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta.services.discuss_service.transports.DiscussServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         adc.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.DiscussServiceTransport()
         adc.assert_called_once()
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/test_model_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/test_model_service.py`

 * *Files 11% similar despite different names*

```diff
@@ -1171,14 +1171,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest()
 
 
 def test_get_model_non_empty_request_with_auto_populated_field():
@@ -1194,22 +1197,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.GetModelRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest(
             name="name_value",
         )
 
 
+def test_get_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_model] = mock_rpc
+        request = {}
+        client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1236,14 +1277,58 @@
         response = await client.get_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_model_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_model
+        ] = mock_object
+
+        request = {}
+        await client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.GetModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1481,14 +1566,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_models), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest()
 
 
 def test_list_models_non_empty_request_with_auto_populated_field():
@@ -1504,22 +1592,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.ListModelsRequest(
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_models), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_models(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest(
             page_token="page_token_value",
         )
 
 
+def test_list_models_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_models] = mock_rpc
+        request = {}
+        client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_models_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1536,14 +1662,60 @@
         response = await client.list_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListModelsRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_models_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_models
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_models
+        ] = mock_object
+
+        request = {}
+        await client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_models_async(
     transport: str = "grpc_asyncio", request_type=model_service.ListModelsRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1912,14 +2084,17 @@
     client = ModelServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_tuned_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetTunedModelRequest()
 
 
 def test_get_tuned_model_non_empty_request_with_auto_populated_field():
@@ -1935,22 +2110,60 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.GetTunedModelRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_tuned_model), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_tuned_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetTunedModelRequest(
             name="name_value",
         )
 
 
+def test_get_tuned_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_tuned_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_tuned_model] = mock_rpc
+        request = {}
+        client.get_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_tuned_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1973,14 +2186,60 @@
         response = await client.get_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.GetTunedModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_tuned_model_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_tuned_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_tuned_model
+        ] = mock_object
+
+        request = {}
+        await client.get_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_tuned_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.GetTunedModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2216,14 +2475,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.list_tuned_models), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_tuned_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListTunedModelsRequest()
 
 
 def test_list_tuned_models_non_empty_request_with_auto_populated_field():
@@ -2241,22 +2503,62 @@
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.list_tuned_models), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_tuned_models(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListTunedModelsRequest(
             page_token="page_token_value",
         )
 
 
+def test_list_tuned_models_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_tuned_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.list_tuned_models
+        ] = mock_rpc
+        request = {}
+        client.list_tuned_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_tuned_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_tuned_models_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2275,14 +2577,60 @@
         response = await client.list_tuned_models()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.ListTunedModelsRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_tuned_models_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_tuned_models
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_tuned_models
+        ] = mock_object
+
+        request = {}
+        await client.list_tuned_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_tuned_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_tuned_models_async(
     transport: str = "grpc_asyncio", request_type=model_service.ListTunedModelsRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2653,14 +3001,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.create_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.CreateTunedModelRequest()
 
 
 def test_create_tuned_model_non_empty_request_with_auto_populated_field():
@@ -2678,22 +3029,68 @@
         tuned_model_id="tuned_model_id_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.create_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_tuned_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.CreateTunedModelRequest(
             tuned_model_id="tuned_model_id_value",
         )
 
 
+def test_create_tuned_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.create_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.create_tuned_model
+        ] = mock_rpc
+        request = {}
+        client.create_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        # Operation methods build a cached wrapper on first rpc call
+        # subsequent calls should use the cached wrapper
+        wrapper_fn.reset_mock()
+
+        client.create_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_create_tuned_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2710,14 +3107,64 @@
         response = await client.create_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.CreateTunedModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_create_tuned_model_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.create_tuned_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.create_tuned_model
+        ] = mock_object
+
+        request = {}
+        await client.create_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        # Operation methods build a cached wrapper on first rpc call
+        # subsequent calls should use the cached wrapper
+        wrapper_fn.reset_mock()
+
+        await client.create_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_create_tuned_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.CreateTunedModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2930,14 +3377,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.update_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.UpdateTunedModelRequest()
 
 
 def test_update_tuned_model_non_empty_request_with_auto_populated_field():
@@ -2953,20 +3403,62 @@
     # if they meet the requirements of AIP 4235.
     request = model_service.UpdateTunedModelRequest()
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.update_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_tuned_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.UpdateTunedModelRequest()
 
 
+def test_update_tuned_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.update_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.update_tuned_model
+        ] = mock_rpc
+        request = {}
+        client.update_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_update_tuned_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2991,14 +3483,60 @@
         response = await client.update_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.UpdateTunedModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_update_tuned_model_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.update_tuned_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.update_tuned_model
+        ] = mock_object
+
+        request = {}
+        await client.update_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.update_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_update_tuned_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.UpdateTunedModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -3275,14 +3813,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.delete_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.DeleteTunedModelRequest()
 
 
 def test_delete_tuned_model_non_empty_request_with_auto_populated_field():
@@ -3300,22 +3841,64 @@
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.delete_tuned_model), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_tuned_model(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.DeleteTunedModelRequest(
             name="name_value",
         )
 
 
+def test_delete_tuned_model_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.delete_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.delete_tuned_model
+        ] = mock_rpc
+        request = {}
+        client.delete_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_delete_tuned_model_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -3330,14 +3913,60 @@
         response = await client.delete_tuned_model()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == model_service.DeleteTunedModelRequest()
 
 
 @pytest.mark.asyncio
+async def test_delete_tuned_model_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = ModelServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.delete_tuned_model
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.delete_tuned_model
+        ] = mock_object
+
+        request = {}
+        await client.delete_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.delete_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_delete_tuned_model_async(
     transport: str = "grpc_asyncio", request_type=model_service.DeleteTunedModelRequest
 ):
     client = ModelServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -3574,14 +4203,50 @@
         "supported_generation_methods_value"
     ]
     assert math.isclose(response.temperature, 0.1198, rel_tol=1e-6)
     assert math.isclose(response.top_p, 0.546, rel_tol=1e-6)
     assert response.top_k == 541
 
 
+def test_get_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_model] = mock_rpc
+
+        request = {}
+        client.get_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_model_rest_required_fields(request_type=model_service.GetModelRequest):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
     request = request_type(**request_init)
     pb_request = request_type.pb(request)
@@ -3833,14 +4498,50 @@
         response = client.list_models(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListModelsPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_models_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.list_models] = mock_rpc
+
+        request = {}
+        client.list_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_list_models_rest_interceptors(null_interceptor):
     transport = transports.ModelServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
         if null_interceptor
         else transports.ModelServiceRestInterceptor(),
@@ -4085,14 +4786,50 @@
     assert response.description == "description_value"
     assert math.isclose(response.temperature, 0.1198, rel_tol=1e-6)
     assert math.isclose(response.top_p, 0.546, rel_tol=1e-6)
     assert response.top_k == 541
     assert response.state == tuned_model.TunedModel.State.CREATING
 
 
+def test_get_tuned_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_tuned_model in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_tuned_model] = mock_rpc
+
+        request = {}
+        client.get_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_tuned_model_rest_required_fields(
     request_type=model_service.GetTunedModelRequest,
 ):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -4350,14 +5087,52 @@
         response = client.list_tuned_models(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListTunedModelsPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_tuned_models_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_tuned_models in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.list_tuned_models
+        ] = mock_rpc
+
+        request = {}
+        client.list_tuned_models(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_tuned_models(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.parametrize("null_interceptor", [True, False])
 def test_list_tuned_models_rest_interceptors(null_interceptor):
     transport = transports.ModelServiceRestTransport(
         credentials=ga_credentials.AnonymousCredentials(),
         interceptor=None
         if null_interceptor
         else transports.ModelServiceRestInterceptor(),
@@ -4688,14 +5463,58 @@
         req.return_value = response_value
         response = client.create_tuned_model(request)
 
     # Establish that the response is the type that we expect.
     assert response.operation.name == "operations/spam"
 
 
+def test_create_tuned_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.create_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.create_tuned_model
+        ] = mock_rpc
+
+        request = {}
+        client.create_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        # Operation methods build a cached wrapper on first rpc call
+        # subsequent calls should use the cached wrapper
+        wrapper_fn.reset_mock()
+
+        client.create_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_create_tuned_model_rest_required_fields(
     request_type=model_service.CreateTunedModelRequest,
 ):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -5073,14 +5892,54 @@
     assert response.description == "description_value"
     assert math.isclose(response.temperature, 0.1198, rel_tol=1e-6)
     assert math.isclose(response.top_p, 0.546, rel_tol=1e-6)
     assert response.top_k == 541
     assert response.state == gag_tuned_model.TunedModel.State.CREATING
 
 
+def test_update_tuned_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.update_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.update_tuned_model
+        ] = mock_rpc
+
+        request = {}
+        client.update_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_update_tuned_model_rest_required_fields(
     request_type=model_service.UpdateTunedModelRequest,
 ):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -5350,14 +6209,54 @@
         req.return_value = response_value
         response = client.delete_tuned_model(request)
 
     # Establish that the response is the type that we expect.
     assert response is None
 
 
+def test_delete_tuned_model_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = ModelServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.delete_tuned_model in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.delete_tuned_model
+        ] = mock_rpc
+
+        request = {}
+        client.delete_tuned_model(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_tuned_model(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_delete_tuned_model_rest_required_fields(
     request_type=model_service.DeleteTunedModelRequest,
 ):
     transport_class = transports.ModelServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/test_permission_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta/test_permission_service.py`

 * *Files 3% similar despite different names*

```diff
@@ -40,23 +40,23 @@
 from grpc.experimental import aio
 from proto.marshal.rules import wrappers
 from proto.marshal.rules.dates import DurationRule, TimestampRule
 import pytest
 from requests import PreparedRequest, Request, Response
 from requests.sessions import Session
 
-from google.ai.generativelanguage_v1beta3.services.permission_service import (
+from google.ai.generativelanguage_v1beta.services.permission_service import (
     PermissionServiceAsyncClient,
     PermissionServiceClient,
     pagers,
     transports,
 )
-from google.ai.generativelanguage_v1beta3.types import permission as gag_permission
-from google.ai.generativelanguage_v1beta3.types import permission
-from google.ai.generativelanguage_v1beta3.types import permission_service
+from google.ai.generativelanguage_v1beta.types import permission as gag_permission
+from google.ai.generativelanguage_v1beta.types import permission
+from google.ai.generativelanguage_v1beta.types import permission_service
 
 
 def client_cert_source_callback():
     return b"cert bytes", b"key bytes"
 
 
 # If default endpoint is localhost, then default mtls endpoint will be the same.
@@ -1066,15 +1066,15 @@
             always_use_jwt_access=True,
             api_audience=None,
         )
 
 
 def test_permission_service_client_client_options_from_dict():
     with mock.patch(
-        "google.ai.generativelanguage_v1beta3.services.permission_service.transports.PermissionServiceGrpcTransport.__init__"
+        "google.ai.generativelanguage_v1beta.services.permission_service.transports.PermissionServiceGrpcTransport.__init__"
     ) as grpc_transport:
         grpc_transport.return_value = None
         client = PermissionServiceClient(
             client_options={"api_endpoint": "squid.clam.whelk"}
         )
         grpc_transport.assert_called_once_with(
             credentials=None,
@@ -1210,14 +1210,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.create_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.CreatePermissionRequest()
 
 
 def test_create_permission_non_empty_request_with_auto_populated_field():
@@ -1235,22 +1238,62 @@
         parent="parent_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.create_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.create_permission(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.CreatePermissionRequest(
             parent="parent_value",
         )
 
 
+def test_create_permission_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.create_permission
+        ] = mock_rpc
+        request = {}
+        client.create_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_create_permission_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1272,14 +1315,60 @@
         response = await client.create_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.CreatePermissionRequest()
 
 
 @pytest.mark.asyncio
+async def test_create_permission_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.create_permission
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.create_permission
+        ] = mock_object
+
+        request = {}
+        await client.create_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.create_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_create_permission_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.CreatePermissionRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1532,14 +1621,17 @@
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_permission), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.GetPermissionRequest()
 
 
 def test_get_permission_non_empty_request_with_auto_populated_field():
@@ -1555,22 +1647,60 @@
     # if they meet the requirements of AIP 4235.
     request = permission_service.GetPermissionRequest(
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.get_permission), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.get_permission(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.GetPermissionRequest(
             name="name_value",
         )
 
 
+def test_get_permission_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_permission] = mock_rpc
+        request = {}
+        client.get_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_get_permission_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1590,14 +1720,60 @@
         response = await client.get_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.GetPermissionRequest()
 
 
 @pytest.mark.asyncio
+async def test_get_permission_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.get_permission
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.get_permission
+        ] = mock_object
+
+        request = {}
+        await client.get_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.get_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_get_permission_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.GetPermissionRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -1824,14 +2000,17 @@
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_permissions), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_permissions()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.ListPermissionsRequest()
 
 
 def test_list_permissions_non_empty_request_with_auto_populated_field():
@@ -1848,23 +2027,63 @@
     request = permission_service.ListPermissionsRequest(
         parent="parent_value",
         page_token="page_token_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.list_permissions), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.list_permissions(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.ListPermissionsRequest(
             parent="parent_value",
             page_token="page_token_value",
         )
 
 
+def test_list_permissions_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_permissions in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.list_permissions
+        ] = mock_rpc
+        request = {}
+        client.list_permissions(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_permissions(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_list_permissions_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1881,14 +2100,60 @@
         response = await client.list_permissions()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.ListPermissionsRequest()
 
 
 @pytest.mark.asyncio
+async def test_list_permissions_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.list_permissions
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.list_permissions
+        ] = mock_object
+
+        request = {}
+        await client.list_permissions(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.list_permissions(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_list_permissions_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.ListPermissionsRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -2309,14 +2574,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.update_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.UpdatePermissionRequest()
 
 
 def test_update_permission_non_empty_request_with_auto_populated_field():
@@ -2332,20 +2600,60 @@
     # if they meet the requirements of AIP 4235.
     request = permission_service.UpdatePermissionRequest()
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.update_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.update_permission(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.UpdatePermissionRequest()
 
 
+def test_update_permission_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.update_permission
+        ] = mock_rpc
+        request = {}
+        client.update_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_update_permission_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2367,14 +2675,60 @@
         response = await client.update_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.UpdatePermissionRequest()
 
 
 @pytest.mark.asyncio
+async def test_update_permission_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.update_permission
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.update_permission
+        ] = mock_object
+
+        request = {}
+        await client.update_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.update_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_update_permission_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.UpdatePermissionRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -2622,14 +2976,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.delete_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.DeletePermissionRequest()
 
 
 def test_delete_permission_non_empty_request_with_auto_populated_field():
@@ -2647,22 +3004,62 @@
         name="name_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.delete_permission), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.delete_permission(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.DeletePermissionRequest(
             name="name_value",
         )
 
 
+def test_delete_permission_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.delete_permission
+        ] = mock_rpc
+        request = {}
+        client.delete_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_delete_permission_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2677,14 +3074,60 @@
         response = await client.delete_permission()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.DeletePermissionRequest()
 
 
 @pytest.mark.asyncio
+async def test_delete_permission_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.delete_permission
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.delete_permission
+        ] = mock_object
+
+        request = {}
+        await client.delete_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.delete_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_delete_permission_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.DeletePermissionRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -2907,14 +3350,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.transfer_ownership), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.transfer_ownership()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.TransferOwnershipRequest()
 
 
 def test_transfer_ownership_non_empty_request_with_auto_populated_field():
@@ -2933,23 +3379,65 @@
         email_address="email_address_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.transfer_ownership), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.transfer_ownership(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.TransferOwnershipRequest(
             name="name_value",
             email_address="email_address_value",
         )
 
 
+def test_transfer_ownership_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.transfer_ownership in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.transfer_ownership
+        ] = mock_rpc
+        request = {}
+        client.transfer_ownership(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.transfer_ownership(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_transfer_ownership_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2966,14 +3454,60 @@
         response = await client.transfer_ownership()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == permission_service.TransferOwnershipRequest()
 
 
 @pytest.mark.asyncio
+async def test_transfer_ownership_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = PermissionServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.transfer_ownership
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.transfer_ownership
+        ] = mock_object
+
+        request = {}
+        await client.transfer_ownership(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.transfer_ownership(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_transfer_ownership_async(
     transport: str = "grpc_asyncio",
     request_type=permission_service.TransferOwnershipRequest,
 ):
     client = PermissionServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
@@ -3188,14 +3722,52 @@
     assert isinstance(response, gag_permission.Permission)
     assert response.name == "name_value"
     assert response.grantee_type == gag_permission.Permission.GranteeType.USER
     assert response.email_address == "email_address_value"
     assert response.role == gag_permission.Permission.Role.OWNER
 
 
+def test_create_permission_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.create_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.create_permission
+        ] = mock_rpc
+
+        request = {}
+        client.create_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.create_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_create_permission_rest_required_fields(
     request_type=permission_service.CreatePermissionRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["parent"] = ""
@@ -3398,15 +3970,15 @@
         client.create_permission(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta3/{parent=tunedModels/*}/permissions" % client.transport._host,
+            "%s/v1beta/{parent=tunedModels/*}/permissions" % client.transport._host,
             args[1],
         )
 
 
 def test_create_permission_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -3471,14 +4043,50 @@
     assert isinstance(response, permission.Permission)
     assert response.name == "name_value"
     assert response.grantee_type == permission.Permission.GranteeType.USER
     assert response.email_address == "email_address_value"
     assert response.role == permission.Permission.Role.OWNER
 
 
+def test_get_permission_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.get_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.get_permission] = mock_rpc
+
+        request = {}
+        client.get_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.get_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_get_permission_rest_required_fields(
     request_type=permission_service.GetPermissionRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -3671,15 +4279,15 @@
         client.get_permission(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta3/{name=tunedModels/*/permissions/*}" % client.transport._host,
+            "%s/v1beta/{name=tunedModels/*/permissions/*}" % client.transport._host,
             args[1],
         )
 
 
 def test_get_permission_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -3737,14 +4345,52 @@
         response = client.list_permissions(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, pagers.ListPermissionsPager)
     assert response.next_page_token == "next_page_token_value"
 
 
+def test_list_permissions_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.list_permissions in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.list_permissions
+        ] = mock_rpc
+
+        request = {}
+        client.list_permissions(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.list_permissions(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_list_permissions_rest_required_fields(
     request_type=permission_service.ListPermissionsRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["parent"] = ""
@@ -3952,15 +4598,15 @@
         client.list_permissions(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta3/{parent=tunedModels/*}/permissions" % client.transport._host,
+            "%s/v1beta/{parent=tunedModels/*}/permissions" % client.transport._host,
             args[1],
         )
 
 
 def test_list_permissions_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -4154,14 +4800,52 @@
     assert isinstance(response, gag_permission.Permission)
     assert response.name == "name_value"
     assert response.grantee_type == gag_permission.Permission.GranteeType.USER
     assert response.email_address == "email_address_value"
     assert response.role == gag_permission.Permission.Role.OWNER
 
 
+def test_update_permission_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.update_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.update_permission
+        ] = mock_rpc
+
+        request = {}
+        client.update_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.update_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_update_permission_rest_required_fields(
     request_type=permission_service.UpdatePermissionRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request = request_type(**request_init)
@@ -4363,15 +5047,15 @@
         client.update_permission(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta3/{permission.name=tunedModels/*/permissions/*}"
+            "%s/v1beta/{permission.name=tunedModels/*/permissions/*}"
             % client.transport._host,
             args[1],
         )
 
 
 def test_update_permission_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
@@ -4426,14 +5110,52 @@
         req.return_value = response_value
         response = client.delete_permission(request)
 
     # Establish that the response is the type that we expect.
     assert response is None
 
 
+def test_delete_permission_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.delete_permission in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.delete_permission
+        ] = mock_rpc
+
+        request = {}
+        client.delete_permission(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.delete_permission(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_delete_permission_rest_required_fields(
     request_type=permission_service.DeletePermissionRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -4613,15 +5335,15 @@
         client.delete_permission(**mock_args)
 
         # Establish that the underlying call was made with the expected
         # request object values.
         assert len(req.mock_calls) == 1
         _, args, _ = req.mock_calls[0]
         assert path_template.validate(
-            "%s/v1beta3/{name=tunedModels/*/permissions/*}" % client.transport._host,
+            "%s/v1beta/{name=tunedModels/*/permissions/*}" % client.transport._host,
             args[1],
         )
 
 
 def test_delete_permission_rest_flattened_error(transport: str = "rest"):
     client = PermissionServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
@@ -4676,14 +5398,54 @@
         req.return_value = response_value
         response = client.transfer_ownership(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, permission_service.TransferOwnershipResponse)
 
 
+def test_transfer_ownership_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = PermissionServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._transport.transfer_ownership in client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.transfer_ownership
+        ] = mock_rpc
+
+        request = {}
+        client.transfer_ownership(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.transfer_ownership(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_transfer_ownership_rest_required_fields(
     request_type=permission_service.TransferOwnershipRequest,
 ):
     transport_class = transports.PermissionServiceRestTransport
 
     request_init = {}
     request_init["name"] = ""
@@ -4990,15 +5752,15 @@
             credentials_file="credentials.json",
         )
 
 
 def test_permission_service_base_transport():
     # Instantiate the base transport.
     with mock.patch(
-        "google.ai.generativelanguage_v1beta3.services.permission_service.transports.PermissionServiceTransport.__init__"
+        "google.ai.generativelanguage_v1beta.services.permission_service.transports.PermissionServiceTransport.__init__"
     ) as Transport:
         Transport.return_value = None
         transport = transports.PermissionServiceTransport(
             credentials=ga_credentials.AnonymousCredentials(),
         )
 
     # Every method on the transport should just blindly
@@ -5028,15 +5790,15 @@
 
 
 def test_permission_service_base_transport_with_credentials_file():
     # Instantiate the base transport with a credentials file
     with mock.patch.object(
         google.auth, "load_credentials_from_file", autospec=True
     ) as load_creds, mock.patch(
-        "google.ai.generativelanguage_v1beta3.services.permission_service.transports.PermissionServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta.services.permission_service.transports.PermissionServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         load_creds.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.PermissionServiceTransport(
             credentials_file="credentials.json",
             quota_project_id="octopus",
         )
@@ -5047,15 +5809,15 @@
             quota_project_id="octopus",
         )
 
 
 def test_permission_service_base_transport_with_adc():
     # Test the default credentials are used if credentials and credentials_file are None.
     with mock.patch.object(google.auth, "default", autospec=True) as adc, mock.patch(
-        "google.ai.generativelanguage_v1beta3.services.permission_service.transports.PermissionServiceTransport._prep_wrapped_messages"
+        "google.ai.generativelanguage_v1beta.services.permission_service.transports.PermissionServiceTransport._prep_wrapped_messages"
     ) as Transport:
         Transport.return_value = None
         adc.return_value = (ga_credentials.AnonymousCredentials(), None)
         transport = transports.PermissionServiceTransport()
         adc.assert_called_once()
 
 
@@ -5433,129 +6195,109 @@
     path = PermissionServiceClient.permission_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_permission_path(path)
     assert expected == actual
 
 
-def test_tuned_model_path():
-    tuned_model = "oyster"
-    expected = "tunedModels/{tuned_model}".format(
-        tuned_model=tuned_model,
-    )
-    actual = PermissionServiceClient.tuned_model_path(tuned_model)
-    assert expected == actual
-
-
-def test_parse_tuned_model_path():
-    expected = {
-        "tuned_model": "nudibranch",
-    }
-    path = PermissionServiceClient.tuned_model_path(**expected)
-
-    # Check that the path construction is reversible.
-    actual = PermissionServiceClient.parse_tuned_model_path(path)
-    assert expected == actual
-
-
 def test_common_billing_account_path():
-    billing_account = "cuttlefish"
+    billing_account = "oyster"
     expected = "billingAccounts/{billing_account}".format(
         billing_account=billing_account,
     )
     actual = PermissionServiceClient.common_billing_account_path(billing_account)
     assert expected == actual
 
 
 def test_parse_common_billing_account_path():
     expected = {
-        "billing_account": "mussel",
+        "billing_account": "nudibranch",
     }
     path = PermissionServiceClient.common_billing_account_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_billing_account_path(path)
     assert expected == actual
 
 
 def test_common_folder_path():
-    folder = "winkle"
+    folder = "cuttlefish"
     expected = "folders/{folder}".format(
         folder=folder,
     )
     actual = PermissionServiceClient.common_folder_path(folder)
     assert expected == actual
 
 
 def test_parse_common_folder_path():
     expected = {
-        "folder": "nautilus",
+        "folder": "mussel",
     }
     path = PermissionServiceClient.common_folder_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_folder_path(path)
     assert expected == actual
 
 
 def test_common_organization_path():
-    organization = "scallop"
+    organization = "winkle"
     expected = "organizations/{organization}".format(
         organization=organization,
     )
     actual = PermissionServiceClient.common_organization_path(organization)
     assert expected == actual
 
 
 def test_parse_common_organization_path():
     expected = {
-        "organization": "abalone",
+        "organization": "nautilus",
     }
     path = PermissionServiceClient.common_organization_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_organization_path(path)
     assert expected == actual
 
 
 def test_common_project_path():
-    project = "squid"
+    project = "scallop"
     expected = "projects/{project}".format(
         project=project,
     )
     actual = PermissionServiceClient.common_project_path(project)
     assert expected == actual
 
 
 def test_parse_common_project_path():
     expected = {
-        "project": "clam",
+        "project": "abalone",
     }
     path = PermissionServiceClient.common_project_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_project_path(path)
     assert expected == actual
 
 
 def test_common_location_path():
-    project = "whelk"
-    location = "octopus"
+    project = "squid"
+    location = "clam"
     expected = "projects/{project}/locations/{location}".format(
         project=project,
         location=location,
     )
     actual = PermissionServiceClient.common_location_path(project, location)
     assert expected == actual
 
 
 def test_parse_common_location_path():
     expected = {
-        "project": "oyster",
-        "location": "nudibranch",
+        "project": "whelk",
+        "location": "octopus",
     }
     path = PermissionServiceClient.common_location_path(**expected)
 
     # Check that the path construction is reversible.
     actual = PermissionServiceClient.parse_common_location_path(path)
     assert expected == actual
```

### Comparing `google-ai-generativelanguage-0.6.2/tests/unit/gapic/generativelanguage_v1beta3/test_text_service.py` & `google-ai-generativelanguage-0.6.3/tests/unit/gapic/generativelanguage_v1beta3/test_text_service.py`

 * *Files 6% similar despite different names*

```diff
@@ -1118,14 +1118,17 @@
     client = TextServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.GenerateTextRequest()
 
 
 def test_generate_text_non_empty_request_with_auto_populated_field():
@@ -1141,22 +1144,60 @@
     # if they meet the requirements of AIP 4235.
     request = text_service.GenerateTextRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.generate_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.generate_text(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.GenerateTextRequest(
             model="model_value",
         )
 
 
+def test_generate_text_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.generate_text] = mock_rpc
+        request = {}
+        client.generate_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_generate_text_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1171,14 +1212,60 @@
         response = await client.generate_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.GenerateTextRequest()
 
 
 @pytest.mark.asyncio
+async def test_generate_text_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.generate_text
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.generate_text
+        ] = mock_object
+
+        request = {}
+        await client.generate_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.generate_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_generate_text_async(
     transport: str = "grpc_asyncio", request_type=text_service.GenerateTextRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1444,14 +1531,17 @@
     client = TextServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.EmbedTextRequest()
 
 
 def test_embed_text_non_empty_request_with_auto_populated_field():
@@ -1468,23 +1558,61 @@
     request = text_service.EmbedTextRequest(
         model="model_value",
         text="text_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.embed_text(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.EmbedTextRequest(
             model="model_value",
             text="text_value",
         )
 
 
+def test_embed_text_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_text] = mock_rpc
+        request = {}
+        client.embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_embed_text_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1499,14 +1627,58 @@
         response = await client.embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.EmbedTextRequest()
 
 
 @pytest.mark.asyncio
+async def test_embed_text_async_use_cached_wrapped_rpc(transport: str = "grpc_asyncio"):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.embed_text
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.embed_text
+        ] = mock_object
+
+        request = {}
+        await client.embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_embed_text_async(
     transport: str = "grpc_asyncio", request_type=text_service.EmbedTextRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -1730,14 +1902,17 @@
     client = TextServiceClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.batch_embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.BatchEmbedTextRequest()
 
 
 def test_batch_embed_text_non_empty_request_with_auto_populated_field():
@@ -1753,22 +1928,62 @@
     # if they meet the requirements of AIP 4235.
     request = text_service.BatchEmbedTextRequest(
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(type(client.transport.batch_embed_text), "__call__") as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.batch_embed_text(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.BatchEmbedTextRequest(
             model="model_value",
         )
 
 
+def test_batch_embed_text_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.batch_embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_embed_text
+        ] = mock_rpc
+        request = {}
+        client.batch_embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_batch_embed_text_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -1783,14 +1998,60 @@
         response = await client.batch_embed_text()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.BatchEmbedTextRequest()
 
 
 @pytest.mark.asyncio
+async def test_batch_embed_text_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.batch_embed_text
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.batch_embed_text
+        ] = mock_object
+
+        request = {}
+        await client.batch_embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.batch_embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_batch_embed_text_async(
     transport: str = "grpc_asyncio", request_type=text_service.BatchEmbedTextRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2021,14 +2282,17 @@
         transport="grpc",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_text_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_text_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.CountTextTokensRequest()
 
 
 def test_count_text_tokens_non_empty_request_with_auto_populated_field():
@@ -2046,22 +2310,62 @@
         model="model_value",
     )
 
     # Mock the actual call within the gRPC stub, and fake the request.
     with mock.patch.object(
         type(client.transport.count_text_tokens), "__call__"
     ) as call:
+        call.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
         client.count_text_tokens(request=request)
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.CountTextTokensRequest(
             model="model_value",
         )
 
 
+def test_count_text_tokens_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="grpc",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.count_text_tokens in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_text_tokens
+        ] = mock_rpc
+        request = {}
+        client.count_text_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_text_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 @pytest.mark.asyncio
 async def test_count_text_tokens_empty_call_async():
     # This test is a coverage failsafe to make sure that totally empty calls,
     # i.e. request == None and no flattened fields passed, work.
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport="grpc_asyncio",
@@ -2080,14 +2384,60 @@
         response = await client.count_text_tokens()
         call.assert_called()
         _, args, _ = call.mock_calls[0]
         assert args[0] == text_service.CountTextTokensRequest()
 
 
 @pytest.mark.asyncio
+async def test_count_text_tokens_async_use_cached_wrapped_rpc(
+    transport: str = "grpc_asyncio",
+):
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method_async.wrap_method") as wrapper_fn:
+        client = TextServiceAsyncClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport=transport,
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert (
+            client._client._transport.count_text_tokens
+            in client._client._transport._wrapped_methods
+        )
+
+        # Replace cached wrapped function with mock
+        class AwaitableMock(mock.AsyncMock):
+            def __await__(self):
+                self.await_count += 1
+                return iter([])
+
+        mock_object = AwaitableMock()
+        client._client._transport._wrapped_methods[
+            client._client._transport.count_text_tokens
+        ] = mock_object
+
+        request = {}
+        await client.count_text_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_object.call_count == 1
+
+        await client.count_text_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_object.call_count == 2
+
+
+@pytest.mark.asyncio
 async def test_count_text_tokens_async(
     transport: str = "grpc_asyncio", request_type=text_service.CountTextTokensRequest
 ):
     client = TextServiceAsyncClient(
         credentials=ga_credentials.AnonymousCredentials(),
         transport=transport,
     )
@@ -2318,14 +2668,50 @@
         req.return_value = response_value
         response = client.generate_text(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.GenerateTextResponse)
 
 
+def test_generate_text_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.generate_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.generate_text] = mock_rpc
+
+        request = {}
+        client.generate_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.generate_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_generate_text_rest_required_fields(
     request_type=text_service.GenerateTextRequest,
 ):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -2601,14 +2987,50 @@
         req.return_value = response_value
         response = client.embed_text(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.EmbedTextResponse)
 
 
+def test_embed_text_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[client._transport.embed_text] = mock_rpc
+
+        request = {}
+        client.embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_embed_text_rest_required_fields(request_type=text_service.EmbedTextRequest):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
     request_init["text"] = ""
     request = request_type(**request_init)
@@ -2874,14 +3296,52 @@
         req.return_value = response_value
         response = client.batch_embed_text(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.BatchEmbedTextResponse)
 
 
+def test_batch_embed_text_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.batch_embed_text in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.batch_embed_text
+        ] = mock_rpc
+
+        request = {}
+        client.batch_embed_text(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.batch_embed_text(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_batch_embed_text_rest_required_fields(
     request_type=text_service.BatchEmbedTextRequest,
 ):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
@@ -3155,14 +3615,52 @@
         response = client.count_text_tokens(request)
 
     # Establish that the response is the type that we expect.
     assert isinstance(response, text_service.CountTextTokensResponse)
     assert response.token_count == 1193
 
 
+def test_count_text_tokens_rest_use_cached_wrapped_rpc():
+    # Clients should use _prep_wrapped_messages to create cached wrapped rpcs,
+    # instead of constructing them on each call
+    with mock.patch("google.api_core.gapic_v1.method.wrap_method") as wrapper_fn:
+        client = TextServiceClient(
+            credentials=ga_credentials.AnonymousCredentials(),
+            transport="rest",
+        )
+
+        # Should wrap all calls on client creation
+        assert wrapper_fn.call_count > 0
+        wrapper_fn.reset_mock()
+
+        # Ensure method has been cached
+        assert client._transport.count_text_tokens in client._transport._wrapped_methods
+
+        # Replace cached wrapped function with mock
+        mock_rpc = mock.Mock()
+        mock_rpc.return_value.name = (
+            "foo"  # operation_request.operation in compute client(s) expect a string.
+        )
+        client._transport._wrapped_methods[
+            client._transport.count_text_tokens
+        ] = mock_rpc
+
+        request = {}
+        client.count_text_tokens(request)
+
+        # Establish that the underlying gRPC stub method was called.
+        assert mock_rpc.call_count == 1
+
+        client.count_text_tokens(request)
+
+        # Establish that a new wrapper was not created for this call
+        assert wrapper_fn.call_count == 0
+        assert mock_rpc.call_count == 2
+
+
 def test_count_text_tokens_rest_required_fields(
     request_type=text_service.CountTextTokensRequest,
 ):
     transport_class = transports.TextServiceRestTransport
 
     request_init = {}
     request_init["model"] = ""
```

